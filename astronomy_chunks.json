[
  {
    "source": "ABC (newspaper).txt",
    "chunk_id": "ABC (newspaper).txt_0",
    "chunk": "# ABC (newspaper) ABC (Spanish pronunciation: [aˈβeθe]) is a Spanish national daily newspaper. Along with El Mundo and El País, it is one of Spain's three newspapers of record. ABC was first published in Madrid on 1 January 1903 by Torcuato Luca de Tena y Álvarez-Ossorio. The founding publishing house was Prensa Española, which was led by the founder of the paper, Luca de Tena. The paper started as a weekly newspaper, turning daily in June 1905. In 1928 ABC had two editions, one for Madrid and the other for Seville. The latter was named ABC de Sevilla. On 20 July 1936, shortly after the Spanish Civil War began, ABC in Madrid was seized by the republican government, which changed the paper's politics to support the Republicans. The same year Blanco y Negro, a magazine, became its supplement. The ABC printed in Seville was supportive of the Nationalists. In 1939 ABC in Madrid was given back to its original owners by Francisco Franco. During this period the paper was one of two major dailies in the country together with La Vanguardia Española. ABC publishes in compact-sized stapled sheets, noticeably smaller than the loose tabloid format favoured by most Spanish dailies,"
  },
  {
    "source": "ABC (newspaper).txt",
    "chunk_id": "ABC (newspaper).txt_1",
    "chunk": "including El País and El Mundo. Its cover distinctively features a full-size picture. In the late 1970s and 1980s, ABC had close connections with first Popular Alliance and later Popular Party. In the 1990s, the publisher of ABC was Editorial Española. The paper later moved from its historic landmark offices in Madrid by Paseo de la Castellana, which is now a shopping mall. The paper is part of Grupo Vocento, which also owns El Correo Español, El Diario Vasco, La Verdad and Las Provincias, among others. ABC is known for generally supporting conservative political views, and defending the Spanish monarchy. The paper also has a right-wing stance. Its director since 1983, Luis María Ansón, left the paper in 1997; he founded another daily, La Razón, which initially catered to even more conservative readers. Historically, it was noted in its heavy use of photography, and the front page is typically a large photo taking up to one third of the area. It has been recognized for its coverage of Spanish culture and arts. On 25 September 2009, ABC made its complete archives, dating back to 1903, available online, giving modern readers a chance to see contemporaneous news about the Spanish Civil"
  },
  {
    "source": "ABC (newspaper).txt",
    "chunk_id": "ABC (newspaper).txt_2",
    "chunk": "War or Francisco Franco's death. In February 1970 ABC had a circulation of 212,536 copies. It was 178,979 copies in February 1975, 171,382 copies in 1976, 145,162 copies in 1977. and 126,952 copies in 1978. The circulation of the paper was 135,380 copies in February 1980. The 1993 circulation of ABC was 334,317 copies, making it the second-best-selling newspaper in Spain. In 1994. it was again the second-best-selling newspaper in the country with a circulation of 321,571 copies. In the period of 1995–1996 the paper had a circulation of 321,573 copies, making again it the second-best-selling paper in the country. The circulation of ABC was 292,000 copies in 2001 and 262,874 copies in 2002. The paper had a circulation of 263,000 copies in 2003, being the fourth best-selling newspaper in the country. Based on the findings of the European Business Readership Survey ABC had 5,685 readers per issue in 2006. Between June 2006 and July 2007 the daily had a circulation of 230,422 copies. The 2008 circulation of the paper was 228,258 copies. It was 243,154 copies between July 2010 and June 2011."
  },
  {
    "source": "Abd al-Rahman al-Sufi.txt",
    "chunk_id": "Abd al-Rahman al-Sufi.txt_0",
    "chunk": "# Abd al-Rahman al-Sufi ʿAbd al-Raḥmān al-Ṣūfī (Persian: عبدالرحمن الصوفی; 7 December 903 – 25 May 986) was a Persian astronomer. His work Kitāb ṣuwar al-kawākib (\"The Book of Fixed Stars\"), written in 964, included both textual descriptions and illustrations. The Persian polymath Al-Biruni wrote that al-Ṣūfī's work on the ecliptic was carried out in Shiraz. Al-Ṣūfī lived at the Buyid court in Isfahan. ʿAbd al-Rahmān al-Ṣūfī (full name: Abū’l-Ḥusayn ʿAbd al-Raḥmān ibn ʿUmar ibn Sahl al-Ṣūfī al-Rāzī) was one of the nine famous Muslim astronomers. He lived at the court of Emir 'Adud al-Dawla in Isfahan, and worked on translating and expanding ancient Greek astronomical works, especially the Almagest of Ptolemy. He made corrections to Ptolemy's star list, and his estimations of star brightness and magnitude deviated from those by Ptolemy; just over half of al-Ṣūfī's magnitudes being identical to Ptolemy's. A Persian, al-Ṣūfī wrote in Arabic, the lingua franca of the scientific Muslim world. Al-Ṣūfī was a major contributor to the translation into Arabic of the Hellenistic astronomy that had been centered in Alexandria, Egypt. His was the first to attempt to relate the Greek with the traditional Arabic star names and constellations, which were completely unrelated and"
  },
  {
    "source": "Abd al-Rahman al-Sufi.txt",
    "chunk_id": "Abd al-Rahman al-Sufi.txt_1",
    "chunk": "overlapped in complicated ways. Al-Ṣūfī made his astronomical observations at a latitude of 32.7N° in Isfahan. It has been claimed that he identified the Large Magellanic Cloud, but this seems to be a misunderstanding of a reference to some stars south of Canopus which he admits he has not seen. He also made the earliest recorded observation of the Andromeda Galaxy in 964, describing it as a \"small cloud\". This was the first galaxy other than the Milky Way to be mentioned in writing. Al-Ṣūfī also wrote about the astrolabe, finding numerous additional uses for it: According to American Near Eastern scholar Adam L. Bean, Al-Ṣūfī's work reportedly described over 1000 different uses in areas as diverse as astronomy, astrology, horoscopes, navigation, surveying, timekeeping, Qibla and Salat prayer. Al-Ṣūfī published Kitāb ṣuwar al-kawākib (\"The Book of Fixed Stars\") in 964, and dedicated it to Adud al-Dawla, the ruler of Buwayhid at the time. This book describes 48 constellations and the stars within them. Al-Ṣūfī compared Greek constellations and stars as described in Ptolemy’s Almagest with Arabic ones, linking those that were the same. He included two illustrations of each constellation, one showing the orientation of the stars from the perspective"
  },
  {
    "source": "Abd al-Rahman al-Sufi.txt",
    "chunk_id": "Abd al-Rahman al-Sufi.txt_2",
    "chunk": "outside the celestial globe, and the other from the perspective of looking at the sky while standing on the Earth. He separated them into three groups; 21 seen from the north, 15 seen from the south, and the 12 zodiac constellations. He included a complete set of star charts, that included the names and numbers of the individual stars in each of the 48 constellations, and each star's longitudinal and latitudinal coordinates, magnitude, and location north or south of the ecliptic. Scribal errors within the 35 surviving copies of The Book of Fixed Stars have caused the value of the magnitude for a particular star to vary from manuscript to manuscript. Al-Ṣūfī organized the stars in each of his drawings into two groups: those that form the image depicted, and others that are in close proximity to the image. He identified and described stars not included by Ptolemy, but he did not include them in his own star charts. Stating that his charts were modelled after Ptolemy, he left the stars excluded in Ptolemy's catalogue out of his charts as well. To allow for the longitudinal placement of the stars within constellations having changed over the eight centuries since the"
  },
  {
    "source": "Abd al-Rahman al-Sufi.txt",
    "chunk_id": "Abd al-Rahman al-Sufi.txt_3",
    "chunk": "Almagest was written, Al-Ṣūfī added 12° 42' to all the longitudes values provided by Ptolemy. Al-Ṣūfī differed from Ptolemy by having a three level scale to measure the magnitude of stars instead of a two level scale. This extra level increased the precision of his measurements. His methodology for determining these magnitude measurements cannot be found in any of his extant texts. Despite the importance of The Book of Fixed Stars in the history of astronomy, it took more than 1000 years until the first partial English translation of the book was published in 2010. Al-Ṣūfī's astronomical work was subsequently used by many other astronomers, including Ulugh Beg who was both a prince and astronomer. The lunar crater Azophi and the minor planet 12621 Alsufi are named after Al-Ṣūfī. The Astronomy Society of Iran – Amateur Committee has held international Sufi Observing Competitions in memory of the astronomer. The first competition was held in 2006 in the north of Semnan Province, and the second was held in the summer of 2008 in Ladiz near the Zahedan. More than 100 attendees from Iran and Iraq participated in these events."
  },
  {
    "source": "Accademia dei Lincei.txt",
    "chunk_id": "Accademia dei Lincei.txt_0",
    "chunk": "# Accademia dei Lincei The Accademia dei Lincei (Italian pronunciation: [akkaˈdɛːmja dei linˈtʃɛi]; literally the \"Academy of the Lynx-Eyed\"), anglicised as the Lincean Academy, is one of the oldest and most prestigious European scientific institutions, located at the Palazzo Corsini on the Via della Lungara in Rome, Italy. Founded in the Papal States in 1603 by Federico Cesi, the academy was named after the lynx, an animal whose sharp vision symbolizes the observational prowess that science requires. Galileo Galilei was the intellectual centre of the academy and adopted \"Galileo Galilei Linceo\" as his signature. \"The Lincei did not long survive the death in 1630 of Cesi, its founder and patron\", and \"disappeared in 1651.\" During the nineteenth century, it was revived, first in the Papal States and later in the nation of Italy. Thus the Pontifical Academy of Sciences, established in 1936, claims this heritage as the Accademia Pontificia dei Nuovi Lincei (\"Pontifical Academy of the New Lynxes\"), founded in 1847, descending from the first two incarnations of the Academy. Similarly, a lynx-eyed academy of the 1870s became the national academy of Italy, encompassing both literature and science among its concerns. The first Accademia dei Lincei was founded in 1603"
  },
  {
    "source": "Accademia dei Lincei.txt",
    "chunk_id": "Accademia dei Lincei.txt_1",
    "chunk": "by Federico Cesi, an aristocrat from Umbria (the son of Duke of Acquasparta and a member of an important family from Rome) who was passionately interested in natural history – particularly botany. Cesi's father disapproved of the research career that Federico was pursuing. His mother, Olimpia Orsini, supported him both financially and morally. The Academy struggled due to this disapproval, but after the death of Frederico's father, he had enough money to allow the academy to flourish. The academy, hosted in Palazzo Cesi-Armellini near Saint Peter, replaced the first scientific community ever, Giambattista della Porta's Academia Secretorum Naturae in Naples that had been closed by the Inquisition. Cesi founded the Accademia dei Lincei with three friends: the Dutch physician Johannes van Heeck (Italianized to Giovanni Ecchio) and two fellow Umbrians, mathematician Francesco Stelluti and polymath Anastasio de Filiis. At the time of the Accademia's founding Cesi was only 18, and the others were only 8 years older. Cesi and his friends aimed to understand all of the natural sciences. The literary and antiquarian emphasis set the \"Lincei\" apart from the host of sixteenth and seventeenth-century Italian Academies. Cesi envisioned a program of free experiment that was respectful of tradition, yet"
  },
  {
    "source": "Accademia dei Lincei.txt",
    "chunk_id": "Accademia dei Lincei.txt_2",
    "chunk": "unfettered by blind obedience to authority, even that of Aristotle and Ptolemy, whose theories the new science called into question. While originally a private association, the Academy became a semi-public establishment during the Napoleonic domination of Rome. This shift allowed the local scientific elite to carve out a place for themselves in larger scientific networks. However, as a semi-public establishment, the Academy's focus was directed by Napoleonic politics. This focus directed the member's efforts towards stimulating industry, turning public opinion in favour of the French regime and secularizing the country. The name \"Lincei\" 'the lynx-like (i. e., lynx-eyed, sharp-eyed) ones' came from Giambattista della Porta's book Magia Naturalis, which had an illustration of the fabled cat on the cover and the words \"[...] with lynx-like eyes, examining those things which manifest themselves, so that having observed them, he may zealously use them\". Accademia dei Lincei's symbols were both a lynx and an eagle; animals with, or reputed to have, keen sight (in classical and medieval bestiaries the lynx was reputed to be able to see through rock and \"new walls\"). The academy's motto, chosen by Cesi, was: \"Take care of small things if you want to obtain the greatest results\""
  },
  {
    "source": "Accademia dei Lincei.txt",
    "chunk_id": "Accademia dei Lincei.txt_3",
    "chunk": "(minima cura si maxima vis). According to T. O'Conor Sloane, their other motto was Sagacius ista. When Cesi visited Naples, he met with many scientists in fields of interest to him including the botanist, Fabio Colonna, the natural history writer, Ferrante Imperato, and the polymath della Porta. Della Porta was impressed with Cesi, and dedicated three works to the Linceans including a treatise on distillation called De Distillatione, a book on curvilinear geometry called Elementa Curvilinea, and The Transformations of the Atmosphere. Della Porta encouraged Cesi to continue with his endeavours. Giambattista della Porta joined Cesi's academy in 1610. While in Naples, Cesi also met with Nardo Antonio Recchi to negotiate the acquisition of a collection of material describing Aztec plants and animals written by Francisco Hernández de Toledo. This collection of material would eventually become the Tesoro Messicano (Mexican Treasury). The goal was nothing less than the assembly of modern science reflected on the method of observation: the church of knowledge. The Academy was to possess in each quarter of the global communes with adequate endowments to retain membership. These communes were complete with libraries, laboratories, museums, printing presses, and botanical gardens. Members frequently wrote letters about their observations."
  },
  {
    "source": "Accademia dei Lincei.txt",
    "chunk_id": "Accademia dei Lincei.txt_4",
    "chunk": "The Lyncæis denounced marriage as a \"mollis et effeminata requies\" (i.e. a soft and feminine rest) which would pose an \"obstacle to a life of research\". Membership was banned to monks. Members were ordered to \"penetrate into the interior of things in order to know the causes and operations of nature, as it is said the lynx does, which sees not only what is outside, but what is hidden within.\" Galileo was inducted to the exclusive Academy on April 25, 1611, and became its intellectual centre. Galileo clearly felt honoured by his association with the Academy for he adopted Galileo Galilei Linceo as his signature. The Academy published his works and supported him during his disputes with the Roman Inquisition. Among the Academy's early publications in the fields of astronomy, physics and botany were Galileo's \"Letters on Sunspots\" and \"The Assayer\", and the Tesoro Messicano describing the flora, fauna and drugs of the New World, which took decades of labour, down to 1651. With this publication, the first, most famous phase of the Lincei was concluded. The new usage of microscopy, with \"references to magnification tools can be found in the works of Galileo and several Lincei, Harvey, Gassendi, Marco"
  },
  {
    "source": "Accademia dei Lincei.txt",
    "chunk_id": "Accademia dei Lincei.txt_5",
    "chunk": "Aurelio Severino—who was probably also in contact with the Lincie—and Nathanial Highmore.\" Domenico Bertoloni Meli, in Mechanism, Experiment, Disease: Marcello Malpighi and Seventeenth-Century Anatomy (Johns Hopkins University Press: 2011; p. 41). Microscopes were not just by the Lincei for astronomical and mathematical work, but were also used for new experimentations in anatomy, as this was the time of the rise of mechanistic anatomy, and the theories of atomism. Experimentation proliferated across the board. Cesi's own intense activity was cut short by his sudden death in 1630 at forty-five. The Linceans produced an important collection of micrographs or drawings made with the help of the newly invented microscope. After Cesi's death, the Accademia dei Lincei closed and the drawings were collected by Cassiano dal Pozzo, a Roman antiquarian, whose heirs sold them. The majority of the collection was procured by George III of the United Kingdom, in 1763. The drawings were discovered in Windsor Castle in 1986, by art historian David Freedberg. They are being published as part of The Paper Museum of Cassiano dal Pozzo. In 1801, Abbot Feliciano Scarpellini and Gioacchino Pessuti, with the patronage of Francesco Caetani, founded the Accademia Caetani which took the name of Accademia dei"
  },
  {
    "source": "Accademia dei Lincei.txt",
    "chunk_id": "Accademia dei Lincei.txt_6",
    "chunk": "Lincei. The period from 1801 to 1840 has been termed the \"Second Renaissance\" of the Accademia. Conflicting goals and general shifts in the \"geo-political scale\" left the Academy in a state of limbo, which ultimately led to its collapse in the 1840s. During the French domination of the Accademia, the institution saw a transition from a private association to a municipal institution. Despite efforts from the early 1800s onward, the Accademia underwent a true revival in 1847, when Pope Pius IX re-founded it as the Pontificia Accademia dei Nuovi Lincei, anglicised as the Pontifical Academy of New Lincei. In 1874, Quintino Sella turned it into the Accademia Nazionale Reale dei Lincei, anglicised as the Royal National Lincean Academy. This incarnation broadened its scope to include moral and humanistic sciences, and regained the high prestige associated with the original Lincean Academy. After the unification of Italy, the Piedmontese Quintino Sella infused new life into the Nuovi Lincei, reaffirming its ideals of secular science, but broadening its scope to include humanistic studies: history, philology, archaeology, philosophy, economics and law, in two classes of Soci (Fellows). During the Italian fascist period, the Lincean Academy was effectively replaced by the new Accademia d'Italia, the"
  },
  {
    "source": "Accademia dei Lincei.txt",
    "chunk_id": "Accademia dei Lincei.txt_7",
    "chunk": "Italian Academy, but was not fully absorbed by that institution until 1939. In 1949, after the fall of the fascist regime, at the suggestion of Benedetto Croce, the Lincean Academy recovered its independence. A brief history of this period of the Accademia, as well as the complete inventory of publications and documents produced in the same period, can be found in the book by Cagiano De Azevedo & Gerardi (2005). In 1986, the Academy was placed under a statute that says it shall be composed of 540 members, of whom 180 are ordinary Italian members, 180 are foreigners, and 180 are Italian corresponding members. The members are divided into two classes: one for mathematical, physical, and natural sciences; the other for moral, historical, and philological sciences. In 2001, the natural sciences were re-divided into five categories: mathematics, mechanics and applications; astronomy, geodesy, geophysics and applications; physics, chemistry and applications; geology, paleontology, mineralogy and applications; and biological sciences and applications. At the same time, the moral sciences were divided into seven categories: philology and linguistics; archaeology; criticism of art and of poetry; history, historical geography, and anthropology; philosophical science; juridical science; social and political science. The Accademia regularly awards prestigious prizes"
  },
  {
    "source": "Accademia dei Lincei.txt",
    "chunk_id": "Accademia dei Lincei.txt_8",
    "chunk": "to talented researchers and scholars. Notable prizes include:"
  },
  {
    "source": "Achromatic lens.txt",
    "chunk_id": "Achromatic lens.txt_0",
    "chunk": "# Achromatic lens An achromatic lens or achromat is a lens that is designed to limit the effects of chromatic and spherical aberration. Achromatic lenses are corrected to bring two wavelengths (typically red and blue) into focus on the same plane. Wavelengths in between these two then have better focus error than could be obtained with a simple lens. The most common type of achromat is the achromatic doublet, which is composed of two individual lenses made from glasses with different amounts of dispersion. Typically, one element is a negative (concave) element made out of flint glass such as F2, which has relatively high dispersion, and the other is a positive (convex) element made of crown glass such as BK7, which has lower dispersion. The lens elements are mounted next to each other, often cemented together, and shaped so that the chromatic aberration of one is counterbalanced by that of the other. In the most common type (shown), the positive power of the crown lens element is not quite equalled by the negative power of the flint lens element. Together they form a weak positive lens that will bring two different wavelengths of light to a common focus. Negative doublets,"
  },
  {
    "source": "Achromatic lens.txt",
    "chunk_id": "Achromatic lens.txt_1",
    "chunk": "in which the negative-power element predominates, are also made. Theoretical considerations of the feasibility of correcting chromatic aberration were debated in the 18th century following Newton's statement that such a correction was impossible (see History of the telescope). Credit for the invention of the first achromatic doublet is often given to an English barrister and amateur optician named Chester Moore Hall. Hall wished to keep his work on the achromatic lenses a secret and contracted the manufacture of the crown and flint lenses to two different opticians, Edward Scarlett and James Mann. They in turn sub-contracted the work to the same person, George Bass. He realized the two components were for the same client and, after fitting the two parts together, noted the achromatic properties. Hall used the achromatic lens to build the first achromatic telescope, but his invention did not become widely known at the time. In the late 1750s, Bass mentioned Hall's lenses to John Dollond, who understood their potential and was able to reproduce their design. Dollond applied for and was granted a patent on the technology in 1758, which led to bitter fights with other opticians over the right to make and sell achromatic doublets. Dollond's"
  },
  {
    "source": "Achromatic lens.txt",
    "chunk_id": "Achromatic lens.txt_2",
    "chunk": "son Peter invented the apochromat, an improvement on the achromat, in 1763. Several different types of achromat have been devised. They differ in the shape of the included lens elements as well as in the optical properties of their glass (most notably in their optical dispersion or Abbe number). In the following, R denotes the radius of the spheres that define the optically relevant refracting lens surfaces. By convention, R1 denotes the first lens surface counted from the object. A doublet lens has four surfaces with radii R1 through R2 . Surfaces with positive radii curve away from the object (R1 positive is a convex first surface); negative radii curve toward the object (R1 negative is a concave first surface). The descriptions of the achromat lens designs mention advantages of designs that do not produce \"ghost\" images. Historically, this was indeed a driving concern for lens makers up to the 19th century and a primary criterion for early optical designs. However, in the mid 20th century, the development of advanced optical coatings for the most part has eliminated the issue of ghost images, and modern optical designs are preferred for other merits. Uses an equiconvex crown glass lens (i.e. R1"
  },
  {
    "source": "Achromatic lens.txt",
    "chunk_id": "Achromatic lens.txt_3",
    "chunk": "> 0 with −R1 = R2 ) and a complementary-curved second flint glass lens (with R3 = R2 ). The back of the flint glass lens is flat ( R4 = ∞ ). A Littrow doublet can produce a ghost image between R2 and R3 because the lens surfaces of the two lenses have the same radii. The first lens has positive refractive power, the second negative. R1 > 0 is set greater than −R2 , and R3 is set close to, but not quite equal to, −R2 . R4 is usually greater than −R3 . In a Fraunhofer doublet, the dissimilar curvatures of −R2 and R3 are mounted close, but not quite in contact. This design yields more degrees of freedom (one more free radius, length of the air space) to correct for optical aberrations. Early Clark lenses follow the Fraunhofer design. After the late 1860s, they changed to the Littrow design, approximately equiconvex crown, R1 = R2 , and a flint with R3 ≃ R2 and R4 ≫ R3 . By about 1880, Clark lenses had R3 set slightly shorter than R2 to create a focus mismatch between R2 and R3, thereby avoiding ghosting caused by reflections within"
  },
  {
    "source": "Achromatic lens.txt",
    "chunk_id": "Achromatic lens.txt_4",
    "chunk": "the airspace. The use of oil between the crown and flint eliminates the effect of ghosting, particularly where R2 ≈ R3 . It can also increase light transmission slightly and reduce the impact of errors in R2 and R3 . The Steinheil doublet, devised by Carl August von Steinheil, is a flint-first doublet. In contrast to the Fraunhofer doublet, it has a negative lens first followed by a positive lens. It needs stronger curvature than the Fraunhofer doublet. Dialyte lenses have a wide air space between the two elements. They were originally devised in the 19th century to allow much smaller flint glass elements down stream since flint glass was hard to produce and expensive. They are also lenses where the elements can not be cemented because R2 and R3 have different absolute values. The first-order design of an achromat involves choosing the overall power 1 f d b l t {\\displaystyle \\ {\\frac {1}{\\ f_{\\mathsf {dblt}}\\ }}\\ } of the doublet and the two glasses to use. The choice of glass gives the mean refractive index, often written as n d {\\displaystyle n_{d}} (for the refractive index at the Fraunhofer \"d\" spectral line wavelength), and the Abbe number V"
  },
  {
    "source": "Achromatic lens.txt",
    "chunk_id": "Achromatic lens.txt_5",
    "chunk": "{\\displaystyle V} (for the reciprocal of the glass dispersion). To make the linear dispersion of the system zero, the system must satisfy the equations where the lens power is 1 f {\\displaystyle \\ {\\frac {1}{\\ f\\ }}\\ } for a lens with focal length f {\\displaystyle f} . Solving these two equations for f 1 {\\displaystyle \\ f_{1}\\ } and f 2 {\\displaystyle \\ f_{2}\\ } gives Since f 1 = − f 2 V 2 V 1 , {\\displaystyle \\ f_{1}=-f_{2}\\ {\\frac {\\ V_{2}\\ }{V_{1}}}\\ ,} and the Abbe numbers are positive-valued, the power of the second element in the doublet is negative when the first element is positive, and vice versa. Optical aberrations other than just color are present in all lenses. For example, coma remains after spherical and chromatic aberrations are corrected. In order to correct other aberrations, the front and back curvatures of each of the two lenses remain free parameters, since the color correction design only prescribes the net focal length of each lens, f 1 {\\displaystyle \\ f_{1}\\ } and separately f 2 . {\\displaystyle \\ f_{2}~.} This leaves a continuum of different combinations of front and back lens curvatures for design tweaks ("
  },
  {
    "source": "Achromatic lens.txt",
    "chunk_id": "Achromatic lens.txt_6",
    "chunk": "R 1 {\\displaystyle \\ R_{1}\\ } and R 2 {\\displaystyle \\ R_{2}\\ } for lens 1; and R 3 {\\displaystyle \\ R_{3}\\ } and R 4 {\\displaystyle \\ R_{4}\\ } for lens 2) that will all produce the same f 1 {\\displaystyle \\ f_{1}\\ } and f 2 {\\displaystyle \\ f_{2}\\ } required by the achromat design. Other adjustable lens parameters include the thickness of each lens and the space between the two, all constrained only by the two required focal lengths. Normally, the free parameters are adjusted to minimize non-color-related optical aberrations. Lens designs more complex than achromatic can improve the precision of color images by bringing more wavelengths into exact focus, but require more expensive types of glass, and more careful shaping and spacing of the combination of simple lenses: In theory, the process can continue indefinitely: Compound lenses used in cameras typically have six or more simple lenses (e.g. double-Gauss lens); several of those lenses can be made with different types of glass, with slightly altered curvatures, in order to bring more colors into focus. The constraint is extra manufacturing cost, and diminishing returns of improved image for the effort."
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_0",
    "chunk": "# Air mass (astronomy) In astronomy, air mass or airmass is a measure of the amount of air along the line of sight when observing a star or other celestial source from below Earth's atmosphere (Green 1992). It is formulated as the integral of air density along the light ray. As it penetrates the atmosphere, light is attenuated by scattering and absorption; the thicker atmosphere through which it passes, the greater the attenuation. Consequently, celestial bodies when nearer the horizon appear less bright than when nearer the zenith. This attenuation, known as atmospheric extinction, is described quantitatively by the Beer–Lambert law. \"Air mass\" normally indicates relative air mass, the ratio of absolute air masses (as defined above) at oblique incidence relative to that at zenith. So, by definition, the relative air mass at the zenith is 1. Air mass increases as the angle between the source and the zenith increases, reaching a value of approximately 38 at the horizon. Air mass can be less than one at an elevation greater than sea level; however, most closed-form expressions for air mass do not include the effects of the observer's elevation, so adjustment must usually be accomplished by other means. Tables of"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_1",
    "chunk": "air mass have been published by numerous authors, including Bemporad (1904), Allen (1973), and Kasten & Young (1989). The absolute air mass is defined as: σ = ∫ ρ d s . {\\displaystyle \\sigma =\\int \\rho \\,\\mathrm {d} s\\,.} where ρ {\\displaystyle \\rho } is volumetric density of air. Thus σ {\\displaystyle \\sigma } is a type of oblique column density. In the vertical direction, the absolute air mass at zenith is: σ z e n = ∫ ρ d z {\\displaystyle \\sigma _{\\mathrm {zen} }=\\int \\rho \\,\\mathrm {d} z} So σ z e n {\\displaystyle \\sigma _{\\mathrm {zen} }} is a type of vertical column density. Finally, the relative air mass is: X = σ σ z e n {\\displaystyle X={\\frac {\\sigma }{\\sigma _{\\mathrm {zen} }}}} Assuming air density to be uniform allows removing it from the integrals. The absolute air mass then simplifies to a product: σ = ρ ¯ s {\\displaystyle \\sigma ={\\bar {\\rho }}s} where ρ ¯ = c o n s t . {\\displaystyle {\\bar {\\rho }}=\\mathrm {const.} } is the average density and the arc length s {\\displaystyle s} of the oblique and zenith light paths are: s = ∫ d s s z"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_2",
    "chunk": "e n = ∫ d z {\\displaystyle {\\begin{aligned}s&=\\int \\,\\mathrm {d} s\\\\s_{\\mathrm {zen} }&=\\int \\,\\mathrm {d} z\\end{aligned}}} In the corresponding simplified relative air mass, the average density cancels out in the fraction, leading to the ratio of path lengths: X = s s z e n . {\\displaystyle X={\\frac {s}{s_{\\mathrm {zen} }}}\\,.} Further simplifications are often made, assuming straight-line propagation (neglecting ray bending), as discussed below. The angle of a celestial body with the zenith is the zenith angle (in astronomy, commonly referred to as the zenith distance). A body's angular position can also be given in terms of altitude, the angle above the geometric horizon; the altitude h {\\displaystyle h} and the zenith angle z {\\displaystyle z} are thus related by h = 90 ∘ − z . {\\displaystyle h=90^{\\circ }-z\\,.} Atmospheric refraction causes light entering the atmosphere to follow an approximately circular path that is slightly longer than the geometric path. Air mass must take into account the longer path (Young 1994). Additionally, refraction causes a celestial body to appear higher above the horizon than it actually is; at the horizon, the difference between the true zenith angle and the apparent zenith angle is approximately 34 minutes of arc."
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_3",
    "chunk": "Most air mass formulas are based on the apparent zenith angle, but some are based on the true zenith angle, so it is important to ensure that the correct value is used, especially near the horizon. When the zenith angle is small to moderate, a good approximation is given by assuming a homogeneous plane-parallel atmosphere (i.e., one in which density is constant and Earth's curvature is ignored). The air mass X {\\displaystyle X} then is simply the secant of the zenith angle z {\\displaystyle z} : X = sec z . {\\displaystyle X=\\sec \\,z\\,.} At a zenith angle of 60°, the air mass is approximately 2. However, because the Earth is not flat, this formula is only usable for zenith angles up to about 60° to 75°, depending on accuracy requirements. At greater zenith angles, the accuracy degrades rapidly, with X = sec z {\\displaystyle X=\\sec \\,z} becoming infinite at the horizon; the horizon air mass in the more realistic spherical atmosphere is usually less than 40. Many formulas have been developed to fit tabular values of air mass; one by Young & Irvine (1967) included a simple corrective term: X = sec z t [ 1 − 0.0012 ("
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_4",
    "chunk": "sec 2 ⁡ z t − 1 ) ] , {\\displaystyle X=\\sec \\,z_{\\mathrm {t} }\\,\\left[1-0.0012\\,(\\sec ^{2}z_{\\mathrm {t} }-1)\\right]\\,,} where z t {\\displaystyle z_{\\mathrm {t} }} is the true zenith angle. This gives usable results up to approximately 80°, but the accuracy degrades rapidly at greater zenith angles. The calculated air mass reaches a maximum of 11.13 at 86.6°, becomes zero at 88°, and approaches negative infinity at the horizon. The plot of this formula on the accompanying graph includes a correction for atmospheric refraction so that the calculated air mass is for apparent rather than true zenith angle. Hardie (1962) introduced a polynomial in sec z − 1 {\\displaystyle \\sec \\,z-1} : X = sec z − 0.0018167 ( sec z − 1 ) − 0.002875 ( sec z − 1 ) 2 − 0.0008083 ( sec z − 1 ) 3 {\\displaystyle X=\\sec \\,z\\,-\\,0.0018167\\,(\\sec \\,z\\,-\\,1)\\,-\\,0.002875\\,(\\sec \\,z\\,-\\,1)^{2}\\,-\\,0.0008083\\,(\\sec \\,z\\,-\\,1)^{3}} which gives usable results for zenith angles of up to perhaps 85°. As with the previous formula, the calculated air mass reaches a maximum, and then approaches negative infinity at the horizon. Rozenberg (1966) suggested X = ( cos z + 0.025 e − 11 cos z ) − 1 , {\\displaystyle"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_5",
    "chunk": "X=\\left(\\cos \\,z+0.025e^{-11\\cos \\,z}\\right)^{-1}\\,,} which gives reasonable results for high zenith angles, with a horizon air mass of 40. X = 1 cos z + 0.50572 ( 6.07995 ∘ + 90 ∘ − z ) − 1.6364 , {\\displaystyle X={\\frac {1}{\\cos \\,z+0.50572\\,(6.07995^{\\circ }+90^{\\circ }-z)^{-1.6364}}}\\,,} which gives reasonable results for zenith angles of up to 90°, with an air mass of approximately 38 at the horizon. Here the second z {\\displaystyle z} term is in degrees. Young (1994) developed X = 1.002432 cos 2 ⁡ z t + 0.148386 cos z t + 0.0096467 cos 3 ⁡ z t + 0.149864 cos 2 ⁡ z t + 0.0102963 cos z t + 0.000303978 {\\displaystyle X={\\frac {1.002432\\,\\cos ^{2}z_{\\mathrm {t} }+0.148386\\,\\cos \\,z_{\\mathrm {t} }+0.0096467}{\\cos ^{3}z_{\\mathrm {t} }+0.149864\\,\\cos ^{2}z_{\\mathrm {t} }+0.0102963\\,\\cos \\,z_{\\mathrm {t} }+0.000303978}}\\,} in terms of the true zenith angle z t {\\displaystyle z_{\\mathrm {t} }} , for which he claimed a maximum error (at the horizon) of 0.0037 air mass. Pickering (2002) developed X = 1 sin ⁡ ( h + 244 / ( 165 + 47 h 1.1 ) ) , {\\displaystyle X={\\frac {1}{\\sin(h+{244}/(165+47h^{1.1}))}}\\,,} where h {\\displaystyle h} is apparent altitude ( 90 ∘ − z ) {\\displaystyle (90^{\\circ }-z)} in degrees."
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_6",
    "chunk": "Pickering claimed his equation to have a tenth the error of Schaefer (1998) near the horizon. Interpolative formulas attempt to provide a good fit to tabular values of air mass using minimal computational overhead. The tabular values, however, must be determined from measurements or atmospheric models that derive from geometrical and physical considerations of Earth and its atmosphere. If atmospheric refraction is ignored, it can be shown from simple geometrical considerations (Schoenberg 1929, 173) that the path s {\\displaystyle s} of a light ray at zenith angle z {\\displaystyle z} through a radially symmetrical atmosphere of height y a t m {\\displaystyle y_{\\mathrm {atm} }} above the Earth is given by s = R E 2 cos 2 ⁡ z + 2 R E y a t m + y a t m 2 − R E cos z {\\displaystyle s={\\sqrt {R_{\\mathrm {E} }^{2}\\cos ^{2}z+2R_{\\mathrm {E} }y_{\\mathrm {atm} }+y_{\\mathrm {atm} }^{2}}}-R_{\\mathrm {E} }\\cos \\,z\\,} or alternatively, s = ( R E + y a t m ) 2 − R E 2 sin 2 ⁡ z − R E cos z {\\displaystyle s={\\sqrt {\\left(R_{\\mathrm {E} }+y_{\\mathrm {atm} }\\right)^{2}-R_{\\mathrm {E} }^{2}\\sin ^{2}z}}-R_{\\mathrm {E} }\\cos \\,z\\,} where R E {\\displaystyle R_{\\mathrm {E}"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_7",
    "chunk": "}} is the radius of the Earth. The relative air mass is then: X = s y a t m = R E y a t m cos 2 ⁡ z + 2 y a t m R E + ( y a t m R E ) 2 − R E y a t m cos z . {\\displaystyle X={\\frac {s}{y_{\\mathrm {atm} }}}={\\frac {R_{\\mathrm {E} }}{y_{\\mathrm {atm} }}}{\\sqrt {\\cos ^{2}z+2{\\frac {y_{\\mathrm {atm} }}{R_{\\mathrm {E} }}}+\\left({\\frac {y_{\\mathrm {atm} }}{R_{\\mathrm {E} }}}\\right)^{2}}}-{\\frac {R_{\\mathrm {E} }}{y_{\\mathrm {atm} }}}\\cos \\,z\\,.} If the atmosphere is homogeneous (i.e., density is constant), the atmospheric height y a t m {\\displaystyle y_{\\mathrm {atm} }} follows from hydrostatic considerations as: y a t m = k T 0 m g , {\\displaystyle y_{\\mathrm {atm} }={\\frac {kT_{0}}{mg}}\\,,} where k {\\displaystyle k} is the Boltzmann constant, T 0 {\\displaystyle T_{0}} is the sea-level temperature, m {\\displaystyle m} is the molecular mass of air, and g {\\displaystyle g} is the acceleration due to gravity. Although this is the same as the pressure scale height of an isothermal atmosphere, the implication is slightly different. In an isothermal atmosphere, 37% (1/e) of the atmosphere is above the pressure scale height; in a homogeneous"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_8",
    "chunk": "atmosphere, there is no atmosphere above the atmospheric height. Taking T 0 = 288.15 K {\\displaystyle T_{0}=\\mathrm {288.15~K} } , m = 28.9644 × 1.6605 × 10 − 27 k g {\\displaystyle m=\\mathrm {28.9644\\times 1.6605\\times 10^{-27}~kg} } , and g = 9.80665 m / s 2 {\\displaystyle g=\\mathrm {9.80665~m/s^{2}} } gives y a t m ≈ 8435 m {\\displaystyle y_{\\mathrm {atm} }\\approx \\mathrm {8435~m} } . Using Earth's mean radius of 6371 km, the sea-level air mass at the horizon is X h o r i z = 1 + 2 R E y a t m ≈ 38.87 . {\\displaystyle X_{\\mathrm {horiz} }={\\sqrt {1+2{\\frac {R_{\\mathrm {E} }}{y_{\\mathrm {atm} }}}}}\\approx 38.87\\,.} The homogeneous spherical model slightly underestimates the rate of increase in air mass near the horizon; a reasonable overall fit to values determined from more rigorous models can be had by setting the air mass to match a value at a zenith angle less than 90°. The air mass equation can be rearranged to give R E y a t m = X 2 − 1 2 ( 1 − X cos ⁡ z ) ; {\\displaystyle {\\frac {R_{\\mathrm {E} }}{y_{\\mathrm {atm} }}}={\\frac {X^{2}-1}{2\\left(1-X\\cos z\\right)}}\\,;} matching Bemporad's value of"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_9",
    "chunk": "19.787 at z {\\displaystyle z} = 88° gives R E / y a t m {\\displaystyle R_{\\mathrm {E} }/y_{\\mathrm {atm} }} ≈ 631.01 and X h o r i z {\\displaystyle X_{\\mathrm {horiz} }} ≈ 35.54. With the same value for R E {\\displaystyle R_{\\mathrm {E} }} as above, y a t m {\\displaystyle y_{\\mathrm {atm} }} ≈ 10,096 m. While a homogeneous atmosphere is not a physically realistic model, the approximation is reasonable as long as the scale height of the atmosphere is small compared to the radius of the planet. The model is usable (i.e., it does not diverge or go to zero) at all zenith angles, including those greater than 90° (see § Homogeneous spherical atmosphere with elevated observer). The model requires comparatively little computational overhead, and if high accuracy is not required, it gives reasonable results. However, for zenith angles less than 90°, a better fit to accepted values of air mass can be had with several of the interpolative formulas. In a real atmosphere, density is not constant (it decreases with elevation above mean sea level. The absolute air mass for the geometrical light path discussed above, becomes, for a sea-level observer, σ = ∫"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_10",
    "chunk": "0 y a t m ρ ( R E + y ) d y R E 2 cos 2 ⁡ z + 2 R E y + y 2 . {\\displaystyle \\sigma =\\int _{0}^{y_{\\mathrm {atm} }}{\\frac {\\rho \\,\\left(R_{\\mathrm {E} }+y\\right)\\mathrm {d} y}{\\sqrt {R_{\\mathrm {E} }^{2}\\cos ^{2}z+2R_{\\mathrm {E} }y+y^{2}}}}\\,.} Several basic models for density variation with elevation are commonly used. The simplest, an isothermal atmosphere, gives ρ = ρ 0 e − y / H , {\\displaystyle \\rho =\\rho _{0}e^{-y/H}\\,,} where ρ 0 {\\displaystyle \\rho _{0}} is the sea-level density and H {\\displaystyle H} is the density scale height. When the limits of integration are zero and infinity, the result is known as Chapman function. An approximate result is obtained if some high-order terms are dropped, yielding (Young 1974, p. 147), X ≈ π R 2 H exp ⁡ ( R cos 2 ⁡ z 2 H ) e r f c ( R cos 2 ⁡ z 2 H ) . {\\displaystyle X\\approx {\\sqrt {\\frac {\\pi R}{2H}}}\\exp {\\left({\\frac {R\\cos ^{2}z}{2H}}\\right)}\\,\\mathrm {erfc} \\left({\\sqrt {\\frac {R\\cos ^{2}z}{2H}}}\\right)\\,.} An approximate correction for refraction can be made by taking (Young 1974, p. 147) R = 7 / 6 R E , {\\displaystyle R=7/6\\,R_{\\mathrm {E}"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_11",
    "chunk": "}\\,,} where R E {\\displaystyle R_{\\mathrm {E} }} is the physical radius of the Earth. At the horizon, the approximate equation becomes X h o r i z ≈ π R 2 H . {\\displaystyle X_{\\mathrm {horiz} }\\approx {\\sqrt {\\frac {\\pi R}{2H}}}\\,.} Using a scale height of 8435 m, Earth's mean radius of 6371 km, and including the correction for refraction, X h o r i z ≈ 37.20 . {\\displaystyle X_{\\mathrm {horiz} }\\approx 37.20\\,.} The assumption of constant temperature is simplistic; a more realistic model is the polytropic atmosphere, for which T = T 0 − α y , {\\displaystyle T=T_{0}-\\alpha y\\,,} where T 0 {\\displaystyle T_{0}} is the sea-level temperature and α {\\displaystyle \\alpha } is the temperature lapse rate. The density as a function of elevation is ρ = ρ 0 ( 1 − α T 0 y ) 1 / ( κ − 1 ) , {\\displaystyle \\rho =\\rho _{0}\\left(1-{\\frac {\\alpha }{T}}_{0}y\\right)^{1/(\\kappa -1)}\\,,} where κ {\\displaystyle \\kappa } is the polytropic exponent (or polytropic index). The air mass integral for the polytropic model does not lend itself to a closed-form solution except at the zenith, so the integration usually is performed numerically. Earth's atmosphere consists of"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_12",
    "chunk": "multiple layers with different temperature and density characteristics; common atmospheric models include the International Standard Atmosphere and the US Standard Atmosphere. A good approximation for many purposes is a polytropic troposphere of 11 km height with a lapse rate of 6.5 K/km and an isothermal stratosphere of infinite height (Garfinkel 1967), which corresponds very closely to the first two layers of the International Standard Atmosphere. More layers can be used if greater accuracy is required. When atmospheric refraction is considered, ray tracing becomes necessary (Kivalov 2007), and the absolute air mass integral becomes σ = ∫ r o b s r a t m ρ d r 1 − ( n o b s n r o b s r ) 2 sin 2 ⁡ z {\\displaystyle \\sigma =\\int _{r_{\\mathrm {obs} }}^{r_{\\mathrm {atm} }}{\\frac {\\rho \\,\\mathrm {d} r}{\\sqrt {1-\\left({\\frac {n_{\\mathrm {obs} }}{n}}{\\frac {r_{\\mathrm {obs} }}{r}}\\right)^{2}\\sin ^{2}z}}}\\,} where n o b s {\\displaystyle n_{\\mathrm {obs} }} is the index of refraction of air at the observer's elevation y o b s {\\displaystyle y_{\\mathrm {obs} }} above sea level, n {\\displaystyle n} is the index of refraction at elevation y {\\displaystyle y} above sea level, r o b s = R E"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_13",
    "chunk": "+ y o b s {\\displaystyle r_{\\mathrm {obs} }=R_{\\mathrm {E} }+y_{\\mathrm {obs} }} , r = R E + y {\\displaystyle r=R_{\\mathrm {E} }+y} is the distance from the center of the Earth to a point at elevation y {\\displaystyle y} , and r a t m = R E + y a t m {\\displaystyle r_{\\mathrm {atm} }=R_{\\mathrm {E} }+y_{\\mathrm {atm} }} is distance to the upper limit of the atmosphere at elevation y a t m {\\displaystyle y_{\\mathrm {atm} }} . The index of refraction in terms of density is usually given to sufficient accuracy (Garfinkel 1967) by the Gladstone–Dale relation n − 1 n o b s − 1 = ρ ρ o b s . {\\displaystyle {\\frac {n-1}{n_{\\mathrm {obs} }-1}}={\\frac {\\rho }{\\rho _{\\mathrm {obs} }}}\\,.} Rearrangement and substitution into the absolute air mass integral gives σ = ∫ r o b s r a t m ρ d r 1 − ( n o b s 1 + ( n o b s − 1 ) ρ / ρ o b s ) 2 ( r o b s r ) 2 sin 2 ⁡ z . {\\displaystyle \\sigma =\\int _{r_{\\mathrm {obs} }}^{r_{\\mathrm {atm} }}{\\frac {\\rho"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_14",
    "chunk": "\\,\\mathrm {d} r}{\\sqrt {1-\\left({\\frac {n_{\\mathrm {obs} }}{1+(n_{\\mathrm {obs} }-1)\\rho /\\rho _{\\mathrm {obs} }}}\\right)^{2}\\left({\\frac {r_{\\mathrm {obs} }}{r}}\\right)^{2}\\sin ^{2}z}}}\\,.} The quantity n o b s − 1 {\\displaystyle n_{\\mathrm {obs} }-1} is quite small; expanding the first term in parentheses, rearranging several times, and ignoring terms in ( n o b s − 1 ) 2 {\\displaystyle (n_{\\mathrm {obs} }-1)^{2}} after each rearrangement, gives (Kasten & Young 1989) σ = ∫ r o b s r a t m ρ d r 1 − [ 1 + 2 ( n o b s − 1 ) ( 1 − ρ ρ o b s ) ] ( r o b s r ) 2 sin 2 ⁡ z . {\\displaystyle \\sigma =\\int _{r_{\\mathrm {obs} }}^{r_{\\mathrm {atm} }}{\\frac {\\rho \\,\\mathrm {d} r}{\\sqrt {1-\\left[1+2(n_{\\mathrm {obs} }-1)(1-{\\frac {\\rho }{\\rho _{\\mathrm {obs} }}})\\right]\\left({\\frac {r_{\\mathrm {obs} }}{r}}\\right)^{2}\\sin ^{2}z}}}\\,.} In the figure at right, an observer at O is at an elevation y obs {\\displaystyle y_{\\text{obs}}} above sea level in a uniform radially symmetrical atmosphere of height y a t m {\\displaystyle y_{\\mathrm {atm} }} . The path length of a light ray at zenith angle z {\\displaystyle z} is s {\\displaystyle s} ; R E {\\displaystyle R_{\\mathrm"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_15",
    "chunk": "{E} }} is the radius of the Earth. Applying the law of cosines to triangle OAC, ( R E + y atm ) 2 = s 2 + ( R E + y obs ) 2 − 2 ( R E + y obs ) s cos ⁡ ( 180 ∘ − z ) = s 2 + ( R E + y obs ) 2 + 2 ( R E + y obs ) s cos ⁡ z {\\displaystyle {\\begin{aligned}\\left(R_{E}+y_{\\text{atm}}\\right)^{2}&=s^{2}+\\left(R_{E}+y_{\\text{obs}}\\right)^{2}-2\\left(R_{E}+y_{\\text{obs}}\\right)s\\cos \\left(180^{\\circ }-z\\right)\\\\&=s^{2}+\\left(R_{E}+y_{\\text{obs}}\\right)^{2}+2\\left(R_{E}+y_{\\text{obs}}\\right)s\\cos z\\end{aligned}}} expanding the left- and right-hand sides, eliminating the common terms, and rearranging gives s 2 + 2 ( R E + y obs ) s cos ⁡ z − 2 R E y atm − y atm 2 + 2 R E y obs + y obs 2 = 0 . {\\displaystyle {{s}^{2}}+2\\left({R_{\\text{E}}}+{y_{\\text{obs}}}\\right)s\\cos z-2{R_{\\text{E}}}{y_{\\text{atm}}}-y_{\\text{atm}}^{2}+2{R_{\\text{E}}}{y_{\\text{obs}}}+y_{\\text{obs}}^{2}=0\\,.} Solving the quadratic for the path length s, factoring, and rearranging, s = ± ( R E + y obs ) 2 cos 2 z + 2 R E ( y atm − y obs ) + y atm 2 − y obs 2 − ( R E + y obs ) cos ⁡ z . {\\displaystyle s=\\pm {\\sqrt {{{\\left({R_{\\text{E}}}+{y_{\\text{obs}}}\\right)}^{2}}{{\\cos }^{2}}z+2{R_{\\text{E}}}\\left({y_{\\text{atm }}}-{y_{\\text{obs}}}\\right)+y_{\\text{atm}}^{2}-y_{\\text{obs}}^{2}}}-({R_{\\text{E}}}+{y_{\\text{obs}}})\\cos"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_16",
    "chunk": "z\\,.} The negative sign of the radical gives a negative result, which is not physically meaningful. Using the positive sign, dividing by y a t m {\\displaystyle y_{\\mathrm {atm} }} , and cancelling common terms and rearranging gives the relative air mass: X = ( R E + y obs y atm ) 2 cos 2 z + 2 R E y atm 2 ( y atm − y obs ) − ( y obs y atm ) 2 + 1 − R E + y obs y atm cos ⁡ z . {\\displaystyle X={\\sqrt {{{\\left({\\frac {{R_{\\text{E}}}+{y_{\\text{obs}}}}{y_{\\text{atm}}}}\\right)}^{2}}{{\\cos }^{2}}z+{\\frac {2{R_{\\text{E}}}}{y_{\\text{atm}}^{2}}}\\left({y_{\\text{atm}}}-{y_{\\text{obs}}}\\right)-{{\\left({\\frac {y_{\\text{obs}}}{y_{\\text{atm}}}}\\right)}^{2}}+1}}-{\\frac {{R_{\\text{E}}}+{y_{\\text{obs}}}}{y_{\\text{atm}}}}\\cos z\\,.} With the substitutions r ^ = R E / y a t m {\\displaystyle {\\hat {r}}=R_{\\mathrm {E} }/y_{\\mathrm {atm} }} and y ^ = y o b s / y a t m {\\displaystyle {\\hat {y}}=y_{\\mathrm {obs} }/y_{\\mathrm {atm} }} , this can be given as X = ( r ^ + y ^ ) 2 cos 2 ⁡ z + 2 r ^ ( 1 − y ^ ) − y ^ 2 + 1 − ( r ^ + y ^ ) cos ⁡ z . {\\displaystyle X={\\sqrt {{({\\hat {r}}+{\\hat {y}})}^{2}\\cos ^{2}z+2{\\hat {r}}(1-{\\hat {y}})-{\\hat {y}}^{2}+1}}\\;-\\;({\\hat {r}}+{\\hat"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_17",
    "chunk": "{y}})\\cos z\\,.} When the observer's elevation is zero, the air mass equation simplifies to X = ( R E y atm ) 2 cos 2 ⁡ z + 2 R E y atm + 1 − R E y atm cos ⁡ z . {\\displaystyle X={\\sqrt {{{\\left({\\frac {R_{\\text{E}}}{y_{\\text{atm}}}}\\right)}^{2}}\\cos ^{2}z+{\\frac {2R_{\\text{E}}}{y_{\\text{atm}}}}+1}}-{\\frac {R_{\\text{E}}}{y_{\\text{atm}}}}\\cos z\\,.} In the limit of grazing incidence, the absolute airmass equals the distance to the horizon. Furthermore, if the observer is elevated, the horizon zenith angle can be greater than 90°. Atmospheric models that derive from hydrostatic considerations assume an atmosphere of constant composition and a single mechanism of extinction, which isn't quite correct. There are three main sources of attenuation (Hayes & Latham 1975): Rayleigh scattering by air molecules, Mie scattering by aerosols, and molecular absorption (primarily by ozone). The relative contribution of each source varies with elevation above sea level, and the concentrations of aerosols and ozone cannot be derived simply from hydrostatic considerations. Rigorously, when the extinction coefficient depends on elevation, it must be determined as part of the air mass integral, as described by Thomason, Herman & Reagan (1983). A compromise approach often is possible, however. Methods for separately calculating the extinction from each species"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_18",
    "chunk": "using closed-form expressions are described in Schaefer (1993) and Schaefer (1998). The latter reference includes source code for a BASIC program to perform the calculations. Reasonably accurate calculation of extinction can sometimes be done by using one of the simple air mass formulas and separately determining extinction coefficients for each of the attenuating species (Green 1992, Pickering 2002). In optical astronomy, the air mass provides an indication of the deterioration of the observed image, not only as regards direct effects of spectral absorption, scattering and reduced brightness, but also an aggregation of visual aberrations, e.g. resulting from atmospheric turbulence, collectively referred to as the quality of the \"seeing\". On bigger telescopes, such as the WHT (Wynne & Worswick 1988) and VLT (Avila, Rupprecht & Beckers 1997), the atmospheric dispersion can be so severe that it affects the pointing of the telescope to the target. In such cases an atmospheric dispersion compensator is used, which usually consists of two prisms. The Greenwood frequency and Fried parameter, both relevant for adaptive optics, depend on the air mass above them (or more specifically, on the zenith angle). In radio astronomy the air mass (which influences the optical path length) is not relevant. The"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_19",
    "chunk": "lower layers of the atmosphere, modeled by the air mass, do not significantly impede radio waves, which are of much lower frequency than optical waves. Instead, some radio waves are affected by the ionosphere in the upper atmosphere. Newer aperture synthesis radio telescopes are especially affected by this as they “see” a much larger portion of the sky and thus the ionosphere. In fact, LOFAR needs to explicitly calibrate for these distorting effects (van der Tol & van der Veen 2007; de Vos, Gunst & Nijboer 2009), but on the other hand can also study the ionosphere by instead measuring these distortions (Thidé 2007). In some fields, such as solar energy and photovoltaics, air mass is indicated by the acronym AM; additionally, the value of the air mass is often given by appending its value to AM, so that AM1 indicates an air mass of 1, AM2 indicates an air mass of 2, and so on. The region above Earth's atmosphere, where there is no atmospheric attenuation of solar radiation, is considered to have \"air mass zero\" (AM0). Atmospheric attenuation of solar radiation is not the same for all wavelengths; consequently, passage through the atmosphere not only reduces intensity but"
  },
  {
    "source": "Air mass (astronomy).txt",
    "chunk_id": "Air mass (astronomy).txt_20",
    "chunk": "also alters the spectral irradiance. Photovoltaic modules are commonly rated using spectral irradiance for an air mass of 1.5 (AM1.5); tables of these standard spectra are given in ASTM G 173-03. The extraterrestrial spectral irradiance (i.e., that for AM0) is given in ASTM E 490-00a. For many solar energy applications when high accuracy near the horizon is not required, air mass is commonly determined using the simple secant formula described in § Plane-parallel atmosphere."
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_0",
    "chunk": "# Al-Biruni Abu Rayhan Muhammad ibn Ahmad al-Biruni /ælbɪˈruːni/ (Persian: ابوریحان بیرونی; Arabic: أبو الريحان البيروني; 973 – after 1050), known as al-Biruni, was a Khwarazmian Iranian scholar and polymath during the Islamic Golden Age. He has been called variously \"Father of Comparative Religion\", \"Father of modern geodesy\", Founder of Indology and the first anthropologist. Al-Biruni was well versed in physics, mathematics, astronomy, and natural sciences, and also distinguished himself as a historian, chronologist, and linguist. He studied almost all the sciences of his day and was rewarded abundantly for his tireless research in many fields of knowledge. Royalty and other powerful elements in society funded al-Biruni's research and sought him out with specific projects in mind. Influential in his own right, al-Biruni was himself influenced by the scholars of other nations, such as the Greeks, from whom he took inspiration when he turned to the study of philosophy. A gifted linguist, he was conversant in Khwarezmian, Persian, Arabic, and Sanskrit, and also knew Greek, Hebrew, and Syriac. He spent much of his life in Ghazni, then capital of the Ghaznavids, in modern-day central-eastern Afghanistan. In 1017, he travelled to the Indian subcontinent and wrote a treatise on Indian culture"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_1",
    "chunk": "entitled Tārīkh al-Hind (\"The History of India\"), after exploring the Hindu faith practiced in India. He was, for his time, an admirably impartial writer on the customs and creeds of various nations, his scholarly objectivity earning him the title al-Ustadh (\"The Master\") in recognition of his remarkable description of early 11th-century India. Al-Biruni's name is derived from the Persian word bērūn or bīrūn (\"outskirts\"), as he was born in an outlying district of Kath, the capital of the Afrighid kingdom of Khwarazm. The city, now called Beruniy, is part of the autonomous republic of Karakalpakstan in northwest Uzbekistan. Al-Biruni spent the first twenty-five years of his life in Khwarezm where he studied Islamic jurisprudence, theology, grammar, mathematics, astronomy, medicine and philosophy and dabbled not only in the field of physics, but also in those of most of the other sciences. The Iranian Khwarezmian language, which was Biruni's mother tongue, survived for several centuries after Islam until the Turkification of the region – at least some of the culture of ancient Khwarezm endured – for it is hard to imagine that the commanding figure of Biruni, a repository of so much knowledge, should have appeared in a cultural vacuum. He was"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_2",
    "chunk": "sympathetic to the Afrighids, who were overthrown by the rival dynasty of Ma'munids in 995. He left his homeland for Bukhara, then under the Samanid ruler Mansur II the son of Nuh II. He corresponded with Avicenna, and there are extant exchanges of views between these two scholars. In 998, he went to the court of the Ziyarid amir of Tabaristan, Qabus (r. 977–981, 997–1012). There he wrote his first important work, al-Athar al-Baqqiya 'an al-Qorun al-Khaliyya (\"The remaining traces of past centuries\", translated as \"Chronology of ancient nations\" or \"Vestiges of the Past\") on historical and scientific chronology, probably around 1000, though he later made some amendments to the book. He also visited the court of the Bavandid ruler Al-Marzuban. Accepting the definite demise of the Afrighids at the hands of the Ma'munids, he made peace with the latter who then ruled Khwarezm. Their court at Gorganj (also in Khwarezm) was gaining fame for its gathering of brilliant scientists. In 1017, Mahmud of Ghazni captured Rey. Most scholars, including al-Biruni, were taken to Ghazni, the capital of the Ghaznavid dynasty. Biruni was made court astrologer and accompanied Mahmud on his invasions into India, living there for a few years."
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_3",
    "chunk": "He was 44 when he went on the journeys with Mahmud of Ghazni. Biruni became acquainted with all things related to India. During this time he wrote his study of India, finishing it around 1030. Along with his writing, Al-Biruni also made sure to extend his study to sciences while on the expeditions. He sought to find a method to measure the height of the sun, and created a makeshift quadrant for that purpose. Al-Biruni was able to make much progress in his study over the frequent travels that he went on throughout the lands of India. Belonging to the Sunni Ash'ari school, al-Biruni nevertheless also associated with Maturidi theologians. He was however, very critical of the Mu'tazila, particularly criticising al-Jahiz and Zurqan. He also repudiated Avicenna for his views on the eternality of the universe. Of the 146 books written by al-Bīrūnī, 95 are devoted to astronomy, mathematics, and related subjects like mathematical geography. He lived during the Islamic Golden Age, when the Abbasid Caliphs promoted astronomical research, because such research possessed not only a scientific but also a religious dimension: in Islam worship and prayer require a knowledge of the precise directions of sacred locations, which can be"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_4",
    "chunk": "determined accurately only through the use of astronomical data. In carrying out his research, al-Biruni used a variety of different techniques dependent upon the particular field of study involved. His major work on astrology is primarily an astronomical and mathematical text; he states: \"I have begun with Geometry and proceeded to Arithmetic and the Science of Numbers, then to the structure of the Universe and finally to Judicial Astrology [sic], for no one who is worthy of the style and title of Astrologer [sic] who is not thoroughly conversant with these for sciences.\" In these earlier chapters he lays the foundations for the final chapter, on astrological prognostication, which he criticises. In a later work, he wrote a refutation of astrology, in contradistinction to the legitimate science of astronomy, for which he expresses wholehearted support. Some suggest that his reasons for refuting astrology relate to the methods used by astrologers being based upon pseudoscience rather than empiricism and also to a conflict between the views of the astrologers and those of the orthodox theologians of Sunni Islam. He wrote an extensive commentary on Indian astronomy in the Taḥqīq mā li-l-Hind mostly translation of Aryabhatta's work, in which he claims to"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_5",
    "chunk": "have resolved the matter of Earth's rotation in a work on astronomy that is no longer extant, his Miftah-ilm-alhai'a (\"Key to Astronomy\"): [T]he rotation of the earth does in no way impair the value of astronomy, as all appearances of an astronomic character can quite as well be explained according to this theory as to the other. There are, however, other reasons which make it impossible. This question is most difficult to solve. The most prominent of both modern and ancient astronomers have deeply studied the question of the moving of the earth, and tried to refute it. We, too, have composed a book on the subject called Miftah-ilm-alhai'a (Key to Astronomy), in which we think we have surpassed our predecessors, if not in the words, at all events in the matter. In his major astronomical work, the Mas'ud Canon, Biruni observed that, contrary to Ptolemy, the Sun's apogee (highest point in the heavens) was mobile, not fixed. He wrote a treatise on the astrolabe, describing how to use it to tell the time and as a quadrant for surveying. One particular diagram of an eight-geared device could be considered an ancestor of later Muslim astrolabes and clocks. More recently,"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_6",
    "chunk": "Biruni's eclipse data was used by Dunthorne in 1749 to help determine the acceleration of the Moon, and his data on equinox times and eclipses was used as part of a study of Earth's past rotation. Like later adherents of the Ash'ari school, such as al-Ghazali, al-Biruni is famous for vehemently defending the majority Sunni position that the universe had a beginning, being a strong supporter of creatio ex nihilo, specifically refuting the philosopher Ibn Sina in a multiple letter correspondence. Al-Biruni stated: \"Other people, besides, hold this foolish persuasion, that time has no terminus quo at all.\" He further stated that Aristotle, whose arguments Avicenna uses, contradicted himself when he stated that the universe and matter has a start whilst holding on to the idea that matter is pre-eternal. In his letters to Avicenna, he stated the argument of Aristotle, that there is a change in the creator. He further argued that stating there is a change in the creator would mean there is a change in the effect (meaning the universe has change) and that the universe coming into being after not being is such a change (and so arguing there is no change – no beginning –"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_7",
    "chunk": "means Aristotle believes the creator is negated). Al-Biruni was proud of the fact that he followed the textual evidence of the religion without being influenced by Greek philosophers such as Aristotle. Al-Biruni contributed to the introduction of the scientific method to medieval mechanics. He developed experimental methods to determine density, using a particular type of hydrostatic balance. Al-Biruni's method of using the hydrostatic balance was precise, and he was able to measure the density of many different substances, including precious metals, gems, and even air. He also used this method to determine the radius of the earth, which he did by measuring the angle of elevation of the horizon from the top of a mountain and comparing it to the angle of elevation of the horizon from a nearby plain. In addition to developing the hydrostatic balance, Al-Biruni also wrote extensively on the topic of density, including the different types of densities and how they are measured. His work on the subject was very influential and was later used by scientists like Galileo and Newton in their own research. Bīrūnī devised a novel method of determining the Earth's radius by means of the observation of the height of a mountain."
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_8",
    "chunk": "He carried it out at Nandana in Pind Dadan Khan (present-day Pakistan). He used trigonometry to calculate the radius of the Earth using measurements of the height of a hill and measurement of the dip in the horizon from the top of that hill. His calculated radius for the Earth of 3928.77 miles was 2% higher than the actual mean radius of 3847.80 miles. His estimate was given as 12,803,337 cubits, so the accuracy of his estimate compared to the modern value depends on what conversion is used for cubits. The exact length of a cubit is not clear; with an 18-inch cubit his estimate would be 3,600 miles, whereas with a 22-inch cubit his estimate would be 4,200 miles. One significant problem with this approach is that Al-Biruni was not aware of atmospheric refraction and made no allowance for it. He used a dip angle of 34 arc minutes in his calculations, but refraction can typically alter the measured dip angle by about 1/6, making his calculation only accurate to within about 20% of the true value. In his Codex Masudicus (1037), Al-Biruni theorized the existence of a landmass along the vast ocean between Asia and Europe, or what"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_9",
    "chunk": "is today known as the Americas. He argued for its existence on the basis of his accurate estimations of the Earth's circumference and Afro-Eurasia's size, which he found spanned only two-fifths of the Earth's circumference, reasoning that the geological processes that gave rise to Eurasia must surely have given rise to lands in the vast ocean between Asia and Europe. He also theorized that at least some of the unknown landmass would lie within the known latitudes which humans could inhabit, and therefore would be inhabited. Biruni wrote a pharmacopoeia, the Kitab al-saydala fi al-tibb (\"Book on the Pharmacopoeia of Medicine\"). It lists synonyms for drug names in Syriac, Persian, Greek, Baluchi, Afghan, Kurdish, and some Indian languages. He used a hydrostatic balance to determine the density and purity of metals and precious stones. He classified gems by what he considered their primary physical properties, such as specific gravity and hardness, rather than the common practice of the time of classifying them by colour. Biruni's main essay on political history, Kitāb al-musāmara fī aḵbār Ḵᵛārazm (\"Book of nightly conversation concerning the affairs of Ḵᵛārazm\") is now known only from quotations in Bayhaqī's Tārīkh-e Masʿūdī. In addition to this various discussions"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_10",
    "chunk": "of historical events and methodology are found in connection with the lists of kings in his al-Āthār al-bāqiya and in the Qānūn as well as elsewhere in the Āthār, in India, and scattered throughout his other works. Al-Biruni's Chronology of Ancient Nations attempted to accurately establish the length of various historical eras. Biruni is widely considered to be one of the most important Muslim authorities on the history of religion. He is known as a pioneer in the field of comparative religion in his study of, among other creeds, Zoroastrianism, Judaism, Hinduism, Christianity, Buddhism and Islam. He assumed the superiority of Islam: \"We have here given an account of these things in order that the reader may learn by the comparative treatment of the subject how much superior the institutions of Islam are, and how more plainly this contrast brings out all customs and usages, differing from those of Islam, in their essential foulness.\" However he was happy on occasion to express admiration for other cultures, and quoted directly from the sacred texts of other religions when reaching his conclusions. He strove to understand them on their own terms rather than trying to prove them wrong. His underlying concept was"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_11",
    "chunk": "that all cultures are at least distant relatives of all other cultures because they are all human constructs. \"Rather, what Al-Biruni seems to be arguing is that there is a common human element in every culture that makes all cultures distant relatives, however foreign they might seem to one another.\" Al-Biruni divides Hindus into an educated and an uneducated class. He describes the educated as monotheistic, believing that God is one, eternal, and omnipotent and eschewing all forms of idol worship. He recognizes that uneducated Hindus worshiped a multiplicity of idols yet points out that even some Muslims (such as the Jabriyah) have adopted anthropomorphic concepts of God. Al-Biruni wrote about the peoples, customs and religions of the Indian subcontinent. According to Akbar S. Ahmed, like modern anthropologists, he engaged in extensive participant observation with a given group of people, learnt their language and studied their primary texts, presenting his findings with objectivity and neutrality using cross-cultural comparisons. Akhbar S. Ahmed concluded that Al-Biruni can be considered as the first anthropologist, others, however, have argued that he can hardly be considered an anthropologist in the conventional sense. Biruni's fame as an Indologist rests primarily on two texts. Biruni wrote an"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_12",
    "chunk": "encyclopedic work on India called Taḥqīq mā li-l-Hind min maqūlah maqbūlah fī al-ʿaql aw mardhūlah (variously translated as Verifying All That the Indians Recount, the Reasonable and the Unreasonable, or The book confirming what pertains to India, whether rational or despicable, in which he explored nearly every aspect of Indian life. During his journey through India, military and political history were not Biruni's main focus: he decided rather to document the civilian and scholarly aspects of Hindu life, examining culture, science, and religion. He explored religion within a rich cultural context. He expressed his objectives with simple eloquence: He also translated the yoga sutras of Indian sage Patanjali with the title Tarjamat ketāb Bātanjalī fi’l-ḵalāṣ men al-ertebāk: I shall not produce the arguments of our antagonists in order to refute such of them, as I believe to be in the wrong. My book is nothing but a simple historic record of facts. I shall place before the reader the theories of the Hindus exactly as they are, and I shall mention in connection with them similar theories of the Greeks in order to show the relationship existing between them. An example of Biruni's analysis is his summary of why many"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_13",
    "chunk": "Hindus hate Muslims. Biruni notes in the beginning of his book how the Muslims had a hard time learning about Hindu knowledge and culture. He explains that Hinduism and Islam are totally different from each other. Moreover, Hindus in 11th century India had suffered waves of destructive attacks on many of its cities, and Islamic armies had taken numerous Hindu slaves to Persia, which – claimed Biruni – contributed to Hindus becoming suspicious of all foreigners, not just Muslims. Hindus considered Muslims violent and impure, and did not want to share anything with them. Over time, Biruni won the welcome of Hindu scholars. Al-Biruni collected books and studied with these Hindu scholars to become fluent in Sanskrit, discover and translate into Arabic the mathematics, science, medicine, astronomy and other fields of arts as practiced in 11th-century India. He was inspired by the arguments offered by Indian scholars who believed earth must be globular in shape, which they felt was the only way to fully explain the difference in daylight hours by latitude, seasons and Earth's relative positions with Moon and stars. At the same time, Biruni was also critical of Indian scribes, who he believed carelessly corrupted Indian documents while"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_14",
    "chunk": "making copies of older documents. He also criticized the Hindus on what he saw them do and not do, for example finding them deficient in curiosity about history and religion. One of the specific aspects of Hindu life that Biruni studied was the Hindu calendar. His scholarship on the topic exhibited great determination and focus, not to mention the excellence in his approach of the in-depth research he performed. He developed a method for converting the dates of the Hindu calendar to the dates of the three different calendars that were common in the Islamic countries of his time period, the Greek, the Arab/Muslim, and the Persian. Biruni also employed astronomy in the determination of his theories, which were complex mathematical equations and scientific calculation that allows one to convert dates and years between the different calendars. The book does not limit itself to tedious records of battle because Biruni found the social culture to be more important. The work includes research on a vast array of topics of Indian culture, including descriptions of their traditions and customs. Although he tried to stay away from political and military history, Biruni did indeed record important dates and noted actual sites of"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_15",
    "chunk": "where significant battles occurred. Additionally, he chronicled stories of Indian rulers and told of how they ruled over their people with their beneficial actions and acted in the interests of the nation. His details are brief and mostly just list rulers without referring to their real names, and he did not go on about deeds that each one carried out during their reign, which keeps in line with Biruni's mission to try to stay away from political histories. Biruni also described the geography of India in his work. He documented different bodies of water and other natural phenomena. These descriptions are useful to today's modern historians because they are able to use Biruni's scholarship to locate certain destinations in modern-day India. Historians are able to make some matches while also concluding that certain areas seem to have disappeared and been replaced with different cities. Different forts and landmarks were able to be located, legitimizing Biruni's contributions with their usefulness to even modern history and archeology. The dispassionate account of Hinduism given by Biruni was remarkable for its time. He stated that he was fully objective in his writings, remaining unbiased like a proper historian should. Biruni documented everything about India"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_16",
    "chunk": "just as it happened. But, he did note how some of the accounts of information that he was given by natives of the land may not have been reliable in terms of complete accuracy, however, he did try to be as honest as possible in his writing. Eduard Sachau compares it to \"a magic island of quiet, impartial research in the midst of a world of clashing swords, burning towns, and plundered temples.\" Biruni's writing was very poetic, which may diminish some of the historical value of the work for modern times. The lack of description of battle and politics makes those parts of the picture completely lost. However, many have used Biruni's work to check facts of history in other works that may have been ambiguous or had their validity questioned. Most of the works of Al-Biruni are in Arabic although he seemingly wrote the Kitab al-Tafhim in both Persian and Arabic, showing his mastery over both languages. Bīrūnī's catalogue of his own literary production up to his 65th lunar/63rd solar year (the end of 427/1036) lists 103 titles divided into 12 categories: astronomy, mathematical geography, mathematics, astrological aspects and transits, astronomical instruments, chronology, comets, an untitled category, astrology,"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_17",
    "chunk": "anecdotes, religion, and books he no longer possesses. Biruni wrote most of his works in Arabic, the scientific language of his age, but al-Tafhim is one of the most important of the early works of science in Persian, and is a rich source for Persian prose and lexicography. The book covers the Quadrivium in a detailed and skilled fashion. Following Al-Biruni's death, his work was neither built upon or referenced by scholars. Centuries later, his writings about India, which had become of interest to the British Raj, were revisited. The lunar crater Al-Biruni and the asteroid 9936 Al-Biruni are named in his honour. Biruni Island in Antarctica is named after al-Biruni. In Iran, surveying engineers are celebrated on al-Biruni's birthday. In June 2009, Iran donated a pavilion to the United Nations Office in Vienna—placed in the central Memorial Plaza of the Vienna International Center. Named the Scholars Pavilion, it features the statues of four prominent Iranian scholars: Avicenna, Abu Rayhan Biruni, Zakariya Razi (Rhazes) and Omar Khayyam. A film about the life of Al-Biruni, Abu Raykhan Beruni, was released in the Soviet Union in 1974. Irrfan Khan portrayed Al-Biruni in the 1988 Doordarshan historical drama Bharat Ek Khoj. He has"
  },
  {
    "source": "Al-Biruni.txt",
    "chunk_id": "Al-Biruni.txt_18",
    "chunk": "been portrayed by Cüneyt Uzunlar in the Turkish television series Alparslan: Büyük Selçuklu on TRT 1."
  },
  {
    "source": "Albert A. Michelson.txt",
    "chunk_id": "Albert A. Michelson.txt_0",
    "chunk": "# Albert A. Michelson Albert Abraham Michelson (/ˈmɪkəlsən/; December 19, 1852 – May 9, 1931) was an American physicist known for his work on measuring the speed of light and especially for the Michelson–Morley experiment. In 1907, he received the Nobel Prize in Physics, becoming the first American to win the Nobel Prize in a science. He was the founder and the first head of the physics departments of Case School of Applied Science (now Case Western Reserve University) and the University of Chicago. Michelson was born in Strelno, Posen, Kingdom of Prussia (modern-day Strzelno, Poland), to Jewish parents, the son of Samuel Michelson and his wife, Rozalia Przyłubska. He moved to the US with his parents in 1855, at the age of two. He grew up in the mining towns of Murphy's Camp, California, and Virginia City, Nevada, where his father was a merchant. His family was non-religious, and Michelson himself was a lifelong agnostic. He spent his high school years in San Francisco in the home of his aunt, Henriette Levy (née Michelson), who was the mother of author Harriet Lane Levy. His sister was the novelist Miriam Michelson. President Ulysses S. Grant awarded Michelson a special appointment"
  },
  {
    "source": "Albert A. Michelson.txt",
    "chunk_id": "Albert A. Michelson.txt_1",
    "chunk": "to the U.S. Naval Academy in 1869. During his four years as a midshipman at the Academy, Michelson excelled in optics, heat, climatology and technical drawing. After graduating in 1873 and two years at sea, he returned to the Naval Academy in 1875 to become an instructor in physics and chemistry until 1879. In 1879, he was posted to the Nautical Almanac Office, Washington (part of the United States Naval Observatory), to work with Simon Newcomb. In the following year he obtained leave of absence to continue his studies in Europe. He visited the Universities of Berlin and Heidelberg, and the Collège de France and École Polytechnique in Paris. Michelson was fascinated with the sciences, and the problem of measuring the speed of light in particular. While at Annapolis, he conducted his first experiments on the speed of light, as part of a class demonstration in 1877. His Annapolis experiment was refined, and in 1879, he measured the speed of light in air to be 299864±51 kilometres per second, and estimated the speed of light in vacuum as 299940 km/s, or 186380 mi/s. After two years of studies in Europe, he resigned from the Navy in 1881. In 1883 he"
  },
  {
    "source": "Albert A. Michelson.txt",
    "chunk_id": "Albert A. Michelson.txt_2",
    "chunk": "accepted a position as professor of physics at the Case School of Applied Science in Cleveland, Ohio, and concentrated on developing an improved interferometer. In 1887 he and Edward Morley carried out the famous Michelson–Morley experiment which failed to detect evidence of the existence of the luminiferous ether. He later moved on to use astronomical interferometers in the measurement of stellar diameters and in measuring the separations of binary stars. In 1889 Michelson became a professor at Clark University at Worcester, Massachusetts, and in 1892 was appointed professor and the first head of the department of physics at the newly organized University of Chicago. In 1902, he was elected as a member of the American Philosophical Society. In 1907, Michelson had the honor of being the first American to receive a Nobel Prize in Physics \"for his optical precision instruments and the spectroscopic and metrological investigations carried out with their aid\". He also won the Copley Medal in 1907, the Henry Draper Medal in 1916 and the Gold Medal of the Royal Astronomical Society in 1923. A crater on the Moon is named after him. He returned to military service in the closing months of World War One as a"
  },
  {
    "source": "Albert A. Michelson.txt",
    "chunk_id": "Albert A. Michelson.txt_3",
    "chunk": "Lieutenant Commander in the Naval Reserve, serving in the Bureau of Ordnance. He was promoted to Commander in the reserve in May 1919 and was recalled briefly to active duty in the 9th Naval District before being released from service on 30 September 1921. Michelson died in Pasadena, California, at the age of 78. The University of Chicago Residence Halls remembered Michelson and his achievements by dedicating 'Michelson House' in his honor. Case Western Reserve has dedicated a Michelson House to him, and Michelson Hall (an academic building of science classrooms, laboratories and offices) at the United States Naval Academy also bears his name. Michelson Laboratory at Naval Air Weapons Station China Lake in Ridgecrest, California, is named for him. There is a display in the publicly accessible area of the Lab which includes facsimiles of Michelson's Nobel Prize medal, the prize document, and examples of his diffraction gratings. In 2017, a newly renovated physics research center at the University of Chicago was renamed in honor of Michelson as well. Numerous awards, lectures, and honors have been created in Albert A. Michelson's name. Some of the current awards and lectures named for Michelson include the following: the Bomem-Michelson Award and"
  },
  {
    "source": "Albert A. Michelson.txt",
    "chunk_id": "Albert A. Michelson.txt_4",
    "chunk": "Lecture annually presented until 2017 by the Coblentz Society; the Michelson–Morley Award and Lecture, along with the Michelson Lecture Series, and the Michelson Postdoctoral Prize Lectureship, all of which are given annually by Case Western Reserve University; the A.A. Michelson Award presented every year by the Computer Measurement Group; the Albert A. Michelson Award given by the Navy League of the United States; and the Michelson Memorial Lecture Series presented annually by the Division of Mathematics and Science at the U.S. Naval Academy. In 1877 Michelson married Margaret Hemingway, daughter of a wealthy New York stockbroker and lawyer and the niece of his commander William T. Sampson. They had two sons and a daughter. Michelson was fascinated by light all his life. Once asked why he studied light, he reputedly said, \"because it's so much fun\". As early as 1869, while serving as an officer in the United States Navy, Michelson started planning a repeat of the rotating-mirror method of Léon Foucault for measuring the speed of light, using improved optics and a longer baseline. He conducted some preliminary measurements using largely improvised equipment in 1878, about the same time that his work came to the attention of Simon Newcomb,"
  },
  {
    "source": "Albert A. Michelson.txt",
    "chunk_id": "Albert A. Michelson.txt_5",
    "chunk": "director of the Nautical Almanac Office who was already advanced in planning his own study. Michelson's formal experiments took place in June and July 1879. He constructed a frame building along the north sea wall of the Naval Academy to house the machinery. Michelson published his result of 299,910 ± 50 km/s in 1879 before joining Newcomb in Washington DC to assist with his measurements there. Thus began a long professional collaboration and friendship between the two. Simon Newcomb, with his more adequately funded project, obtained a value of 299,860 ± 30 km/s, just at the extreme edge of consistency with Michelson's. Michelson continued to \"refine\" his method and in 1883 published a measurement of 299,853 ± 60 km/s, rather closer to that of his mentor. In 1906, a novel electrical method was used by E. B. Rosa and the National Bureau of Standards to obtain a value for the speed of light of 299,781 ± 10 km/s. Though this result has subsequently been shown to be severely biased by the poor electrical standards in use at the time, it seems to have set a fashion for rather lower measured values. From 1920, Michelson started planning a definitive measurement from"
  },
  {
    "source": "Albert A. Michelson.txt",
    "chunk_id": "Albert A. Michelson.txt_6",
    "chunk": "the Mount Wilson Observatory, using a baseline to Lookout Mountain, a prominent bump on the south ridge of Mount San Antonio (\"Old Baldy\"), some 22 miles distant. In 1922, the United States Coast and Geodetic Survey began two years of painstaking measurement of the baseline using the recently available invar tapes. With the baseline length established in 1924, measurements were carried out over the next two years to obtain the published value of 299,796 ± 4 km/s. Famous as the measurement is, it was beset by problems, not least of which was the haze created by the smoke from forest fires which blurred the mirror image. It is also probable that the intensively detailed work of the geodetic survey, with an estimated error of less than one part in 1 million, was compromised by a shift in the baseline arising from the Santa Barbara earthquake of June 29, 1925, which was an estimated magnitude of 6.3 on the Richter scale. The now-famous Michelson–Morley experiment also influenced the affirmation attempts of peer Albert Einstein's theory of general relativity and special relativity, using similar optical instrumentation. These instruments and related collaborations included the participation of fellow physicists Dayton Miller, Hendrik Lorentz, and"
  },
  {
    "source": "Albert A. Michelson.txt",
    "chunk_id": "Albert A. Michelson.txt_7",
    "chunk": "Robert Shankland. The period after 1927 marked the advent of new measurements of the speed of light using novel electro-optic devices, all substantially lower than Michelson's 1926 value. Michelson sought another measurement, but this time in an evacuated tube to avoid difficulties in interpreting the image owing to atmospheric effects. In 1929, he began a collaboration with Francis G. Pease and Fred Pearson to perform a measurement in a 1.6 km tube 3 feet in diameter at the Irvine Ranch near Santa Ana, California. In multiple reflections the light path was increased to 5 miles. For the first time in history the speed of light was measured in an almost perfect vacuum of 0.5 mm of mercury. Michelson died with only 36 of the 233 measurement series completed and the experiment was subsequently beset by geological instability and condensation problems before the result of 299774±11 km/s, consistent with the prevailing electro-optic values, was published posthumously in 1935. During June and early July 1879, Michelson refined experimental arrangements from those developed by Hippolyte Fizeau and Léon Foucault. The experimental setup was as follows: Light generated from a source is directed towards a rotating mirror through a slit on a fixed plate;"
  },
  {
    "source": "Albert A. Michelson.txt",
    "chunk_id": "Albert A. Michelson.txt_8",
    "chunk": "the rotating mirror reflects the incoming light and at a certain angle, towards the direction where another fixed flat mirror is placed whose surface is perpendicular to the incoming ray of light; the rotating mirror should have rotated by an angle α by the time the ray of light travels back and is reflected again towards the fixed plate (the distance between the fixed mirror and the rotating one is recorded as D); a displacement from the slit is detected on the plate which measures d; the distance from the rotating mirror to the fixed plate is designated as the radius r while the number of revolutions per second of the mirror is recorded as ω. In this way, tan(2α) = d/r; Δt = (α/2π)/ω; speed of light can be derived as c = 2D/Δt. While at plain sight, four measured quantities are involved: distance D, radius r, displacement d and rotating mirror revolution per second ω, which seems simple; yet based on the limitation of the measurement technology at that time, great efforts were made by Michelson to reduce systematic errors and apply subsequent corrections. For instance, he adopted a steel measuring tape with a said length of 100"
  },
  {
    "source": "Albert A. Michelson.txt",
    "chunk_id": "Albert A. Michelson.txt_9",
    "chunk": "feet and he intended to measure tens of times across the distance; still, he measured its length against a copy of the official standard yard to find out it as 100.006 feet, thus eliminating a systematic error, albeit small. Aside from the efforts to reduce as much as possible the systematic errors, repeated measurements were performed at multiple levels to obtain more accurate results. As R.J. MacKay and R.W. Oldford remarked in their article, 'It is clear that Michelson appreciated the power of averaging to reduce variability in measurement', it is clear that Michelson had in mind the property that averages vary less which should be formally described as: the standard deviation of the average of n independent random variables is less than that of a single random variable by a factor of the square root of n. To realize that, he also strived to have each measurement not influencing each other, thus being mutually independent random variables. A statistical model for repeated measurements with the assumption of independence or identical distributions is unrealistic. In the case of light speed study, each measurement is approached as the sum of quantity of interest and measurement error. In the absence of systematic"
  },
  {
    "source": "Albert A. Michelson.txt",
    "chunk_id": "Albert A. Michelson.txt_10",
    "chunk": "error, the measurement error of speed of light can be modeled by a random sample from a distribution with unknown expectation and finite variance; thus, the speed of light is represented by the expectation of the model distribution and the ultimate goal is to estimate the expectation of the model distribution on the acquired dataset. The law of large numbers suggests to estimate the expectation by the sample mean. In 1887 he collaborated with colleague Edward Williams Morley of Western Reserve University, now part of Case Western Reserve University, in the Michelson–Morley experiment. Their experiment for the expected motion of the Earth relative to the aether, the hypothetical medium in which light was supposed to travel, resulted in a null result. Surprised, Michelson repeated the experiment with greater and greater precision over the next years, but continued to find no ability to measure the aether. The Michelson–Morley results were immensely influential in the physics community, leading Hendrik Lorentz to devise his now-famous Lorentz contraction equations as a means of explaining the null result. There has been some historical controversy over whether Albert Einstein was aware of the Michelson–Morley results when he developed his theory of special relativity, which pronounced the"
  },
  {
    "source": "Albert A. Michelson.txt",
    "chunk_id": "Albert A. Michelson.txt_11",
    "chunk": "aether to be \"superfluous\". In a later interview, Einstein said of the Michelson–Morley experiment, \"I was not conscious it had influenced me directly ... I guess I just took it for granted that it was true.\" Regardless of Einstein's specific knowledge, the experiment is today considered the canonical experiment in regards to showing the lack of a detectable aether. The precision of their equipment allowed Michelson and Morley to be the first to get precise values for the fine structure in the atomic spectral lines for which in 1916 Arnold Sommerfeld gave a theoretical explanation, introducing the fine-structure constant. In 1920 Michelson and Francis G. Pease made the first measurement of the diameter of a star other than the Sun. Michelson had invented astronomical interferometry and built such an instrument at the Mount Wilson Observatory which was used to measure the diameter of the red giant Betelgeuse. A periscope arrangement was used to direct light from two subpupils, separated by up to 20 feet (6m), into the main pupil of the 100 inch (2.5m) Hooker Telescope, producing interference fringes observed through the eyepiece. The measurement of stellar diameters and the separations of binary stars took up an increasing amount of"
  },
  {
    "source": "Albert A. Michelson.txt",
    "chunk_id": "Albert A. Michelson.txt_12",
    "chunk": "Michelson's life after this. Beginning in the 1970s, astronomical interferometry was revived, with the configurations using two (or more) separate apertures (with diameters small compared to their separation) being often referred to as \"Michelson Stellar Interferometry\". This was to distinguish it from speckle interferometry, but should not be confused with the Michelson interferometer which is one common laboratory interferometer configuration of which the interferometer used in the Michelson–Morley experiment was an instance. Michelson's concept of interfering light from two relatively small apertures separated by a substantial distance (but with that distance, or baseline, now often as long as hundreds of meters) is employed at modern operational observatories such as VLTI, CHARA and the U.S. Navy's NPOI. Gravitational waves are detected using a Michelson interferometer with a laser light source. In 2020 there were three Michelson interferometer gravitational wave detectors operational, and a fourth under construction. These Michelson interferometers have arms 4 kilometers in length, set at 90 degree angles to each other, with the light passing through 1 m diameter vacuum tubes running their entire length. A passing gravitational wave will slightly stretch one arm as it shortens the other. This is precisely the motion to which these Michelson interferometers"
  },
  {
    "source": "Albert A. Michelson.txt",
    "chunk_id": "Albert A. Michelson.txt_13",
    "chunk": "are most sensitive. As of 2020 fifteen gravitational wave events had been observed using these Michelson interferometers. In the 1890s Michelson built a mechanical device called the harmonic analyzer, for computing coefficients of Fourier series and drawing graphs of their partial sums. He and S. W. Stratton published a paper about this machine in the American Journal of Science in 1898. In Season 3 Episode 26 of the television series Bonanza (\"Look to the Stars\", broadcast March 18, 1962), Ben Cartwright (Lorne Greene) helps the 16-year-old Michelson (portrayed by 25-year-old Douglas Lambert (1936–1986)) obtain an appointment to the U.S. Naval Academy, despite the opposition of the bigoted town schoolteacher (played by William Schallert). Bonanza was set in and around Virginia City, Nevada, where Michelson lived with his parents prior to leaving for the Naval Academy. In a voice-over at the end of the episode, Greene mentions Michelson's 1907 Nobel Prize. The home in which Michelson lived as a child in Murphys Camp, California, was in the store of his father, first on Main Street, across from the Sperry & Perry Hotel, and after the 1859 fire in a store next to the hotel. His aunt Bertha Meyers owned a house"
  },
  {
    "source": "Albert A. Michelson.txt",
    "chunk_id": "Albert A. Michelson.txt_14",
    "chunk": "on Main Street toward the east end of town and Michelson probably visited her family there frequently. New Beast Theater Works in collaboration with High Concept Laboratories produced a 'semi-opera' about Michelson, his obsessive working style and its effect on his family life. The production ran from February 11 to February 26, 2011, in Chicago at The Building Stage. Michelson was portrayed by Jon Stutzman. The play was directed by David Maral with music composed by Joshua Dumas. Norman Fitzroy Maclean wrote an essay \"Billiards Is a Good Game\"; published in The Norman Maclean Reader (ed. O. Alan Weltzien, 2008), it is an appreciation of Michelson from Maclean's vantage point as a graduate student regularly watching him play billiards. Michelson was a member of the Royal Society, the National Academy of Sciences, the American Physical Society and the American Association for the Advancement of Science."
  },
  {
    "source": "Allen Telescope Array.txt",
    "chunk_id": "Allen Telescope Array.txt_0",
    "chunk": "# Allen Telescope Array The Allen Telescope Array (ATA), formerly known as the One Hectare Telescope (1hT), is a radio telescope array dedicated to astronomical observations and a simultaneous search for extraterrestrial intelligence (SETI). The array is situated at the Hat Creek Radio Observatory in Shasta County, 290 miles (470 km) northeast of San Francisco, California. The project was originally developed as a joint effort between the SETI Institute and the Radio Astronomy Laboratory (RAL) at the University of California, Berkeley (UC Berkeley), with funds obtained from an initial US$12.5 million donation by the Paul G. Allen Family Foundation and Nathan Myhrvold. The first phase of construction was completed and the ATA finally became operational on 11 October 2007 with 42 antennas (ATA-42), after Paul Allen (co-founder of Microsoft) had pledged an additional $13.5 million to support the construction of the first and second phases. Although overall Allen has contributed more than $30 million to the project, it has not succeeded in building the 350 6.1 m (20 ft) dishes originally conceived, and the project suffered an operational hiatus due to funding shortfalls between April and August 2011, after which observations resumed. Subsequently, UC Berkeley exited the project, completing divestment"
  },
  {
    "source": "Allen Telescope Array.txt",
    "chunk_id": "Allen Telescope Array.txt_1",
    "chunk": "in April 2012. The facility is now managed by SRI International (formerly Stanford Research Institute), an independent, nonprofit research institute. As of 2016, the SETI Institute performs observations with the ATA between the hours of 6 pm and 6 am daily. In August 2014, the installation was threatened by a forest fire in the area and was briefly forced to shut down, but ultimately emerged largely unscathed. First conceived by SETI pioneer Frank Drake, the idea has been a dream of the SETI Institute for years. However, it was not until early 2001 that research and development began, after a donation of $11.5 million by the Paul G. Allen Family Foundation. In March 2004, following the successful completion of a three-year research and development phase, the SETI Institute unveiled a three-tier construction plan for the telescope. Construction began immediately, thanks to the pledge of $13.5 million by Paul Allen (co-founder of Microsoft) to support the construction of the first and second phases. The SETI Institute named the telescope in Allen's honor. Overall, Paul Allen contributed more than $30 million to the project. The ATA is a centimeter-wave array which pioneers the Large-Number Small-Diameter concept of building radio telescopes. Compared to"
  },
  {
    "source": "Allen Telescope Array.txt",
    "chunk_id": "Allen Telescope Array.txt_2",
    "chunk": "a large dish antenna, large numbers of smaller dishes are cheaper for the same collecting area. To get similar sensitivity, the signals from all telescopes must be combined. This requires high-performance electronics, which had been prohibitively expensive. Due to the declining cost of electronic components, the required electronics became practicable, resulting in a large cost-saving over telescopes of more conventional design. This is informally referred to as \"replacing steel with silicon\". The ATA has four primary technical capabilities that make it well suited for a range of scientific investigations: a very wide field of view (2.45° at λ = 21 cm, the wavelength of the hydrogen line), complete instantaneous frequency coverage from 0.5 to 11.2 gigahertz (GHz), multiple simultaneous backends, and active interference mitigation. The area of sky which can be instantaneously imaged is 17 times that obtainable by the Very Large Array telescope. The instantaneous frequency coverage of more than four octaves is unprecedented in radio astronomy, and is the result of a unique feed, input amplifier and signal path design. Active interference mitigation will make it possible to observe even at the frequencies of many terrestrial radio emitters. All-sky surveys are an important part of the science program,"
  },
  {
    "source": "Allen Telescope Array.txt",
    "chunk_id": "Allen Telescope Array.txt_3",
    "chunk": "and the ATA will have increased efficiency through its ability to conduct extraterrestrial intelligence searches (SETI) and other radio astronomy observations simultaneously. The telescope can do this by splitting the recorded signals in the control room prior to final processing. Simultaneous observations are possible because for SETI, wherever the telescope is pointed, several target stars will lie within the large field of view afforded by the 6 m dishes. By agreement between the UC Berkeley Radio Astronomy Laboratory (RAL) and the SETI Institute, the needs of conventional radio astronomy determined the pointing of the array up until 2012. The ATA is planned to comprise 350 6 m dishes and will make possible large, deep radio surveys that were not previously feasible. The telescope design incorporates many new features, including hydroformed antenna surfaces, a log-periodic feed covering the entire range of frequencies from 500 megahertz (MHz) to 11.2 GHz, and low-noise, wide-band amplifiers with a flat response over the entire band, thus making it possible to amplify the sky signal directly. This amplified signal, containing the entire received bandwidth, is brought from each antenna to the processing room via optical fiber cables. This means that as electronics improve and wider bandwidths"
  },
  {
    "source": "Allen Telescope Array.txt",
    "chunk_id": "Allen Telescope Array.txt_4",
    "chunk": "are obtainable, only the central processor needs to change, and not the antennas or feeds. The instrument was operated and maintained by RAL until development of the array was put on hold in 2011. RAL worked hand in hand with the SETI Institute during design and prototyping and was the primary designer of the feed, antenna surfaces, beamforming, correlator, and imaging system for radio astronomy observations. The panel for the Astronomy and Astrophysics Decadal Survey in its fifth report, Astronomy and Astrophysics in the New Millennium (2001), endorsed SETI and recognized the ATA (then called the 1-Hectare Telescope) as an important stepping stone towards the building of the Square Kilometer Array telescope (SKA). The most recent Decadal report recommended ending the US's financial support of the SKA, although US participation in SKA precursors such as MeerKAT, the Hydrogen Epoch of Reionization Array and the Murchison Widefield Array. Although cost estimates of unbuilt projects are always dubious, and the specifications are not identical (conventional telescopes have lower noise temperature, but the ATA has a larger field of view, for example), the ATA has potential promise as a much cheaper radio telescope technology for a given effective aperture. For example, the amount"
  },
  {
    "source": "Allen Telescope Array.txt",
    "chunk_id": "Allen Telescope Array.txt_5",
    "chunk": "spent on the first ATA-42 phase, including technology development, is roughly one third of the cost of a new copy of a Deep Space Network 34 m antenna of similar collecting area. Similarly, the estimated total cost of building the remaining 308 dishes was estimated (as of October 2007) at about $41 million. This is about two times cheaper than the $85 million cost of the last large radio astronomy antenna built in the US, the Green Bank Telescope, of similar collecting area. The contractor filed for a $29 million overrun, but only $4 million of this was allowed. The ATA aspires to be among the world's largest and fastest observing instruments, and to permit astronomers to search many different target stars simultaneously. If completed as originally envisioned, it will be one of the largest and most powerful telescopes in the world. Since its inception, the ATA has been a development tool for astronomical interferometer technology (specifically, for the Square Kilometer Array). The ATA was originally planned to be constructed in four stages, ATA-42, ATA-98, ATA-206 and ATA-350, each number representing the number of dishes in the array at a given time. (See Table 1). The ATA is planned to"
  },
  {
    "source": "Allen Telescope Array.txt",
    "chunk_id": "Allen Telescope Array.txt_6",
    "chunk": "comprise 350 dishes with a diameter of 6 m each. Regular operations with 42 dishes started on 11 October 2007. Funding for building additional antennas is currently being sought by the SETI Institute from various sources, including the United States Navy, Defense Advanced Research Projects Agency (DARPA), National Science Foundation (NSF) and private donors. Simultaneous astronomical and SETI observations are performed with two 32-input dual polarization imaging correlators. Numerous articles reporting conventional radio astronomy observations have been published. Three phased array beamformers utilizing the Berkeley Emulation Engine 2 (BEE2) were deployed in June 2007 and have been integrated into the system to allow for simultaneous astronomical and SETI observations. As of April 2008, the first pulsar observations were conducted using the beamformer and a purpose-built pulsar spectrometer. The workhorse SETI search system (SETI on ATA or SonATA) performs fully automated SETI observations. SonATA follows up on detected signals in real time and continues to track them until 1) the signal is shown to have been generated on Earth or rarely, 2) the source sets, which triggers follow up the next day. As of 2016, more than two hundred million signals have been followed up and classified using the ATA. Not"
  },
  {
    "source": "Allen Telescope Array.txt",
    "chunk_id": "Allen Telescope Array.txt_7",
    "chunk": "one of these signals had all the characteristics expected for an ETI signal. The results of SETI Institute's observations are published in a number of papers. In April 2011, the ATA was put into hibernation owing to funding shortfalls, meaning that it was no longer available for use. Operation of the ATA resumed on 5 December 2011. Efforts are now led by Andrew Siemion. In 2012, the ATA was funded by a $3.6 million philanthropic donation by Franklin Antonio, cofounder and Chief Scientist of Qualcomm Incorporated. This gift supports upgrades of all the receivers on the ATA dishes to have dramatically greater sensitivity (2 − 10× from 1–8 GHz) than before and support sensitive observations over a wider frequency range, from 1–15 GHz, when initially the radio frequency electronics went to only 11 GHz. By July 2016, the first ten of these receivers had been installed and proven. Full installation on all 42 antennas is planned as of June 2017. In November 2015, the ATA studied the anomalous star KIC 8462852, and in autumn 2017 the Allen Telescope Array examined the interstellar asteroid 'Oumuamua for signs of technology, but detected no unusual radio emissions. The science goals listed below represent"
  },
  {
    "source": "Allen Telescope Array.txt",
    "chunk_id": "Allen Telescope Array.txt_8",
    "chunk": "the most important projects to be conducted with the ATA. Each of these goals is associated with one of the four stages of development mentioned earlier. (See Table 1). Also listed is some of the science that it is hoped each will produce. Since construction of the array began, a few science goals not specifically drawn up for it have been suggested. For example, the Allen Telescope Array has offered to provide the mooncast data downlink for any contestants in the Google Lunar X Prize. This is practical, since the array, with no modifications, covers the main space communications bands (S-band and X-band). A telemetry decoder would be the only needed addition. Also, the ATA was mentioned as a candidate for searching for a new type of radio transient. It is an excellent choice for this owing to its large field of view and wide instantaneous bandwidth. Following this suggestion, Andrew Siemion and an international team of astronomers and engineers developed an instrument called \"Fly's Eye\" that allowed the ATA to search for bright radio transients, and observations were carried out between February and April 2008. The ATA-42 configuration will provide a maximum baseline of 300 m (and ultimately for"
  },
  {
    "source": "Allen Telescope Array.txt",
    "chunk_id": "Allen Telescope Array.txt_9",
    "chunk": "the ATA-350, 900 m). A cooled log-periodic feed on each antenna is designed to provide a system temperature of ~45K from 1–10 GHz, with reduced sensitivity in the ranges of 0.5–1.0 GHz and 10–11.2 GHz. Four separate frequency tunings (IFs) are available to produce 4 x 672 MHz intermediate frequency bands. Two IFs support correlators for imaging; two will support SETI observing. All tunings can produce four dual polarization phased array beams which can be independently pointed within the primary beam and can be used with a variety of detectors. The ATA can therefore synthesize up to 32 phased array beams. The wide field of view of the ATA gives it an unparalleled capability for large surveys (Fig. 4). The time required for mapping a large area to a given sensitivity is proportional to (ND), where N is the number of elements and D is the diameter of the dish. This leads to the surprising result that a large array of small dishes can outperform an array with a smaller number of elements but considerably greater collecting area in the task of large surveys. As a consequence, even the ATA-42 is competitive with much larger telescopes in its capability for"
  },
  {
    "source": "Allen Telescope Array.txt",
    "chunk_id": "Allen Telescope Array.txt_10",
    "chunk": "both brightness temperature and point source surveys. For point source surveys, the ATA-42 is comparable in speed to Arecibo and the Green Bank Telescope (GBT), but three times slower than the Very Large Array (VLA). The ATA-350, on the other hand, will be one order of magnitude faster than the Very Large Array for point source surveys, and is comparable to the Expanded Very Large Array (EVLA) in survey speed. For surveys up to a specified brightness temperature sensitivity, the ATA-98 will exceed the survey speed of even the VLA-D configuration. The ATA-206 should match the brightness temperature sensitivity of Arecibo and the GBT. The ATA, however, provides better resolution than either of these single-dish telescopes. The antennas for the ATA are 6.1 x 7.0 meters (20.0 ft x 23.0 ft) hydroformed offset Gregorian telescopes, each with a 2.4 meter sub-reflector with an effective focal length/diameter (f/D) ratio of 0.65. (See DeBoer, 2001). The offset geometry eliminates blockage, which increases efficiency and decreases the side lobes. It also allows for the large sub-reflector, providing good low frequency performance. The hydroforming technology used to make these surfaces is the same as that used by Andersen Manufacturing of Idaho Falls, Idaho to"
  },
  {
    "source": "Allen Telescope Array.txt",
    "chunk_id": "Allen Telescope Array.txt_11",
    "chunk": "generate low-cost satellite reflectors. The unique interior frame rim-supported compact mount allows excellent performance at low cost. The drive system employs a spring-loaded passive anti-backlash azimuth drive train. Most components designed by Matthew Fleming and manufactured at Minex Engineering Corp. in Antioch, CA. As with other arrays, the huge amount of incoming sensory information requires real-time array processing capability in order to reduce data volume for storage. For ATA-256, the average data rates and total data volume for the correlator are estimated to be 100 Mbyte/s and 15 Pbytes for the five-year survey period. Experiments such as transient surveys will exceed this rate significantly. The beamformers produce data at a much higher rate (8 gigabytes per second (Gb/s)) but only a very small fraction of this data is archived. In 2009, the signal detection hardware and software was called Prelude, which was composed of rack-mounted PCs augmented by two custom accelerator cards based on digital signal processing (DSP) and field-programmable gate array (FPGA) chips. Each Programmable Detection Module (one of 28 PCs) can analyze 2 MHz of dual-polarization input data to generate spectra with spectral resolution of 0.7 Hz and time samples of 1.4 seconds. In 2009, the array had"
  },
  {
    "source": "Allen Telescope Array.txt",
    "chunk_id": "Allen Telescope Array.txt_12",
    "chunk": "a 40 Mbit/s internet connection, adequate for remote access and transferring of data products for ATA-256. An upgrade to 40 Gbit/s was planned, which would enable direct distribution of raw data for offsite computing. Like other array systems the ATA has a computational complexity and cross-connect which scales as O(N) with the number of antennas N {\\displaystyle N} . The computation requirement, for example, for correlating the full ATA bandwidth ( B {\\displaystyle B} = 11 GHz) for the proposed N {\\displaystyle N} = 350 dual-polarization antenna build-out, using an efficient frequency-multiply (FX) architecture and a modest 500 kHz channel width (with number of channels F {\\displaystyle F} = 2200), is given by: 2 B ⟨ N log 2 ⁡ ( F ) ( 10 O P s ) + ( N N + 1 2 ) × 4 ( 8 O P s ) ⟩ {\\displaystyle 2B\\langle N\\log _{2}(F)(10OPs)+(N{\\frac {N+1}{2}})\\times 4(8OPs)\\rangle } = 44 Peta-OPs per second where O p s {\\displaystyle Ops} is an operation. Note that since each dish has a dual polarization antenna, each signal sample is actually a two data set, hence 2 B {\\displaystyle 2B} ."
  },
  {
    "source": "Ancient Greek.txt",
    "chunk_id": "Ancient Greek.txt_0",
    "chunk": "# Ancient Greek Ancient Greek (Ἑλληνῐκή, Hellēnikḗ; [hellɛːnikɛ́ː]) includes the forms of the Greek language used in ancient Greece and the ancient world from around 1500 BC to 300 BC. It is often roughly divided into the following periods: Mycenaean Greek (c. 1400–1200 BC), Dark Ages (c. 1200–800 BC), the Archaic or Homeric period (c. 800–500 BC), and the Classical period (c. 500–300 BC). Ancient Greek was the language of Homer and of fifth-century Athenian historians, playwrights, and philosophers. It has contributed many words to English vocabulary and has been a standard subject of study in educational institutions of the Western world since the Renaissance. This article primarily contains information about the Epic and Classical periods of the language, which are the best-attested periods and considered most typical of Ancient Greek. From the Hellenistic period (c. 300 BC), Ancient Greek was followed by Koine Greek, which is regarded as a separate historical stage, though its earliest form closely resembles Attic Greek, and its latest form approaches Medieval Greek, and Koine may be classified as Ancient Greek in a wider sense. There were several regional dialects of Ancient Greek; Attic Greek developed into Koine. Ancient Greek was a pluricentric language, divided"
  },
  {
    "source": "Ancient Greek.txt",
    "chunk_id": "Ancient Greek.txt_1",
    "chunk": "into many dialects. The main dialect groups are Attic and Ionic, Aeolic, Arcadocypriot, and Doric, many of them with several subdivisions. Some dialects are found in standardized literary forms in literature, while others are attested only in inscriptions. There are also several historical forms. Homeric Greek is a literary form of Archaic Greek (derived primarily from Ionic and Aeolic) used in the epic poems, the Iliad and the Odyssey, and in later poems by other authors. Homeric Greek had significant differences in grammar and pronunciation from Classical Attic and other Classical-era dialects. The origins, early form and development of the Hellenic language family are not well understood because of a lack of contemporaneous evidence. Several theories exist about what Hellenic dialect groups may have existed between the divergence of early Greek-like speech from the common Proto-Indo-European language and the Classical period. They have the same general outline but differ in some of the detail. The only attested dialect from this period is Mycenaean Greek, but its relationship to the historical dialects and the historical circumstances of the times imply that the overall groups already existed in some form. Scholars assume that major Ancient Greek period dialect groups developed not later"
  },
  {
    "source": "Ancient Greek.txt",
    "chunk_id": "Ancient Greek.txt_2",
    "chunk": "than 1120 BC, at the time of the Dorian invasions—and that their first appearances as precise alphabetic writing began in the 8th century BC. The invasion would not be \"Dorian\" unless the invaders had some cultural relationship to the historical Dorians. The invasion is known to have displaced population to the later Attic-Ionic regions, who regarded themselves as descendants of the population displaced by or contending with the Dorians. The Greeks of this period believed there were three major divisions of all Greek people – Dorians, Aeolians, and Ionians (including Athenians), each with their own defining and distinctive dialects. Allowing for their oversight of Arcadian, an obscure mountain dialect, and Cypriot, far from the center of Greek scholarship, this division of people and language is quite similar to the results of modern archaeological-linguistic investigation. West vs. non-West Greek is the strongest-marked and earliest division, with non-West in subsets of Ionic-Attic (or Attic-Ionic) and Aeolic vs. Arcadocypriot, or Aeolic and Arcado-Cypriot vs. Ionic-Attic. Often non-West is called 'East Greek'. Arcadocypriot apparently descended more closely from the Mycenaean Greek of the Bronze Age. Boeotian Greek had come under a strong Northwest Greek influence, and can in some respects be considered a transitional"
  },
  {
    "source": "Ancient Greek.txt",
    "chunk_id": "Ancient Greek.txt_3",
    "chunk": "dialect, as exemplified in the poems of the Boeotian poet Pindar who wrote in Doric with a small Aeolic admixture. Thessalian likewise had come under Northwest Greek influence, though to a lesser degree. Pamphylian Greek, spoken in a small area on the southwestern coast of Anatolia and little preserved in inscriptions, may be either a fifth major dialect group, or it is Mycenaean Greek overlaid by Doric, with a non-Greek native influence. Regarding the speech of the ancient Macedonians diverse theories have been put forward, but the epigraphic activity and the archaeological discoveries in the Greek region of Macedonia during the last decades has brought to light documents, among which the first texts written in Macedonian, such as the Pella curse tablet, as Hatzopoulos and other scholars note. Based on the conclusions drawn by several studies and findings such as Pella curse tablet, Emilio Crespo and other scholars suggest that ancient Macedonian was a Northwest Doric dialect, which shares isoglosses with its neighboring Thessalian dialects spoken in northeastern Thessaly. Some have also suggested an Aeolic Greek classification. The Lesbian dialect was Aeolic. For example, fragments of the works of the poet Sappho from the island of Lesbos are in Aeolian."
  },
  {
    "source": "Ancient Greek.txt",
    "chunk_id": "Ancient Greek.txt_4",
    "chunk": "Most of the dialect sub-groups listed above had further subdivisions, generally equivalent to a city-state and its surrounding territory, or to an island. Doric notably had several intermediate divisions as well, into Island Doric (including Cretan Doric), Southern Peloponnesus Doric (including Laconian, the dialect of Sparta), and Northern Peloponnesus Doric (including Corinthian). All the groups were represented by colonies beyond Greece proper as well, and these colonies generally developed local characteristics, often under the influence of settlers or neighbors speaking different Greek dialects. After the conquests of Alexander the Great in the late 4th century BC, a new international dialect known as Koine or Common Greek developed, largely based on Attic Greek, but with influence from other dialects. This dialect slowly replaced most of the older dialects, although the Doric dialect has survived in the Tsakonian language, which is spoken in the region of modern Sparta. Doric has also passed down its aorist terminations into most verbs of Demotic Greek. By about the 6th century AD, the Koine had slowly metamorphosed into Medieval Greek. Phrygian is an extinct Indo-European language of West and Central Anatolia, which is considered by some linguists to have been closely related to Greek. Among Indo-European"
  },
  {
    "source": "Ancient Greek.txt",
    "chunk_id": "Ancient Greek.txt_5",
    "chunk": "branches with living descendants, Greek is often argued to have the closest genetic ties with Armenian (see also Graeco-Armenian) and Indo-Iranian languages (see Graeco-Aryan). Ancient Greek differs from Proto-Indo-European (PIE) and other Indo-European languages in certain ways. In phonotactics, ancient Greek words could end only in a vowel or /n s r/; final stops were lost, as in γάλα \"milk\", compared with γάλακτος \"of milk\" (genitive). Ancient Greek of the classical period also differed in both the inventory and distribution of original PIE phonemes due to numerous sound changes, notably the following: The pronunciation of Ancient Greek was very different from that of Modern Greek. Ancient Greek had long and short vowels; many diphthongs; double and single consonants; voiced, voiceless, and aspirated stops; and a pitch accent. In Modern Greek, all vowels and consonants are short. Many vowels and diphthongs once pronounced distinctly are pronounced as /i/ (iotacism). Some of the stops and glides in diphthongs have become fricatives, and the pitch accent has changed to a stress accent. Many of the changes took place in the Koine Greek period. The writing system of Modern Greek, however, does not reflect all pronunciation changes. The examples below represent Attic Greek in"
  },
  {
    "source": "Ancient Greek.txt",
    "chunk_id": "Ancient Greek.txt_6",
    "chunk": "the 5th century BC. Ancient pronunciation cannot be reconstructed with certainty, but Greek from the period is well documented, and there is little disagreement among linguists as to the general nature of the sounds that the letters represent. Greek, like all of the older Indo-European languages, is highly inflected. It is highly archaic in its preservation of Proto-Indo-European forms. In ancient Greek, nouns (including proper nouns) have five cases (nominative, genitive, dative, accusative, and vocative), three genders (masculine, feminine, and neuter), and three numbers (singular, dual, and plural). Verbs have four moods (indicative, imperative, subjunctive, and optative) and three voices (active, middle, and passive), as well as three persons (first, second, and third) and various other forms. Verbs are conjugated through seven combinations of tenses and aspect (generally simply called \"tenses\"): the present, future, and imperfect are imperfective in aspect; the aorist, present perfect, pluperfect and future perfect are perfective in aspect. Most tenses display all four moods and three voices, although there is no future subjunctive or imperative. Also, there is no imperfect subjunctive, optative or imperative. The infinitives and participles correspond to the finite combinations of tense, aspect, and voice. The indicative of past tenses adds (conceptually, at"
  },
  {
    "source": "Ancient Greek.txt",
    "chunk_id": "Ancient Greek.txt_7",
    "chunk": "least) a prefix /e-/, called the augment. This was probably originally a separate word, meaning something like \"then\", added because tenses in PIE had primarily aspectual meaning. The augment is added to the indicative of the aorist, imperfect, and pluperfect, but not to any of the other forms of the aorist (no other forms of the imperfect and pluperfect exist). The two kinds of augment in Greek are syllabic and quantitative. The syllabic augment is added to stems beginning with consonants, and simply prefixes e (stems beginning with r, however, add er). The quantitative augment is added to stems beginning with vowels, and involves lengthening the vowel: Some verbs augment irregularly; the most common variation is e → ei. The irregularity can be explained diachronically by the loss of s between vowels, or that of the letter w, which affected the augment when it was word-initial. In verbs with a preposition as a prefix, the augment is placed not at the start of the word, but between the preposition and the original verb. For example, προσ(-)βάλλω (I attack) goes to προσέβαλoν in the aorist. However compound verbs consisting of a prefix that is not a preposition retain the augment at"
  },
  {
    "source": "Ancient Greek.txt",
    "chunk_id": "Ancient Greek.txt_8",
    "chunk": "the start of the word: αὐτο(-)μολῶ goes to ηὐτομόλησα in the aorist. Following Homer's practice, the augment is sometimes not made in poetry, especially epic poetry. Almost all forms of the perfect, pluperfect, and future perfect reduplicate the initial syllable of the verb stem. (A few irregular forms of perfect do not reduplicate, whereas a handful of irregular aorists reduplicate.) The three types of reduplication are: Irregular duplication can be understood diachronically. For example, lambanō (root lab) has the perfect stem eilēpha (not *lelēpha) because it was originally slambanō, with perfect seslēpha, becoming eilēpha through compensatory lengthening. Reduplication is also visible in the present tense stems of certain verbs. These stems add a syllable consisting of the root's initial consonant followed by i. A nasal stop appears after the reduplication in some verbs. The earliest extant examples of ancient Greek writing (c. 1450 BC) are in the syllabic script Linear B. Beginning in the 8th century BC, however, the Greek alphabet became standard, albeit with some variation among dialects. Early texts are written in boustrophedon style, but left-to-right became standard during the classic period. Modern editions of ancient Greek texts are usually written with accents and breathing marks, interword spacing,"
  },
  {
    "source": "Ancient Greek.txt",
    "chunk_id": "Ancient Greek.txt_9",
    "chunk": "modern punctuation, and sometimes mixed case, but these were all introduced later. The beginning of Homer's Iliad exemplifies the Archaic period of ancient Greek (see Homeric Greek for more details): Μῆνιν ἄειδε, θεά, Πηληϊάδεω Ἀχιλῆος οὐλομένην, ἣ μυρί' Ἀχαιοῖς ἄλγε' ἔθηκε, πολλὰς δ' ἰφθίμους ψυχὰς Ἄϊδι προΐαψεν ἡρώων, αὐτοὺς δὲ ἑλώρια τεῦχε κύνεσσιν οἰωνοῖσί τε πᾶσι· Διὸς δ' ἐτελείετο βουλή· ἐξ οὗ δὴ τὰ πρῶτα διαστήτην ἐρίσαντε Ἀτρεΐδης τε ἄναξ ἀνδρῶν καὶ δῖος Ἀχιλλεύς. The beginning of Apology by Plato exemplifies Attic Greek from the Classical period of ancient Greek. (The second line is the IPA, the third is transliterated into the Latin alphabet using a modern version of the Erasmian scheme.) Ὅτι μὲν ὑμεῖς, {} ὦ ἄνδρες Ἀθηναῖοι, {} πεπόνθατε {} ὑπὸ τῶν ἐμῶν κατηγόρων, {} οὐκ οἶδα· {} ἐγὼ {δ' οὖν} καὶ αὐτὸς {} ὑπ' αὐτῶν ὀλίγου ἐμαυτοῦ {} ἐπελαθόμην, {} οὕτω πιθανῶς ἔλεγον. {} Καίτοι ἀληθές γε {} ὡς ἔπος εἰπεῖν {} οὐδὲν εἰρήκασιν. {} [hóti men hyːmêːs | ɔ̂ː ándres atʰɛːnaî̯i̯oi | pepóntʰate | hypo tɔ̂ːn emɔ̂ːŋ katɛːɡórɔːn | oːk oî̯da ‖ eɡɔ́ː dûːŋ kai̯ au̯tos | hyp au̯tɔ̂ːn olíɡoː emau̯tûː | epelatʰómɛːn | hǔːtɔː pitʰanɔ̂ːs éleɡon ‖ kaí̯toi̯ alɛːtʰéz ɡe | hɔːs épos eːpêːn |"
  },
  {
    "source": "Ancient Greek.txt",
    "chunk_id": "Ancient Greek.txt_10",
    "chunk": "oːden eːrɛ̌ːkaːsin ‖] Hóti mèn hūmeîs, {} ô ándres Athēnaîoi, {} pepónthate {} hupò tôn emôn katēgórōn, {} ouk oîda: {} egṑ {d' oûn} kaì autòs {} hup' autōn olígou emautoû {} epelathómēn, {} hoútō pithanôs élegon. {} Kaítoi alēthés ge {} hōs épos eipeîn {} oudèn eirḗkāsin. {} How you, men of Athens, are feeling under the power of my accusers, I do not know: actually, even I myself almost forgot who I was because of them, they spoke so persuasively. And yet, loosely speaking, nothing they have said is true. The study of Ancient Greek in European countries in addition to Latin occupied an important place in the syllabus from the Renaissance until the beginning of the 20th century. This was true as well in the United States, where many of the nation's Founders received a classically based education. Latin was emphasized in American colleges, but Greek also was required in the Colonial and Early National eras, and the study of ancient Greece became increasingly popular in the mid-to-late Nineteenth Century, the age of American philhellenism. In particular, female intellectuals of the era designated the mastering of ancient Greek as essential in becoming a \"woman of letters.\" Ancient"
  },
  {
    "source": "Ancient Greek.txt",
    "chunk_id": "Ancient Greek.txt_11",
    "chunk": "Greek is still taught as a compulsory or optional subject especially at traditional or elite schools throughout Europe, such as public schools and grammar schools in the United Kingdom. It is compulsory in the liceo classico in Italy, in the gymnasium in the Netherlands, in some classes in Austria, in klasična gimnazija (grammar school – orientation: classical languages) in Croatia, in classical studies in ASO in Belgium and it is optional in the humanities-oriented gymnasium in Germany, usually as a third language after Latin and English, from the age of 14 to 18. In 2006/07, 15,000 pupils studied ancient Greek in Germany according to the Federal Statistical Office of Germany, and 280,000 pupils studied it in Italy. It is a compulsory subject alongside Latin in the humanities branch of the Spanish Baccalaureate. Ancient Greek is taught at most major universities worldwide, often combined with Latin as part of the study of classics. In 2010 it was offered in three primary schools in the UK, to boost children's language skills, and was one of seven foreign languages which primary schools could teach 2014 as part of a major drive to boost education standards. Ancient Greek is taught as a compulsory subject"
  },
  {
    "source": "Ancient Greek.txt",
    "chunk_id": "Ancient Greek.txt_12",
    "chunk": "in all gymnasiums and lyceums in Greece. Starting in 2001, an annual international competition \"Exploring the Ancient Greek Language and Culture\" (Greek: Διαγωνισμός στην Αρχαία Ελληνική Γλώσσα και Γραμματεία) was run for upper secondary students through the Greek Ministry of National Education and Religious Affairs, with Greek language and cultural organisations as co-organisers. It appears to have ceased in 2010, having failed to gain the recognition and acceptance of teachers. Modern authors rarely write in ancient Greek, though Jan Křesadlo wrote some poetry and prose in the language, and Harry Potter and the Philosopher's Stone, some volumes of Asterix, and The Adventures of Alix have been translated into ancient Greek. Ὀνόματα Kεχιασμένα (Onomata Kechiasmena) is the first magazine of crosswords and puzzles in ancient Greek. Its first issue appeared in April 2015 as an annex to Hebdomada Aenigmatum. Alfred Rahlfs included a preface, a short history of the Septuagint text, and other front matter translated into ancient Greek in his 1935 edition of the Septuagint; Robert Hanhart also included the introductory remarks to the 2006 revised Rahlfs–Hanhart edition in the language as well. Akropolis World News reports weekly a summary of the most important news in ancient Greek. Ancient Greek"
  },
  {
    "source": "Ancient Greek.txt",
    "chunk_id": "Ancient Greek.txt_13",
    "chunk": "is also used by organizations and individuals, mainly Greek, who wish to denote their respect, admiration or preference for the use of this language. This use is sometimes considered graphical, nationalistic or humorous. In any case, the fact that modern Greeks can still wholly or partly understand texts written in non-archaic forms of ancient Greek shows the affinity of the modern Greek language to its ancestral predecessor. Ancient Greek is often used in the coinage of modern technical terms in the European languages: see English words of Greek origin. Latinized forms of ancient Greek roots are used in many of the scientific names of species and in scientific terminology."
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_0",
    "chunk": "# Antenna (radio) In radio-frequency engineering, an antenna (American English) or aerial (British English) is an electronic device that converts an alternating electric current into radio waves (transmitting), or radio waves into an electric current (receiving). It is the interface between radio waves propagating through space and electric currents moving in metal conductors, used with a transmitter or receiver. In transmission, a radio transmitter supplies an electric current to the antenna's terminals, and the antenna radiates the energy from the current as electromagnetic waves (radio waves). In reception, an antenna intercepts some of the power of a radio wave in order to produce an electric current at its terminals, that is applied to a receiver to be amplified. Antennas are essential components of all radio equipment. An antenna is an array of conductor segments (elements), electrically connected to the receiver or transmitter. Antennas can be designed to transmit and receive radio waves in all horizontal directions equally (omnidirectional antennas), or preferentially in a particular direction (directional, or high-gain, or \"beam\" antennas). An antenna may include components not connected to the transmitter, parabolic reflectors, horns, or parasitic elements, which serve to direct the radio waves into a beam or other desired"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_1",
    "chunk": "radiation pattern. Strong directivity and good efficiency when transmitting are hard to achieve with antennas with dimensions that are much smaller than a half wavelength. The first antennas were built in 1886 by German physicist Heinrich Hertz in his pioneering experiments to prove the existence of electromagnetic waves predicted by the 1867 electromagnetic theory of James Clerk Maxwell. Hertz placed dipole antennas at the focal point of parabolic reflectors for both transmitting and receiving. Starting in 1895, Guglielmo Marconi began development of antennas practical for long-distance wireless telegraphy and opened a factory in Chelmsford, England, to manufacture his invention in 1898. The words antenna and aerial are used interchangeably. Occasionally the equivalent term \"aerial\" is used to specifically mean an elevated horizontal wire antenna. The origin of the word antenna relative to wireless apparatus is attributed to Italian radio pioneer Guglielmo Marconi. In the summer of 1895, Marconi began testing his wireless system outdoors on his father's estate near Bologna and soon began to experiment with long wire \"aerials\" suspended from a pole. In Italian a tent pole is known as l'antenna centrale, and the pole with the wire was simply called l'antenna. Until then wireless radiating transmitting and receiving"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_2",
    "chunk": "elements were known simply as \"terminals\". Because of his prominence, Marconi's use of the word antenna spread among wireless researchers and enthusiasts, and later to the general public. Antenna may refer broadly to an entire assembly including support structure, enclosure (if any), etc., in addition to the actual RF current-carrying components. A receiving antenna may include not only the passive metal receiving elements, but also an integrated preamplifier or mixer, especially at and above microwave frequencies. Antennas are required by any radio receiver or transmitter to couple its electrical connection to the electromagnetic field. Radio waves are electromagnetic waves which carry signals through space at the speed of light with almost no transmission loss. Antennas can be classified as omnidirectional, radiating energy approximately equally in all horizontal directions, or directional, where radio waves are concentrated in some direction(s). A so-called beam antenna is unidirectional, designed for maximum response in the direction of the other station, whereas many other antennas are intended to accommodate stations in various directions but are not truly omnidirectional. Since antennas obey reciprocity the same radiation pattern applies to transmission as well as reception of radio waves. A hypothetical antenna that radiates equally in all directions (vertical"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_3",
    "chunk": "as well as all horizontal angles) is called an isotropic radiator; however, these cannot exist in practice nor would they be particularly desired. For most terrestrial communications, rather, there is an advantage in reducing radiation toward the sky or ground in favor of horizontal direction(s). A dipole antenna oriented horizontally sends no energy in the direction of the conductor – this is called the antenna null – but is usable in most other directions. A number of such dipole elements can be combined into an antenna array such as the Yagi–Uda in order to favor a single horizontal direction, thus termed a beam antenna. The dipole antenna, which is the basis for most antenna designs, is a balanced component, with equal but opposite voltages and currents applied at its two terminals. The vertical antenna is a monopole antenna, not balanced with respect to ground. The ground (or any large conductive surface) plays the role of the second conductor of a monopole. Since monopole antennas rely on a conductive surface, they may be mounted with a ground plane to approximate the effect of being mounted on the Earth's surface. More complex antennas increase the directivity of the antenna. Additional elements in"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_4",
    "chunk": "the antenna structure, which need not be directly connected to the receiver or transmitter, increase its directionality. Antenna \"gain\" describes the concentration of radiated power into a particular solid angle of space. \"Gain\" is perhaps an unfortunately chosen term, by comparison with amplifier \"gain\" which implies a net increase in power. In contrast, for antenna \"gain\", the power increased in the desired direction is at the expense of power reduced in undesired directions. Unlike amplifiers, antennas are electrically \"passive\" devices which conserve total power, and there is no increase in total power above that delivered from the power source (the transmitter), only improved distribution of that fixed total. A phased array consists of two or more simple antennas which are connected together through an electrical network. This often involves a number of parallel dipole antennas with a certain spacing. Depending on the relative phase introduced by the network, the same combination of dipole antennas can operate as a \"broadside array\" (directional normal to a line connecting the elements) or as an \"end-fire array\" (directional along the line connecting the elements). Antenna arrays may employ any basic (omnidirectional or weakly directional) antenna type, such as dipole, loop or slot antennas. These"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_5",
    "chunk": "elements are often identical. Log-periodic and frequency-independent antennas employ self-similarity in order to be operational over a wide range of bandwidths. The most familiar example is the log-periodic dipole array which can be seen as a number (typically 10 to 20) of connected dipole elements with progressive lengths in an endfire array making it rather directional; it finds use especially as a rooftop antenna for television reception. On the other hand, a Yagi–Uda antenna (or simply \"Yagi\"), with a somewhat similar appearance, has only one dipole element with an electrical connection; the other parasitic elements interact with the electromagnetic field in order to realize a highly directional antenna but with a narrow bandwidth. Even greater directionality can be obtained using aperture antennas such as the parabolic reflector or horn antenna. Since high directivity in an antenna depends on it being large compared to the wavelength, highly directional antennas (thus with high antenna gain) become more practical at higher frequencies (UHF and above). At low frequencies (such as AM broadcast), arrays of vertical towers are used to achieve directionality and they will occupy large areas of land. For reception, a long Beverage antenna can have significant directivity. For non directional portable"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_6",
    "chunk": "use, a short vertical antenna or small loop antenna works well, with the main design challenge being that of impedance matching. With a vertical antenna a loading coil at the base of the antenna may be employed to cancel the reactive component of impedance; small loop antennas are tuned with parallel capacitors for this purpose. An antenna lead-in is the transmission line, or feed line, which connects the antenna to a transmitter or receiver. The \"antenna feed\" may refer to all components connecting the antenna to the transmitter or receiver, such as an impedance matching network in addition to the transmission line. In a so-called \"aperture antenna\", such as a horn or parabolic dish, the \"feed\" may also refer to a basic radiating antenna embedded in the entire system of reflecting elements (normally at the focus of the parabolic dish or at the throat of a horn) which could be considered the one active element in that antenna system. A microwave antenna may also be fed directly from a waveguide in place of a (conductive) transmission line. An antenna counterpoise, or ground plane, is a structure of conductive material which improves or substitutes for the ground. It may be connected"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_7",
    "chunk": "to or insulated from the natural ground. In a monopole antenna, this aids in the function of the natural ground, particularly where variations (or limitations) of the characteristics of the natural ground interfere with its proper function. Such a structure is normally connected to the return connection of an unbalanced transmission line such as the shield of a coaxial cable. An electromagnetic wave refractor in some aperture antennas is a component which due to its shape and position functions to selectively delay or advance portions of the electromagnetic wavefront passing through it. The refractor alters the spatial characteristics of the wave on one side relative to the other side. It can, for instance, bring the wave to a focus or alter the wave front in other ways, generally in order to maximize the directivity of the antenna system. This is the radio equivalent of an optical lens. An antenna coupling network is a passive network (generally a combination of inductive and capacitive circuit elements) used for impedance matching in between the antenna and the transmitter or receiver. This may be used to minimize losses on the feed line, by reducing transmission line's standing wave ratio, and to present the transmitter"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_8",
    "chunk": "or receiver with a standard resistive impedance needed for its optimum operation. The feed point location(s) is selected, and antenna elements electrically similar to tuner components may be incorporated in the antenna structure itself, to improve the match. It is a fundamental property of antennas that most of the electrical characteristics of an antenna, such as those described in the next section (e.g. gain, radiation pattern, impedance, bandwidth, resonant frequency and polarization), are the same whether the antenna is transmitting or receiving. For example, the \"receiving pattern\" (sensitivity to incoming signals as a function of direction) of an antenna when used for reception is identical to the radiation pattern of the antenna when it is driven and functions as a radiator, even though the current and voltage distributions on the antenna itself are different for receiving and sending. This is a consequence of the reciprocity theorem of electromagnetics. Therefore, in discussions of antenna properties no distinction is usually made between receiving and transmitting terminology, and the antenna can be viewed as either transmitting or receiving, whichever is more convenient. A necessary condition for the aforementioned reciprocity property is that the materials in the antenna and transmission medium are linear and"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_9",
    "chunk": "reciprocal. Reciprocal (or bilateral) means that the material has the same response to an electric current or magnetic field in one direction, as it has to the field or current in the opposite direction. Most materials used in antennas meet these conditions, but some microwave antennas use high-tech components such as isolators and circulators, made of nonreciprocal materials such as ferrite. These can be used to give the antenna a different behavior on receiving than it has on transmitting, which can be useful in applications like radar. The majority of antenna designs are based on the resonance principle. This relies on the behaviour of moving electrons, which reflect off surfaces where the dielectric constant changes, in a fashion similar to the way light reflects when optical properties change. In these designs, the reflective surface is created by the end of a conductor, normally a thin metal wire or rod, which in the simplest case has a feed point at one end where it is connected to a transmission line. The conductor, or element, is aligned with the electrical field of the desired signal, normally meaning it is perpendicular to the line from the antenna to the source (or receiver in"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_10",
    "chunk": "the case of a broadcast antenna). The radio signal's electric component induces a voltage in the conductor. This causes an electrical current to begin flowing in the direction of the signal's instantaneous field. When the resulting current reaches the end of the conductor, it reflects, which is equivalent to a 180 degree change in phase. If the conductor is ⁠ 1 /4⁠ of a wavelength long, current from the feed point will undergo 90 degree phase change by the time it reaches the end of the conductor, reflect through 180 degrees, and then another 90 degrees as it travels back. That means it has undergone a total 360 degree phase change, returning it to the original signal. The current in the element thus adds to the current being created from the source at that instant. This process creates a standing wave in the conductor, with the maximum current at the feed. The ordinary half-wave dipole is probably the most widely used antenna design. This consists of two ⁠ 1 /4⁠ wavelength elements arranged end-to-end, and lying along essentially the same axis (or collinear), each feeding one side of a two-conductor transmission wire. The physical arrangement of the two elements places"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_11",
    "chunk": "them 180 degrees out of phase, which means that at any given instant one of the elements is driving current into the transmission line while the other is pulling it out. The monopole antenna is essentially one half of the half-wave dipole, a single ⁠ 1 /4⁠ wavelength element with the other side connected to ground or an equivalent ground plane (or counterpoise). Monopoles, which are one-half the size of a dipole, are common for long-wavelength radio signals where a dipole would be impractically large. Another common design is the folded dipole which consists of two (or more) half-wave dipoles placed side by side and connected at their ends but only one of which is driven. The standing wave forms with this desired pattern at the design operating frequency, fo, and antennas are normally designed to be this size. However, feeding that element with 3 fo (whose wavelength is ⁠ 1 /3⁠ that of fo) will also lead to a standing wave pattern. Thus, an antenna element is also resonant when its length is ⁠ 3 /4⁠ of a wavelength. This is true for all odd multiples of ⁠ 1 /4⁠ wavelength. This allows some flexibility of design in terms"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_12",
    "chunk": "of antenna lengths and feed points. Antennas used in such a fashion are known to be harmonically operated. Resonant antennas usually use a linear conductor (or element), or pair of such elements, each of which is about a quarter of the wavelength in length (an odd multiple of quarter wavelengths will also be resonant). Antennas that are required to be small compared to the wavelength sacrifice efficiency and cannot be very directional. Since wavelengths are so small at higher frequencies (UHF, microwaves) trading off performance to obtain a smaller physical size is usually not required. The quarter-wave elements imitate a series-resonant electrical element due to the standing wave present along the conductor. At the resonant frequency, the standing wave has a current peak and voltage node (minimum) at the feed. In electrical terms, this means that at that position, the element has minimum impedance magnitude, generating the maximum current for minimum voltage. This is the ideal situation, because it produces the maximum output for the minimum input, producing the highest possible efficiency. Contrary to an ideal (lossless) series-resonant circuit, a finite resistance remains (corresponding to the relatively small voltage at the feed-point) due to the antenna's resistance to radiating, as"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_13",
    "chunk": "well as any conventional electrical losses from producing heat. Recall that a current will reflect when there are changes in the electrical properties of the material. In order to efficiently transfer the received signal into the transmission line, it is important that the transmission line has the same impedance as its connection point on the antenna, otherwise some of the signal will be reflected backwards into the body of the antenna; likewise part of the transmitter's signal power will be reflected back to transmitter, if there is a change in electrical impedance where the feedline joins the antenna. This leads to the concept of impedance matching, the design of the overall system of antenna and transmission line so the impedance is as close as possible, thereby reducing these losses. Impedance matching is accomplished by a circuit called an antenna tuner or impedance matching network between the transmitter and antenna. The impedance match between the feedline and antenna is measured by a parameter called the standing wave ratio (SWR) on the feedline. Consider a half-wave dipole designed to work with signals with wavelength 1 m, meaning the antenna would be approximately 50 cm from tip to tip. If the element has"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_14",
    "chunk": "a length-to-diameter ratio of 1000, it will have an inherent impedance of about 63 ohms resistive. Using the appropriate transmission wire or balun, we match that resistance to ensure minimum signal reflection. Feeding that antenna with a current of 1 Ampere will require 63 Volts, and the antenna will radiate 63 Watts (ignoring losses) of radio frequency power. Now consider the case when the antenna is fed a signal with a wavelength of 1.25 m; in this case the current induced by the signal would arrive at the antenna's feedpoint out-of-phase with the signal, causing the net current to drop while the voltage remains the same. Electrically this appears to be a very high impedance. The antenna and transmission line no longer have the same impedance, and the signal will be reflected back into the antenna, reducing output. This could be addressed by changing the matching system between the antenna and transmission line, but that solution only works well at the new design frequency. The result is that the resonant antenna will efficiently feed a signal into the transmission line only when the source signal's frequency is close to that of the design frequency of the antenna, or one of"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_15",
    "chunk": "the resonant multiples. This makes resonant antenna designs inherently narrow-band: Only useful for a small range of frequencies centered around the resonance(s). It is possible to use simple impedance matching techniques to allow the use of monopole or dipole antennas substantially shorter than the ⁠ 1 /4⁠ or ⁠ 1 /2⁠ wave, respectively, at which they are resonant. As these antennas are made shorter (for a given frequency) their impedance becomes dominated by a series capacitive (negative) reactance; by adding an appropriate size \"loading coil\" – a series inductance with equal and opposite (positive) reactance – the antenna's capacitive reactance may be cancelled leaving only a pure resistance. Sometimes the resulting (lower) electrical resonant frequency of such a system (antenna plus matching network) is described using the concept of electrical length, so an antenna used at a lower frequency than its resonant frequency is called an electrically short antenna For example, at 30 MHz (10 m wavelength) a true resonant ⁠ 1 /4⁠ wave monopole would be almost 2.5 meters long, and using an antenna only 1.5 meters tall would require the addition of a loading coil. Then it may be said that the coil has lengthened the antenna to"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_16",
    "chunk": "achieve an electrical length of 2.5 meters. However, the resulting resistive impedance achieved will be quite a bit lower than that of a true ⁠ 1 /4⁠ wave (resonant) monopole, often requiring further impedance matching (a transformer) to the desired transmission line. For ever shorter antennas (requiring greater \"electrical lengthening\") the radiation resistance plummets (approximately according to the square of the antenna length), so that the mismatch due to a net reactance away from the electrical resonance worsens. Or one could as well say that the equivalent resonant circuit of the antenna system has a higher Q factor and thus a reduced bandwidth, which can even become inadequate for the transmitted signal's spectrum. Resistive losses due to the loading coil, relative to the decreased radiation resistance, entail a reduced electrical efficiency, which can be of great concern for a transmitting antenna, but bandwidth is the major factor that sets the size of antennas at 1 MHz and lower frequencies. The radiant flux as a function of the distance from the transmitting antenna varies according to the inverse-square law, since that describes the geometrical divergence of the transmitted wave. For a given incoming flux, the power acquired by a receiving antenna"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_17",
    "chunk": "is proportional to its effective area. This parameter compares the amount of power captured by a receiving antenna in comparison to the flux of an incoming wave (measured in terms of the signal's power density in watts per square metre). A half-wave dipole has an effective area of about 0.13 λ seen from the broadside direction. If higher gain is needed one cannot simply make the antenna larger. Due to the constraint on the effective area of a receiving antenna detailed below, one sees that for an already-efficient antenna design, the only way to increase gain (effective area) is by reducing the antenna's gain in another direction. If a half-wave dipole is not connected to an external circuit but rather shorted out at the feedpoint, then it becomes a resonant half-wave element which efficiently produces a standing wave in response to an impinging radio wave. Because there is no load to absorb that power, it retransmits all of that power, possibly with a phase shift which is critically dependent on the element's exact length. Thus such a conductor can be arranged in order to transmit a second copy of a transmitter's signal in order to affect the radiation pattern (and"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_18",
    "chunk": "feedpoint impedance) of the element electrically connected to the transmitter. Antenna elements used in this way are known as passive radiators. A Yagi–Uda array uses passive elements to greatly increase gain in one direction (at the expense of other directions). A number of parallel approximately half-wave elements (of very specific lengths) are situated parallel to each other, at specific positions, along a boom; the boom is only for support and not involved electrically. Only one of the elements is electrically connected to the transmitter or receiver, while the remaining elements are passive. The Yagi produces a fairly large gain (depending on the number of passive elements) and is widely used as a directional antenna with an antenna rotor to control the direction of its beam. It suffers from having a rather limited bandwidth, restricting its use to certain applications. Rather than using one driven antenna element along with passive radiators, one can build an array antenna in which multiple elements are all driven by the transmitter through a system of power splitters and transmission lines in relative phases so as to concentrate the RF power in a single direction. What's more, a phased array can be made \"steerable\", that is,"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_19",
    "chunk": "by changing the phases applied to each element the radiation pattern can be shifted without physically moving the antenna elements. Another common array antenna is the log-periodic dipole array which has an appearance similar to the Yagi (with a number of parallel elements along a boom) but is totally dissimilar in operation as all elements are connected electrically to the adjacent element with a phase reversal; using the log-periodic principle it obtains the unique property of maintaining its performance characteristics (gain and impedance) over a very large bandwidth. When a radio wave hits a large conducting sheet it is reflected (with the phase of the electric field reversed) just as a mirror reflects light. Placing such a reflector behind an otherwise non-directional antenna will insure that the power that would have gone in its direction is redirected toward the desired direction, increasing the antenna's gain by a factor of at least 2. Likewise, a corner reflector can insure that all of the antenna's power is concentrated in only one quadrant of space (or less) with a consequent increase in gain. Practically speaking, the reflector need not be a solid metal sheet, but can consist of a curtain of rods aligned"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_20",
    "chunk": "with the antenna's polarization; this greatly reduces the reflector's weight and wind load. Specular reflection of radio waves is also employed in a parabolic reflector antenna, in which a curved reflecting surface effects focussing of an incoming wave toward a so-called feed antenna; this results in an antenna system with an effective area comparable to the size of the reflector itself. Other concepts from geometrical optics are also employed in antenna technology, such as with the lens antenna. The antenna's power gain (or simply \"gain\") also takes into account the antenna's efficiency, and is often the primary figure of merit. Antennas are characterized by a number of performance measures which a user would be concerned with in selecting or designing an antenna for a particular application. A plot of the directional characteristics in the space surrounding the antenna is its radiation pattern. The frequency range or bandwidth over which an antenna functions well can be very wide (as in a log-periodic antenna) or narrow (as in a small loop antenna); outside this range the antenna impedance becomes a poor match to the transmission line and transmitter (or receiver). Use of the antenna well away from its design frequency affects its"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_21",
    "chunk": "radiation pattern, reducing its directive gain. Generally an antenna will not have a feed-point impedance that matches that of a transmission line; a matching network between antenna terminals and the transmission line will improve power transfer to the antenna. A non-adjustable matching network will most likely place further limits the usable bandwidth of the antenna system. It may be desirable to use tubular elements, instead of thin wires, to make an antenna; these will allow a greater bandwidth. Or, several thin wires can be grouped in a cage to simulate a thicker element. This widens the bandwidth of the resonance. Amateur radio antennas that operate at several frequency bands which are widely separated from each other may connect elements resonant at those different frequencies in parallel. Most of the transmitter's power will flow into the resonant element while the others present a high impedance. Another solution uses traps, parallel resonant circuits which are strategically placed in breaks created in long antenna elements. When used at the trap's particular resonant frequency the trap presents a very high impedance (parallel resonance) effectively truncating the element at the location of the trap; if positioned correctly, the truncated element makes a proper resonant antenna"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_22",
    "chunk": "at the trap frequency. At substantially higher or lower frequencies the trap allows the full length of the broken element to be employed, but with a resonant frequency shifted by the net reactance added by the trap. The bandwidth characteristics of a resonant antenna element can be characterized according to its Q where the resistance involved is the radiation resistance, which represents the emission of energy from the resonant antenna to free space. The Q of a narrow band antenna can be as high as 15. On the other hand, the reactance at the same off-resonant frequency of one using thick elements is much less, consequently resulting in a Q as low as 5. These two antennas may perform equivalently at the resonant frequency, but the second antenna will perform over a bandwidth 3 times as wide as the antenna consisting of a thin conductor. Antennas for use over much broader frequency ranges are achieved using further techniques. Adjustment of a matching network can, in principle, allow for any antenna to be matched at any frequency. Thus the small loop antenna built into most AM broadcast (medium wave) receivers has a very narrow bandwidth, but is tuned using a parallel"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_23",
    "chunk": "capacitance which is adjusted according to the receiver tuning. On the other hand, log-periodic antennas are not resonant at any single frequency but can (in principle) be built to attain similar characteristics (including feedpoint impedance) over any frequency range. These are therefore commonly used (in the form of directional log-periodic dipole arrays) as television antennas. Gain is a parameter which measures the degree of directivity of the antenna's radiation pattern. A high-gain antenna will radiate most of its power in a particular direction, while a low-gain antenna will radiate over a wide angle. The antenna gain, or power gain of an antenna is defined as the ratio of the intensity (power per unit surface area) I {\\displaystyle I} radiated by the antenna in the direction of its maximum output, at an arbitrary distance, divided by the intensity I iso {\\displaystyle I_{\\text{iso}}} radiated at the same distance by a hypothetical isotropic antenna which radiates equal power in all directions. This dimensionless ratio is usually expressed logarithmically in decibels, these units are called decibels-isotropic (dBi) A second unit used to measure gain is the ratio of the power radiated by the antenna to the power radiated by a half-wave dipole antenna I"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_24",
    "chunk": "dipole {\\displaystyle I_{\\text{dipole}}} ; these units are called decibels-dipole (dBd) Since the gain of a half-wave dipole is 2.15 dBi and the logarithm of a product is additive, the gain in dBi is just 2.15 decibels greater than the gain in dBd High-gain antennas have the advantage of longer range and better signal quality, but must be aimed carefully at the other antenna. An example of a high-gain antenna is a parabolic dish such as a satellite television antenna. Low-gain antennas have shorter range, but the orientation of the antenna is relatively unimportant. An example of a low-gain antenna is the whip antenna found on portable radios and cordless phones. Antenna gain should not be confused with amplifier gain, a separate parameter measuring the increase in signal power due to an amplifying device placed at the front-end of the system, such as a low-noise amplifier. The effective area or effective aperture of a receiving antenna expresses the portion of the power of a passing electromagnetic wave which the antenna delivers to its terminals, expressed in terms of an equivalent area. For instance, if a radio wave passing a given location has a flux of 1 pW / m (10 Watts"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_25",
    "chunk": "per square meter) and an antenna has an effective area of 12 m, then the antenna would deliver 12 pW of RF power to the receiver (30 microvolts RMS at 75 ohms). Since the receiving antenna is not equally sensitive to signals received from all directions, the effective area is a function of the direction to the source. Due to reciprocity (discussed above) the gain of an antenna used for transmitting must be proportional to its effective area when used for receiving. Consider an antenna with no loss, that is, one whose electrical efficiency is 100%. It can be shown that its effective area averaged over all directions must be equal to λ/4π, the wavelength squared divided by 4π. Gain is defined such that the average gain over all directions for an antenna with 100% electrical efficiency is equal to 1. Therefore, the effective area Aeff in terms of the gain G in a given direction is given by: For an antenna with an efficiency of less than 100%, both the effective area and gain are reduced by that same amount. Therefore, the above relationship between gain and effective area still holds. These are thus two different ways of expressing"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_26",
    "chunk": "the same quantity. Aeff is especially convenient when computing the power that would be received by an antenna of a specified gain, as illustrated by the above example. The radiation pattern of an antenna is a plot of the relative field strength of the radio waves emitted by the antenna at different angles in the far field. It is typically represented by a three-dimensional graph, or polar plots of the horizontal and vertical cross sections. The pattern of an ideal isotropic antenna, which radiates equally in all directions, would look like a sphere. Many nondirectional antennas, such as monopoles and dipoles, emit equal power in all horizontal directions, with the power dropping off at higher and lower angles; this is called an omnidirectional pattern and when plotted looks like a torus or donut. The radiation of many antennas shows a pattern of maxima or \"lobes\" at various angles, separated by \"nulls\", angles where the radiation falls to zero. This is because the radio waves emitted by different parts of the antenna typically interfere, causing maxima at angles where the radio waves arrive at distant points in phase, and zero radiation at other angles where the radio waves arrive out of"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_27",
    "chunk": "phase. In a directional antenna designed to project radio waves in a particular direction, the lobe in that direction is designed larger than the others and is called the \"main lobe\". The other lobes usually represent unwanted radiation and are called \"sidelobes\". The axis through the main lobe is called the \"principal axis\" or \"boresight axis\". The polar diagrams (and therefore the efficiency and gain) of Yagi antennas are tighter if the antenna is tuned for a narrower frequency range, e.g. the grouped antenna compared to the wideband. Similarly, the polar plots of horizontally polarized yagis are tighter than for those vertically polarized. The space surrounding an antenna can be divided into three concentric regions: The reactive near-field (also called the inductive near-field), the radiating near-field (Fresnel region) and the far-field (Fraunhofer) regions. These regions are useful to identify the field structure in each, although the transitions between them are gradual; there are no clear boundaries. The far-field region is far enough from the antenna to ignore its size and shape: It can be assumed that the electromagnetic wave is purely a radiating plane wave (electric and magnetic fields are in phase and perpendicular to each other and to the"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_28",
    "chunk": "direction of propagation). This simplifies the mathematical analysis of the radiated field. Efficiency of a transmitting antenna is the ratio of power actually radiated (in all directions) to the power absorbed by the antenna terminals. The power supplied to the antenna terminals which is not radiated is converted into heat. This is usually through loss resistance in the antenna's conductors, or loss between the reflector and feed horn of a parabolic antenna. Antenna efficiency is separate from impedance matching, which may also reduce the amount of power radiated using a given transmitter. If an SWR meter reads 150 W of incident power and 50 W of reflected power, that means 100 W have actually been absorbed by the antenna (ignoring transmission line losses). How much of that power has actually been radiated cannot be directly determined through electrical measurements at (or before) the antenna terminals, but would require (for instance) careful measurement of field strength. The loss resistance and efficiency of an antenna can be calculated once the field strength is known, by comparing it to the power supplied to the antenna. The loss resistance will generally affect the feedpoint impedance, adding to its resistive component. That resistance will consist"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_29",
    "chunk": "of the sum of the radiation resistance Rrad and the loss resistance Rloss. If a current I is delivered to the terminals of an antenna, then a power of IRrad will be radiated and a power of IRloss will be lost as heat. Therefore, the efficiency of an antenna is equal to ⁠Rrad/(Rrad + Rloss)⁠. Only the total resistance Rrad + Rloss can be directly measured. According to reciprocity, the efficiency of an antenna used as a receiving antenna is identical to its efficiency as a transmitting antenna, described above. The power that an antenna will deliver to a receiver (with a proper impedance match) is reduced by the same amount. In some receiving applications, the very inefficient antennas may have little impact on performance. At low frequencies, for example, atmospheric or man-made noise can mask antenna inefficiency. For example, CCIR Rep. 258-3 indicates man-made noise in a residential setting at 40 MHz is about 28 dB above the thermal noise floor. Consequently, an antenna with a 20 dB loss (due to inefficiency) would have little impact on system noise performance. The loss within the antenna will affect the intended signal and the noise/interference identically, leading to no reduction in"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_30",
    "chunk": "signal to noise ratio (SNR). Antennas which are not a significant fraction of a wavelength in size are inevitably inefficient due to their small radiation resistance. AM broadcast radios include a small loop antenna for reception which has an extremely poor efficiency. This has little effect on the receiver's performance, but simply requires greater amplification by the receiver's electronics. Contrast this tiny component to the massive and very tall towers used at AM broadcast stations for transmitting at the very same frequency, where every percentage point of reduced antenna efficiency entails a substantial cost. The definition of antenna gain or power gain already includes the effect of the antenna's efficiency. Therefore, if one is trying to radiate a signal toward a receiver using a transmitter of a given power, one need only compare the gain of various antennas rather than considering the efficiency as well. This is likewise true for a receiving antenna at very high (especially microwave) frequencies, where the point is to receive a signal which is strong compared to the receiver's noise temperature. However, in the case of a directional antenna used for receiving signals with the intention of rejecting interference from different directions, one is no"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_31",
    "chunk": "longer concerned with the antenna efficiency, as discussed above. In this case, rather than quoting the antenna gain, one would be more concerned with the directive gain, or simply directivity which does not include the effect of antenna (in)efficiency. The directive gain of an antenna can be computed from the published gain divided by the antenna's efficiency. In equation form, gain = directivity × efficiency. The orientation and physical structure of an antenna determine the polarization of the electric field of the radio wave transmitted by it. For instance, an antenna composed of a linear conductor (such as a dipole or whip antenna) oriented vertically will result in vertical polarization; if turned on its side the same antenna's polarization will be horizontal. Reflections generally affect polarization. Radio waves reflected off the ionosphere can change the wave's polarization. For line-of-sight communications or ground wave propagation, horizontally or vertically polarized transmissions generally remain in about the same polarization state at the receiving location. Using a vertically polarized antenna to receive a horizontally polarized wave (or visa-versa) results in relatively poor reception. An antenna's polarization can sometimes be inferred directly from its geometry. When the antenna's conductors viewed from a reference location appear"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_32",
    "chunk": "along one line, then the antenna's polarization will be linear in that very direction. In the more general case, the antenna's polarization must be determined through analysis. For instance, a turnstile antenna mounted horizontally (as is usual), from a distant location on Earth, appears as a horizontal line segment, so its radiation received there is horizontally polarized. But viewed at a downward angle from an airplane, the same antenna does not meet this requirement; in fact its radiation is elliptically polarized when viewed from that direction. In some antennas the state of polarization will change with the frequency of transmission. The polarization of a commercial antenna is an essential specification. In the most general case, polarization is elliptical, meaning that over each cycle the electric field vector traces out an ellipse. Two special cases are linear polarization (the ellipse collapses into a line) as discussed above, and circular polarization (in which the two axes of the ellipse are equal). In linear polarization the electric field of the radio wave oscillates along one direction. In circular polarization, the electric field of the radio wave rotates around the axis of propagation. Circular or elliptically polarized radio waves are designated as right-handed or"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_33",
    "chunk": "left-handed using the \"thumb in the direction of the propagation\" rule. Note that for circular polarization, optical researchers use the opposite right-hand rule from the one used by radio engineers. It is best for the receiving antenna to match the polarization of the transmitted wave for optimum reception. Otherwise there will be a loss of signal strength: when a linearly polarized antenna receives linearly polarized radiation at a relative angle of θ, then there will be a power loss of cosθ . A circularly polarized antenna can be used to equally well match vertical or horizontal linear polarizations, suffering a 3 dB signal reduction. However it will be blind to a circularly polarized signal of the opposite orientation. Maximum power transfer requires matching the impedance of an antenna system (as seen looking into the transmission line) to the complex conjugate of the impedance of the receiver or transmitter. In the case of a transmitter, however, the desired matching impedance might not exactly correspond to the dynamic output impedance of the transmitter as analyzed as a source impedance but rather the design value (typically 50 Ohms) required for efficient and safe operation of the transmitting circuitry. The intended impedance is normally"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_34",
    "chunk": "resistive, but a transmitter (and some receivers) may have limited additional adjustments to cancel a certain amount of reactance, in order to \"tweak\" the match. When a transmission line is used in between the antenna and the transmitter (or receiver) one generally would like an antenna system whose impedance is resistive and nearly the same as the characteristic impedance of that transmission line, in addition to matching the impedance that the transmitter (or receiver) expects. The match is sought to minimize the amplitude of standing waves (measured via the standing wave ratio; SWR) that a mismatch raises on the line, and the increase in transmission line losses it entails. Antenna tuning, in the strict sense of modifying the antenna itself, generally refers only to cancellation of any reactance seen at the antenna terminals, leaving only a resistive impedance which might or might not be exactly the desired impedance (that of the available transmission line). Although an antenna may be designed to have a purely resistive feedpoint impedance (such as a dipole 97% of a half wavelength long) at just one frequency, this will very likely not be exactly true at other frequencies that the antenna is eventually used for. In"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_35",
    "chunk": "most cases, in principle the physical length of the antenna can be \"trimmed\" to obtain a pure resistance, although this is rarely convenient. On the other hand, the addition of a contrary inductance or capacitance can be used to cancel a residual capacitive or inductive reactance, respectively, and may be more convenient than lowering and trimming or extending the antenna, then hoisting it back. Antenna reactance may be removed using lumped elements, such as capacitors or inductors in the main path of current traversing the antenna, often near the feedpoint, or by incorporating capacitive or inductive structures into the conducting body of the antenna to cancel the feedpoint reactance – such as open-ended \"spoke\" radial wires, or looped parallel wires – hence genuinely tune the antenna to resonance. In addition to those reactance-neutralizing add-ons, antennas of any kind may include a transformer and / or transformer balun at their feedpoint, to change the resistive part of the impedance to more nearly match the feedline's characteristic impedance. Antenna tuning in the loose sense, performed by an impedance matching device (somewhat inappropriately named an \"antenna tuner\", or the older, more appropriate term transmatch) goes beyond merely removing reactance and includes transforming the"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_36",
    "chunk": "remaining resistance to match the feedline and radio. An additional problem is matching the remaining resistive impedance to the characteristic impedance of the transmission line: A general impedance matching network (an \"antenna tuner\" or ATU) will have at least two adjustable elements to correct both components of impedance. Any matching network will have both power losses and power restrictions when used for transmitting. Commercial antennas are generally designed to approximately match standard 50 Ohm coaxial cables, at standard frequencies; the design expectation is that a matching network will be merely used to 'tweak' any residual mismatch. In some cases matching is done in a more extreme manner, not simply to cancel a small amount of residual reactance, but to resonate an antenna whose resonance frequency is quite different from the intended frequency of operation. The radiation pattern and even the driving point impedance of an antenna can be influenced by the dielectric constant and especially conductivity of nearby objects. For a terrestrial antenna, the ground is usually one such object of importance. The antenna's height above the ground, as well as the electrical properties (permittivity and conductivity) of the ground, can then be important. Also, in the particular case of"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_37",
    "chunk": "a monopole antenna, the ground (or an artificial ground plane) serves as the return connection for the antenna current thus having an additional effect, particularly on the impedance seen by the feed line. When an electromagnetic wave strikes a plane surface such as the ground, part of the wave is transmitted into the ground and part of it is reflected, according to the Fresnel coefficients. If the ground is a very good conductor then almost all of the wave is reflected (180° out of phase), whereas a ground modeled as a (lossy) dielectric can absorb a large amount of the wave's power. The power remaining in the reflected wave, and the phase shift upon reflection, strongly depend on the wave's angle of incidence and polarization. The dielectric constant and conductivity (or simply the complex dielectric constant) is dependent on the soil type and is a function of frequency. For very low frequencies to high frequencies (< 30 MHz), the ground behaves as a lossy dielectric, thus the ground is characterized both by a conductivity and permittivity (dielectric constant) which can be measured for a given soil (but is influenced by fluctuating moisture levels) or can be estimated from certain maps."
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_38",
    "chunk": "At lower mediumwave frequencies the ground acts mainly as a good conductor, which AM broadcast (0.5–1.7 MHz) antennas depend on. At frequencies between 3–30 MHz, a large portion of the energy from a horizontally polarized antenna reflects off the ground, with almost total reflection at the grazing angles important for ground wave propagation. That reflected wave, with its phase reversed, can either cancel or reinforce the direct wave, depending on the antenna height in wavelengths and elevation angle (for a sky wave). On the other hand, vertically polarized radiation is not well reflected by the ground except at grazing incidence or over very highly conducting surfaces such as sea water. However the grazing angle reflection important for ground wave propagation, using vertical polarization, is in phase with the direct wave, providing a boost of up to 6 dB, as is detailed below. At VHF and above (> 30 MHz) the ground becomes a poorer reflector. However, for shortwave frequencies, especially below ~15 MHz, it remains a good reflector especially for horizontal polarization and grazing angles of incidence. That is important as these higher frequencies usually depend on horizontal line-of-sight propagation (except for satellite communications), the ground then behaving almost as"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_39",
    "chunk": "a mirror. The net quality of a ground reflection depends on the topography of the surface. When the irregularities of the surface are much smaller than the wavelength, the dominant regime is that of specular reflection, and the receiver sees both the real antenna and an image of the antenna under the ground due to reflection. But if the ground has irregularities not small compared to the wavelength, reflections will not be coherent but shifted by random phases. With shorter wavelengths (higher frequencies), this is generally the case. Whenever both the receiving or transmitting antenna are placed at significant heights above the ground (relative to the wavelength), waves reflected specularly by the ground will travel a longer distance than direct waves, inducing a phase shift which can sometimes be significant. When a sky wave is launched by such an antenna, that phase shift is always significant unless the antenna is very close to the ground (compared to the wavelength). The phase of reflection of electromagnetic waves depends on the polarization of the incident wave. Given the larger refractive index of the ground (typically n ≈ 2) compared to air (n = 1), the phase of horizontally polarized radiation is reversed"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_40",
    "chunk": "upon reflection (a phase shift of π radians, or 180°). On the other hand, the vertical component of the wave's electric field is reflected at grazing angles of incidence approximately in phase. These phase shifts apply as well to a ground modeled as a good electrical conductor. This means that a receiving antenna \"sees\" an image of the emitting antenna but with 'reversed' currents (opposite in direction and phase) if the emitting antenna is horizontally oriented (and thus horizontally polarized). However, the received current will be in the same absolute direction and phase if the emitting antenna is vertically polarized. The actual antenna which is transmitting the original wave then also may receive a strong signal from its own image from the ground. This will induce an additional current in the antenna element, changing the current at the feedpoint for a given feedpoint voltage. Thus the antenna's impedance, given by the ratio of feedpoint voltage to current, is altered due to the antenna's proximity to the ground. This can be quite a significant effect when the antenna is within a wavelength or two of the ground. But as the antenna height is increased, the reduced power of the reflected wave"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_41",
    "chunk": "(due to the inverse square law) allows the antenna to approach its asymptotic feedpoint impedance given by theory. At lower heights, the effect on the antenna's impedance is very sensitive to the exact distance from the ground, as this affects the phase of the reflected wave relative to the currents in the antenna. Changing the antenna's height by a quarter wavelength, then changes the phase of the reflection by 180°, with a completely different effect on the antenna's impedance. The ground reflection has an important effect on the net far field radiation pattern in the vertical plane, that is, as a function of elevation angle, which is thus different between a vertically and horizontally polarized antenna. Consider an antenna at a height h above the ground, transmitting a wave considered at the elevation angle θ. For a vertically polarized transmission the magnitude of the electric field of the electromagnetic wave produced by the direct ray plus the reflected ray is: Thus the power received can be as high as 4 times that due to the direct wave alone (such as when θ = 0), following the square of the cosine. The sign inversion for the reflection of horizontally polarized emission"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_42",
    "chunk": "instead results in: For horizontal propagation between transmitting and receiving antennas situated near the ground reasonably far from each other, the distances traveled by the direct and reflected rays are nearly the same. There is almost no relative phase shift. If the emission is polarized vertically, the two fields (direct and reflected) add and there is maximum of received signal. If the signal is polarized horizontally, the two signals subtract and the received signal is largely cancelled. The vertical plane radiation patterns are shown in the image at right. With vertical polarization there is always a maximum for θ = 0, horizontal propagation (left pattern). For horizontal polarization, there is cancellation at that angle. The above formulae and these plots assume the ground as a perfect conductor. These plots of the radiation pattern correspond to a distance between the antenna and its image of 2.5 λ . As the antenna height is increased, the number of lobes increases as well. The difference in the above factors for the case of θ = 0 is the reason that most broadcasting (transmissions intended for the public) uses vertical polarization. For receivers near the ground, horizontally polarized transmissions suffer cancellation. For best reception"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_43",
    "chunk": "the receiving antennas for these signals are likewise vertically polarized. In some applications where the receiving antenna must work in any position, as in mobile phones, the base station antennas use mixed polarization, such as linear polarization at an angle (with both vertical and horizontal components) or circular polarization. On the other hand, analog television transmissions are usually horizontally polarized, because in urban areas buildings can reflect the electromagnetic waves and create ghost images due to multipath propagation. Using horizontal polarization, ghosting is reduced because the amount of reflection in the horizontal polarization off the side of a building is generally less than in the vertical direction. Vertically polarized analog television have been used in some rural areas. In digital terrestrial television such reflections are less problematic, due to robustness of binary transmissions and error correction. The flow of current in wire antennas is identical to the solution of counter-propagating waves in a single conductor transmission line, which can be solved using the telegrapher's equations. Solutions of currents along antenna elements are more conveniently and accurately obtained by numerical methods, so transmission-line techniques have largely been abandoned for precision modelling, but they continue to be a widely used source of"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_44",
    "chunk": "useful, simple approximations that describe well the impedance profiles of antennas. Unlike transmission lines, currents in antennas contribute power to the radiated part electromagnetic field, which can be modeled using radiation resistance. The end of an antenna element corresponds to an unterminated (open) end of a single-conductor transmission line, resulting in a reflected wave identical to the incident wave, with its voltage in phase with the incident wave and its current in the opposite phase (thus net zero current, where there is, after all, no further conductor). The combination of the incident and reflected wave, just as in a transmission line, forms a standing wave with a current node at the conductor's end, and a voltage node one-quarter wavelength from the end (if the element is at least that long). In a resonant antenna, the feedpoint of the antenna is at one of those voltage nodes. Due to discrepancies from the simplified version of the transmission line model, the voltage one quarter wavelength from the current node is not exactly zero, but it is near a minimum, and small compared to the much large voltage at the conductor's end. Hence, a feed point matching the antenna at that spot requires"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_45",
    "chunk": "a relatively small voltage but large current (the currents from the two waves add in-phase there), thus a relatively low feedpoint impedance. Feeding the antenna at other points involves a large voltage, thus a large impedance, and usually one that is primarily reactive (low power factor), which is a terrible impedance match to available transmission lines. Therefore, it is usually desired for an antenna to operate as a resonant element with each conductor having a length of one quarter wavelength (or any other odd multiples of a quarter wavelength). For instance, a half-wave dipole has two such elements (one connected to each conductor of a balanced transmission line) about one quarter wavelength long. Depending on the conductors' diameters, a small deviation from this length is adopted in order to reach the point where the antenna current and the (small) feedpoint voltage are exactly in phase. Then the antenna presents a purely resistive impedance, and ideally one close to the characteristic impedance of an available transmission line. Despite these useful properties, resonant antennas have the disadvantage that they achieve resonance (purely resistive feedpoint impedance) only at a fundamental frequency, and perhaps some of its harmonics, and the feedpoint resistance is larger"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_46",
    "chunk": "at higher-order resonances. Therefore, resonant antennas can only achieve their good performance within a limited bandwidth, depending on the Q at the resonance. The electric and magnetic fields emanating from a driven antenna element will generally affect the voltages and currents in nearby antennas, antenna elements, or other conductors. This is particularly true when the affected conductor is a resonant element (multiple of half-wavelengths in length) at about the same frequency, as is the case where the conductors are all part of the same active or passive antenna array. Because the affected conductors are in the near-field, one can not just treat two antennas as transmitting and receiving a signal according to the Friis transmission formula for instance, but must calculate the mutual impedance matrix which takes into account both voltages and currents (interactions through both the electric and magnetic fields). Thus using the mutual impedances calculated for a specific geometry, one can solve for the radiation pattern of a Yagi–Uda antenna or the currents and voltages for each element of a phased array. Such an analysis can also describe in detail reflection of radio waves by a ground plane or by a corner reflector and their effect on the"
  },
  {
    "source": "Antenna (radio).txt",
    "chunk_id": "Antenna (radio).txt_47",
    "chunk": "impedance (and radiation pattern) of an antenna in its vicinity. Often such near-field interactions are undesired and pernicious. Currents in random metal objects near a transmitting antenna will often be in poor conductors, causing loss of RF power in addition to unpredictably altering the characteristics of the antenna. By careful design, it is possible to reduce the electrical interaction between nearby conductors. For instance, the 90 degree angle in between the two dipoles composing the turnstile antenna insures no interaction between these, allowing these to be driven independently (but actually with the same signal in quadrature phases in the turnstile antenna design). Antennas can be classified by operating principles or by their application. Different authorities placed antennas in narrower or broader categories. Generally these include These antenna types and others are summarized in greater detail in the overview article, Antenna types, as well as in each of the linked articles in the list above, and in even more detail in articles which those link to."
  },
  {
    "source": "Antipodal point.txt",
    "chunk_id": "Antipodal point.txt_0",
    "chunk": "# Antipodal point In mathematics, two points of a sphere (or n-sphere, including a circle) are called antipodal or diametrically opposite if they are the endpoints of a diameter, a straight line segment between two points on a sphere and passing through its center. Given any point on a sphere, its antipodal point is the unique point at greatest distance, whether measured intrinsically (great-circle distance on the surface of the sphere) or extrinsically (chordal distance through the sphere's interior). Every great circle on a sphere passing through a point also passes through its antipodal point, and there are infinitely many great circles passing through a pair of antipodal points (unlike the situation for any non-antipodal pair of points, which have a unique great circle passing through both). Many results in spherical geometry depend on choosing non-antipodal points, and degenerate if antipodal points are allowed; for example, a spherical triangle degenerates to an underspecified lune if two of the vertices are antipodal. The point antipodal to a given point is called its antipodes, from the Greek ἀντίποδες (antípodes) meaning \"opposite feet\"; see Antipodes § Etymology. Sometimes the s is dropped, and this is rendered antipode, a back-formation. The concept of antipodal"
  },
  {
    "source": "Antipodal point.txt",
    "chunk_id": "Antipodal point.txt_1",
    "chunk": "points is generalized to spheres of any dimension: two points on the sphere are antipodal if they are opposite through the centre. Each line through the centre intersects the sphere in two points, one for each ray emanating from the centre, and these two points are antipodal. The Borsuk–Ulam theorem is a result from algebraic topology dealing with such pairs of points. It says that any continuous function from S n {\\displaystyle S^{n}} to R n {\\displaystyle \\mathbb {R} ^{n}} maps some pair of antipodal points in S n {\\displaystyle S^{n}} to the same point in R n . {\\displaystyle \\mathbb {R} ^{n}.} Here, S n {\\displaystyle S^{n}} denotes the n {\\displaystyle n} -dimensional sphere and R n {\\displaystyle \\mathbb {R} ^{n}} is n {\\displaystyle n} -dimensional real coordinate space. The antipodal map A : S n → S n {\\displaystyle A:S^{n}\\to S^{n}} sends every point on the sphere to its antipodal point. If points on the n {\\displaystyle n} -sphere are represented as displacement vectors from the sphere's center in Euclidean ( n + 1 ) {\\displaystyle (n+1)} -space, then two antipodal points are represented by additive inverses v {\\displaystyle \\mathbf {v} } and − v , {\\displaystyle -\\mathbf"
  },
  {
    "source": "Antipodal point.txt",
    "chunk_id": "Antipodal point.txt_2",
    "chunk": "{v} ,} and the antipodal map can be defined as A ( x ) = − x . {\\displaystyle A(\\mathbf {x} )=-\\mathbf {x} .} The antipodal map preserves orientation (is homotopic to the identity map) when n {\\displaystyle n} is odd, and reverses it when n {\\displaystyle n} is even. Its degree is ( − 1 ) n + 1 . {\\displaystyle (-1)^{n+1}.} If antipodal points are identified (considered equivalent), the sphere becomes a model of real projective space."
  },
  {
    "source": "Apsis.txt",
    "chunk_id": "Apsis.txt_0",
    "chunk": "# Apsis An apsis (from Ancient Greek ἁψίς (hapsís) 'arch, vault'; pl. apsides /ˈæpsɪˌdiːz/ AP-sih-deez) is the farthest or nearest point in the orbit of a planetary body about its primary body. The line of apsides (also called apse line, or major axis of the orbit) is the line connecting the two extreme values. Apsides pertaining to orbits around different bodies have distinct names to differentiate themselves from other apsides. Apsides pertaining to geocentric orbits, orbits around the Earth, are at the farthest point called the apogee, and at the nearest point the perigee, like with orbits of satellites and the Moon around Earth. Apsides pertaining to orbits around the Sun are named aphelion for the farthest and perihelion for the nearest point in a heliocentric orbit. Earth's two apsides are the farthest point, aphelion, and the nearest point, perihelion, of its orbit around the host Sun. The terms aphelion and perihelion apply in the same way to the orbits of Jupiter and the other planets, the comets, and the asteroids of the Solar System. There are two apsides in any elliptic orbit. The name for each apsis is created from the prefixes ap-, apo- (from ἀπ(ό), (ap(o)-) 'away from')"
  },
  {
    "source": "Apsis.txt",
    "chunk_id": "Apsis.txt_1",
    "chunk": "for the farthest or peri- (from περί (peri-) 'near') for the closest point to the primary body, with a suffix that describes the primary body. The suffix for Earth is -gee, so the apsides' names are apogee and perigee. For the Sun, the suffix is -helion, so the names are aphelion and perihelion. According to Newton's laws of motion, all periodic orbits are ellipses. The barycenter of the two bodies may lie well within the bigger body—e.g., the Earth–Moon barycenter is about 75% of the way from Earth's center to its surface. If, compared to the larger mass, the smaller mass is negligible (e.g., for satellites), then the orbital parameters are independent of the smaller mass. When used as a suffix—that is, -apsis—the term can refer to the two distances from the primary body to the orbiting body when the latter is located: 1) at the periapsis point, or 2) at the apoapsis point (compare both graphics, second figure). The line of apsides denotes the distance of the line that joins the nearest and farthest points across an orbit; it also refers simply to the extreme range of an object orbiting a host body (see top figure; see third figure)."
  },
  {
    "source": "Apsis.txt",
    "chunk_id": "Apsis.txt_2",
    "chunk": "In orbital mechanics, the apsides technically refer to the distance measured between the barycenter of the 2-body system and the center of mass of the orbiting body. However, in the case of a spacecraft, the terms are commonly used to refer to the orbital altitude of the spacecraft above the surface of the central body (assuming a constant, standard reference radius). The words \"pericenter\" and \"apocenter\" are often seen, although periapsis/apoapsis are preferred in technical usage. The words perihelion and aphelion were coined by Johannes Kepler to describe the orbital motions of the planets around the Sun. The words are formed from the prefixes peri- (Greek: περί, near) and apo- (Greek: ἀπό, away from), affixed to the Greek word for the Sun, (ἥλιος, or hēlíos). Various related terms are used for other celestial objects. The suffixes -gee, -helion, -astron and -galacticon are frequently used in the astronomical literature when referring to the Earth, Sun, stars, and the Galactic Center respectively. The suffix -jove is occasionally used for Jupiter, but -saturnium has very rarely been used in the last 50 years for Saturn. The -gee form is also used as a generic closest-approach-to \"any planet\" term—instead of applying it only to"
  },
  {
    "source": "Apsis.txt",
    "chunk_id": "Apsis.txt_3",
    "chunk": "Earth. During the Apollo program, the terms pericynthion and apocynthion were used when referring to orbiting the Moon; they reference Cynthia, an alternative name for the Greek Moon goddess Artemis. More recently, during the Artemis program, the terms perilune and apolune have been used. Regarding black holes, the term peribothron was first used in a 1976 paper by J. Frank and M. J. Rees, who credit W. R. Stoeger for suggesting creating a term using the greek word for pit: \"bothron\". The terms perimelasma and apomelasma (from a Greek root) were used by physicist and science-fiction author Geoffrey A. Landis in a story published in 1998, thus appearing before perinigricon and aponigricon (from Latin) in the scientific literature in 2002. The suffixes shown below may be added to prefixes peri- or apo- to form unique names of apsides for the orbiting bodies of the indicated host/(primary) system. However, only for the Earth, Moon and Sun systems are the unique suffixes commonly used. Exoplanet studies commonly use -astron, but typically, for other host systems the generic suffix, -apsis, is used instead. The perihelion (q) and aphelion (Q) are the nearest and farthest points respectively of a body's direct orbit around the"
  },
  {
    "source": "Apsis.txt",
    "chunk_id": "Apsis.txt_4",
    "chunk": "Sun. Comparing osculating elements at a specific epoch to those at a different epoch will generate differences. The time-of-perihelion-passage as one of six osculating elements is not an exact prediction (other than for a generic two-body model) of the actual minimum distance to the Sun using the full dynamical model. Precise predictions of perihelion passage require numerical integration. The two images below show the orbits, orbital nodes, and positions of perihelion (q) and aphelion (Q) for the planets of the Solar System as seen from above the northern pole of Earth's ecliptic plane, which is coplanar with Earth's orbital plane. The planets travel counterclockwise around the Sun and for each planet, the blue part of their orbit travels north of the ecliptic plane, the pink part travels south, and dots mark perihelion (green) and aphelion (orange). The first image (below-left) features the inner planets, situated outward from the Sun as Mercury, Venus, Earth, and Mars. The reference Earth-orbit is colored yellow and represents the orbital plane of reference. At the time of vernal equinox, the Earth is at the bottom of the figure. The second image (below-right) shows the outer planets, being Jupiter, Saturn, Uranus, and Neptune. The orbital nodes"
  },
  {
    "source": "Apsis.txt",
    "chunk_id": "Apsis.txt_5",
    "chunk": "are the two end points of the \"line of nodes\" where a planet's tilted orbit intersects the plane of reference; here they may be 'seen' as the points where the blue section of an orbit meets the pink. The chart shows the extreme range—from the closest approach (perihelion) to farthest point (aphelion)—of several orbiting celestial bodies of the Solar System: the planets, the known dwarf planets, including Ceres, and Halley's Comet. The length of the horizontal bars correspond to the extreme range of the orbit of the indicated body around the Sun. These extreme distances (between perihelion and aphelion) are the lines of apsides of the orbits of various objects around a host body. Distances of selected bodies of the Solar System from the Sun. The left and right edges of each bar correspond to the perihelion and aphelion of the body, respectively, hence long bars denote high orbital eccentricity. The radius of the Sun is 0.7 million km, and the radius of Jupiter (the largest planet) is 0.07 million km, both too small to resolve on this image. Currently, the Earth reaches perihelion in early January, approximately 14 days after the December solstice. At perihelion, the Earth's center is"
  },
  {
    "source": "Apsis.txt",
    "chunk_id": "Apsis.txt_6",
    "chunk": "about 0.98329 astronomical units (AU) or 147,098,070 km (91,402,500 mi) from the Sun's center. In contrast, the Earth reaches aphelion currently in early July, approximately 14 days after the June solstice. The aphelion distance between the Earth's and Sun's centers is currently about 1.01671 AU or 152,097,700 km (94,509,100 mi). The dates of perihelion and aphelion change over time due to precession and other orbital factors, which follow cyclical patterns known as Milankovitch cycles. In the short term, such dates can vary up to 2 days from one year to another. This significant variation is due to the presence of the Moon: while the Earth–Moon barycenter is moving on a stable orbit around the Sun, the position of the Earth's center which is on average about 4,700 kilometres (2,900 mi) from the barycenter, could be shifted in any direction from it—and this affects the timing of the actual closest approach between the Sun's and the Earth's centers (which in turn defines the timing of perihelion in a given year). Because of the increased distance at aphelion, only 93.55% of the radiation from the Sun falls on a given area of Earth's surface as does at perihelion, but this does not"
  },
  {
    "source": "Apsis.txt",
    "chunk_id": "Apsis.txt_7",
    "chunk": "account for the seasons, which result instead from the tilt of Earth's axis of 23.4° away from perpendicular to the plane of Earth's orbit. Indeed, at both perihelion and aphelion it is summer in one hemisphere while it is winter in the other one. Winter falls on the hemisphere where sunlight strikes least directly, and summer falls where sunlight strikes most directly, regardless of the Earth's distance from the Sun. In the northern hemisphere, summer occurs at the same time as aphelion, when solar radiation is lowest. Despite this, summers in the northern hemisphere are on average 2.3 °C (4 °F) warmer than in the southern hemisphere, because the northern hemisphere contains larger land masses, which are easier to heat than the seas. Perihelion and aphelion do however have an indirect effect on the seasons: because Earth's orbital speed is minimum at aphelion and maximum at perihelion, the planet takes longer to orbit from June solstice to September equinox than it does from December solstice to March equinox. Therefore, summer in the northern hemisphere lasts slightly longer (93 days) than summer in the southern hemisphere (89 days). Astronomers commonly express the timing of perihelion relative to the First Point of"
  },
  {
    "source": "Apsis.txt",
    "chunk_id": "Apsis.txt_8",
    "chunk": "Aries not in terms of days and hours, but rather as an angle of orbital displacement, the so-called longitude of the periapsis (also called longitude of the pericenter). For the orbit of the Earth, this is called the longitude of perihelion, and in 2000 it was about 282.895°; by 2010, this had advanced by a small fraction of a degree to about 283.067°, i.e. a mean increase of 62\" per year. For the orbit of the Earth around the Sun, the time of apsis is often expressed in terms of a time relative to seasons, since this determines the contribution of the elliptical orbit to seasonal variations. The variation of the seasons is primarily controlled by the annual cycle of the elevation angle of the Sun, which is a result of the tilt of the axis of the Earth measured from the plane of the ecliptic. The Earth's eccentricity and other orbital elements are not constant, but vary slowly due to the perturbing effects of the planets and other objects in the solar system (Milankovitch cycles). On a very long time scale, the dates of the perihelion and of the aphelion progress through the seasons, and they make one complete"
  },
  {
    "source": "Apsis.txt",
    "chunk_id": "Apsis.txt_9",
    "chunk": "cycle in 22,000 to 26,000 years. There is a corresponding movement of the position of the stars as seen from Earth, called the apsidal precession. (This is closely related to the precession of the axes.) The dates and times of the perihelions and aphelions for several past and future years are listed in the following table: The following table shows the distances of the planets and dwarf planets from the Sun at their perihelion and aphelion. While, in accordance with Kepler's laws of planetary motion (based on the conservation of angular momentum) and the conservation of energy, these two quantities are constant for a given orbit: Note that for conversion from heights above the surface to distances between an orbit and its primary, the radius of the central body has to be added, and conversely. The arithmetic mean of the two limiting distances is the length of the semi-major axis a. The geometric mean of the two distances is the length of the semi-minor axis b. which is the speed of a body in a circular orbit whose radius is a {\\displaystyle a} . Orbital elements such as the time of perihelion passage are defined at the epoch chosen using"
  },
  {
    "source": "Apsis.txt",
    "chunk_id": "Apsis.txt_10",
    "chunk": "an unperturbed two-body solution that does not account for the n-body problem. To get an accurate time of perihelion passage you need to use an epoch close to the perihelion passage. For example, using an epoch of 1996, Comet Hale–Bopp shows perihelion on 1 April 1997. Using an epoch of 2008 shows a less accurate perihelion date of 30 March 1997. Short-period comets can be even more sensitive to the epoch selected. Using an epoch of 2005 shows 101P/Chernykh coming to perihelion on 25 December 2005, but using an epoch of 2012 produces a less accurate unperturbed perihelion date of 20 January 2006. Numerical integration shows dwarf planet Eris will come to perihelion around December 2257. Using an epoch of 2021, which is 236 years early, less accurately shows Eris coming to perihelion in 2260. 4 Vesta came to perihelion on 26 December 2021, but using a two-body solution at an epoch of July 2021 less accurately shows Vesta came to perihelion on 25 December 2021. Trans-Neptunian objects discovered when 80+ AU from the Sun need dozens of observations over multiple years to well constrain their orbits because they move very slowly against the background stars. Due to statistics of"
  },
  {
    "source": "Apsis.txt",
    "chunk_id": "Apsis.txt_11",
    "chunk": "small numbers, trans-Neptunian objects such as 2015 TH367 when it had only 8 observations over an observation arc of 1 year that have not or will not come to perihelion for roughly 100 years can have a 1-sigma uncertainty of 77.3 years (28,220 days) in the perihelion date."
  },
  {
    "source": "Arecibo Observatory.txt",
    "chunk_id": "Arecibo Observatory.txt_0",
    "chunk": "# Arecibo Observatory The Arecibo Observatory, also known as the National Astronomy and Ionosphere Center (NAIC) and formerly known as the Arecibo Ionosphere Observatory, is an observatory in Barrio Esperanza, Arecibo, Puerto Rico owned by the US National Science Foundation (NSF). The observatory's main instrument was the Arecibo Telescope, a 305 m (1,000 ft) spherical reflector dish built into a natural sinkhole, with a cable-mount steerable receiver and several radar transmitters for emitting signals mounted 150 m (492 ft) above the dish. Completed in 1963, it was the world's largest single-aperture telescope for 53 years, surpassed in July 2016 by the Five-hundred-meter Aperture Spherical Telescope (FAST) in China. On August 10 and November 6, 2020, two of the receiver's support cables broke and the NSF announced that it would decommission the telescope. The telescope collapsed on December 1, 2020. In 2022, the NSF announced the telescope will not be rebuilt, with an educational facility to be established on the site. The observatory also includes a smaller radio telescope, a LIDAR facility, and a visitor center, which remained operational after the telescope's collapse. The asteroid 4337 Arecibo is named after the observatory by Steven J. Ostro, in recognition of the observatory's"
  },
  {
    "source": "Arecibo Observatory.txt",
    "chunk_id": "Arecibo Observatory.txt_1",
    "chunk": "contributions to the characterization of Solar System bodies. In the 1950s, the United States Department of Defense (DoD)'s Advanced Research Projects Agency (ARPA) was seeking a means to detect missiles in Earth's ionosphere. On November 6, 1959, Cornell University entered into a contract with ARPA to carry out development studies for a large-scale ionospheric radar probe. The Arecibo Telescope was consequently built to study the ionosphere as well as to serve as a general-purpose radio telescope. Construction of the telescope was started in September 1960. The telescope and supporting observatory were formally opened as the Arecibo Ionospheric Observatory on November 1, 1963. DoD transferred the observatory to the National Science Foundation on October 1, 1969. NSF appointed Cornell University to manage the observatory. By September 1971, NSF had renamed the observatory the National Astronomy and Ionosphere Center (NAIC) and had made it a federally funded research and development center (FFRDC). NASA began contributing funds to the observatory alongside NSF for its planetary radar mission. In the early 2000s, NASA eliminated funding for the Arecibo Observatory. In 2006, NSF indicated that it would reduce funding for the observatory, and decommission it if other funding could not be found. Academics and politicians"
  },
  {
    "source": "Arecibo Observatory.txt",
    "chunk_id": "Arecibo Observatory.txt_2",
    "chunk": "lobbied to stave off its closure, and NASA recommitted funding in 2011 for study of near-earth objects. In 2011, NSF delisted Arecibo as an FFRDC, which allowed the observatory to seek funding from a wider variety of sources; the agency also replaced Cornell as the site operator with a team led by SRI International. Damage to the telescope by 2017's Hurricane Maria led NSF again to suggest closing the observatory. A consortium led by the University of Central Florida (UCF) proposed to manage the observatory and cover much of the operations and maintenance costs, and in 2018, NSF made UCF's consortium the new site operators, though no specific actions or funding were announced. On August 6, 2020, an auxiliary cable broke on the telescope, followed by a main cable on November 7. The NSF announced that they would decommission the telescope through controlled demolition, but that the other facilities on the observatory would remain operational. Before demolition could occur, remaining support cables from one tower rapidly failed in the morning of December 1, 2020, causing the instrument platform to crash through the dish, shearing off the tops of the support towers, and partially damaging some of the other buildings, though"
  },
  {
    "source": "Arecibo Observatory.txt",
    "chunk_id": "Arecibo Observatory.txt_3",
    "chunk": "with no injuries. NSF officials said in 2020 that they aimed to have the other observatory facilities operational as soon as possible and were considering rebuilding a new telescope instrument in its place. However, in 2022, the NSF announced the telescope will not be rebuilt but an educational facility would be established on the site. The following year, NSF picked a consortium of universities—Cold Spring Harbor Laboratory in New York; the University of Maryland, Baltimore County; the University of Puerto Rico, Río Piedras Campus in San Juan; and the University of the Sacred Heart, also in San Juan—to set up and run an education center called Arecibo C3 (Arecibo Center for Culturally Relevant and Inclusive Science Education, Computational Skills, and Community Engagement). The observatory's main feature was its large radio telescope, whose main collecting dish was an inverted spherical dome 1,000 feet (305 m) in diameter with an 869-foot (265 m) radius of curvature, constructed inside a karst sinkhole. The dish's surface was made of 38,778 perforated aluminum panels, each about 3 by 7 feet (1 by 2 m), supported by a mesh of steel cables. The ground beneath supported shade-tolerant vegetation. Since its completion in November 1963, the Telescope"
  },
  {
    "source": "Arecibo Observatory.txt",
    "chunk_id": "Arecibo Observatory.txt_4",
    "chunk": "had been used for radar astronomy and radio astronomy, and had been part of the Search for extraterrestrial intelligence (SETI) program. It was also used by NASA for Near-Earth object detection. Since around 2006, NSF funding support for the telescope had waned as the Foundation directed funds to newer instruments, though academics petitioned to the NSF and Congress to continue support for the telescope. Numerous hurricanes, including Hurricane Maria, had damaged parts of the telescope, straining the reduced budget. Two cable breaks, one in August 2020 and a second in November 2020, threatened the structural integrity of the support structure for the suspended platform and damaged the dish. The NSF determined in November 2020 that it was safer to decommission the telescope rather than to try to repair it, but the telescope collapsed before a controlled demolition could be carried out. The remaining support cables from one tower failed around 7:56 a.m. local time on December 1, 2020, causing the receiver platform to fall into the dish and collapsing the telescope. NASA led an extensive failure investigation and reported the findings, along with a technical bulletin with industry recommendations. The investigation concluded that \"a combination of low socket design margin"
  },
  {
    "source": "Arecibo Observatory.txt",
    "chunk_id": "Arecibo Observatory.txt_5",
    "chunk": "and a high percentage of sustained loading revealed an unexpected vulnerability to zinc creep and environments, resulting in long-term cumulative damage and progressive zinc/wire failure\". The Arecibo Observatory also has other facilities beyond the main telescope, including a 12-meter (39 ft) radio telescope intended for very-long-baseline interferometry (VLBI) with the main telescope; and a LIDAR facility whose research has continued since the main telescope's collapse. Opened in 1997, the Ángel Ramos Foundation Visitor Center features interactive exhibits and displays about the operations of the radio telescope, astronomy and atmospheric sciences. The center is named after the financial foundation that honors Ángel Ramos, owner of the El Mundo newspaper and founder of Telemundo. The Foundation provided half of the funds to build the Visitor Center, with the remainder received from private donations and Cornell University. The center, in collaboration with the Caribbean Astronomical Society, hosts a series of Astronomical Nights throughout the year, which feature diverse discussions regarding exoplanets, astronomical phenomena, and discoveries (such as Comet ISON). The purposes of the center are to increase public interest in astronomy, the observatory's research successes, and space endeavors."
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_0",
    "chunk": "# Aristotle Aristotle (Attic Greek: Ἀριστοτέλης, romanized: Aristotélēs; 384–322 BC) was an Ancient Greek philosopher and polymath. His writings cover a broad range of subjects spanning the natural sciences, philosophy, linguistics, economics, politics, psychology, and the arts. As the founder of the Peripatetic school of philosophy in the Lyceum in Athens, he began the wider Aristotelian tradition that followed, which set the groundwork for the development of modern science. Little is known about Aristotle's life. He was born in the city of Stagira in northern Greece during the Classical period. His father, Nicomachus, died when Aristotle was a child, and he was brought up by a guardian. At around eighteen years old, he joined Plato's Academy in Athens and remained there until the age of thirty seven (c. 347 BC). Shortly after Plato died, Aristotle left Athens and, at the request of Philip II of Macedon, tutored his son Alexander the Great beginning in 343 BC. He established a library in the Lyceum, which helped him to produce many of his hundreds of books on papyrus scrolls. Though Aristotle wrote many treatises and dialogues for publication, only around a third of his original output has survived, none of it intended"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_1",
    "chunk": "for publication. Aristotle provided a complex synthesis of the various philosophies existing prior to him. His teachings and methods of inquiry have had a significant impact across the world, and remain a subject of contemporary philosophical discussion. Aristotle's views profoundly shaped medieval scholarship. The influence of his physical science extended from late antiquity and the Early Middle Ages into the Renaissance, and was not replaced systematically until the Enlightenment and theories such as classical mechanics were developed. He influenced Judeo-Islamic philosophies during the Middle Ages, as well as Christian theology, especially the Neoplatonism of the Early Church and the scholastic tradition of the Catholic Church. Aristotle was revered among medieval Muslim scholars as \"The First Teacher\", and among medieval Christians like Thomas Aquinas as simply \"The Philosopher\", while the poet Dante called him \"the master of those who know\". He has been referred to as the first scientist. His works contain the earliest known formal study of logic, and were studied by medieval scholars such as Peter Abelard and Jean Buridan. His influence on logic continued well into the 19th century. In addition, his ethics, although always influential, has gained renewed interest with the modern advent of virtue ethics. In"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_2",
    "chunk": "general, the details of Aristotle's life are not well-established. The biographies written in ancient times are often speculative and historians only agree on a few salient points. Aristotle was born in 384 BC in Stagira, Chalcidice, about 55 km (34 miles) east of modern-day Thessaloniki. He was the son of Nicomachus, the personal physician of King Amyntas of Macedon, and Phaestis, a woman with origins from Chalcis, Euboea. Nicomachus was said to have belonged to the medical guild of Asclepiadae and was likely responsible for Aristotle's early interest in biology and medicine. Ancient tradition held that Aristotle's family descended from the legendary physician Asclepius and his son Machaon. Both of Aristotle's parents died when he was still at a young age and Proxenus of Atarneus became his guardian. Although little information about Aristotle's childhood has survived, he probably spent some time in the Macedonian capital, making his first connections with the Macedonian monarchy. At the age of seventeen or eighteen, Aristotle moved to Athens to continue his education at Plato's Academy. He became distinguished as a researcher and lecturer, earning for himself the nickname \"mind of the school\" by his tutor Plato. In Athens, he probably experienced the Eleusinian Mysteries"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_3",
    "chunk": "as he wrote when describing the sights one viewed at the Mysteries, \"to experience is to learn\" (παθεĩν μαθεĩν). Aristotle remained in Athens for nearly twenty years before leaving in 348/47 BC after Plato's death. The traditional story about his departure records that he was disappointed with the academy's direction after control passed to Plato's nephew Speusippus, although it is possible that the anti-Macedonian sentiments in Athens could have also influenced his decision. Aristotle left with Xenocrates to Assos in Asia Minor, where he was invited by his former fellow student Hermias of Atarneus; he stayed there for a few years and left around the time of Hermias' death. While at Assos, Aristotle and his colleague Theophrastus did extensive research in botany and marine biology, which they later continued at the near-by island of Lesbos. During this time, Aristotle married Pythias, Hermias's adoptive daughter and niece, and had a daughter whom they also named Pythias. In 343/42 BC, Aristotle was invited to Pella by Philip II of Macedon to become the tutor to his thirteen-year-old son Alexander; a choice perhaps influenced by the relationship of Aristotle's family with the Macedonian dynasty. Aristotle taught Alexander at the private school of Mieza,"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_4",
    "chunk": "in the gardens of the Nymphs, the royal estate near Pella. Alexander's education probably included a number of subjects, such as ethics and politics, as well as standard literary texts, like Euripides and Homer. It is likely that during Aristotle's time in the Macedonian court, other prominent nobles, like Ptolemy and Cassander, would have occasionally attended his lectures. Aristotle encouraged Alexander toward eastern conquest, and his own attitude towards Persia was strongly ethnocentric. In one famous example, he counsels Alexander to be \"a leader to the Greeks and a despot to the barbarians\". Alexander's education under the guardianship of Aristotle likely lasted for only a few years, as at around the age of sixteen he returned to Pella and was appointed regent of Macedon by his father Philip. During this time, Aristotle gifted Alexander an annotated copy of the Iliad, which is said to have become one of Alexander's most prized possessions. Scholars speculate that two of Aristotle's now lost works, On kingship and On behalf of the Colonies, were composed by the philosopher for the young prince. Aristotle returned to Athens for the second and final time a year after Philip II's assassination in 336 BC. As a metic,"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_5",
    "chunk": "Aristotle could not own property in Athens and thus rented a building known as the Lyceum (named after the sacred grove of Apollo Lykeios), in which he established his own school. The building included a gymnasium and a colonnade (peripatos), from which the school acquired the name Peripatetic. Aristotle conducted courses and research at the school for the next twelve years. He often lectured small groups of distinguished students and, along with some of them, such as Theophrastus, Eudemus, and Aristoxenus, Aristotle built a large library which included manuscripts, maps, and museum objects. While in Athens, his wife Pythias died and Aristotle became involved with Herpyllis of Stagira. They had a son whom Aristotle named after his father, Nicomachus. This period in Athens, between 335 and 323 BC, is when Aristotle is believed to have composed many of his philosophical works. He wrote many dialogues, of which only fragments have survived. Those works that have survived are in treatise form and were not, for the most part, intended for widespread publication; they are generally thought to be lecture aids for his students. His most important treatises include Physics, Metaphysics, Nicomachean Ethics, Politics, On the Soul and Poetics. Aristotle studied and"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_6",
    "chunk": "made significant contributions to \"logic, metaphysics, mathematics, physics, biology, botany, ethics, politics, agriculture, medicine, dance, and theatre.\" While Alexander deeply admired Aristotle, near the end of his life, the two men became estranged having diverging opinions over issues, like the optimal administration of city-states, the treatment of conquered populations, such as the Persians, and philosophical questions, like the definition of braveness. A widespread speculation in antiquity suggested that Aristotle played a role in Alexander's death, but the only evidence of this is an unlikely claim made some six years after the death. Following Alexander's death, anti-Macedonian sentiment in Athens was rekindled. In 322 BC, Demophilus and Eurymedon the Hierophant reportedly denounced Aristotle for impiety, prompting him to flee to his mother's family estate in Chalcis, Euboea, at which occasion he was said to have stated \"I will not allow the Athenians to sin twice against philosophy\" – a reference to Athens's trial and execution of Socrates. He died in Chalcis, Euboea of natural causes later that same year, having named his student Antipater as his chief executor and leaving a will in which he asked to be buried next to his wife. Aristotle left his works to Theophrastus, his successor"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_7",
    "chunk": "as the head of the Lyceum, who in turn passed them down to Neleus of Scepsis in Asia Minor. There, the papers remained hidden for protection until they were purchased by the collector Apellicon. In the meantime, many copies of Aristotle's major works had already begun to circulate and be used in the Lyceum of Athens, Alexandria, and later in Rome. With the Prior Analytics, Aristotle is credited with the earliest study of formal logic, and his conception of it was the dominant form of Western logic until 19th-century advances in mathematical logic. Kant stated in the Critique of Pure Reason that with Aristotle, logic reached its completion. Most of Aristotle's work is probably not in its original form, because it was most likely edited by students and later lecturers. The logical works of Aristotle were compiled into a set of six books called the Organon around 40 BC by Andronicus of Rhodes or others among his followers. The books are: The order of the books (or the teachings from which they are composed) is not certain, but this list was derived from analysis of Aristotle's writings. It goes from the basics, the analysis of simple terms in the Categories,"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_8",
    "chunk": "the analysis of propositions and their elementary relations in On Interpretation, to the study of more complex forms, namely, syllogisms (in the Analytics) and dialectics (in the Topics and Sophistical Refutations). The first three treatises form the core of the logical theory stricto sensu: the grammar of the language of logic and the correct rules of reasoning. The Rhetoric is not conventionally included, but it states that it relies on the Topics. What is today called Aristotelian logic with its types of syllogism (methods of logical argument), Aristotle himself would have labelled \"analytics\". The term \"logic\" he reserved to mean dialectics. The word \"metaphysics\" appears to have been coined by the first century AD editor who assembled various small selections of Aristotle's works to create the treatise we know by the name Metaphysics. Aristotle called it \"first philosophy\", and distinguished it from mathematics and natural science (physics) as the contemplative (theoretikē) philosophy which is \"theological\" and studies the divine. He wrote in his Metaphysics (1026a16): If there were no other independent things besides the composite natural ones, the study of nature would be the primary kind of knowledge; but if there is some motionless independent thing, the knowledge of this"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_9",
    "chunk": "precedes it and is first philosophy, and it is universal in just this way, because it is first. And it belongs to this sort of philosophy to study being as being, both what it is and what belongs to it just by virtue of being. Aristotle examines the concepts of substance (ousia) and essence (to ti ên einai, \"the what it was to be\") in his Metaphysics (Book VII), and he concludes that a particular substance is a combination of both matter and form, a philosophical theory called hylomorphism. In Book VIII, he distinguishes the matter of the substance as the substratum, or the stuff of which it is composed. For example, the matter of a house is the bricks, stones, timbers, etc., or whatever constitutes the potential house, while the form of the substance is the actual house, namely 'covering for bodies and chattels' or any other differentia that let us define something as a house. The formula that gives the components is the account of the matter, and the formula that gives the differentia is the account of the form. Like his teacher Plato, Aristotle's philosophy aims at the universal. Aristotle's ontology places the universal (katholou) in particulars"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_10",
    "chunk": "(kath' hekaston), things in the world, whereas for Plato the universal is a separately existing form which actual things imitate. For Aristotle, \"form\" is still what phenomena are based on, but is \"instantiated\" in a particular substance. Plato argued that all things have a universal form, which could be either a property or a relation to other things. When one looks at an apple, for example, one sees an apple, and one can also analyse a form of an apple. In this distinction, there is a particular apple and a universal form of an apple. Moreover, one can place an apple next to a book, so that one can speak of both the book and apple as being next to each other. Plato argued that there are some universal forms that are not a part of particular things. For example, it is possible that there is no particular good in existence, but \"good\" is still a proper universal form. Aristotle disagreed with Plato on this point, arguing that all universals are instantiated at some period of time, and that there are no universals that are unattached to existing things. In addition, Aristotle disagreed with Plato about the location of universals."
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_11",
    "chunk": "Where Plato spoke of the forms as existing separately from the things that participate in them, Aristotle maintained that universals exist within each thing on which each universal is predicated. So, according to Aristotle, the form of apple exists within each apple, rather than in the world of the forms. Concerning the nature of change (kinesis) and its causes, as he outlines in his Physics and On Generation and Corruption (319b–320a), he distinguishes coming-to-be (genesis, also translated as 'generation') from: Coming-to-be is a change where the substrate of the thing that has undergone the change has itself changed. In that particular change he introduces the concept of potentiality (dynamis) and actuality (entelecheia) in association with the matter and the form. Referring to potentiality, this is what a thing is capable of doing or being acted upon if the conditions are right and it is not prevented by something else. For example, the seed of a plant in the soil is potentially (dynamei) a plant, and if it is not prevented by something, it will become a plant. Potentially, beings can either 'act' (poiein) or 'be acted upon' (paschein), which can be either innate or learned. For example, the eyes possess"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_12",
    "chunk": "the potentiality of sight (innate – being acted upon), while the capability of playing the flute can be possessed by learning (exercise – acting). Actuality is the fulfilment of the end of the potentiality. Because the end (telos) is the principle of every change, and potentiality exists for the sake of the end, actuality, accordingly, is the end. Referring then to the previous example, it can be said that an actuality is when a plant does one of the activities that plants do. For that for the sake of which (to hou heneka) a thing is, is its principle, and the becoming is for the sake of the end; and the actuality is the end, and it is for the sake of this that the potentiality is acquired. For animals do not see in order that they may have sight, but they have sight that they may see. In summary, the matter used to make a house has potentiality to be a house and both the activity of building and the form of the final house are actualities, which is also a final cause or end. Then Aristotle proceeds and concludes that the actuality is prior to potentiality in formula,"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_13",
    "chunk": "in time and in substantiality. With this definition of the particular substance (i.e., matter and form), Aristotle tries to solve the problem of the unity of the beings, for example, \"what is it that makes a man one\"? Since, according to Plato there are two Ideas: animal and biped, how then is man a unity? However, according to Aristotle, the potential being (matter) and the actual one (form) are one and the same. Aristotle's immanent realism means his epistemology is based on the study of things that exist or happen in the world, and rises to knowledge of the universal, whereas for Plato epistemology begins with knowledge of universal Forms (or ideas) and descends to knowledge of particular imitations of these. Aristotle uses induction from examples alongside deduction, whereas Plato relies on deduction from a priori principles. Aristotle's \"natural philosophy\" spans a wide range of natural phenomena including those now covered by physics, biology and other natural sciences. In Aristotle's terminology, \"natural philosophy\" is a branch of philosophy examining the phenomena of the natural world, and includes fields that would be regarded today as physics, biology and other natural sciences. Aristotle's work encompassed virtually all facets of intellectual inquiry. Aristotle"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_14",
    "chunk": "makes philosophy in the broad sense coextensive with reasoning, which he also would describe as \"science\". However, his use of the term science carries a different meaning than that covered by the term \"scientific method\". For Aristotle, \"all science (dianoia) is either practical, poetical or theoretical\" (Metaphysics 1025b25). His practical science includes ethics and politics; his poetical science means the study of fine arts including poetry; his theoretical science covers physics, mathematics and metaphysics. In his On Generation and Corruption, Aristotle related each of the four elements proposed earlier by Empedocles, earth, water, air, and fire, to two of the four sensible qualities, hot, cold, wet, and dry. In the Empedoclean scheme, all matter was made of the four elements, in differing proportions. Aristotle's scheme added the heavenly aether, the divine substance of the heavenly spheres, stars and planets. Aristotle describes two kinds of motion: \"violent\" or \"unnatural motion\", such as that of a thrown stone, in the Physics (254b10), and \"natural motion\", such as of a falling object, in On the Heavens (300a20). In violent motion, as soon as the agent stops causing it, the motion stops also: in other words, the natural state of an object is to"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_15",
    "chunk": "be at rest, since Aristotle does not address friction. With this understanding, it can be observed that, as Aristotle stated, heavy objects (on the ground, say) require more force to make them move; and objects pushed with greater force move faster. This would imply the equation Natural motion depends on the element concerned: the aether naturally moves in a circle around the heavens, while the 4 Empedoclean elements move vertically up (like fire, as is observed) or down (like earth) towards their natural resting places. In the Physics (215a25), Aristotle effectively states a quantitative law, that the speed, v, of a falling body is proportional (say, with constant c) to its weight, W, and inversely proportional to the density, ρ, of the fluid in which it is falling:; Aristotle implies that in a vacuum the speed of fall would become infinite, and concludes from this apparent absurdity that a vacuum is not possible. Opinions have varied on whether Aristotle intended to state quantitative laws. Henri Carteron held the \"extreme view\" that Aristotle's concept of force was basically qualitative, but other authors reject this. Archimedes corrected Aristotle's theory that bodies move towards their natural resting places; metal boats can float if"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_16",
    "chunk": "they displace enough water; floating depends in Archimedes' scheme on the mass and volume of the object, not, as Aristotle thought, its elementary composition. Aristotle's writings on motion remained influential until the early modern period. John Philoponus (in late antiquity) and Galileo (in the early modern period) are said to have shown by experiment that Aristotle's claim that a heavier object falls faster than a lighter object is incorrect. A contrary opinion is given by Carlo Rovelli, who argues that Aristotle's physics of motion is correct within its domain of validity, that of objects in the Earth's gravitational field immersed in a fluid such as air. In this system, heavy bodies in steady fall indeed travel faster than light ones (whether friction is ignored, or not), and they do fall more slowly in a denser medium. Newton's \"forced\" motion corresponds to Aristotle's \"violent\" motion with its external agent, but Aristotle's assumption that the agent's effect stops immediately it stops acting (e.g., the ball leaves the thrower's hand) has awkward consequences: he has to suppose that surrounding fluid helps to push the ball along to make it continue to rise even though the hand is no longer acting on it, resulting"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_17",
    "chunk": "in the Medieval theory of impetus. Aristotle suggested that the reason for anything coming about can be attributed to four different types of simultaneously active factors. His term aitia is traditionally translated as \"cause\", but it does not always refer to temporal sequence; it might be better translated as \"explanation\", but the traditional rendering will be employed here. Aristotle describes experiments in optics using a camera obscura in Problems, book 15. The apparatus consisted of a dark chamber with a small aperture that let light in. With it, he saw that whatever shape he made the hole, the sun's image always remained circular. He also noted that increasing the distance between the aperture and the image surface magnified the image. According to Aristotle, spontaneity and chance are causes of some things, distinguishable from other types of cause such as simple necessity. Chance as an incidental cause lies in the realm of accidental things, \"from what is spontaneous\". There is also more a specific kind of chance, which Aristotle names \"luck\", that only applies to people's moral choices. In astronomy, Aristotle refuted Democritus's claim that the Milky Way was made up of \"those stars which are shaded by the earth from"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_18",
    "chunk": "the sun's rays,\" pointing out partly correctly that if \"the size of the sun is greater than that of the earth and the distance of the stars from the earth many times greater than that of the sun, then... the sun shines on all the stars and the earth screens none of them.\" He also wrote descriptions of comets, including the Great Comet of 371 BC. Aristotle was one of the first people to record any geological observations. He stated that geological change was too slow to be observed in one person's lifetime. The geologist Charles Lyell noted that Aristotle described such change, including \"lakes that had dried up\" and \"deserts that had become watered by rivers\", giving as examples the growth of the Nile delta since the time of Homer, and \"the upheaving of one of the Aeolian islands, previous to a volcanic eruption.\"' Meteorologica lends its name to the modern study of meteorology, but its modern usage diverges from the content of Aristotle's ancient treatise on meteors. The ancient Greeks did use the term for a range of atmospheric phenomena, but also for earthquakes and volcanic eruptions. Aristotle proposed that the cause of earthquakes was a gas or"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_19",
    "chunk": "vapor (anathymiaseis) that was trapped inside the earth and trying to escape, following other Greek authors Anaxagoras, Empedocles and Democritus. Aristotle also made many observations about the hydrologic cycle. For example, he made some of the earliest observations about desalination: he observed early – and correctly – that when seawater is heated, freshwater evaporates and that the oceans are then replenished by the cycle of rainfall and river runoff (\"I have proved by experiment that salt water evaporated forms fresh and the vapor does not when it condenses condense into sea water again.\") Aristotle was the first person to study biology systematically, and biology forms a large part of his writings. He spent two years observing and describing the zoology of Lesbos and the surrounding seas, including in particular the Pyrrha lagoon in the centre of Lesbos. His data in History of Animals, Generation of Animals, Movement of Animals, and Parts of Animals are assembled from his own observations, statements given by people with specialized knowledge, such as beekeepers and fishermen, and less accurate accounts provided by travellers from overseas. His apparent emphasis on animals rather than plants is a historical accident: his works on botany have been lost, but"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_20",
    "chunk": "two books on plants by his pupil Theophrastus have survived. Aristotle reports on the sea-life visible from observation on Lesbos and the catches of fishermen. He describes the catfish, electric ray, and frogfish in detail, as well as cephalopods such as the octopus and paper nautilus. His description of the hectocotyl arm of cephalopods, used in sexual reproduction, was widely disbelieved until the 19th century. He gives accurate descriptions of the four-chambered fore-stomachs of ruminants, and of the ovoviviparous embryological development of the hound shark. He notes that an animal's structure is well matched to function so birds like the heron (which live in marshes with soft mud and live by catching fish) have a long neck, long legs, and a sharp spear-like beak, whereas ducks that swim have short legs and webbed feet. Darwin, too, noted these sorts of differences between similar kinds of animal, but unlike Aristotle used the data to come to the theory of evolution. Aristotle's writings can seem to modern readers close to implying evolution, but while Aristotle was aware that new mutations or hybridizations could occur, he saw these as rare accidents. For Aristotle, accidents, like heat waves in winter, must be considered distinct"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_21",
    "chunk": "from natural causes. He was thus critical of Empedocles's materialist theory of a \"survival of the fittest\" origin of living things and their organs, and ridiculed the idea that accidents could lead to orderly results. To put his views into modern terms, he nowhere says that different species can have a common ancestor, or that one kind can change into another, or that kinds can become extinct. Aristotle did not do experiments in the modern sense. He used the ancient Greek term pepeiramenoi to mean observations, or at most investigative procedures like dissection. In Generation of Animals, he finds a fertilized hen's egg of a suitable stage and opens it to see the embryo's heart beating inside. Instead, he practiced a different style of science: systematically gathering data, discovering patterns common to whole groups of animals, and inferring possible causal explanations from these. This style is common in modern biology when large amounts of data become available in a new field, such as genomics. It does not result in the same certainty as experimental science, but it sets out testable hypotheses and constructs a narrative explanation of what is observed. In this sense, Aristotle's biology is scientific. From the data"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_22",
    "chunk": "he collected and documented, Aristotle inferred quite a number of rules relating the life-history features of the live-bearing tetrapods (terrestrial placental mammals) that he studied. Among these correct predictions are the following. Brood size decreases with (adult) body mass, so that an elephant has fewer young (usually just one) per brood than a mouse. Lifespan increases with gestation period, and also with body mass, so that elephants live longer than mice, have a longer period of gestation, and are heavier. As a final example, fecundity decreases with lifespan, so long-lived kinds like elephants have fewer young in total than short-lived kinds like mice. Aristotle distinguished about 500 species of animals, arranging these in the History of Animals in a graded scale of perfection, a nonreligious version of the scala naturae, with man at the top. His system had eleven grades of animal, from highest potential to lowest, expressed in their form at birth: the highest gave live birth to hot and wet creatures, the lowest laid cold, dry mineral-like eggs. Animals came above plants, and these in turn were above minerals. He grouped what the modern zoologist would call vertebrates as the hotter \"animals with blood\", and below them the"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_23",
    "chunk": "colder invertebrates as \"animals without blood\". Those with blood were divided into the live-bearing (mammals), and the egg-laying (birds, reptiles, fish). Those without blood were insects, crustacea (non-shelled – cephalopods, and shelled) and the hard-shelled molluscs (bivalves and gastropods). He recognised that animals did not exactly fit into a linear scale, and noted various exceptions, such as that sharks had a placenta like the tetrapods. To a modern biologist, the explanation, not available to Aristotle, is convergent evolution. Philosophers of science have generally concluded that Aristotle was not interested in taxonomy, but zoologists who studied this question in the early 21st century think otherwise. He believed that purposive final causes guided all natural processes; this teleological view justified his observed data as an expression of formal design. Aristotle's psychology, given in his treatise On the Soul (peri psychēs), posits three kinds of soul (psyches): the vegetative soul, the sensitive soul, and the rational soul. Humans have all three. The vegetative soul is concerned with growth and nourishment. The sensitive soul experiences sensations and movement. The unique part of the human, rational soul is its ability to receive forms of other things and to compare them using the nous (intellect) and"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_24",
    "chunk": "logos (reason). For Aristotle, the soul is the form of a living being. Because all beings are composites of form and matter, the form of living beings is that which endows them with what is specific to living beings, e.g. the ability to initiate movement (or in the case of plants, growth and transformations, which Aristotle considers types of movement). In contrast to earlier philosophers, but in accordance with the Egyptians, he placed the rational soul in the heart, rather than the brain. Notable is Aristotle's division of sensation and thought, which generally differed from the concepts of previous philosophers, with the exception of Alcmaeon. In On the Soul, Aristotle famously criticizes Plato's theory of the soul and develops his own in response. The first criticism is against Plato's view of the soul in the Timaeus that the soul takes up space and is able to come into physical contact with bodies. 20th-century scholarship overwhelmingly opposed Aristotle's interpretation of Plato and maintained that he had misunderstood him. Today's scholars have tended to re-assess Aristotle's interpretation and been more positive about it. Aristotle's other criticism is that Plato's view of reincarnation entails that it is possible for a soul and its"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_25",
    "chunk": "body to be mis-matched; in principle, Aristotle alleges, any soul can go with any body, according to Plato's theory. Aristotle's claim that the soul is the form of a living being eliminates that possibility and thus rules out reincarnation. According to Aristotle in On the Soul, memory is the ability to hold a perceived experience in the mind and to distinguish between the internal \"appearance\" and an occurrence in the past. In other words, a memory is a mental picture (phantasm) that can be recovered. Aristotle believed an impression is left on a semi-fluid bodily organ that undergoes several changes in order to make a memory. A memory occurs when stimuli such as sights or sounds are so complex that the nervous system cannot receive all the impressions at once. These changes are the same as those involved in the operations of sensation, Aristotelian 'common sense', and thinking. Aristotle uses the term 'memory' for the actual retaining of an experience in the impression that can develop from sensation, and for the intellectual anxiety that comes with the impression because it is formed at a particular time and processing specific contents. Memory is of the past, prediction is of the future,"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_26",
    "chunk": "and sensation is of the present. Retrieval of impressions cannot be performed suddenly. A transitional channel is needed and located in past experiences, both for previous experience and present experience. Because Aristotle believes people receive all kinds of sense perceptions and perceive them as impressions, people are continually weaving together new impressions of experiences. To search for these impressions, people search the memory itself. Within the memory, if one experience is offered instead of a specific memory, that person will reject this experience until they find what they are looking for. Recollection occurs when one retrieved experience naturally follows another. If the chain of \"images\" is needed, one memory will stimulate the next. When people recall experiences, they stimulate certain previous experiences until they reach the one that is needed. Recollection is thus the self-directed activity of retrieving the information stored in a memory impression. Only humans can remember impressions of intellectual activity, such as numbers and words. Animals that have perception of time can retrieve memories of their past observations. Remembering involves only perception of the things remembered and of the time passed. Aristotle believed the chain of thought, which ends in recollection of certain impressions, was connected systematically"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_27",
    "chunk": "in relationships such as similarity, contrast, and contiguity, described in his laws of association. Aristotle believed that past experiences are hidden within the mind. A force operates to awaken the hidden material to bring up the actual experience. According to Aristotle, association is the power innate in a mental state, which operates upon the unexpressed remains of former experiences, allowing them to rise and be recalled. Aristotle describes sleep in On Sleep and Wakefulness. Sleep takes place as a result of overuse of the senses or of digestion, so it is vital to the body. While a person is asleep, the critical activities, which include thinking, sensing, recalling and remembering, do not function as they do during wakefulness. Since a person cannot sense during sleep, they cannot have desire, which is the result of sensation. However, the senses are able to work during sleep, albeit differently, unless they are weary. Dreams do not involve actually sensing a stimulus. In dreams, sensation is still involved, but in an altered manner. Aristotle explains that when a person stares at a moving stimulus such as the waves in a body of water, and then looks away, the next thing they look at appears"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_28",
    "chunk": "to have a wavelike motion. When a person perceives a stimulus and the stimulus is no longer the focus of their attention, it leaves an impression. When the body is awake and the senses are functioning properly, a person constantly encounters new stimuli to sense and so the impressions of previously perceived stimuli are ignored. However, during sleep the impressions made throughout the day are noticed as there are no new distracting sensory experiences. So, dreams result from these lasting impressions. Since impressions are all that are left and not the exact stimuli, dreams do not resemble the actual waking experience. During sleep, a person is in an altered state of mind. Aristotle compares a sleeping person to a person who is overtaken by strong feelings toward a stimulus. For example, a person who has a strong infatuation with someone may begin to think they see that person everywhere because they are so overtaken by their feelings. Since a person sleeping is in a suggestible state and unable to make judgements, they become easily deceived by what appears in their dreams, like the infatuated person. This leads the person to believe the dream is real, even when the dreams are"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_29",
    "chunk": "absurd in nature. In De Anima iii 3, Aristotle ascribes the ability to create, to store, and to recall images in the absence of perception to the faculty of imagination, phantasia. One component of Aristotle's theory of dreams disagrees with previously held beliefs. He claimed that dreams are not foretelling and not sent by a divine being. Aristotle reasoned naturalistically that instances in which dreams do resemble future events are simply coincidences. Aristotle claimed that a dream is first established by the fact that the person is asleep when they experience it. If a person had an image appear for a moment after waking up or if they see something in the dark it is not considered a dream because they were awake when it occurred. Secondly, any sensory experience that is perceived while a person is asleep does not qualify as part of a dream. For example, if, while a person is sleeping, a door shuts and in their dream they hear a door is shut, this sensory experience is not part of the dream. Lastly, the images of dreams must be a result of lasting impressions of waking sensory experiences. Aristotle's practical philosophy covers areas such as ethics,"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_30",
    "chunk": "politics, economics, and rhetoric. Aristotle considered ethics to be a practical rather than theoretical study, i.e., one aimed at becoming good and doing good rather than knowing for its own sake. He wrote several treatises on ethics, most notably including the Nicomachean Ethics. Aristotle taught that virtue has to do with the proper function (ergon) of a thing. An eye is only a good eye in so much as it can see because the proper function of an eye is sight. Aristotle reasoned that humans must have a function specific to humans, and that this function must be an activity of the psuchē (soul) in accordance with reason (logos). Aristotle identified such an optimum activity (the virtuous mean, between the accompanying vices of excess or deficiency) of the soul as the aim of all human deliberate action, eudaimonia, generally translated as \"happiness\" or sometimes \"well-being\". To have the potential of ever being happy in this way necessarily requires a good character (ēthikē aretē), often translated as moral or ethical virtue or excellence. Aristotle taught that to achieve a virtuous and potentially happy character requires a first stage of having the fortune to be habituated not deliberately, but by teachers, and"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_31",
    "chunk": "experience, leading to a later stage in which one consciously chooses to do the best things. When the best people come to live life this way their practical wisdom (phronesis) and their intellect (nous) can develop with each other towards the highest possible human virtue, the wisdom of an accomplished theoretical or speculative thinker, or in other words, a philosopher. In addition to his works on ethics, which address the individual, Aristotle addressed the city in his work titled Politics. Aristotle considered the city to be a natural community. Moreover, he considered the city to be prior in importance to the family, which in turn is prior to the individual, \"for the whole must of necessity be prior to the part\". He famously stated that \"man is by nature a political animal\" and argued that humanity's defining factor among others in the animal kingdom is its rationality. Aristotle conceived of politics as being like an organism rather than like a machine, and as a collection of parts none of which can exist without the others. Aristotle's conception of the city is organic, and he is considered one of the first to conceive of the city in this manner. The common"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_32",
    "chunk": "modern understanding of a political community as a modern state is quite different from Aristotle's understanding. Although he was aware of the existence and potential of larger empires, the natural community according to Aristotle was the city (polis) which functions as a political \"community\" or \"partnership\" (koinōnia). The aim of the city is not just to avoid injustice or for economic stability, but rather to allow at least some citizens the possibility to live a good life, and to perform beautiful acts: \"The political partnership must be regarded, therefore, as being for the sake of noble actions, not for the sake of living together.\" This is distinguished from modern approaches, beginning with social contract theory, according to which individuals leave the state of nature because of \"fear of violent death\" or its \"inconveniences\". For we all agree that the most excellent man should rule, i.e., the supreme by nature, and that the law rules and alone is authoritative; but the law is a kind of intelligence, i.e. a discourse based on intelligence. And again, what standard do we have, what criterion of good things, that is more precise than the intelligent man? For all that this man will choose, if"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_33",
    "chunk": "the choice is based on his knowledge, are good things and their contraries are bad. And since everybody chooses most of all what conforms to their own proper dispositions (a just man choosing to live justly, a man with bravery to live bravely, likewise a self-controlled man to live with self-control), it is clear that the intelligent man will choose most of all to be intelligent; for this is the function of that capacity. Hence it's evident that, according to the most authoritative judgment, intelligence is supreme among goods. As Plato's disciple Aristotle was rather critical concerning democracy and, following the outline of certain ideas from Plato's Statesman, he developed a coherent theory of integrating various forms of power into a so-called mixed state: It is ... constitutional to take ... from oligarchy that offices are to be elected, and from democracy that this is not to be on a property-qualification. This then is the mode of the mixture; and the mark of a good mixture of democracy and oligarchy is when it is possible to speak of the same constitution as a democracy and as an oligarchy. Aristotle made substantial contributions to economic thought, especially to thought in the"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_34",
    "chunk": "Middle Ages. In Politics, Aristotle addresses the city, property, and trade. His response to criticisms of private property, in Lionel Robbins's view, anticipated later proponents of private property among philosophers and economists, as it related to the overall utility of social arrangements. Aristotle believed that although communal arrangements may seem beneficial to society, and that although private property is often blamed for social strife, such evils in fact come from human nature. In Politics, Aristotle offers one of the earliest accounts of the origin of money. Money came into use because people became dependent on one another, importing what they needed and exporting the surplus. For the sake of convenience, people then agreed to deal in something that is intrinsically useful and easily applicable, such as iron or silver. Aristotle's discussions on retail and interest was a major influence on economic thought in the Middle Ages. He had a low opinion of retail, believing that contrary to using money to procure things one needs in managing the household, retail trade seeks to make a profit. It thus uses goods as a means to an end, rather than as an end unto itself. He believed that retail trade was in this"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_35",
    "chunk": "way unnatural. Similarly, Aristotle considered making a profit through interest unnatural, as it makes a gain out of the money itself, and not from its use. Aristotle gave a summary of the function of money that was perhaps remarkably precocious for his time. He wrote that because it is impossible to determine the value of every good through a count of the number of other goods it is worth, the necessity arises of a single universal standard of measurement. Money thus allows for the association of different goods and makes them \"commensurable\". He goes on to state that money is also useful for future exchange, making it a sort of security. That is, \"if we do not want a thing now, we shall be able to get it when we do want it\". Aristotle's Rhetoric proposes that a speaker can use three basic kinds of appeals to persuade his audience: ethos (an appeal to the speaker's character), pathos (an appeal to the audience's emotion), and logos (an appeal to logical reasoning). He also categorizes rhetoric into three genres: epideictic (ceremonial speeches dealing with praise or blame), forensic (judicial speeches over guilt or innocence), and deliberative (speeches calling on an audience"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_36",
    "chunk": "to decide on an issue). Aristotle also outlines two kinds of rhetorical proofs: enthymeme (proof by syllogism) and paradeigma (proof by example). Aristotle writes in his Poetics that epic poetry, tragedy, comedy, dithyrambic poetry, painting, sculpture, music, and dance are all fundamentally acts of mimesis (\"imitation\"), each varying in imitation by medium, object, and manner. He applies the term mimesis both as a property of a work of art and also as the product of the artist's intention and contends that the audience's realisation of the mimesis is vital to understanding the work itself. Aristotle states that mimesis is a natural instinct of humanity that separates humans from animals and that all human artistry \"follows the pattern of nature\". Because of this, Aristotle believed that each of the mimetic arts possesses what Stephen Halliwell calls \"highly structured procedures for the achievement of their purposes.\" For example, music imitates with the media of rhythm and harmony, whereas dance imitates with rhythm alone, and poetry with language. The forms also differ in their object of imitation. Comedy, for instance, is a dramatic imitation of men worse than average; whereas tragedy imitates men slightly better than average. Lastly, the forms differ in their"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_37",
    "chunk": "manner of imitation – through narrative or character, through change or no change, and through drama or no drama. While it is believed that Aristotle's Poetics originally comprised two books – one on comedy and one on tragedy – only the portion that focuses on tragedy has survived. Aristotle taught that tragedy is composed of six elements: plot-structure, character, style, thought, spectacle, and lyric poetry. The characters in a tragedy are merely a means of driving the story; and the plot, not the characters, is the chief focus of tragedy. Tragedy is the imitation of action arousing pity and fear, and is meant to effect the catharsis of those same emotions. Aristotle concludes Poetics with a discussion on which, if either, is superior: epic or tragic mimesis. He suggests that because tragedy possesses all the attributes of an epic, possibly possesses additional attributes such as spectacle and music, is more unified, and achieves the aim of its mimesis in shorter scope, it can be considered superior to epic. Aristotle was a keen systematic collector of riddles, folklore, and proverbs; he and his school had a special interest in the riddles of the Delphic Oracle and studied the fables of Aesop."
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_38",
    "chunk": "Aristotle never wrote a specific work on women. However, he asserted the existence of differences between men and women throughout his biological, political, and ethical works. For most female animals, including human women, Aristotle maintains that they are for the most part physically smaller and of a more cowardly constitution. From these comments in his biological works, he often connects the idea that women are inferior with their need to be ruled over by men. Proponents of feminist philosophy question the extent to which Aristotle's philosophy relies on misogynistic and sexist tenets. Within the same works, however, there is still concern for women's happiness and participation within the city. For instance, women are meant to be consulted on household decisions, are praised for their tenderness to children, and expected to participate in religious festivals. More than 2300 years after his death, Aristotle remains one of the most influential people who ever lived. He contributed to almost every field of human knowledge then in existence, and he was the founder of many new fields. According to the philosopher Bryan Magee, \"it is doubtful whether any human being has ever known as much as he did\". Aristotle has been regarded as the"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_39",
    "chunk": "first scientist. Among countless other achievements, Aristotle was the founder of formal logic, pioneered the study of zoology, and left every future scientist and philosopher in his debt through his contributions to the scientific method. Taneli Kukkonen, observes that his achievement in founding two sciences is unmatched, and his reach in influencing \"every branch of intellectual enterprise\" including Western ethical and political theory, theology, rhetoric, and literary analysis is equally long. As a result, Kukkonen argues, any analysis of reality today \"will almost certainly carry Aristotelian overtones ... evidence of an exceptionally forceful mind.\" Jonathan Barnes wrote that \"an account of Aristotle's intellectual afterlife would be little less than a history of European thought\". Aristotle has been called the father of logic, biology, political science, zoology, embryology, natural law, scientific method, rhetoric, psychology, realism, criticism, individualism, teleology, and meteorology. The scholar Taneli Kukkonen notes that \"in the best 20th-century scholarship Aristotle comes alive as a thinker wrestling with the full weight of the Greek philosophical tradition.\" What follows is an overview of the transmission and influence of his texts and ideas into the modern era. Aristotle's pupil and successor, Theophrastus, wrote the History of Plants, a pioneering work in botany."
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_40",
    "chunk": "Some of his technical terms remain in use, such as carpel from carpos, fruit, and pericarp, from pericarpion, seed chamber. Theophrastus was much less concerned with formal causes than Aristotle was, instead pragmatically describing how plants functioned. The immediate influence of Aristotle's work was felt as the Lyceum grew into the Peripatetic school. Aristotle's students included Aristoxenus, Dicaearchus, Demetrius of Phalerum, Eudemos of Rhodes, Harpalus, Hephaestion, Mnason of Phocis, Nicomachus, and Theophrastus. Aristotle's influence over Alexander the Great is seen in the latter's bringing with him on his expedition a host of zoologists, botanists, and researchers. He had also learned a great deal about Persian customs and traditions from his teacher. Although his respect for Aristotle was diminished as his travels made it clear that much of Aristotle's geography was clearly wrong, when the old philosopher released his works to the public, Alexander complained \"Thou hast not done well to publish thy acroamatic doctrines; for in what shall I surpass other men if those doctrines wherein I have been trained are to be all men's common property?\" After Theophrastus, the Lyceum failed to produce any original work. Though interest in Aristotle's ideas survived, they were generally taken unquestioningly. It is"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_41",
    "chunk": "not until the age of Alexandria under the Ptolemies that advances in biology can be again found. The first medical teacher at Alexandria, Herophilus of Chalcedon, corrected Aristotle, placing intelligence in the brain, and connected the nervous system to motion and sensation. Herophilus also distinguished between veins and arteries, noting that the latter pulse while the former do not. Though a few ancient atomists such as Lucretius challenged the teleological viewpoint of Aristotelian ideas about life, teleology (and after the rise of Christianity, natural theology) would remain central to biological thought essentially until the 18th and 19th centuries. Ernst Mayr states that there was \"nothing of any real consequence in biology after Lucretius and Galen until the Renaissance.\" Following the decline of the Roman Empire, Aristotle's vast philosophical and scientific corpus lay largely dormant in the West. However, his works underwent a remarkable revival in the Abbasid Caliphate. Translated into Arabic alongside other Greek classics, Aristotle's logic, ethics, and natural philosophy ignited the minds of early Islamic scholars. Through meticulous commentaries and critical engagements, figures like Al-Farabi and Ibn Sina (Avicenna) breathed new life into Aristotle's ideas. They harmonized his logic with Islamic theology, employed his scientific methodologies to explore"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_42",
    "chunk": "the natural world, and even reinterpreted his ethics within the framework of Islamic morality. This revival was not mere imitation. Islamic thinkers embraced Aristotle's rigorous methods while simultaneously challenging his conclusions where they diverged from their own religious beliefs. Greek Christian scribes played a crucial role in the preservation of Aristotle by copying all the extant Greek language manuscripts of the corpus. The first Greek Christians to comment extensively on Aristotle were Philoponus, Elias, and David in the sixth century, and Stephen of Alexandria in the early seventh century. John Philoponus stands out for having attempted a fundamental critique of Aristotle's views on the eternity of the world, movement, and other elements of Aristotelian thought. Philoponus questioned Aristotle's teaching of physics, noting its flaws and introducing the theory of impetus to explain his observations. After a hiatus of several centuries, formal commentary by Eustratius and Michael of Ephesus reappeared in the late eleventh and early twelfth centuries, apparently sponsored by Anna Comnena. Aristotle is considered the most influential figure in the history of Arabic philosophy and was one of the most revered thinkers in early Islamic theology. Most of the still extant works of Aristotle, as well as a number"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_43",
    "chunk": "of the original Greek commentaries, were translated into Arabic and studied by Muslim philosophers, scientists, and scholars. Averroes, Avicenna, and Alpharabius, who wrote on Aristotle in great depth, also influenced Thomas Aquinas and other Western Christian scholastic philosophers. Alkindus greatly admired Aristotle's philosophy, and Averroes spoke of Aristotle as the \"exemplar\" for all future philosophers. Medieval Muslim scholars regularly described Aristotle as the \"First Teacher\". The title was later used by Western philosophers (as in the famous poem of Dante) who were influenced by the tradition of Islamic philosophy. With the loss of the study of ancient Greek in the early medieval Latin West, Aristotle was practically unknown there from c. CE 600 to c. 1100 except through the Latin translation of the Organon made by Boethius. In the twelfth and thirteenth centuries, interest in Aristotle revived and Latin Christians had translations made, both from Arabic translations, such as those by Gerard of Cremona, and from the original Greek, such as those by James of Venice and William of Moerbeke. After the Scholastic Thomas Aquinas wrote his Summa Theologica, working from Moerbeke's translations and calling Aristotle \"The Philosopher\", the demand for Aristotle's writings grew, and the Greek manuscripts returned to"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_44",
    "chunk": "the West, stimulating a revival of Aristotelianism in Europe that continued into the Renaissance. These thinkers blended Aristotelian philosophy with Christianity, bringing the thought of Ancient Greece into the Middle Ages. Scholars such as Boethius, Peter Abelard, and John Buridan worked on Aristotelian logic. According to scholar Roger Theodore Lafferty, Dante built up the philosophy of the Comedy with the works of Aristotle as a foundation, just as the scholastics used Aristotle as the basis for their thinking. Dante knew Aristotle directly from Latin translations of his works and indirectly through quotations in the works of Albert Magnus. Dante even acknowledges Aristotle's influence explicitly in the poem, specifically when Virgil justifies the Inferno's structure by citing the Nicomachean Ethics. Dante famously refers to him as \"he / Who is acknowledged Master of those who know\". Moses Maimonides (considered to be the foremost intellectual figure of medieval Judaism) adopted Aristotelianism from the Islamic scholars and based his Guide for the Perplexed on it and that became the basis of Jewish scholastic philosophy. Maimonides also considered Aristotle to be the greatest philosopher that ever lived, and styled him as the \"chief of the philosophers\". Also, in his letter to Samuel ibn Tibbon,"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_45",
    "chunk": "Maimonides observes that there is no need for Samuel to study the writings of philosophers who preceded Aristotle because the works of the latter are \"sufficient by themselves and [superior] to all that were written before them. His intellect, Aristotle's is the extreme limit of human intellect, apart from him upon whom the divine emanation has flowed forth to such an extent that they reach the level of prophecy, there being no level higher\". In the early modern period, scientists such as William Harvey in England and Galileo Galilei in Italy reacted against the theories of Aristotle and other classical era thinkers like Galen, establishing new theories based to some degree on observation and experiment. Harvey demonstrated the circulation of the blood, establishing that the heart functioned as a pump rather than being the seat of the soul and the controller of the body's heat, as Aristotle thought. Galileo used more doubtful arguments to displace Aristotle's physics, proposing that bodies all fall at the same speed whatever their weight. The English mathematician George Boole fully accepted Aristotle's logic, but decided \"to go under, over, and beyond\" it with his system of algebraic logic in his 1854 book The Laws of"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_46",
    "chunk": "Thought. This gives logic a mathematical foundation with equations, enables it to solve equations as well as check validity, and allows it to handle a wider class of problems by expanding propositions of any number of terms, not just two. Charles Darwin regarded Aristotle as the most important contributor to the subject of biology. In an 1882 letter he wrote that \"Linnaeus and Cuvier have been my two gods, though in very different ways, but they were mere schoolboys to old Aristotle\". Also, in later editions of the book \"On the Origin of Species', Darwin traced evolutionary ideas as far back as Aristotle; the text he cites is a summary by Aristotle of the ideas of the earlier Greek philosopher Empedocles. The philosopher Bertrand Russell claims that \"almost every serious intellectual advance has had to begin with an attack on some Aristotelian doctrine\". Russell calls Aristotle's ethics \"repulsive\", and labelled his logic \"as definitely antiquated as Ptolemaic astronomy\". Russell states that these errors make it difficult to do historical justice to Aristotle, until one remembers what an advance he made upon all of his predecessors. The Dutch historian of science Eduard Jan Dijksterhuis writes that Aristotle and his predecessors showed"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_47",
    "chunk": "the difficulty of science by \"proceed[ing] so readily to frame a theory of such a general character\" on limited evidence from their senses. In 1985, the biologist Peter Medawar could still state in \"pure seventeenth century\" tones that Aristotle had assembled \"a strange and generally speaking rather tiresome farrago of hearsay, imperfect observation, wishful thinking and credulity amounting to downright gullibility\". Zoologists have frequently mocked Aristotle for errors and unverified secondhand reports. However, modern observation has confirmed several of his more surprising claims. Aristotle's work remains largely unknown to modern scientists, though zoologists sometimes mention him as the father of biology or in particular of marine biology. Practising zoologists are unlikely to adhere to Aristotle's chain of being, but its influence is still perceptible in the use of the terms \"lower\" and \"upper\" to designate taxa such as groups of plants. The evolutionary biologist Armand Marie Leroi has reconstructed Aristotle's biology, while Niko Tinbergen's four questions, based on Aristotle's four causes, are used to analyse animal behaviour; they examine function, phylogeny, mechanism, and ontogeny. The concept of homology began with Aristotle; the evolutionary developmental biologist Lewis I. Held commented that he would be interested in the concept of deep homology."
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_48",
    "chunk": "In systematics too, recent studies suggest that Aristotle made important contributions in taxonomy and biological nomenclature. The works of Aristotle that have survived from antiquity through medieval manuscript transmission are collected in the Corpus Aristotelicum. These texts, as opposed to Aristotle's lost works, are technical philosophical treatises from within Aristotle's school. Reference to them is made according to the organization of Immanuel Bekker's Royal Prussian Academy edition (Aristotelis Opera edidit Academia Regia Borussica, Berlin, 1831–1870), which in turn is based on ancient classifications of these works. Aristotle wrote his works on papyrus scrolls, the common writing medium of that era. His writings are divisible into two groups: the \"exoteric\", intended for the public, and the \"esoteric\", for use within the Lyceum school. Aristotle's \"lost\" works stray considerably in characterization from the surviving Aristotelian corpus. Whereas the lost works appear to have been originally written with a view to subsequent publication, the surviving works mostly resemble lecture notes not intended for publication. Cicero's description of Aristotle's literary style as \"a river of gold\" must have applied to the published works, not the surviving notes. A major question in the history of Aristotle's works is how the exoteric writings were all lost,"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_49",
    "chunk": "and how the ones now possessed came to be found. The consensus is that Andronicus of Rhodes collected the esoteric works of Aristotle's school which existed in the form of smaller, separate works, distinguished them from those of Theophrastus and other Peripatetics, edited them, and finally compiled them into the more cohesive, larger works as they are known today. According to Strabo and Plutarch, after Aristotle's death, his library and writings went to Theophrastus (Aristotle's successor as head of the Lyceum and the Peripatetic school). After the death of Theophrastus, the peripatetic library went to Neleus of Scepsis. Some time later, the Kingdom of Pergamon began conscripting books for a royal library, and the heirs of Neleus hid their collection in a cellar to prevent it from being seized for that purpose. The library was stored there for about a century and a half, in conditions that were not ideal for document preservation. On the death of Attalus III, which also ended the royal library ambitions, the existence of Aristotelian library was disclosed, and it was purchased by Apellicon and returned to Athens c. 100 BC. Apellicon sought to recover the texts, many of which were seriously degraded at this"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_50",
    "chunk": "point due to the conditions in which they were stored. He had them copied out into new manuscripts, and used his best guesswork to fill in the gaps where the originals were unreadable. When Sulla seized Athens in 86 BC, he seized the library and transferred it to Rome. There, Andronicus of Rhodes organized the texts into the first complete edition of Aristotle's works (and works attributed to him). The Aristotelian texts we have today are based on these. Aristotle has been depicted by major artists including Lucas Cranach the Elder, Justus van Gent, Raphael, Paolo Veronese, Jusepe de Ribera, Rembrandt, and Francesco Hayez over the centuries. Among the best-known depictions is Raphael's fresco The School of Athens, in the Vatican's Apostolic Palace, where the figures of Plato and Aristotle are central to the image, at the architectural vanishing point, reflecting their importance. Rembrandt's Aristotle with a Bust of Homer, too, is a celebrated work, showing the knowing philosopher and the blind Homer from an earlier age: as the art critic Jonathan Jones writes, \"this painting will remain one of the greatest and most mysterious in the world, ensnaring us in its musty, glowing, pitch-black, terrible knowledge of time.\" The"
  },
  {
    "source": "Aristotle.txt",
    "chunk_id": "Aristotle.txt_51",
    "chunk": "Aristotle Mountains in Antarctica are named after Aristotle. He was the first person known to conjecture, in his book Meteorology, the existence of a landmass in the southern high-latitude region, which he called Antarctica. Aristoteles is a crater on the Moon bearing the classical form of Aristotle's name. (6123) Aristoteles, an asteroid in the main asteroid belt is also bearing the classical form of his name. The secondary literature on Aristotle is vast. The following is only a small selection."
  },
  {
    "source": "arXiv.txt",
    "chunk_id": "arXiv.txt_0",
    "chunk": "# arXiv arXiv (pronounced as \"archive\"—the X represents the Greek letter chi ⟨χ⟩) is an open-access repository of electronic preprints and postprints (known as e-prints) approved for posting after moderation, but not peer reviewed. It consists of scientific papers in the fields of mathematics, physics, astronomy, electrical engineering, computer science, quantitative biology, statistics, mathematical finance, and economics, which can be accessed online. In many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository before publication in a peer-reviewed journal. Some publishers also grant permission for authors to archive the peer-reviewed postprint. Begun on August 14, 1991, arXiv.org passed the half-million-article milestone on October 3, 2008, had hit a million by the end of 2014 and two million by the end of 2021. As of November 2024, the submission rate is about 24,000 articles per month. arXiv was made possible by the compact TeX file format, which allowed scientific papers to be easily transmitted over the Internet and rendered client-side. Around 1990, Joanne Cohn began emailing physics preprints to colleagues as TeX files, but the number of papers being sent soon filled mailboxes to capacity. Paul Ginsparg recognized the need for central storage, and in"
  },
  {
    "source": "arXiv.txt",
    "chunk_id": "arXiv.txt_1",
    "chunk": "August 1991 he created a central repository mailbox stored at the Los Alamos National Laboratory (LANL) that could be accessed from any computer. Additional modes of access were soon added: FTP in 1991, Gopher in 1992, and the World Wide Web in 1993. The term e-print was quickly adopted to describe the articles. It began as a physics archive, called the LANL preprint archive, but soon expanded to include astronomy, mathematics, computer science, quantitative biology and, most recently, statistics. Its original domain name was xxx.lanl.gov. Due to LANL's lack of interest in the rapidly expanding technology, in 2001 Ginsparg changed institutions to Cornell University and changed the name of the repository to arXiv.org. Ginsparg brainstormed the new name with his wife; the domain \"archive\" was already claimed, so \"chi\" was replaced with \"X\" standing in as the Greek letter chi and the \"e\" dropped for symmetry around the \"X\". arXiv was an early adopter and promoter of preprints. Its success in sharing preprints was one of the precipitating factors that led to the later movement in scientific publishing known as open access. Mathematicians and scientists regularly upload their papers to arXiv.org for worldwide access and sometimes for reviews before they"
  },
  {
    "source": "arXiv.txt",
    "chunk_id": "arXiv.txt_2",
    "chunk": "are published in peer-reviewed journals. Ginsparg was awarded a MacArthur Fellowship in 2002 for his establishment of arXiv. The annual budget for arXiv was approximately $826,000 for 2013 to 2017, funded jointly by Cornell University Library, the Simons Foundation (in both gift and challenge grant forms) and annual fee income from member institutions. This model arose in 2010, when Cornell sought to broaden the financial funding of the project by asking institutions to make annual voluntary contributions based on the amount of download usage by each institution. Each member institution pledges a five-year funding commitment to support arXiv. Based on institutional usage ranking, the annual fees are set in four tiers from $1,000 to $4,400. Cornell's goal is to raise at least $504,000 per year through membership fees generated by approximately 220 institutions. In September 2011, Cornell University Library took overall administrative and financial responsibility for arXiv's operation and development. Ginsparg was quoted in the Chronicle of Higher Education as joking that it \"was supposed to be a three-hour tour, not a life sentence\". However, Ginsparg remains on the arXiv's Scientific Advisory Board and its Physics Advisory Committee. In January 2022, arXiv began assigning DOIs to articles, in collaboration with"
  },
  {
    "source": "arXiv.txt",
    "chunk_id": "arXiv.txt_3",
    "chunk": "DataCite. Different versions of the same paper are specified by a version number at the end. For example, 1709.08980v1. If no version number is specified, the default is the latest version. arXiv uses a category system. Each paper is tagged with one or more categories. Some categories have two layers. For example, q-fin.TR is the \"Trading and Market Microstructure\" category within \"quantitative finance\". Other categories have one layer. For example, hep-ex is \"high energy physics experiments\". Although arXiv is not peer reviewed, a collection of moderators for each area review the submissions; they may recategorize any that are deemed off-topic, or reject submissions that are not scientific papers, or sometimes for undisclosed reasons. The lists of moderators for many sections of arXiv are publicly available, but moderators for most of the physics sections remain unlisted. Additionally, an \"endorsement\" system was introduced in 2004 as part of an effort to ensure content is relevant and of interest to current research in the specified disciplines. Under the system, for categories that use it, an author must be endorsed by an established arXiv author before being allowed to submit papers to those categories. Endorsers are not asked to review the paper for errors"
  },
  {
    "source": "arXiv.txt",
    "chunk_id": "arXiv.txt_4",
    "chunk": "but to check whether the paper is appropriate for the intended subject area. New authors from recognized academic institutions generally receive automatic endorsement, which in practice means that they do not need to deal with the endorsement system at all. However, the endorsement system has attracted criticism for allegedly restricting scientific inquiry. A majority of the e-prints are also submitted to journals for publication, but some work, including some very influential papers, remain purely as e-prints and are never published in a peer-reviewed journal. A well-known example of the latter is an outline of a proof of Thurston's geometrization conjecture, including the Poincaré conjecture as a particular case, uploaded by Grigori Perelman in November 2002. Perelman appears content to forgo the traditional peer-reviewed journal process, stating: \"If anybody is interested in my way of solving the problem, it's all there [on the arXiv] – let them go and read about it\". Despite this non-traditional method of publication, other mathematicians recognized this work by offering the Fields Medal and Clay Mathematics Millennium Prizes to Perelman, both of which he refused. While arXiv does contain some dubious e-prints, such as those claiming to refute famous theorems or proving famous conjectures such as"
  },
  {
    "source": "arXiv.txt",
    "chunk_id": "arXiv.txt_5",
    "chunk": "Fermat's Last Theorem using only high-school mathematics, a 2002 article which appeared in Notices of the American Mathematical Society described those as \"surprisingly rare\". arXiv generally re-classifies these works, e.g. in \"General mathematics\", rather than deleting them; however, some authors have voiced concern over the lack of transparency in the arXiv screening process. It has been reported that 14,000 preprints at have been withdrawn at arXiv, most commonly due to \"crucial errors\". A lesser number of the withdrawals were due to the preprint being subsumed by another publication. The report itself was posted at arXiv December, 2024. Papers can be submitted in any of several formats, including LaTeX, and PDF printed from a word processor other than TeX or LaTeX. The submission is rejected by the arXiv software if generating the final PDF file fails, if any image file is too large, or if the total size of the submission is too large. arXiv now allows one to store and modify an incomplete submission, and only finalize the submission when ready. The time stamp on the article is set when the submission is finalized. The standard access route is through the arXiv.org website. Other interfaces and access routes have also"
  },
  {
    "source": "arXiv.txt",
    "chunk_id": "arXiv.txt_6",
    "chunk": "been created by other un-associated organisations. Metadata for arXiv is made available through OAI-PMH, the standard for open access repositories. Content is therefore indexed in all major consumers of such data, such as BASE, CORE and Unpaywall. As of 2020, the Unpaywall dump links over 500,000 arxiv URLs as the open access version of a work found in CrossRef data from the publishers, making arXiv a top 10 global host of green open access. Finally, researchers can select sub-fields and receive daily e-mailings or RSS feeds of all submissions in them."
  },
  {
    "source": "Astronomy & Astrophysics.txt",
    "chunk_id": "Astronomy & Astrophysics.txt_0",
    "chunk": "# Astronomy & Astrophysics Astronomy & Astrophysics (A&A) is a monthly peer-reviewed scientific journal covering theoretical, observational, and instrumental astronomy and astrophysics. It is operated by an editorial team under the supervision of a board of directors representing 27 sponsoring countries plus a representative of the European Southern Observatory. The journal is published by EDP Sciences and the current editors-in-chief are Thierry Forveille and João Alves. Astronomy & Astrophysics was created as an answer to the publishing situation found in Europe in the 1960s. At that time, multiple journals were being published in several countries around the continent. These journals usually had a limited number of subscribers, and articles were written in languages other than English. They were less widely read than American and British journals and the research they reported had therefore less impact in the community. Starting in 1963, conversations between astronomers from European countries assessed the need for a common astronomical journal. On 8 April 1968, leading astronomers from Belgium, Denmark, France, Germany, the Netherlands, and Scandinavian countries met in Leiden University to prepare a possible merging of some of the principal existing journals. It was proposed that the new journal be called Astronomy and Astrophysics, A"
  },
  {
    "source": "Astronomy & Astrophysics.txt",
    "chunk_id": "Astronomy & Astrophysics.txt_1",
    "chunk": "European Journal. The main policy-making body of the new journal was to be the \"Board of Directors\", consisting of senior astronomers or government representatives of the sponsoring countries. The board appoints the editors-in chief, who are responsible for the scientific contents of the journal. The European Southern Observatory was chosen as an additional body that acts on behalf of the board and handles the administrative, financial, and legal matters of the journal. A second meeting held in July 1968 in Brussels cemented the agreement discussed in Leiden. Each nation established an annual monetary contribution and appointed its delegates for the board of directors. Also at this meeting, the first editors-in-chief were appointed: Stuart Pottasch and Jean-Louis Steinberg. The next meeting took place in Paris on 11 October 1968 and is officially regarded as the first meeting of the board of directors. At this meeting, the first chairman of the board, Adriaan Blaauw, was appointed, and the contract with the publisher Springer Science+Business Media was formalized. The first issue of A&A was published in January 1969, merging several national journals of individual European countries into one comprehensive publication. These journals, with their ISSN and date of first publication, are as follows:"
  },
  {
    "source": "Astronomy & Astrophysics.txt",
    "chunk_id": "Astronomy & Astrophysics.txt_2",
    "chunk": "Arkiv för Astronomi (ISSN 0004-2048), established in 1948 in Sweden, was also incorporated in 1973. The publishing of Astronomy & Astrophysics was further extended in 1992 by the incorporation of Bulletin of the Astronomical Institutes of Czechoslovakia (ISSN 0004-6248), established in 1947. There were only four issues of the journal in 1969, but it soon became a monthly publication and one of the four major generalist astronomical journals in the world. Initially, papers were submitted in English, French or German, but it soon became clear that, for a given author, the papers in English were cited twice as often as those in other languages. In addition to regular research papers in several different fields of astrophysics. A&A featured Letters and Research Notes for short manuscripts on a significant result or idea. A Supplement Series for the journal was created in 1970 for publishing extensive tabular material and catalogs. The turn of the century brought important changes to the journal. In 2001, a new contract was signed with EDP Sciences, which replaced Springer as the publishing house. Special Issues featuring results of astronomical surveys and space missions such as XMM-Newton, Planck, Rosetta, and Gaia were introduced. The editorial structure of the"
  },
  {
    "source": "Astronomy & Astrophysics.txt",
    "chunk_id": "Astronomy & Astrophysics.txt_3",
    "chunk": "journal was profoundly changed in 2003 and 2005 to involve more countries in the editorial process and to better handle the increasing number of submissions. Precise criteria for publishing in Astronomy & Astrophysics were explicited in 2004. English language editing was introduced in 2001 as a service to the diverse authorship of the journal. An extensive survey of authors conducted in 2007 showed widespread satisfaction with the new directions of the journal, although the use of structured abstracts proved more controversial. The evolution of electronic publishing resulted in the extinction of the Supplement Series, which was incorporated in the main journal in 2001, and of the printed edition in 2016. The Research Notes section was also discontinued in 2016. In 2023, A&A announced the introduction of links between articles and corresponding ESO datasets. The journal editorial office is located at the Paris Observatory and is supervised by the managing editor. It handles over 2000 papers per year. An archive of the published articles and related material is maintained by the Centre de données astronomiques de Strasbourg. The original sponsoring countries were the four countries whose journals merged to form Astronomy & Astrophysics (France, Germany, the Netherlands and Sweden), together with"
  },
  {
    "source": "Astronomy & Astrophysics.txt",
    "chunk_id": "Astronomy & Astrophysics.txt_4",
    "chunk": "Belgium, Denmark, Finland, and Norway. Norway later withdrew, but Austria, Greece, Italy, Spain, and Switzerland joined during the 1970s and 1980s. The Czech Republic, Estonia, Hungary, Poland, and Slovakia all joined as new members in the 1990s. In 2001 the words \"A European Journal\" were removed from the front cover in recognition of the fact that the journal was becoming increasingly global in scope. In effect, Argentina was admitted as an \"observer\" in 2002. In 2004 the board of directors decided that the journal \"will henceforth consider applications for sponsoring membership from any country in the world with well-documented active and excellent astronomical research\". Argentina became the first non-European country to gain full membership in 2005, followed by Brazil and Chile in 2006 (Brazil withdrew in 2016). Other European countries also joined during the 21st century: Portugal, Croatia, and Bulgaria during the 2010s, and Armenia, Lithuania, Norway, Serbia and Ukraine in the 2010s. The current list of member countries is listed here. Before 2022, the most recent issue of A&A was available free of charge for readers. Authors had the option to pay article processing charges (APC) for immediate and permanent open access. Furthermore, all Letters to the Editor and"
  },
  {
    "source": "Astronomy & Astrophysics.txt",
    "chunk_id": "Astronomy & Astrophysics.txt_5",
    "chunk": "all articles published in Sections 12 to 15 were in free access at no cost to the authors. Articles in the other sections of the journal were made freely available 12 months after publication (delayed open-access), through the publisher's site and via the Astrophysics Data System. Since the beginning of 2022, Astronomy & Astrophysics is published in full open access under the Subscribe to Open (S2O) model. A&A organises Scientific Writing Schools aimed at postgraduate students and young researchers. The purpose of these schools is to teach young authors how to express their scientific results through adequate and efficient science writing. As of 2019, five of these schools were organised in Belgium (2008 and 2009), Hungary (2014), Chile (2016) and China (2019). According to the Journal Citation Reports, the journal has a 2022 impact factor of 6.5."
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_0",
    "chunk": "# Astronomy Astronomy is a natural science that studies celestial objects and the phenomena that occur in the cosmos. It uses mathematics, physics, and chemistry in order to explain their origin and their overall evolution. Objects of interest include planets, moons, stars, nebulae, galaxies, meteoroids, asteroids, and comets. Relevant phenomena include supernova explosions, gamma ray bursts, quasars, blazars, pulsars, and cosmic microwave background radiation. More generally, astronomy studies everything that originates beyond Earth's atmosphere. Cosmology is a branch of astronomy that studies the universe as a whole. Astronomy is one of the oldest natural sciences. The early civilizations in recorded history made methodical observations of the night sky. These include the Egyptians, Babylonians, Greeks, Indians, Chinese, Maya, and many ancient indigenous peoples of the Americas. In the past, astronomy included disciplines as diverse as astrometry, celestial navigation, observational astronomy, and the making of calendars. Professional astronomy is split into observational and theoretical branches. Observational astronomy is focused on acquiring data from observations of astronomical objects. This data is then analyzed using basic principles of physics. Theoretical astronomy is oriented toward the development of computer or analytical models to describe astronomical objects and phenomena. These two fields complement each other. Theoretical"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_1",
    "chunk": "astronomy seeks to explain observational results and observations are used to confirm theoretical results. Astronomy is one of the few sciences in which amateurs play an active role. This is especially true for the discovery and observation of transient events. Amateur astronomers have helped with many important discoveries, such as finding new comets. Astronomy (from the Greek ἀστρονομία from ἄστρον astron, \"star\" and -νομία -nomia from νόμος nomos, \"law\" or \"culture\") means \"law of the stars\" (or \"culture of the stars\" depending on the translation). Astronomy should not be confused with astrology, the belief system which claims that human affairs are correlated with the positions of celestial objects. Although the two fields share a common origin, they are now entirely distinct. \"Astronomy\" and \"astrophysics\" are synonyms. Based on strict dictionary definitions, \"astronomy\" refers to \"the study of objects and matter outside the Earth's atmosphere and of their physical and chemical properties\", while \"astrophysics\" refers to the branch of astronomy dealing with \"the behavior, physical properties, and dynamic processes of celestial objects and phenomena\". In some cases, as in the introduction of the introductory textbook The Physical Universe by Frank Shu, \"astronomy\" may be used to describe the qualitative study of"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_2",
    "chunk": "the subject, whereas \"astrophysics\" is used to describe the physics-oriented version of the subject. However, since most modern astronomical research deals with subjects related to physics, modern astronomy could actually be called astrophysics. Some fields, such as astrometry, are purely astronomy rather than also astrophysics. Various departments in which scientists carry out research on this subject may use \"astronomy\" and \"astrophysics\", partly depending on whether the department is historically affiliated with a physics department, and many professional astronomers have physics rather than astronomy degrees. Some titles of the leading scientific journals in this field include The Astronomical Journal, The Astrophysical Journal, and Astronomy & Astrophysics. In early historic times, astronomy only consisted of the observation and predictions of the motions of objects visible to the naked eye. In some locations, early cultures assembled massive artifacts that may have had some astronomical purpose. In addition to their ceremonial uses, these observatories could be employed to determine the seasons, an important factor in knowing when to plant crops and in understanding the length of the year. As civilizations developed, most notably in Egypt, Mesopotamia, Greece, Persia, India, China, and Central America, astronomical observatories were assembled and ideas on the nature of the"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_3",
    "chunk": "Universe began to develop. Most early astronomy consisted of mapping the positions of the stars and planets, a science now referred to as astrometry. From these observations, early ideas about the motions of the planets were formed, and the nature of the Sun, Moon and the Earth in the Universe were explored philosophically. Mesopotamia is worldwide the place of the earliest known astronomer and poet by name: Enheduanna, Akkadian high priestess to the lunar deity Nanna/Sin and princess, daughter of Sargon the Great (c. 2334 – c. 2279 BCE). She had the Moon tracked in her chambers and wrote poems about her divine Moon. A particularly important early development was the beginning of mathematical and scientific astronomy, which began among the Babylonians, who laid the foundations for the later astronomical traditions that developed in many other civilizations. The Babylonians discovered that lunar eclipses recurred in a repeating cycle known as a saros. Following the Babylonians, significant advances in astronomy were made in ancient Greece and the Hellenistic world. Greek astronomy is characterized from the start by seeking a rational, physical explanation for celestial phenomena. In the 3rd century BC, Aristarchus of Samos estimated the size and distance of the Moon"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_4",
    "chunk": "and Sun, and he proposed a model of the Solar System where the Earth and planets rotated around the Sun, now called the heliocentric model. In the 2nd century BC, Hipparchus discovered precession, calculated the size and distance of the Moon and invented the earliest known astronomical devices such as the astrolabe. Hipparchus also created a comprehensive catalog of 1020 stars, and most of the constellations of the northern hemisphere derive from Greek astronomy. The Antikythera mechanism (c. 150–80 BC) was an early analog computer designed to calculate the location of the Sun, Moon, and planets for a given date. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical astronomical clocks appeared in Europe. The Earth was believed to be the center of the Universe with the Sun, the Moon and the stars rotating around it. This is known as the geocentric model of the Universe, or the Ptolemaic system, named after Ptolemy. Astronomy flourished in the Islamic world and other parts of the world. This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century. In 964, the Andromeda Galaxy, the largest galaxy in the Local"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_5",
    "chunk": "Group, was described by the Persian Muslim astronomer Abd al-Rahman al-Sufi in his Book of Fixed Stars. The SN 1006 supernova, the brightest apparent magnitude stellar event in recorded history, was observed by the Egyptian Arabic astronomer Ali ibn Ridwan and Chinese astronomers in 1006. Iranian scholar Al-Biruni observed that, contrary to Ptolemy, the Sun's apogee (highest point in the heavens) was mobile, not fixed. Some of the prominent Islamic (mostly Persian and Arab) astronomers who made significant contributions to the science include Al-Battani, Thebit, Abd al-Rahman al-Sufi, Biruni, Abū Ishāq Ibrāhīm al-Zarqālī, Al-Birjandi, and the astronomers of the Maragheh and Samarkand observatories. Astronomers during that time introduced many Arabic names now used for individual stars. It is also believed that the ruins at Great Zimbabwe and Timbuktu may have housed astronomical observatories. In Post-classical West Africa, Astronomers studied the movement of stars and relation to seasons, crafting charts of the heavens as well as precise diagrams of orbits of the other planets based on complex mathematical calculations. Songhai historian Mahmud Kati documented a meteor shower in August 1583. Europeans had previously believed that there had been no astronomical observation in sub-Saharan Africa during the pre-colonial Middle Ages, but modern"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_6",
    "chunk": "discoveries show otherwise. For over six centuries (from the recovery of ancient learning during the late Middle Ages into the Enlightenment), the Roman Catholic Church gave more financial and social support to the study of astronomy than probably all other institutions. Among the Church's motives was finding the date for Easter. Medieval Europe housed a number of important astronomers. Richard of Wallingford (1292–1336) made major contributions to astronomy and horology, including the invention of the first astronomical clock, the Rectangulus which allowed for the measurement of angles between planets and other astronomical bodies, as well as an equatorium called the Albion which could be used for astronomical calculations such as lunar, solar and planetary longitudes and could predict eclipses. Nicole Oresme (1320–1382) and Jean Buridan (1300–1361) first discussed evidence for the rotation of the Earth, furthermore, Buridan also developed the theory of impetus (predecessor of the modern scientific theory of inertia) which was able to show planets were capable of motion without the intervention of angels. Georg von Peuerbach (1423–1461) and Regiomontanus (1436–1476) helped make astronomical progress instrumental to Copernicus's development of the heliocentric model decades later. During the Renaissance, Nicolaus Copernicus proposed a heliocentric model of the solar system."
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_7",
    "chunk": "His work was defended by Galileo Galilei and expanded upon by Johannes Kepler. Kepler was the first to devise a system that correctly described the details of the motion of the planets around the Sun. However, Kepler did not succeed in formulating a theory behind the laws he wrote down. It was Isaac Newton, with his invention of celestial dynamics and his law of gravitation, who finally explained the motions of the planets. Newton also developed the reflecting telescope. Improvements in the size and quality of the telescope led to further discoveries. The English astronomer John Flamsteed catalogued over 3000 stars. More extensive star catalogues were produced by Nicolas Louis de Lacaille. The astronomer William Herschel made a detailed catalog of nebulosity and clusters, and in 1781 discovered the planet Uranus, the first new planet found. During the 18–19th centuries, the study of the three-body problem by Leonhard Euler, Alexis Claude Clairaut, and Jean le Rond d'Alembert led to more accurate predictions about the motions of the Moon and planets. This work was further refined by Joseph-Louis Lagrange and Pierre Simon Laplace, allowing the masses of the planets and moons to be estimated from their perturbations. Significant advances in astronomy"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_8",
    "chunk": "came about with the introduction of new technology, including the spectroscope and photography. Joseph von Fraunhofer discovered about 600 bands in the spectrum of the Sun in 1814–15, which, in 1859, Gustav Kirchhoff ascribed to the presence of different elements. Stars were proven to be similar to the Earth's own Sun, but with a wide range of temperatures, masses, and sizes. The existence of the Earth's galaxy, the Milky Way, as its own group of stars was only proven in the 20th century, along with the existence of \"external\" galaxies. The observed recession of those galaxies led to the discovery of the expansion of the Universe. In 1919, when the Hooker Telescope was completed, the prevailing view was that the universe consisted entirely of the Milky Way Galaxy. Using the Hooker Telescope, Edwin Hubble identified Cepheid variables in several spiral nebulae and in 1922–1923 proved conclusively that Andromeda Nebula and Triangulum among others, were entire galaxies outside our own, thus proving that the universe consists of a multitude of galaxies. With this Hubble formulated the Hubble constant, which allowed for the first time a calculation of the age of the Universe and size of the Observable Universe, which became increasingly"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_9",
    "chunk": "precise with better meassurements, starting at 2 billion years and 280 million light-years, until 2006 when data of the Hubble Space Telescope allowed a very accurate calculation of the age of the Universe and size of the Observable Universe. Theoretical astronomy led to speculations on the existence of objects such as black holes and neutron stars, which have been used to explain such observed phenomena as quasars, pulsars, blazars, and radio galaxies. Physical cosmology made huge advances during the 20th century. In the early 1900s the model of the Big Bang theory was formulated, heavily evidenced by cosmic microwave background radiation, Hubble's law, and the cosmological abundances of elements. Space telescopes have enabled measurements in parts of the electromagnetic spectrum normally blocked or blurred by the atmosphere. In February 2016, it was revealed that the LIGO project had detected evidence of gravitational waves in the previous September. The main source of information about celestial bodies and other objects is visible light, or more generally electromagnetic radiation. Observational astronomy may be categorized according to the corresponding region of the electromagnetic spectrum on which the observations are made. Some parts of the spectrum can be observed from the Earth's surface, while other"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_10",
    "chunk": "parts are only observable from either high altitudes or outside the Earth's atmosphere. Specific information on these subfields is given below. Radio astronomy uses radiation with wavelengths greater than approximately one millimeter, outside the visible range. Radio astronomy is different from most other forms of observational astronomy in that the observed radio waves can be treated as waves rather than as discrete photons. Hence, it is relatively easier to measure both the amplitude and phase of radio waves, whereas this is not as easily done at shorter wavelengths. Although some radio waves are emitted directly by astronomical objects, a product of thermal emission, most of the radio emission that is observed is the result of synchrotron radiation, which is produced when electrons orbit magnetic fields. Additionally, a number of spectral lines produced by interstellar gas, notably the hydrogen spectral line at 21 cm, are observable at radio wavelengths. A wide variety of other objects are observable at radio wavelengths, including supernovae, interstellar gas, pulsars, and active galactic nuclei. Infrared astronomy is founded on the detection and analysis of infrared radiation, wavelengths longer than red light and outside the range of our vision. The infrared spectrum is useful for studying objects"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_11",
    "chunk": "that are too cold to radiate visible light, such as planets, circumstellar disks or nebulae whose light is blocked by dust. The longer wavelengths of infrared can penetrate clouds of dust that block visible light, allowing the observation of young stars embedded in molecular clouds and the cores of galaxies. Observations from the Wide-field Infrared Survey Explorer (WISE) have been particularly effective at unveiling numerous galactic protostars and their host star clusters. With the exception of infrared wavelengths close to visible light, such radiation is heavily absorbed by the atmosphere, or masked, as the atmosphere itself produces significant infrared emission. Consequently, infrared observatories have to be located in high, dry places on Earth or in space. Some molecules radiate strongly in the infrared. This allows the study of the chemistry of space; more specifically it can detect water in comets. Historically, optical astronomy, which has been also called visible light astronomy, is the oldest form of astronomy. Images of observations were originally drawn by hand. In the late 19th century and most of the 20th century, images were made using photographic equipment. Modern images are made using digital detectors, particularly using charge-coupled devices (CCDs) and recorded on modern medium. Although"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_12",
    "chunk": "visible light itself extends from approximately 4000 Å to 7000 Å (400 nm to 700 nm), that same equipment can be used to observe some near-ultraviolet and near-infrared radiation. Ultraviolet astronomy employs ultraviolet wavelengths between approximately 100 and 3200 Å (10 to 320 nm). Light at those wavelengths is absorbed by the Earth's atmosphere, requiring observations at these wavelengths to be performed from the upper atmosphere or from space. Ultraviolet astronomy is best suited to the study of thermal radiation and spectral emission lines from hot blue stars (OB stars) that are very bright in this wave band. This includes the blue stars in other galaxies, which have been the targets of several ultraviolet surveys. Other objects commonly observed in ultraviolet light include planetary nebulae, supernova remnants, and active galactic nuclei. However, as ultraviolet light is easily absorbed by interstellar dust, an adjustment of ultraviolet measurements is necessary. X-ray astronomy uses X-ray wavelengths. Typically, X-ray radiation is produced by synchrotron emission (the result of electrons orbiting magnetic field lines), thermal emission from thin gases above 10 (10 million) kelvins, and thermal emission from thick gases above 10 Kelvin. Since X-rays are absorbed by the Earth's atmosphere, all X-ray observations must"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_13",
    "chunk": "be performed from high-altitude balloons, rockets, or X-ray astronomy satellites. Notable X-ray sources include X-ray binaries, pulsars, supernova remnants, elliptical galaxies, clusters of galaxies, and active galactic nuclei. Gamma ray astronomy observes astronomical objects at the shortest wavelengths of the electromagnetic spectrum. Gamma rays may be observed directly by satellites such as the Compton Gamma Ray Observatory or by specialized telescopes called atmospheric Cherenkov telescopes. The Cherenkov telescopes do not detect the gamma rays directly but instead detect the flashes of visible light produced when gamma rays are absorbed by the Earth's atmosphere. Most gamma-ray emitting sources are actually gamma-ray bursts, objects which only produce gamma radiation for a few milliseconds to thousands of seconds before fading away. Only 10% of gamma-ray sources are non-transient sources. These steady gamma-ray emitters include pulsars, neutron stars, and black hole candidates such as active galactic nuclei. In addition to electromagnetic radiation, a few other events originating from great distances may be observed from the Earth. In neutrino astronomy, astronomers use heavily shielded underground facilities such as SAGE, GALLEX, and Kamioka II/III for the detection of neutrinos. The vast majority of the neutrinos streaming through the Earth originate from the Sun, but 24 neutrinos"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_14",
    "chunk": "were also detected from supernova 1987A. Cosmic rays, which consist of very high energy particles (atomic nuclei) that can decay or be absorbed when they enter the Earth's atmosphere, result in a cascade of secondary particles which can be detected by current observatories. Some future neutrino detectors may also be sensitive to the particles produced when cosmic rays hit the Earth's atmosphere. Gravitational-wave astronomy is an emerging field of astronomy that employs gravitational-wave detectors to collect observational data about distant massive objects. A few observatories have been constructed, such as the Laser Interferometer Gravitational Observatory LIGO. LIGO made its first detection on 14 September 2015, observing gravitational waves from a binary black hole. A second gravitational wave was detected on 26 December 2015 and additional observations should continue but gravitational waves require extremely sensitive instruments. The combination of observations made using electromagnetic radiation, neutrinos or gravitational waves and other complementary information, is known as multi-messenger astronomy. One of the oldest fields in astronomy, and in all of science, is the measurement of the positions of celestial objects. Historically, accurate knowledge of the positions of the Sun, Moon, planets and stars has been essential in celestial navigation (the use of celestial"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_15",
    "chunk": "objects to guide navigation) and in the making of calendars. Careful measurement of the positions of the planets has led to a solid understanding of gravitational perturbations, and an ability to determine past and future positions of the planets with great accuracy, a field known as celestial mechanics. More recently the tracking of near-Earth objects will allow for predictions of close encounters or potential collisions of the Earth with those objects. The measurement of stellar parallax of nearby stars provides a fundamental baseline in the cosmic distance ladder that is used to measure the scale of the Universe. Parallax measurements of nearby stars provide an absolute baseline for the properties of more distant stars, as their properties can be compared. Measurements of the radial velocity and proper motion of stars allow astronomers to plot the movement of these systems through the Milky Way galaxy. Astrometric results are the basis used to calculate the distribution of speculated dark matter in the galaxy. During the 1990s, the measurement of the stellar wobble of nearby stars was used to detect large extrasolar planets orbiting those stars. Theoretical astronomers use several tools including analytical models and computational numerical simulations; each has its particular advantages."
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_16",
    "chunk": "Analytical models of a process are better for giving broader insight into the heart of what is going on. Numerical models reveal the existence of phenomena and effects otherwise unobserved. Theorists in astronomy endeavor to create theoretical models that are based on existing observations and known physics, and to predict observational consequences of those models. The observation of phenomena predicted by a model allows astronomers to select between several alternative or conflicting models. Theorists also modify existing models to take into account new observations. In some cases, a large amount of observational data that is inconsistent with a model may lead to abandoning it largely or completely, as for geocentric theory, the existence of luminiferous aether, and the steady-state model of cosmic evolution. Modern theoretical astronomy reflects dramatic advances in observation since the 1990s, including studies of the cosmic microwave background, distant supernovae and galaxy redshifts, which have led to the development of a standard model of cosmology. This model requires the universe to contain large amounts of dark matter and dark energy whose nature is currently not well understood, but the model gives detailed predictions that are in excellent agreement with many diverse observations. Astrophysics is the branch of"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_17",
    "chunk": "astronomy that employs the principles of physics and chemistry \"to ascertain the nature of the astronomical objects, rather than their positions or motions in space\". Among the objects studied are the Sun, other stars, galaxies, extrasolar planets, the interstellar medium and the cosmic microwave background. Their emissions are examined across all parts of the electromagnetic spectrum, and the properties examined include luminosity, density, temperature, and chemical composition. Because astrophysics is a very broad subject, astrophysicists typically apply many disciplines of physics, including mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics. In practice, modern astronomical research often involves a substantial amount of work in the realms of theoretical and observational physics. Some areas of study for astrophysicists include their attempts to determine the properties of dark matter, dark energy, and black holes; whether or not time travel is possible, wormholes can form, or the multiverse exists; and the origin and ultimate fate of the universe. Topics also studied by theoretical astrophysicists include Solar System formation and evolution; stellar dynamics and evolution; galaxy formation and evolution; magnetohydrodynamics; large-scale structure of matter in the universe; origin of cosmic rays; general relativity and physical cosmology,"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_18",
    "chunk": "including string cosmology and astroparticle physics. Astrochemistry is the study of the abundance and reactions of molecules in the Universe, and their interaction with radiation. The discipline is an overlap of astronomy and chemistry. The word \"astrochemistry\" may be applied to both the Solar System and the interstellar medium. The study of the abundance of elements and isotope ratios in Solar System objects, such as meteorites, is also called cosmochemistry, while the study of interstellar atoms and molecules and their interaction with radiation is sometimes called molecular astrophysics. The formation, atomic and chemical composition, evolution and fate of molecular gas clouds is of special interest, because it is from these clouds that solar systems form. Studies in this field contribute to the understanding of the formation of the Solar System, Earth's origin and geology, abiogenesis, and the origin of climate and oceans. Astrobiology is an interdisciplinary scientific field concerned with the origins, early evolution, distribution, and future of life in the universe. Astrobiology considers the question of whether extraterrestrial life exists, and how humans can detect it if it does. The term exobiology is similar. Astrobiology makes use of molecular biology, biophysics, biochemistry, chemistry, astronomy, physical cosmology, exoplanetology and geology"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_19",
    "chunk": "to investigate the possibility of life on other worlds and help recognize biospheres that might be different from that on Earth. The origin and early evolution of life is an inseparable part of the discipline of astrobiology. Astrobiology concerns itself with interpretation of existing scientific data, and although speculation is entertained to give context, astrobiology concerns itself primarily with hypotheses that fit firmly into existing scientific theories. This interdisciplinary field encompasses research on the origin of planetary systems, origins of organic compounds in space, rock-water-carbon interactions, abiogenesis on Earth, planetary habitability, research on biosignatures for life detection, and studies on the potential for life to adapt to challenges on Earth and in outer space. Cosmology (from the Greek κόσμος (kosmos) \"world, universe\" and λόγος (logos) \"word, study\" or literally \"logic\") could be considered the study of the Universe as a whole. Observations of the large-scale structure of the Universe, a branch known as physical cosmology, have provided a deep understanding of the formation and evolution of the cosmos. Fundamental to modern cosmology is the well-accepted theory of the Big Bang, wherein our Universe began at a single point in time, and thereafter expanded over the course of 13.8 billion years"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_20",
    "chunk": "to its present condition. The concept of the Big Bang can be traced back to the discovery of the microwave background radiation in 1965. In the course of this expansion, the Universe underwent several evolutionary stages. In the very early moments, it is theorized that the Universe experienced a very rapid cosmic inflation, which homogenized the starting conditions. Thereafter, nucleosynthesis produced the elemental abundance of the early Universe. (See also nucleocosmochronology.) When the first neutral atoms formed from a sea of primordial ions, space became transparent to radiation, releasing the energy viewed today as the microwave background radiation. The expanding Universe then underwent a Dark Age due to the lack of stellar energy sources. A hierarchical structure of matter began to form from minute variations in the mass density of space. Matter accumulated in the densest regions, forming clouds of gas and the earliest stars, the Population III stars. These massive stars triggered the reionization process and are believed to have created many of the heavy elements in the early Universe, which, through nuclear decay, create lighter elements, allowing the cycle of nucleosynthesis to continue longer. Gravitational aggregations clustered into filaments, leaving voids in the gaps. Gradually, organizations of gas"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_21",
    "chunk": "and dust merged to form the first primitive galaxies. Over time, these pulled in more matter, and were often organized into groups and clusters of galaxies, then into larger-scale superclusters. Fundamental to the structure of the Universe is the existence of dark matter and dark energy. These are now thought to be its dominant components, forming 96% of the mass of the Universe. For this reason, much effort is expended in trying to understand the physics of these components. The study of objects outside our galaxy is a branch of astronomy concerned with the formation and evolution of galaxies, their morphology (description) and classification, the observation of active galaxies, and at a larger scale, the groups and clusters of galaxies. Finally, the latter is important for the understanding of the large-scale structure of the cosmos. Most galaxies are organized into distinct shapes that allow for classification schemes. They are commonly divided into spiral, elliptical and Irregular galaxies. As the name suggests, an elliptical galaxy has the cross-sectional shape of an ellipse. The stars move along random orbits with no preferred direction. These galaxies contain little or no interstellar dust, few star-forming regions, and older stars. Elliptical galaxies may have been"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_22",
    "chunk": "formed by other galaxies merging. A spiral galaxy is organized into a flat, rotating disk, usually with a prominent bulge or bar at the center, and trailing bright arms that spiral outward. The arms are dusty regions of star formation within which massive young stars produce a blue tint. Spiral galaxies are typically surrounded by a halo of older stars. Both the Milky Way and one of our nearest galaxy neighbors, the Andromeda Galaxy, are spiral galaxies. Irregular galaxies are chaotic in appearance, and are neither spiral nor elliptical. About a quarter of all galaxies are irregular, and the peculiar shapes of such galaxies may be the result of gravitational interaction. An active galaxy is a formation that emits a significant amount of its energy from a source other than its stars, dust and gas. It is powered by a compact region at the core, thought to be a supermassive black hole that is emitting radiation from in-falling material. A radio galaxy is an active galaxy that is very luminous in the radio portion of the spectrum, and is emitting immense plumes or lobes of gas. Active galaxies that emit shorter frequency, high-energy radiation include Seyfert galaxies, quasars, and blazars."
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_23",
    "chunk": "Quasars are believed to be the most consistently luminous objects in the known universe. The large-scale structure of the cosmos is represented by groups and clusters of galaxies. This structure is organized into a hierarchy of groupings, with the largest being the superclusters. The collective matter is formed into filaments and walls, leaving large voids between. The Solar System orbits within the Milky Way, a barred spiral galaxy that is a prominent member of the Local Group of galaxies. It is a rotating mass of gas, dust, stars and other objects, held together by mutual gravitational attraction. As the Earth is located within the dusty outer arms, there are large portions of the Milky Way that are obscured from view. In the center of the Milky Way is the core, a bar-shaped bulge with what is believed to be a supermassive black hole at its center. This is surrounded by four primary arms that spiral from the core. This is a region of active star formation that contains many younger, population I stars. The disk is surrounded by a spheroid halo of older, population II stars, as well as relatively dense concentrations of stars known as globular clusters. Between the"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_24",
    "chunk": "stars lies the interstellar medium, a region of sparse matter. In the densest regions, molecular clouds of molecular hydrogen and other elements create star-forming regions. These begin as a compact pre-stellar core or dark nebulae, which concentrate and collapse (in volumes determined by the Jeans length) to form compact protostars. As the more massive stars appear, they transform the cloud into an H II region (ionized atomic hydrogen) of glowing gas and plasma. The stellar wind and supernova explosions from these stars eventually cause the cloud to disperse, often leaving behind one or more young open clusters of stars. These clusters gradually disperse, and the stars join the population of the Milky Way. Kinematic studies of matter in the Milky Way and other galaxies have demonstrated that there is more mass than can be accounted for by visible matter. A dark matter halo appears to dominate the mass, although the nature of this dark matter remains undetermined. The study of stars and stellar evolution is fundamental to our understanding of the Universe. The astrophysics of stars has been determined through observation and theoretical understanding; and from computer simulations of the interior. Star formation occurs in dense regions of dust and"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_25",
    "chunk": "gas, known as giant molecular clouds. When destabilized, cloud fragments can collapse under the influence of gravity, to form a protostar. A sufficiently dense, and hot, core region will trigger nuclear fusion, thus creating a main-sequence star. Almost all elements heavier than hydrogen and helium were created inside the cores of stars. The characteristics of the resulting star depend primarily upon its starting mass. The more massive the star, the greater its luminosity, and the more rapidly it fuses its hydrogen fuel into helium in its core. Over time, this hydrogen fuel is completely converted into helium, and the star begins to evolve. The fusion of helium requires a higher core temperature. A star with a high enough core temperature will push its outer layers outward while increasing its core density. The resulting red giant formed by the expanding outer layers enjoys a brief life span, before the helium fuel in the core is in turn consumed. Very massive stars can also undergo a series of evolutionary phases, as they fuse increasingly heavier elements. The final fate of the star depends on its mass, with stars of mass greater than about eight times the Sun becoming core collapse supernovae; while"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_26",
    "chunk": "smaller stars blow off their outer layers and leave behind the inert core in the form of a white dwarf. The ejection of the outer layers forms a planetary nebula. The remnant of a supernova is a dense neutron star, or, if the stellar mass was at least three times that of the Sun, a black hole. Closely orbiting binary stars can follow more complex evolutionary paths, such as mass transfer onto a white dwarf companion that can potentially cause a supernova. Planetary nebulae and supernovae distribute the \"metals\" produced in the star by fusion to the interstellar medium; without them, all new stars (and their planetary systems) would be formed from hydrogen and helium alone. At a distance of about eight light-minutes, the most frequently studied star is the Sun, a typical main-sequence dwarf star of stellar class G2 V, and about 4.6 billion years (Gyr) old. The Sun is not considered a variable star, but it does undergo periodic changes in activity known as the sunspot cycle. This is an 11-year oscillation in sunspot number. Sunspots are regions of lower-than-average temperatures that are associated with intense magnetic activity. The Sun has steadily increased in luminosity by 40% since"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_27",
    "chunk": "it first became a main-sequence star. The Sun has also undergone periodic changes in luminosity that can have a significant impact on the Earth. The Maunder minimum, for example, is believed to have caused the Little Ice Age phenomenon during the Middle Ages. At the center of the Sun is the core region, a volume of sufficient temperature and pressure for nuclear fusion to occur. Above the core is the radiation zone, where the plasma conveys the energy flux by means of radiation. Above that is the convection zone where the gas material transports energy primarily through physical displacement of the gas known as convection. It is believed that the movement of mass within the convection zone creates the magnetic activity that generates sunspots. The visible outer surface of the Sun is called the photosphere. Above this layer is a thin region known as the chromosphere. This is surrounded by a transition region of rapidly increasing temperatures, and finally by the super-heated corona. A solar wind of plasma particles constantly streams outward from the Sun until, at the outermost limit of the Solar System, it reaches the heliopause. As the solar wind passes the Earth, it interacts with the Earth's"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_28",
    "chunk": "magnetic field (magnetosphere) and deflects the solar wind, but traps some creating the Van Allen radiation belts that envelop the Earth. The aurora are created when solar wind particles are guided by the magnetic flux lines into the Earth's polar regions where the lines then descend into the atmosphere. Planetary science is the study of the assemblage of planets, moons, dwarf planets, comets, asteroids, and other bodies orbiting the Sun, as well as extrasolar planets. The Solar System has been relatively well-studied, initially through telescopes and then later by spacecraft. This has provided a good overall understanding of the formation and evolution of the Sun's planetary system, although many new discoveries are still being made. The Solar System is divided into the inner Solar System (subdivided into the inner planets and the asteroid belt), the outer Solar System (subdivided into the outer planets and centaurs), comets, the trans-Neptunian region (subdivided into the Kuiper belt, and the scattered disc) and the farthest regions (e.g., boundaries of the heliosphere, and the Oort Cloud, which may extend as far as a light-year). The inner terrestrial planets consist of Mercury, Venus, Earth, and Mars. The outer giant planets are the gas giants (Jupiter and"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_29",
    "chunk": "Saturn) and the ice giants (Uranus and Neptune). The planets were formed 4.6 billion years ago in the protoplanetary disk that surrounded the early Sun. Through a process that included gravitational attraction, collision, and accretion, the disk formed clumps of matter that, with time, became protoplanets. The radiation pressure of the solar wind then expelled most of the unaccreted matter, and only those planets with sufficient mass retained their gaseous atmosphere. The planets continued to sweep up, or eject, the remaining matter during a period of intense bombardment, evidenced by the many impact craters on the Moon. During this period, some of the protoplanets may have collided and one such collision may have formed the Moon. Once a planet reaches sufficient mass, the materials of different densities segregate within, during planetary differentiation. This process can form a stony or metallic core, surrounded by a mantle and an outer crust. The core may include solid and liquid regions, and some planetary cores generate their own magnetic field, which can protect their atmospheres from solar wind stripping. A planet or moon's interior heat is produced from the collisions that created the body, by the decay of radioactive materials (e.g. uranium, thorium, and"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_30",
    "chunk": "Al), or tidal heating caused by interactions with other bodies. Some planets and moons accumulate enough heat to drive geologic processes such as volcanism and tectonics. Those that accumulate or retain an atmosphere can also undergo surface erosion from wind or water. Smaller bodies, without tidal heating, cool more quickly; and their geological activity ceases with the exception of impact cratering. Astronomy and astrophysics have developed significant interdisciplinary links with other major scientific fields. Archaeoastronomy is the study of ancient or traditional astronomies in their cultural context, utilizing archaeological and anthropological evidence. Astrobiology is the study of the advent and evolution of biological systems in the Universe, with particular emphasis on the possibility of non-terrestrial life. Astrostatistics is the application of statistics to astrophysics to the analysis of a vast amount of observational astrophysical data. The study of chemicals found in space, including their formation, interaction and destruction, is called astrochemistry. These substances are usually found in molecular clouds, although they may also appear in low-temperature stars, brown dwarfs and planets. Cosmochemistry is the study of the chemicals found within the Solar System, including the origins of the elements and variations in the isotope ratios. Both of these fields represent"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_31",
    "chunk": "an overlap of the disciplines of astronomy and chemistry. As \"forensic astronomy\", finally, methods from astronomy have been used to solve problems of art history and occasionally of law. Collectively, amateur astronomers observe a variety of celestial objects and phenomena sometimes with consumer-level equipment or equipment that they build themselves. Common targets of amateur astronomers include the Sun, the Moon, planets, stars, comets, meteor showers, and a variety of deep-sky objects such as star clusters, galaxies, and nebulae. Astronomy clubs are located throughout the world and many have programs to help their members set up and complete observational programs including those to observe all the objects in the Messier (110 objects) or Herschel 400 catalogues of points of interest in the night sky. One branch of amateur astronomy, astrophotography, involves the taking of photos of the night sky. Many amateurs like to specialize in the observation of particular objects, types of objects, or types of events that interest them. Most amateurs work at visible wavelengths, but many experiment with wavelengths outside the visible spectrum. This includes the use of infrared filters on conventional telescopes, and also the use of radio telescopes. The pioneer of amateur radio astronomy was Karl Jansky,"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_32",
    "chunk": "who started observing the sky at radio wavelengths in the 1930s. A number of amateur astronomers use either homemade telescopes or use radio telescopes which were originally built for astronomy research but which are now available to amateurs (e.g. the One-Mile Telescope). Amateur astronomers continue to make scientific contributions to the field of astronomy and it is one of the few scientific disciplines where amateurs can still make significant contributions. Amateurs can make occultation measurements that are used to refine the orbits of minor planets. They can also discover comets, and perform regular observations of variable stars. Improvements in digital technology have allowed amateurs to make impressive advances in the field of astrophotography. In the 21st century there remain important unanswered questions in astronomy. Some are cosmic in scope: for example, what are dark matter and dark energy? These dominate the evolution and fate of the cosmos, yet their true nature remains unknown. What will be the ultimate fate of the universe? Why is the abundance of lithium in the cosmos four times lower than predicted by the standard Big Bang model? Others pertain to more specific classes of phenomena. For example, is the Solar System normal or atypical? What"
  },
  {
    "source": "Astronomy.txt",
    "chunk_id": "Astronomy.txt_33",
    "chunk": "is the origin of the stellar mass spectrum? That is, why do astronomers observe the same distribution of stellar masses—the initial mass function—apparently regardless of the initial conditions? Likewise, questions remain about the formation of the first galaxies, the origin of supermassive black holes, the source of ultra-high-energy cosmic rays, and more. Is there other life in the Universe? Especially, is there other intelligent life? If so, what is the explanation for the Fermi paradox? The existence of life elsewhere has important scientific and philosophical implications."
  },
  {
    "source": "Astrophysics (journal).txt",
    "chunk_id": "Astrophysics (journal).txt_0",
    "chunk": "# Astrophysics (journal) Astrophysics is a peer-reviewed scientific journal of astrophysics published by Springer. Each volume is published every three months. It was founded in 1965 by the Soviet Armenian astrophysicist Viktor Ambartsumian. It is the English version of the journal Astrofizika, published by the Armenian National Academy of Sciences mostly in Russian. The current editor-in-chief is Arthur Nikoghossian. The focus of this journal is astronomy and is a translation of the peer-reviewed Russian language journal Astrofizika."
  },
  {
    "source": "Astrophysics.txt",
    "chunk_id": "Astrophysics.txt_0",
    "chunk": "# Astrophysics Astrophysics is a science that employs the methods and principles of physics and chemistry in the study of astronomical objects and phenomena. As one of the founders of the discipline, James Keeler, said, astrophysics \"seeks to ascertain the nature of the heavenly bodies, rather than their positions or motions in space—what they are, rather than where they are\", which is studied in celestial mechanics. Among the subjects studied are the Sun (solar physics), other stars, galaxies, extrasolar planets, the interstellar medium, and the cosmic microwave background. Emissions from these objects are examined across all parts of the electromagnetic spectrum, and the properties examined include luminosity, density, temperature, and chemical composition. Because astrophysics is a very broad subject, astrophysicists apply concepts and methods from many disciplines of physics, including classical mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics. In practice, modern astronomical research often involves substantial work in the realms of theoretical and observational physics. Some areas of study for astrophysicists include the properties of dark matter, dark energy, black holes, and other celestial bodies; and the origin and ultimate fate of the universe. Topics also studied by theoretical astrophysicists"
  },
  {
    "source": "Astrophysics.txt",
    "chunk_id": "Astrophysics.txt_1",
    "chunk": "include Solar System formation and evolution; stellar dynamics and evolution; galaxy formation and evolution; magnetohydrodynamics; large-scale structure of matter in the universe; origin of cosmic rays; general relativity, special relativity, and quantum and physical cosmology (the physical study of the largest-scale structures of the universe), including string cosmology and astroparticle physics. Astronomy is an ancient science, long separated from the study of terrestrial physics. In the Aristotelian worldview, bodies in the sky appeared to be unchanging spheres whose only motion was uniform motion in a circle, while the earthly world was the realm which underwent growth and decay and in which natural motion was in a straight line and ended when the moving object reached its goal. Consequently, it was held that the celestial region was made of a fundamentally different kind of matter from that found in the terrestrial sphere; either Fire as maintained by Plato, or Aether as maintained by Aristotle. During the 17th century, natural philosophers such as Galileo, Descartes, and Newton began to maintain that the celestial and terrestrial regions were made of similar kinds of material and were subject to the same natural laws. Their challenge was that the tools had not yet been invented"
  },
  {
    "source": "Astrophysics.txt",
    "chunk_id": "Astrophysics.txt_2",
    "chunk": "with which to prove these assertions. For much of the nineteenth century, astronomical research was focused on the routine work of measuring the positions and computing the motions of astronomical objects. A new astronomy, soon to be called astrophysics, began to emerge when William Hyde Wollaston and Joseph von Fraunhofer independently discovered that, when decomposing the light from the Sun, a multitude of dark lines (regions where there was less or no light) were observed in the spectrum. By 1860 the physicist, Gustav Kirchhoff, and the chemist, Robert Bunsen, had demonstrated that the dark lines in the solar spectrum corresponded to bright lines in the spectra of known gases, specific lines corresponding to unique chemical elements. Kirchhoff deduced that the dark lines in the solar spectrum are caused by absorption by chemical elements in the Solar atmosphere. In this way it was proved that the chemical elements found in the Sun and stars were also found on Earth. Among those who extended the study of solar and stellar spectra was Norman Lockyer, who in 1868 detected radiant, as well as dark lines in solar spectra. Working with chemist Edward Frankland to investigate the spectra of elements at various temperatures and"
  },
  {
    "source": "Astrophysics.txt",
    "chunk_id": "Astrophysics.txt_3",
    "chunk": "pressures, he could not associate a yellow line in the solar spectrum with any known elements. He thus claimed the line represented a new element, which was called helium, after the Greek Helios, the Sun personified. In 1885, Edward C. Pickering undertook an ambitious program of stellar spectral classification at Harvard College Observatory, in which a team of woman computers, notably Williamina Fleming, Antonia Maury, and Annie Jump Cannon, classified the spectra recorded on photographic plates. By 1890, a catalog of over 10,000 stars had been prepared that grouped them into thirteen spectral types. Following Pickering's vision, by 1924 Cannon expanded the catalog to nine volumes and over a quarter of a million stars, developing the Harvard Classification Scheme which was accepted for worldwide use in 1922. In 1895, George Ellery Hale and James E. Keeler, along with a group of ten associate editors from Europe and the United States, established The Astrophysical Journal: An International Review of Spectroscopy and Astronomical Physics. It was intended that the journal would fill the gap between journals in astronomy and physics, providing a venue for publication of articles on astronomical applications of the spectroscope; on laboratory research closely allied to astronomical physics, including"
  },
  {
    "source": "Astrophysics.txt",
    "chunk_id": "Astrophysics.txt_4",
    "chunk": "wavelength determinations of metallic and gaseous spectra and experiments on radiation and absorption; on theories of the Sun, Moon, planets, comets, meteors, and nebulae; and on instrumentation for telescopes and laboratories. Around 1920, following the discovery of the Hertzsprung–Russell diagram still used as the basis for classifying stars and their evolution, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper The Internal Constitution of the Stars. At that time, the source of stellar energy was a complete mystery; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein's equation E = mc. This was a particularly remarkable development since at that time fusion and thermonuclear energy, and even that stars are largely composed of hydrogen (see metallicity), had not yet been discovered. In 1925 Cecilia Helena Payne (later Cecilia Payne-Gaposchkin) wrote an influential doctoral dissertation at Radcliffe College, in which she applied Saha's ionization theory to stellar atmospheres to relate the spectral classes to the temperature of stars. Most significantly, she discovered that hydrogen and helium were the principal components of stars, not the composition of Earth. Despite Eddington's suggestion, discovery was so unexpected that"
  },
  {
    "source": "Astrophysics.txt",
    "chunk_id": "Astrophysics.txt_5",
    "chunk": "her dissertation readers (including Russell) convinced her to modify the conclusion before publication. However, later research confirmed her discovery. By the end of the 20th century, studies of astronomical spectra had expanded to cover wavelengths extending from radio waves through optical, x-ray, and gamma wavelengths. In the 21st century, it further expanded to include observations based on gravitational waves. Observational astronomy is a division of the astronomical science that is concerned with recording and interpreting data, in contrast with theoretical astrophysics, which is mainly concerned with finding out the measurable implications of physical models. It is the practice of observing celestial objects by using telescopes and other astronomical apparatus. Other than electromagnetic radiation, few things may be observed from the Earth that originate from great distances. A few gravitational wave observatories have been constructed, but gravitational waves are extremely difficult to detect. Neutrino observatories have also been built, primarily to study the Sun. Cosmic rays consisting of very high-energy particles can be observed hitting the Earth's atmosphere. Observations can also vary in their time scale. Most optical observations take minutes to hours, so phenomena that change faster than this cannot readily be observed. However, historical data on some objects is"
  },
  {
    "source": "Astrophysics.txt",
    "chunk_id": "Astrophysics.txt_6",
    "chunk": "available, spanning centuries or millennia. On the other hand, radio observations may look at events on a millisecond timescale (millisecond pulsars) or combine years of data (pulsar deceleration studies). The information obtained from these different timescales is very different. The study of the Sun has a special place in observational astrophysics. Due to the tremendous distance of all other stars, the Sun can be observed in a kind of detail unparalleled by any other star. Understanding the Sun serves as a guide to understanding of other stars. The topic of how stars change, or stellar evolution, is often modeled by placing the varieties of star types in their respective positions on the Hertzsprung–Russell diagram, which can be viewed as representing the state of a stellar object, from birth to destruction. Theoretical astrophysicists use a wide variety of tools which include analytical models (for example, polytropes to approximate the behaviors of a star) and computational numerical simulations. Each has some advantages. Analytical models of a process are generally better for giving insight into the heart of what is going on. Numerical models can reveal the existence of phenomena and effects that would otherwise not be seen. Theorists in astrophysics endeavor to"
  },
  {
    "source": "Astrophysics.txt",
    "chunk_id": "Astrophysics.txt_7",
    "chunk": "create theoretical models and figure out the observational consequences of those models. This helps allow observers to look for data that can refute a model or help in choosing between several alternate or conflicting models. Theorists also try to generate or modify models to take into account new data. In the case of an inconsistency, the general tendency is to try to make minimal modifications to the model to fit the data. In some cases, a large amount of inconsistent data over time may lead to total abandonment of a model. Topics studied by theoretical astrophysicists include stellar dynamics and evolution; galaxy formation and evolution; magnetohydrodynamics; large-scale structure of matter in the universe; origin of cosmic rays; general relativity and physical cosmology, including string cosmology and astroparticle physics. Relativistic astrophysics serves as a tool to gauge the properties of large-scale structures for which gravitation plays a significant role in physical phenomena investigated and as the basis for black hole (astro)physics and the study of gravitational waves. Some widely accepted and studied theories and models in astrophysics, now included in the Lambda-CDM model, are the Big Bang, cosmic inflation, dark matter, dark energy and fundamental theories of physics. The roots of"
  },
  {
    "source": "Astrophysics.txt",
    "chunk_id": "Astrophysics.txt_8",
    "chunk": "astrophysics can be found in the seventeenth century emergence of a unified physics, in which the same laws applied to the celestial and terrestrial realms. There were scientists who were qualified in both physics and astronomy who laid the firm foundation for the current science of astrophysics. In modern times, students continue to be drawn to astrophysics due to its popularization by the Royal Astronomical Society and notable educators such as prominent professors Lawrence Krauss, Subrahmanyan Chandrasekhar, Stephen Hawking, Hubert Reeves, Carl Sagan and Patrick Moore. The efforts of the early, late, and present scientists continue to attract young people to study the history and science of astrophysics. The television sitcom show The Big Bang Theory popularized the field of astrophysics with the general public, and featured some well known scientists like Stephen Hawking and Neil deGrasse Tyson."
  },
  {
    "source": "Bahtinov mask.txt",
    "chunk_id": "Bahtinov mask.txt_0",
    "chunk": "# Bahtinov mask The Bahtinov mask is a device used to focus small astronomical telescopes accurately. Although masks have long been used as focusing aids, the distinctive pattern was invented by Russian amateur astrophotographer Pavel Bahtinov (Russian: Павел Бахтинов) in 2005. Precise focusing of telescopes and astrographs is critical to performing astrophotography. The telescope is pointed at a bright star, and a mask is placed in front of the telescope's objective (or in front of the aperture). The mask consists of three separate grids, positioned in such a way that the grids produce three angled diffraction spikes at the focal plane of the instrument for each bright image element. As the instrument's focus is changed, the central spike appears to move from one side of the star to the other. In reality, all three spikes move, but the central spike moves in the opposite direction to the two spikes forming the \"X\". Optimal focus is achieved when the middle spike is centered between the other two spikes. Small deviations from optimal focus are easily visible. For astrophotography, a digital image can be analyzed by software to locate the alignment of the spikes to sub-pixel resolution. The direction of this displacement"
  },
  {
    "source": "Bahtinov mask.txt",
    "chunk_id": "Bahtinov mask.txt_1",
    "chunk": "indicates the direction of the necessary focus correction. Rotating the mask through 180° will reverse the direction of spike movement, so it is easier to use if placed on the telescope with consistent orientation. The mask must be removed after accurate focus is achieved. The mask works by replacing the aperture stop of the optical system (normally the circular shape of the objective itself) with a stop which is asymmetric and periodic. Viewing a point source (such as a star) yields a diffraction pattern at the focal plane representing the Fraunhofer diffraction transform of the aperture shape. This pattern normally would be an Airy disk resulting from a circular aperture, but with the mask in place, the pattern exhibits asymmetric spikes representing the transform of the mask pattern's spatial frequency and orientation. A very bright star and very dark sky are required to produce highly contrasted spikes that are clearly visible. The diffraction effect is similar to producing sunstar patterns in landscape photography with ordinary camera lenses, where the mechanical iris of the lens is adjusted to a small polygonal shape with sharp corners. In the example below, the central pattern shows good focus. The central spike is noticeably displaced"
  },
  {
    "source": "Bahtinov mask.txt",
    "chunk_id": "Bahtinov mask.txt_2",
    "chunk": "from the central position in the left and right images."
  },
  {
    "source": "Baryon number.txt",
    "chunk_id": "Baryon number.txt_0",
    "chunk": "# Baryon number In particle physics, the baryon number (B) is an additive quantum number of a system. It is defined as B = 1 3 ( n q − n q ¯ ) , {\\displaystyle B={\\frac {1}{3}}(n_{\\text{q}}-n_{\\rm {\\overline {q}}}),} where ⁠ n q {\\displaystyle n_{\\rm {q}}} ⁠ is the number of quarks, and ⁠ n q ¯ {\\displaystyle n_{\\rm {\\overline {q}}}} ⁠ is the number of antiquarks. Baryons (three quarks) have B = +1, mesons (one quark, one antiquark) have B = 0, and antibaryons (three antiquarks) have B = −1. Exotic hadrons like pentaquarks (four quarks, one antiquark) and tetraquarks (two quarks, two antiquarks) are also classified as baryons and mesons depending on their baryon number. In the standard model B conservation is an accidental symmetry which means that it appears in the standard model but is often violated when going beyond it. Physics beyond the Standard Model theories that contain baryon number violation are, for example, Standard Model with extra dimensions, Supersymmetry, Grand Unified Theory and String theory. Quarks carry not only electric charge, but also charges such as color charge and weak isospin. Because of a phenomenon known as color confinement, a hadron cannot have a"
  },
  {
    "source": "Baryon number.txt",
    "chunk_id": "Baryon number.txt_1",
    "chunk": "net color charge; that is, the total color charge of a particle has to be zero (\"white\"). A quark can have one of three \"colors\", dubbed \"red\", \"green\", and \"blue\"; while an antiquark may be either \"anti-red\", \"anti-green\" or \"anti-blue\". The baryon number was defined long before the quark model was established, so rather than changing the definitions, particle physicists simply gave quarks one third the baryon number. In theory, exotic hadrons can be formed by adding pairs of quarks and antiquarks, provided that each pair has a matching color/anticolor. For example, a pentaquark (four quarks, one antiquark) could have the individual quark colors: red, green, blue, blue, and antiblue. In 2015, the LHCb collaboration at CERN reported results consistent with pentaquark states in the decay of bottom Lambda baryons (Λb). Baryon number is a 'conserved' quantity in the sense that for perturbutative reactions in the Standard Model the total baryon number of the incoming particles is equal to the baryon number of the outgoing particles. Baryon number violation has never been observed experimentally. However, neither Baryon number nor lepton number can from theory be shown to be conserved quantities due to nonperturbative effects in the Standard Model. These effects"
  },
  {
    "source": "Baryon number.txt",
    "chunk_id": "Baryon number.txt_2",
    "chunk": "are, for example, sphalerons and instantons. The hypothesized Adler–Bell–Jackiw anomaly in electroweak interactions is an example of an electroweak sphaleron. These reactions are massively suppressed at low energies/temperatures. At high temperatures, in for example the early universe, they could explain electroweak baryogenesis and leptogenesis. Sphalerons can only change the baryon and lepton number by 3 or multiples of 3 (the reactions create 3 leptons and 3 baryons or the corresponding antiparticles). This is because the sum of baryon and lepton number (see B − L) is a conserved quantity in the standard model. The hypothetical concepts of grand unified theory (GUT) models and supersymmetry allows for the changing of a baryon into leptons and antiquarks (see B − L), thus violating the conservation of both baryon and lepton numbers. Proton decay would be an example of such a process taking place, but has never been observed. Neutrinoless double beta decay is a reaction that would violate lepton number and neutron-to-antineutron oscillation would violate baryon number by −2 units. The conservation of baryon number is not consistent with the physics of black hole evaporation via Hawking radiation. It is expected in general that quantum gravitational effects violate the conservation of all"
  },
  {
    "source": "Baryon number.txt",
    "chunk_id": "Baryon number.txt_3",
    "chunk": "charges associated to global symmetries. The violation of conservation of baryon number led John Archibald Wheeler to speculate on a principle of mutability for all physical properties."
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_0",
    "chunk": "# Basalt Basalt (UK: /ˈbæsɒlt, -ɔːlt, -əlt/; US: /bəˈsɔːlt, ˈbeɪsɔːlt/) is an aphanitic (fine-grained) extrusive igneous rock formed from the rapid cooling of low-viscosity lava rich in magnesium and iron (mafic lava) exposed at or very near the surface of a rocky planet or moon. More than 90% of all volcanic rock on Earth is basalt. Rapid-cooling, fine-grained basalt is chemically equivalent to slow-cooling, coarse-grained gabbro. The eruption of basalt lava is observed by geologists at about 20 volcanoes per year. Basalt is also an important rock type on other planetary bodies in the Solar System. For example, the bulk of the plains of Venus, which cover ~80% of the surface, are basaltic; the lunar maria are plains of flood-basaltic lava flows; and basalt is a common rock on the surface of Mars. Molten basalt lava has a low viscosity due to its relatively low silica content (between 45% and 52%), resulting in rapidly moving lava flows that can spread over great areas before cooling and solidifying. Flood basalts are thick sequences of many such flows that can cover hundreds of thousands of square kilometres and constitute the most voluminous of all volcanic formations. Basaltic magmas within Earth are thought"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_1",
    "chunk": "to originate from the upper mantle. The chemistry of basalts thus provides clues to processes deep in Earth's interior. Basalt is composed mostly of oxides of silicon, iron, magnesium, potassium, aluminum, titanium, and calcium. Geologists classify igneous rock by its mineral content whenever possible; the relative volume percentages of quartz (crystalline silica (SiO2)), alkali feldspar, plagioclase, and feldspathoid (QAPF) are particularly important. An aphanitic (fine-grained) igneous rock is classified as basalt when its QAPF fraction is composed of less than 10% feldspathoid and less than 20% quartz, and plagioclase makes up at least 65% of its feldspar content. This places basalt in the basalt/andesite field of the QAPF diagram. Basalt is further distinguished from andesite by its silica content of under 52%. It is often not practical to determine the mineral composition of volcanic rocks, due to their very small grain size, in which case geologists instead classify the rocks chemically, with particular emphasis on the total content of alkali metal oxides and silica (TAS); in that context, basalt is defined as volcanic rock with a content of between 45% and 52% silica and no more than 5% alkali metal oxides. This places basalt in the B field of the"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_2",
    "chunk": "TAS diagram. Such a composition is described as mafic. Basalt is usually dark grey to black in colour, due to a high content of augite or other dark-coloured pyroxene minerals, but can exhibit a wide range of shading. Some basalts are quite light-coloured due to a high content of plagioclase; these are sometimes described as leucobasalts. It can be difficult to distinguish between lighter-colored basalt and andesite, so field researchers commonly use a rule of thumb for this purpose, classifying it as basalt if it has a color index of 35 or greater. The physical properties of basalt result from its relatively low silica content and typically high iron and magnesium content. The average density of basalt is 2.9 g/cm, compared, for example, to granite’s typical density of 2.7 g/cm. The viscosity of basaltic magma is relatively low—around 10 to 10 cP—similar to the viscosity of ketchup, but that is still several orders of magnitude higher than the viscosity of water, which is about 1 cP). Basalt is often porphyritic, containing larger crystals (phenocrysts) that formed before the extrusion event that brought the magma to the surface, embedded in a finer-grained matrix. These phenocrysts are usually made of augite, olivine,"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_3",
    "chunk": "or a calcium-rich plagioclase, which have the highest melting temperatures of any of the minerals that can typically crystallize from the melt, and which are therefore the first to form solid crystals. Basalt often contains vesicles; they are formed when dissolved gases bubble out of the magma as it decompresses during its approach to the surface; the erupted lava then solidifies before the gases can escape. When vesicles make up a substantial fraction of the volume of the rock, the rock is described as scoria. The term basalt is at times applied to shallow intrusive rocks with a composition typical of basalt, but rocks of this composition with a phaneritic (coarser) groundmass are more properly referred to either as diabase (also called dolerite) or—when they are more coarse-grained (having crystals over 2 mm across)—as gabbro. Diabase and gabbro are thus the hypabyssal and plutonic equivalents of basalt. During the Hadean, Archean, and early Proterozoic eons of Earth's history, the chemistry of erupted magmas was significantly different from what it is today, due to immature crustal and asthenosphere differentiation. The resulting ultramafic volcanic rocks, with silica (SiO2) contents below 45% and high magnesium oxide (MgO) content, are usually classified as komatiites."
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_4",
    "chunk": "The word \"basalt\" is ultimately derived from Late Latin basaltes, a misspelling of Latin basanites \"very hard stone\", which was imported from Ancient Greek βασανίτης (basanites), from βάσανος (basanos, \"touchstone\"). The modern petrological term basalt, describing a particular composition of lava-derived rock, became standard because of its use by Georgius Agricola in 1546, in his work De Natura Fossilium. Agricola applied the term \"basalt\" to the volcanic black rock beneath the Bishop of Meissen's Stolpen castle, believing it to be the same as the \"basaniten\" described by Pliny the Elder in AD 77 in Naturalis Historiae. On Earth, most basalt is formed by decompression melting of the mantle. The high pressure in the upper mantle (due to the weight of the overlying rock) raises the melting point of mantle rock, so that almost all of the upper mantle is solid. However, mantle rock is ductile (the solid rock slowly deforms under high stress). When tectonic forces cause hot mantle rock to creep upwards, pressure on the ascending rock decreases, and this can lower its melting point enough for the rock to partially melt, producing basaltic magma. Decompression melting can occur in a variety of tectonic settings, including in continental rift"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_5",
    "chunk": "zones, at mid-ocean ridges, above geological hotspots, and in back-arc basins. Basalt also forms in subduction zones, where mantle rock rises into a mantle wedge above the descending slab. The slab releases water vapor and other volatiles as it descends, which further lowers the melting point, further increasing the amount of decompression melting. Each tectonic setting produces basalt with its own distinctive characteristics. The mineralogy of basalt is characterized by a preponderance of calcic plagioclase feldspar and pyroxene. Olivine can also be a significant constituent. Accessory minerals present in relatively minor amounts include iron oxides and iron-titanium oxides, such as magnetite, ulvöspinel, and ilmenite. Because of the presence of such oxide minerals, basalt can acquire strong magnetic signatures as it cools, and paleomagnetic studies have made extensive use of basalt. In tholeiitic basalt, pyroxene (augite and orthopyroxene or pigeonite) and calcium-rich plagioclase are common phenocryst minerals. Olivine may also be a phenocryst, and when present, may have rims of pigeonite. The groundmass contains interstitial quartz or tridymite or cristobalite. Olivine tholeiitic basalt has augite and orthopyroxene or pigeonite with abundant olivine, but olivine may have rims of pyroxene and is unlikely to be present in the groundmass. Alkali basalts typically"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_6",
    "chunk": "have mineral assemblages that lack orthopyroxene but contain olivine. Feldspar phenocrysts typically are labradorite to andesine in composition. Augite is rich in titanium compared to augite in tholeiitic basalt. Minerals such as alkali feldspar, leucite, nepheline, sodalite, phlogopite mica, and apatite may be present in the groundmass. Basalt has high liquidus and solidus temperatures—values at the Earth's surface are near or above 1200 °C (liquidus) and near or below 1000 °C (solidus); these values are higher than those of other common igneous rocks. The majority of tholeiitic basalts are formed at approximately 50–100 km depth within the mantle. Many alkali basalts may be formed at greater depths, perhaps as deep as 150–200 km. The origin of high-alumina basalt continues to be controversial, with disagreement over whether it is a primary melt or derived from other basalt types by fractionation. Relative to most common igneous rocks, basalt compositions are rich in MgO and CaO and low in SiO2 and the alkali oxides, i.e., Na2O + K2O, consistent with their TAS classification. Basalt contains more silica than picrobasalt and most basanites and tephrites but less than basaltic andesite. Basalt has a lower total content of alkali oxides than trachybasalt and most basanites"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_7",
    "chunk": "and tephrites. Basalt generally has a composition of 45–52 wt% SiO2, 2–5 wt% total alkalis, 0.5–2.0 wt% TiO2, 5–14 wt% FeO and 14 wt% or more Al2O3. Contents of CaO are commonly near 10 wt%, those of MgO commonly in the range 5 to 12 wt%. High-alumina basalts have aluminium contents of 17–19 wt% Al2O3; boninites have magnesium (MgO) contents of up to 15 percent. Rare feldspathoid-rich mafic rocks, akin to alkali basalts, may have Na2O + K2O contents of 12% or more. The abundances of the lanthanide or rare-earth elements (REE) can be a useful diagnostic tool to help explain the history of mineral crystallisation as the melt cooled. In particular, the relative abundance of europium compared to the other REE is often markedly higher or lower, and called the europium anomaly. It arises because Eu can substitute for Ca in plagioclase feldspar, unlike any of the other lanthanides, which tend to only form cations. Mid-ocean ridge basalts (MORB) and their intrusive equivalents, gabbros, are the characteristic igneous rocks formed at mid-ocean ridges. They are tholeiitic basalts particularly low in total alkalis and in incompatible trace elements, and they have relatively flat REE patterns normalized to mantle or chondrite"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_8",
    "chunk": "values. In contrast, alkali basalts have normalized patterns highly enriched in the light REE, and with greater abundances of the REE and of other incompatible elements. Because MORB basalt is considered a key to understanding plate tectonics, its compositions have been much studied. Although MORB compositions are distinctive relative to average compositions of basalts erupted in other environments, they are not uniform. For instance, compositions change with position along the Mid-Atlantic Ridge, and the compositions also define different ranges in different ocean basins. Mid-ocean ridge basalts have been subdivided into varieties such as normal (NMORB) and those slightly more enriched in incompatible elements (EMORB). Isotope ratios of elements such as strontium, neodymium, lead, hafnium, and osmium in basalts have been much studied to learn about the evolution of the Earth's mantle. Isotopic ratios of noble gases, such as He/He, are also of great value: for instance, ratios for basalts range from 6 to 10 for mid-ocean ridge tholeiitic basalt (normalized to atmospheric values), but to 15–24 and more for ocean-island basalts thought to be derived from mantle plumes. Source rocks for the partial melts that produce basaltic magma probably include both peridotite and pyroxenite. The shape, structure and texture of"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_9",
    "chunk": "a basalt is diagnostic of how and where it erupted—for example, whether into the sea, in an explosive cinder eruption or as creeping pāhoehoe lava flows, the classic image of Hawaiian basalt eruptions. Basalt that erupts under open air (that is, subaerially) forms three distinct types of lava or volcanic deposits: scoria; ash or cinder (breccia); and lava flows. Basalt in the tops of subaerial lava flows and cinder cones will often be highly vesiculated, imparting a lightweight \"frothy\" texture to the rock. Basaltic cinders are often red, coloured by oxidized iron from weathered iron-rich minerals such as pyroxene. ʻAʻā types of blocky cinder and breccia flows of thick, viscous basaltic lava are common in Hawaiʻi. Pāhoehoe is a highly fluid, hot form of basalt which tends to form thin aprons of molten lava which fill up hollows and sometimes forms lava lakes. Lava tubes are common features of pāhoehoe eruptions. Basaltic tuff or pyroclastic rocks are less common than basaltic lava flows. Usually basalt is too hot and fluid to build up sufficient pressure to form explosive lava eruptions but occasionally this will happen by trapping of the lava within the volcanic throat and buildup of volcanic gases. Hawaiʻi's"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_10",
    "chunk": "Mauna Loa volcano erupted in this way in the 19th century, as did Mount Tarawera, New Zealand in its violent 1886 eruption. Maar volcanoes are typical of small basalt tuffs, formed by explosive eruption of basalt through the crust, forming an apron of mixed basalt and wall rock breccia and a fan of basalt tuff further out from the volcano. Amygdaloidal structure is common in relict vesicles and beautifully crystallized species of zeolites, quartz or calcite are frequently found. During the cooling of a thick lava flow, contractional joints or fractures form. If a flow cools relatively rapidly, significant contraction forces build up. While a flow can shrink in the vertical dimension without fracturing, it cannot easily accommodate shrinking in the horizontal direction unless cracks form; the extensive fracture network that develops results in the formation of columns. These structures, or basalt prisms, are predominantly hexagonal in cross-section, but polygons with three to twelve or more sides can be observed. The size of the columns depends loosely on the rate of cooling; very rapid cooling may result in very small (<1 cm diameter) columns, while slow cooling is more likely to produce large columns. The character of submarine basalt eruptions"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_11",
    "chunk": "is largely determined by depth of water, since increased pressure restricts the release of volatile gases and results in effusive eruptions. It has been estimated that at depths greater than 500 metres (1,600 ft), explosive activity associated with basaltic magma is suppressed. Above this depth, submarine eruptions are often explosive, tending to produce pyroclastic rock rather than basalt flows. These eruptions, described as Surtseyan, are characterised by large quantities of steam and gas and the creation of large amounts of pumice. When basalt erupts underwater or flows into the sea, contact with the water quenches the surface and the lava forms a distinctive pillow shape, through which the hot lava breaks to form another pillow. This \"pillow\" texture is very common in underwater basaltic flows and is diagnostic of an underwater eruption environment when found in ancient rocks. Pillows typically consist of a fine-grained core with a glassy crust and have radial jointing. The size of individual pillows varies from 10 cm up to several metres. When pāhoehoe lava enters the sea it usually forms pillow basalts. However, when ʻaʻā enters the ocean it forms a littoral cone, a small cone-shaped accumulation of tuffaceous debris formed when the blocky ʻaʻā"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_12",
    "chunk": "lava enters the water and explodes from built-up steam. The island of Surtsey in the Atlantic Ocean is a basalt volcano which breached the ocean surface in 1963. The initial phase of Surtsey's eruption was highly explosive, as the magma was quite fluid, causing the rock to be blown apart by the boiling steam to form a tuff and cinder cone. This has subsequently moved to a typical pāhoehoe-type behaviour. Volcanic glass may be present, particularly as rinds on rapidly chilled surfaces of lava flows, and is commonly (but not exclusively) associated with underwater eruptions. Basalt is the most common volcanic rock type on Earth, making up over 90% of all volcanic rock on the planet. The crustal portions of oceanic tectonic plates are composed predominantly of basalt, produced from upwelling mantle below the ocean ridges. Basalt is also the principal volcanic rock in many oceanic islands, including the islands of Hawaiʻi, the Faroe Islands, and Réunion. The eruption of basalt lava is observed by geologists at about 20 volcanoes per year. Basalt is the rock most typical of large igneous provinces. These include continental flood basalts, the most voluminous basalts found on land. Examples of continental flood basalts included"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_13",
    "chunk": "the Deccan Traps in India, the Chilcotin Group in British Columbia, Canada, the Paraná Traps in Brazil, the Siberian Traps in Russia, the Karoo flood basalt province in South Africa, and the Columbia River Plateau of Washington and Oregon. Basalt is also prevalent across extensive regions of the Eastern Galilee, Golan, and Bashan in Israel and Syria. Ancient Precambrian basalts are usually only found in fold and thrust belts, and are often heavily metamorphosed. These are known as greenstone belts, because low-grade metamorphism of basalt produces chlorite, actinolite, epidote and other green minerals. As well as forming large parts of the Earth's crust, basalt also occurs in other parts of the Solar System. Basalt commonly erupts on Io (the third largest moon of Jupiter), and has also formed on the Moon, Mars, Venus, and the asteroid Vesta. The dark areas visible on Earth's moon, the lunar maria, are plains of flood basaltic lava flows. These rocks were sampled both by the crewed American Apollo program and the robotic Russian Luna program, and are represented among the lunar meteorites. Lunar basalts differ from their Earth counterparts principally in their high iron contents, which typically range from about 17 to 22 wt%"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_14",
    "chunk": "FeO. They also possess a wide range of titanium concentrations (present in the mineral ilmenite), ranging from less than 1 wt% TiO2, to about 13 wt.%. Traditionally, lunar basalts have been classified according to their titanium content, with classes being named high-Ti, low-Ti, and very-low-Ti. Nevertheless, global geochemical maps of titanium obtained from the Clementine mission demonstrate that the lunar maria possess a continuum of titanium concentrations, and that the highest concentrations are the least abundant. Lunar basalts show exotic textures and mineralogy, particularly shock metamorphism, lack of the oxidation typical of terrestrial basalts, and a complete lack of hydration. Most of the Moon's basalts erupted between about 3 and 3.5 billion years ago, but the oldest samples are 4.2 billion years old, and the youngest flows, based on the age dating method of crater counting, are estimated to have erupted only 1.2 billion years ago. From 1972 to 1985, five Venera and two VEGA landers successfully reached the surface of Venus and carried out geochemical measurements using X-ray fluorescence and gamma-ray analysis. These returned results consistent with the rock at the landing sites being basalts, including both tholeiitic and highly alkaline basalts. The landers are thought to have landed"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_15",
    "chunk": "on plains whose radar signature is that of basaltic lava flows. These constitute about 80% of the surface of Venus. Some locations show high reflectivity consistent with unweathered basalt, indicating basaltic volcanism within the last 2.5 million years. Basalt is also a common rock on the surface of Mars, as determined by data sent back from the planet's surface, and by Martian meteorites. Analysis of Hubble Space Telescope images of Vesta suggests this asteroid has a basaltic crust covered with a brecciated regolith derived from the crust. Evidence from Earth-based telescopes and the Dawn mission suggest that Vesta is the source of the HED meteorites, which have basaltic characteristics. Vesta is the main contributor to the inventory of basaltic asteroids of the main Asteroid Belt. Lava flows represent a major volcanic terrain on Io. Analysis of the Voyager images led scientists to believe that these flows were composed mostly of various compounds of molten sulfur. However, subsequent Earth-based infrared studies and measurements from the Galileo spacecraft indicate that these flows are composed of basaltic lava with mafic to ultramafic compositions. This conclusion is based on temperature measurements of Io's \"hotspots\", or thermal-emission locations, which suggest temperatures of at least 1,300"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_16",
    "chunk": "K and some as high as 1,600 K. Initial estimates suggesting eruption temperatures approaching 2,000 K have since proven to be overestimates because the wrong thermal models were used to model the temperatures. Compared to granitic rocks exposed at the Earth's surface, basalt outcrops weather relatively rapidly. This reflects their content of minerals that crystallized at higher temperatures and in an environment poorer in water vapor than granite. These minerals are less stable in the colder, wetter environment at the Earth's surface. The finer grain size of basalt and the volcanic glass sometimes found between the grains also hasten weathering. The high iron content of basalt causes weathered surfaces in humid climates to accumulate a thick crust of hematite or other iron oxides and hydroxides, staining the rock a brown to rust-red colour. Because of the low potassium content of most basalts, weathering converts the basalt to calcium-rich clay (montmorillonite) rather than potassium-rich clay (illite). Further weathering, particularly in tropical climates, converts the montmorillonite to kaolinite or gibbsite. This produces the distinctive tropical soil known as laterite. The ultimate weathering product is bauxite, the principal ore of aluminium. Chemical weathering also releases readily water-soluble cations such as calcium, sodium and"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_17",
    "chunk": "magnesium, which give basaltic areas a strong buffer capacity against acidification. Calcium released by basalts binds CO2 from the atmosphere forming CaCO3 acting thus as a CO2 trap. Intense heat or great pressure transforms basalt into its metamorphic rock equivalents. Depending on the temperature and pressure of metamorphism, these may include greenschist, amphibolite, or eclogite. Basalts are important rocks within metamorphic regions because they can provide vital information on the conditions of metamorphism that have affected the region. Metamorphosed basalts are important hosts for a variety of hydrothermal ores, including deposits of gold, copper and volcanogenic massive sulfides. The common corrosion features of underwater volcanic basalt suggest that microbial activity may play a significant role in the chemical exchange between basaltic rocks and seawater. The significant amounts of reduced iron, Fe(II), and manganese, Mn(II), present in basaltic rocks provide potential energy sources for bacteria. Some Fe(II)-oxidizing bacteria cultured from iron-sulfide surfaces are also able to grow with basaltic rock as a source of Fe(II). Fe- and Mn- oxidizing bacteria have been cultured from weathered submarine basalts of Kamaʻehuakanaloa Seamount (formerly Loihi). The impact of bacteria on altering the chemical composition of basaltic glass (and thus, the oceanic crust) and seawater"
  },
  {
    "source": "Basalt.txt",
    "chunk_id": "Basalt.txt_18",
    "chunk": "suggest that these interactions may lead to an application of hydrothermal vents to the origin of life. Basalt is used in construction (e.g. as building blocks or in the groundwork), making cobblestones (from columnar basalt) and in making statues. Heating and extruding basalt yields stone wool, which has potential to be an excellent thermal insulator. Carbon sequestration in basalt has been studied as a means of removing carbon dioxide, produced by human industrialization, from the atmosphere. Underwater basalt deposits, scattered in seas around the globe, have the added benefit of the water serving as a barrier to the re-release of CO2 into the atmosphere."
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_0",
    "chunk": "# BBC The British Broadcasting Corporation (BBC) is a British public service broadcaster headquartered at Broadcasting House in London, England. Originally established in 1922 as the British Broadcasting Company, it evolved into its current state with its current name on New Year's Day 1927. The oldest and largest local and global broadcaster by stature and by number of employees, the BBC employs over 21,000 staff in total, of whom approximately 17,200 are in public-sector broadcasting. The BBC was established under a royal charter, and operates under an agreement with the Secretary of State for Culture, Media and Sport. Its work is funded principally by an annual television licence fee which is charged to all British households, companies, and organisations using any type of equipment to receive or record live television broadcasts or to use the BBC's streaming service, iPlayer. The fee is set by the British government, agreed by Parliament, and is used to fund the BBC's radio, TV, and online services covering the nations and regions of the UK. Since 1 April 2014, it has also funded the BBC World Service (launched in 1932 as the BBC Empire Service), which broadcasts in 28 languages and provides comprehensive TV, radio,"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_1",
    "chunk": "and online services in Arabic and Persian. Some of the BBC's revenue comes from its commercial subsidiary BBC Studios (formerly BBC Worldwide), which sells BBC programmes and services internationally and also distributes the BBC's international 24-hour English-language news services BBC News, and from BBC.com, provided by BBC Global News Ltd. In 2009, the company was awarded the Queen's Award for Enterprise in recognition of its international achievements in business. Since its formation in 1922, the BBC has played a prominent role in British life and culture. It is sometimes informally referred to as the Beeb or Auntie. In 1923 it launched Radio Times (subtitled \"The official organ of the BBC\"), the first broadcast listings magazine; the 1988 Christmas edition sold 11 million copies, the biggest-selling edition of any British magazine in history. Britain's first live public broadcast was made from the factory of Marconi Company in Chelmsford in June 1920. It was sponsored by the Daily Mail's Alfred Harmsworth, 1st Viscount Northcliffe and featured the famous Australian soprano Dame Nellie Melba. The Melba broadcast caught the people's imagination and marked a turning point in the British public's attitude to radio. However, this public enthusiasm was not shared in official circles"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_2",
    "chunk": "where such broadcasts were held to interfere with important military and civil communications. By late 1920, the pressure from these quarters and uneasiness among the staff of the licensing authority, the General Post Office (GPO), was sufficient to lead to a ban on further Chelmsford broadcasts. But by 1922, the GPO had received nearly 100 broadcast licence requests and moved to rescind its ban in the wake of a petition by 63 wireless societies with over 3,000 members. Anxious to avoid the same chaotic expansion experienced in the United States, the GPO proposed that it would issue a single broadcasting licence to a company jointly owned by a consortium of leading wireless receiver manufacturers, to be known as the British Broadcasting Company Ltd, which was formed on 18 October 1922. John Reith, a Scottish Calvinist, was appointed its general manager in December 1922 a few weeks after the company made its first official broadcast. L. Stanton Jefferies was its first director of music. The company was to be financed by a royalty on the sale of BBC wireless receiving sets from approved domestic manufacturers. To this day, the BBC aims to follow the Reithian directive to \"inform, educate and entertain\"."
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_3",
    "chunk": "The financial arrangements soon proved inadequate. Set sales were disappointing as amateurs made their own receivers and listeners bought rival unlicensed sets. By mid-1923, discussions between the GPO and the BBC had become deadlocked and the Postmaster General commissioned a review of broadcasting by the Sykes Committee. The committee recommended a short-term reorganisation of licence fees with improved enforcement in order to address the BBC's immediate financial distress, and an increased share of the licence revenue split between it and the GPO. This was to be followed by a simple 10 shillings licence fee to fund broadcasts. The BBC's broadcasting monopoly was made explicit for the duration of its current broadcast licence, as was the prohibition on advertising. To avoid competition with newspapers, Fleet Street persuaded the government to ban news bulletins before 7 pm and the BBC was required to source all news from external wire services. The Radio Times, the world's first and longest-running radio and television listings magazine, was launched by Reith in September 1923. The first edition, subtitled \"The official organ of the BBC\", was priced at tuppence (two pence) on newsstands, and quickly sold out its run of a quarter of a million copies. Mid-1925"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_4",
    "chunk": "found the future of broadcasting under further consideration, this time by the Crawford committee. By now, the BBC, under Reith's leadership, had forged a consensus favouring a continuation of the unified (monopoly) broadcasting service, but more money was still required to finance rapid expansion. Wireless manufacturers were anxious to exit the loss-making consortium, and Reith was keen that the BBC be seen as a public service rather than a commercial enterprise. The recommendations of the Crawford Committee were published in March the following year and were still under consideration by the GPO when the 1926 United Kingdom general strike broke out in May. The strike temporarily interrupted newspaper production, and with restrictions on news bulletins waived, the BBC suddenly became the primary source of news for the duration of the crisis. The crisis placed the BBC in a delicate position. On the one hand Reith was acutely aware that the government might exercise its right to commandeer the BBC at any time as a mouthpiece of the government if the BBC were to step out of line, but on the other he was anxious to maintain public trust by appearing to be acting independently. The government was divided on how"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_5",
    "chunk": "to handle the BBC, but ended up trusting Reith, whose opposition to the strike mirrored the PM's own. Although Winston Churchill in particular wanted to commandeer the BBC to use it \"to the best possible advantage\", Reith wrote that Stanley Baldwin's government wanted to be able to say \"that they did not commandeer [the BBC], but they know that they can trust us not to be really impartial\". Thus the BBC was granted sufficient leeway to pursue the government's objectives largely in a manner of its own choosing. Supporters of the strike nicknamed the BBC the BFC for British Falsehood Company. Reith personally announced the end of the strike which he marked by reciting from Blake's \"Jerusalem\" signifying that England had been saved. While the BBC tends to characterise its coverage of the general strike by emphasising the positive impression created by its balanced coverage of the views of government and strikers, Seaton has characterised the episode as the invention of \"modern propaganda in its British form\". Reith argued that trust gained by 'authentic impartial news' could then be used. Impartial news was not necessarily an end in itself. The BBC did well out of the crisis, which cemented a"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_6",
    "chunk": "national audience for its broadcasting, and it was followed by the Government's acceptance of the recommendation made by the Crawford Committee (1925–26) that the British Broadcasting Company be replaced by a non-commercial, Crown-chartered organisation: the British Broadcasting Corporation. The British Broadcasting Corporation came into existence on 1 January 1927, and Reith – newly knighted – was appointed its first director general. To represent its purpose and (stated) values, the new corporation adopted the coat of arms, including the motto \"Nation shall speak peace unto Nation\". British radio audiences had little choice apart from BBC's programming approach. Reith was viewed as taking a moralistic approach as an executive, aiming to broadcast \"all that is best in every department of human knowledge, endeavour and achievement\", and putting the programming in moral or ethical terms, advocating \"a high moral tone\" as \"obviously of paramount importance\". Reith succeeded in building a high wall against a more tabloid, free-for-all in radio aimed at merely attracting the largest audience (and advertising revenue). There was no paid advertising on the BBC; all the revenue came from a tax on receiving sets. Highbrow audiences, however, greatly enjoyed it. At a time when American, Australian and Canadian stations were"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_7",
    "chunk": "drawing huge audiences cheering for their local teams with the broadcast of baseball, rugby and hockey, the BBC emphasised service for a national rather than a regional audience. Boat races were well covered along with tennis and horse racing, but the BBC was reluctant to spend its severely limited air time on long football or cricket games, regardless of their popularity. John Reith and the BBC, with support from the Crown, determined the universal needs of the people of Britain and broadcast content according to these perceived standards. Reith effectively censored anything that he felt would be harmful, directly or indirectly. While recounting his time with the BBC in 1935, Raymond Postgate claims that BBC broadcasters were made to submit a draft of their potential broadcast for approval. It was expected that they tailored their content to accommodate the modest, church-going elderly or a member of the Clergy. Until 1928, entertainers broadcasting on the BBC, both singers and \"talkers\" were expected to avoid biblical quotations, Clerical impersonations and references, references to drink or Prohibition in the United States, vulgar and doubtful matter and political allusions. The BBC excluded popular foreign music and musicians from its broadcasts, while promoting British alternatives."
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_8",
    "chunk": "On 5 March 1928, Stanley Baldwin, the Prime Minister, maintained the censorship of editorial opinions on public policy, but allowed the BBC to address matters of religious, political or industrial controversy. The resulting political \"talk series\", designed to inform England on political issues, were criticised by members of parliament, including Winston Churchill, David Lloyd George and Sir Austen Chamberlain. Those who opposed these chats claimed that they silence the opinions of those in Parliament who are not nominated by Party Leaders or Party Whips, thus stifling independent, non-official views. In October 1932, the policemen of the Metropolitan Police Federation marched in protest at a proposed pay cut. Fearing dissent within the police force and public support for the movement, the BBC censored its coverage of the events, only broadcasting official statements from the government. Throughout the 1930s, political broadcasts had been closely monitored by the BBC. In 1935, the BBC censored the broadcasts of Oswald Mosley and Harry Pollitt. Mosley was a leader of the British Union of Fascists, and Pollitt a leader of the Communist Party of Great Britain. They had been contracted to provide a series of five broadcasts on their parties' politics. The BBC, in conjunction with"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_9",
    "chunk": "The Foreign Office of Britain, first suspended this series and ultimately cancelled it without the notice of the public. Less radical politicians faced similar censorship. In 1938, Winston Churchill proposed a series of talks regarding British domestic and foreign politics and affairs but was similarly censored. The censorship of political discourse by the BBC was a precursor to the total shutdown of political debate that manifested over the BBC's wartime airwaves. The Foreign Office maintained that the public should not be aware of their role in the censorship. From 1935 to 1939, the BBC also attempted to unite the British Empire's radio waves, sending staff to Egypt, Palestine, Newfoundland, Jamaica, India, Canada and South Africa. Reith personally visited South Africa, lobbying for state-run radio programmes which was accepted by South African Parliament in 1936. A similar programme was adopted in Canada. Through collaboration with these state-run broadcasting centres, Reith left a legacy of cultural influence across the empire of Great Britain with his departure from the corporation in 1938. Experimental television broadcasts were started in 1929, using an electromechanical 30-line system developed by John Logie Baird. Limited regular broadcasts using this system began in 1932, and an expanded service (now"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_10",
    "chunk": "named the BBC Television Service) started from Alexandra Palace in November 1936, alternating between an improved Baird mechanical 240-line system and the all-electronic 405-line Marconi-EMI system which had been developed by an EMI research team led by Sir Isaac Shoenberg. The superiority of the electronic system saw the mechanical system dropped early the following year, with the Marconi-EMI system the first fully electronic television system in the world to be used in regular broadcasting. The success of broadcasting provoked animosities between the BBC and well-established media such as theatres, concert halls and the recording industry. By 1929, the BBC complained that the agents of many comedians refused to sign contracts for broadcasting, because they feared it harmed the artist \"by making his material stale\" and that it \"reduces the value of the artist as a visible music-hall performer\". On the other hand, the BBC was \"keenly interested\" in a cooperation with the recording companies who \"in recent years ... have not been slow to make records of singers, orchestras, dance bands, etc. who have already proved their power to achieve popularity by wireless.\" Radio plays were so popular that the BBC had received 6,000 manuscripts by 1929, most of them"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_11",
    "chunk": "written for stage and of little value for broadcasting: \"Day in and day out, manuscripts come in, and nearly all go out again through the post, with a note saying 'We regret, etc.'\" In the 1930s music broadcasts also enjoyed great popularity, for example the friendly and wide-ranging BBC Theatre Organ broadcasts at St George's Hall, London by Reginald Foort, who held the official role of BBC Staff Theatre Organist from 1936 to 1938. Television broadcasting was suspended from 1 September 1939 to 7 June 1946, during World War II, and it was left to BBC Radio broadcasters such as Reginald Foort to keep the nation's spirits up. The BBC moved most of its radio operations out of London, initially to Bristol, and then to Bedford. Concerts were broadcast from the Bedford Corn Exchange; the Trinity Chapel in St Paul's Church, Bedford was the studio for the Daily Service (a daily 15 minute religious service first broadcast on the BBC in 1928 which continues today) from 1941 to 1945, and, in the darkest days of the war in 1941, the Archbishops of Canterbury and York came to St Paul's to broadcast to the UK and the world on the National"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_12",
    "chunk": "Day of Prayer. BBC employees during the war included George Orwell who spent two years with the broadcaster. During his role as prime minister during the war, Winston Churchill delivered 33 major wartime speeches by radio, all of which were carried by the BBC within the UK. On 18 June 1940, French general Charles de Gaulle, in exile in London as the leader of the Free French, made a speech, broadcast by the BBC, urging the French people not to capitulate to the Nazis. In October 1940, Princesses Elizabeth and Margaret made their first radio broadcast for the BBC's Children's Hour, addressing other children who had been evacuated from cities. In 1938, John Reith and the Government of the United Kingdom, specifically the Ministry of Information which had been set up for WWII, designed a censorship apparatus for the inevitability of war. Due to the BBC's advancements in shortwave radio technology, the corporation could broadcast across the world during the Second World War. Within Europe, the BBC European Service would gather intelligence and information regarding the current events of the war in English. Regional BBC workers, based on their regional geo-political climate, would then further censor the material their broadcasts"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_13",
    "chunk": "would cover. Nothing was to be added outside the preordained news items. For example, the BBC Polish Service was heavily censored due to fears of jeopardising relations with the Soviet Union. Controversial topics, i.e. the contested Polish and Soviet border, the deportation of Polish citizens, the arrests of Polish Home Army members and the Katyn massacre, were not included in Polish broadcasts. American radio broadcasts were broadcast across Europe on BBC channels. This material also passed through the BBC's censorship office, which surveilled and edited American coverage of British affairs. By 1940, across all BBC broadcasts, music by composers from enemy nations was censored. In total, 99 German, 38 Austrian and 38 Italian composers were censored. The BBC argued that like the Italian or German languages, listeners would be irritated by the inclusion of enemy composers. Any potential broadcasters said to have pacifist, communist or fascist ideologies were not allowed on the BBC's airwaves. In 1937, a MI5 security officer was given a permanent office within the organisation. This officer would examine the files of potential political subversives and mark the files of those deemed a security risk to the organisation, blacklisting them. This was often done on spurious grounds;"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_14",
    "chunk": "even so, the practice would continue and expand during the years of the Cold War. There was a widely reported urban myth that, upon resumption of the BBC television service after the war, announcer Leslie Mitchell started by saying, \"As I was saying before we were so rudely interrupted ...\" In fact, the first person to appear when transmission resumed was Jasmine Bligh and the words said were \"Good afternoon, everybody. How are you? Do you remember me, Jasmine Bligh ... ?\" The European Broadcasting Union was formed on 12 February 1950, in Torquay with the BBC among the 23 founding broadcasting organisations. Competition to the BBC was introduced in 1955, with the commercial and independently operated television network of Independent Television (ITV). However, the BBC monopoly on radio services would persist until 8 October 1973 when under the control of the newly renamed Independent Broadcasting Authority (IBA), the UK's first Independent local radio station, LBC came on-air in the London area. As a result of the Pilkington Committee report of 1962, in which the BBC was praised for the quality and range of its output, and ITV was very heavily criticised for not providing enough quality programming, the decision"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_15",
    "chunk": "was taken to award the BBC a second television channel, BBC2, in 1964, renaming the existing service BBC1. BBC2 used the higher resolution 625-line standard which had been standardised across Europe. BBC2 was broadcast in colour from 1 July 1967 and was joined by BBC1 and ITV on 15 November 1969. The 405-line VHF transmissions of BBC1 (and ITV) were continued for compatibility with older television receivers until 1985. Starting in 1964, a series of pirate radio stations (starting with Radio Caroline) came on the air and forced the British government finally to regulate radio services to permit nationally based advertising-financed services. In response, the BBC reorganised and renamed their radio channels. On 30 September 1967, the Light Programme was split into Radio 1 offering continuous \"Popular\" music and Radio 2 more \"Easy Listening\". The \"Third\" programme became Radio 3 offering classical music and cultural programming. The Home Service became Radio 4 offering news, and non-musical content such as quiz shows, readings, dramas and plays. As well as the four national channels, a series of local BBC radio stations were established in 1967, including Radio London. In 1969, the BBC Enterprises department was formed to exploit BBC brands and programmes"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_16",
    "chunk": "for commercial spin-off products. In 1979, it became a wholly owned limited company, BBC Enterprises Ltd. In 1974, the BBC's teletext service, Ceefax, was introduced, created initially to provide subtitling, but developed into a news and information service. In 1978, BBC staff went on strike just before the Christmas, thus blocking out the transmission of both channels and amalgamating all four radio stations into one. Since the deregulation of the UK television and radio market in the 1980s, the BBC has faced increased competition from the commercial sector (and from the advertiser-funded public service broadcaster Channel 4), especially on satellite television, cable television, and digital television services. In the late 1980s, the BBC began a process of divestment by spinning off and selling parts of its organisation. In 1988, it sold off the Hulton Press Library, a photographic archive which had been acquired from the Picture Post magazine by the BBC in 1957. The archive was sold to Brian Deutsch and is now owned by Getty Images. In 1987, the BBC decided to centralize its operations by the management team with the radio and television divisions joining forces together for the first time, the activities of the news and currents"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_17",
    "chunk": "departments and coordinated jointly under the new directorate. During the 1990s, this process continued with the separation of certain operational arms of the corporation into autonomous but wholly owned subsidiaries, with the aim of generating additional revenue for programme-making. BBC Enterprises was reorganised and relaunched in 1995, as BBC Worldwide Ltd. In 1998, BBC studios, outside broadcasts, post production, design, costumes and wigs were spun off into BBC Resources Ltd. The BBC Research & Development has played a major part in the development of broadcasting and recording techniques. The BBC was also responsible for the development of the NICAM stereo standard. In recent decades, a number of additional channels and radio stations have been launched: Radio 5 was launched in 1990, as a sports and educational station, but was replaced in 1994, with BBC Radio 5 Live to become a live radio station, following the success of the Radio 4 service to cover the 1991 Gulf War. The new station would be a news and sport station. In 1997, BBC News 24, a rolling news channel, launched on digital television services, and the following year, BBC Choice was launched as the third general entertainment channel from the BBC. The BBC"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_18",
    "chunk": "also purchased The Parliamentary Channel, which was renamed BBC Parliament. In 1999, BBC Knowledge launched as a multimedia channel, with services available on the newly launched BBC Text digital teletext service (later rebranded as BBC Red Button), and on BBC Online. The channel had an educational aim, which was modified later on in its life to offer documentaries. In 2002, several television and radio channels were reorganised. BBC Knowledge was replaced by BBC Four and became the BBC's arts and documentaries channel. CBBC, which had been a programming strand as Children's BBC since 1985, was split into CBBC and CBeebies, for younger children, with both new services getting a digital channel: the CBBC Channel and CBeebies Channel. In addition to the television channels, new digital radio stations were created: 1Xtra, 6 Music and Radio 4 Extra. BBC 1Xtra was a sister station to Radio 1 and specialised in modern black music, BBC 6 Music specialised in alternative music genres and BBC7 specialised in archive, speech and children's programming. The following few years resulted in repositioning of some channels to conform to a larger brand: in 2003, BBC Choice was replaced by BBC Three, with programming for younger adults and shocking"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_19",
    "chunk": "real-life documentaries, BBC News 24 became the BBC News Channel in 2008, and BBC Radio 7 became BBC Radio 4 Extra in 2011, with new programmes to supplement those broadcast on Radio 4. In 2008, another channel was launched, BBC Alba, a Scottish Gaelic service. During this decade, the corporation began to sell off a number of its operational divisions to private owners; BBC Broadcast was spun off as a separate company in 2002, and in 2005, it was sold off to Australian-based Macquarie Capital Alliance Group and Macquarie Group Limited and rebranded Red Bee Media. The BBC's IT, telephony and broadcast technology were brought together as BBC Technology Ltd in 2001, and the division was later sold to the German company Siemens IT Solutions and Services (SIS). SIS was subsequently acquired from Siemens by the French company Atos. Further divestments included BBC Books (sold to Random House in 2006); BBC Outside Broadcasts Ltd (sold in 2008 to Satellite Information Services); Costumes and Wigs (stock sold in 2008 to Angels Costumes); and BBC Magazines (sold to Immediate Media Company in 2011). After the sales of OBs and costumes, the remainder of BBC Resources was reorganised as BBC Studios and Post"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_20",
    "chunk": "Production, which continues today as a wholly owned subsidiary of the BBC. The 2004 Hutton Inquiry and the subsequent report raised questions about the BBC's journalistic standards and its impartiality. This led to resignations of senior management members at the time including the then Director General, Greg Dyke. In January 2007, the BBC released minutes of the board meeting which led to Greg Dyke's resignation. Unlike the other departments of the BBC, the BBC World Service was funded by the Foreign, Commonwealth and Development Office. The Foreign and Commonwealth Office, more commonly known as the Foreign Office or the FCO, is the British government department responsible for promoting the interests of the United Kingdom abroad. A strike in 2005 by more than 11,000 BBC workers, over a proposal to cut 4,000 jobs, and to privatise parts of the BBC, disrupted much of the BBC's regular programming. In 2006, BBC HD launched as an experimental service and became official in December 2007. The channel broadcast HD simulcasts of programmes on BBC One, BBC Two, BBC Three and BBC Four as well as repeats of some older programmes in HD. In 2010, an HD simulcast of BBC One launched: BBC One HD."
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_21",
    "chunk": "The channel uses HD versions of BBC One's schedule and uses upscaled versions of programmes not currently produced in HD. The BBC HD channel closed in March 2013 and was replaced by BBC Two HD in the same month. On 18 October 2007, BBC Director General Mark Thompson announced a controversial plan to make major cuts and reduce the size of the BBC as an organisation. The plans included a reduction in posts of 2,500; including 1,800 redundancies, consolidating news operations, reducing programming output by 10% and selling off the flagship Television Centre building in London. These plans were fiercely opposed by unions, who threatened a series of strikes; however, the BBC stated that the cuts were essential to move the organisation forward and concentrate on increasing the quality of programming. On 20 October 2010, the Chancellor of the Exchequer George Osborne announced that the television licence fee would be frozen at its current level until the end of the current charter in 2016. The same announcement revealed that the BBC would take on the full cost of running the BBC World Service and the BBC Monitoring service from the Foreign and Commonwealth Office, and partially finance the Welsh broadcaster"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_22",
    "chunk": "S4C. Further cuts were announced on 6 October 2011, so the BBC could reach a total reduction in their budget of 20%, following the licence fee freeze in October 2010, which included cutting staff by 2,000 and sending a further 1,000 to the MediaCityUK development in Salford, with BBC Three moving online only in 2016, the sharing of more programmes between stations and channels, sharing of radio news bulletins, more repeats in schedules, including the whole of BBC Two daytime and for some original programming to be reduced. BBC HD was closed on 26 March 2013, and replaced with an HD simulcast of BBC Two; however, flagship programmes, other channels and full funding for CBBC and CBeebies would be retained. Numerous BBC facilities have been sold off, including New Broadcasting House on Wilmslow Road in Manchester. Many major departments have been relocated to Broadcasting House in central London and MediaCityUK in Salford, particularly since the closure of BBC Television Centre in March 2013. On 16 February 2016, the BBC Three television service was discontinued and replaced by a digital outlet under the same name, targeting its young adult audience with web series and other content. Under the new royal charter"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_23",
    "chunk": "instituted in 2017, the corporation must publish an annual report to Ofcom, outlining its plans and public service obligations for the next year. In its 2017–18 report, released July 2017, the BBC announced plans to \"re-invent\" its output to better compete against commercial streaming services such as Netflix. These plans included increasing the diversity of its content on television and radio, a major increase in investments towards digital children's content, and plans to make larger investments in Wales, Scotland and Northern Ireland to \"rise to the challenge of better reflecting and representing a changing UK\". Since 2017, the BBC has also funded the Local Democracy Reporting Service, with up to 165 journalists employed by independent news organisations to report on local democracy issues on a pooled basis. In 2016, the BBC Director General Tony Hall announced a savings target of £800 million per year by 2021, which is about 23% of annual licence fee revenue. Having to take on the £700 million cost for free TV licences for the over-75 pensioners, and rapid inflation in drama and sport coverage costs, was given as the reason. Duplication of management and content spending would be reduced, and there would be a review"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_24",
    "chunk": "of BBC News. In September 2019, the BBC launched the Trusted News Initiative to work with news and social media companies to combat disinformation about national elections. In 2020, the BBC announced a BBC News savings target of £80 million per year by 2022, involving about 520 staff reductions. The BBC's director of news and current affairs Fran Unsworth said there would be further moves toward digital broadcasting, in part to attract back a youth audience, and more pooling of reporters to stop separate teams covering the same news. In 2020, the BBC reported a £119 million deficit because of delays to cost reduction plans, and the forthcoming ending of the remaining £253 million funding towards pensioner licence fees would increase financial pressures. In January 2021, it was reported that former banker Richard Sharp would succeed David Clementi, as chairman, when he stepped down in February. In March 2023, the BBC was at the centre of a political row with football pundit Gary Lineker, after he criticised the British government's asylum policy on social media. Lineker was suspended from his position on Match of the Day before being re-instated after receiving overwhelming support from his colleagues. The scandal was made"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_25",
    "chunk": "worse due to the connections between BBC's chairman, Richard Sharp, and the Conservative Party. In April 2023, Richard Sharp resigned as chairman after a report found he did not disclose potential perceived conflicts of interest in his role in the facilitation of a loan to Prime Minister Boris Johnson. Dame Elan Closs Stephens was appointed as acting chairwoman on 27 June 2023, and she would lead the BBC board for a year or until a new permanent chair has been appointed. Samir Shah was subsequently appointed with effect from 4 March 2024. In October 2024 it was announced that the BBC along with Sky Sports signed a deal to broadcast the 2025–26 season of the Women's Super League campaign. The BBC is a chartered corporation, independent from direct government intervention, with its activities being overseen from April 2017 by the BBC Board and regulated by Ofcom. The chairman is Samir Shah. The BBC is a public broadcasting company that operates under a royal charter. The charter is the constitutional basis for the BBC, and sets out the BBC's object, mission and public purposes. It emphasises public service, (limited) editorial independence, prohibits advertising on domestic services and proclaims the BBC is"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_26",
    "chunk": "to \"seek to avoid adverse impacts on competition which are not necessary for the effective fulfilment of the Mission and the promotion of the Public Purposes\". The charter additionally sets out that the BBC is subject to an additional agreement between it and the Culture Secretary, and that its operating licence is to be set by Ofcom, an external regulatory body. It used to be that the Home Secretary be departmental to both the agreement as well as the licence, and regulatory duties fall to the BBC Trust, but the 2017 charter changed those 2007 arrangements. The charter also outlines the BBC's governance and regulatory arrangements as a statutory corporation, including the role and composition of the BBC Board. The current charter took effect on 1 January 2017 and is set to expire on 31 December 2027; the agreement being coterminous. The BBC Board was formed in April 2017. It replaced the previous governing body, the BBC Trust, which itself had replaced the board of governors in 2007. The board sets the strategy for the corporation, assesses the performance of the BBC's executive board in delivering the BBC's services, and appoints the director-general. Ofcom is responsible for the regulation of"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_27",
    "chunk": "the BBC. Samir Shah has served as the chairman since 4 March 2024. The executive committee is responsible for the day-to-day operations of the broadcaster. Consisting of senior managers of the BBC, the committee meets once per month and is responsible for operational management and delivery of services within a framework set by the board, and is chaired by the director-general, currently Tim Davie, who is chief executive and (from 1994) editor-in-chief. The corporation has the following in-house divisions covering the BBC's output and operations: From as early as the 1930s until the 1990s, MI5, the British domestic intelligence service, engaged in the vetting of applicants for BBC jobs, a policy designed to keep out persons deemed subversive. In 1933, BBC executive Colonel Alan Dawnay began to meet the head of MI5, Sir Vernon Kell, to informally trade information; from 1935, a formal arrangement was made whereby job applicants would be secretly vetted by MI5 for their political views (without their knowledge). The BBC took up a policy of denying any suggestion by the press of such a relationship (the very existence of MI5 itself was not officially acknowledged until the Security Service Act 1989). This relationship garnered wider public"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_28",
    "chunk": "attention after an article by David Leigh and Paul Lashmar appeared in The Observer in August 1985, revealing that MI5 had been vetting appointments, running operations from Room 105 in Broadcasting House. At the time of the exposé, the operation was being run by Ronnie Stonham. A memo from 1984 revealed that blacklisted organisations included the far-left Communist Party of Great Britain, the Socialist Workers Party, the Workers Revolutionary Party and the Militant tendency, as well as the far-right National Front and the British National Party. An association with one of these groups could result in a denial of a job application. In October 1985, the BBC announced that it would stop the vetting process, except for a few people in top roles, as well as those in charge of Wartime Broadcasting Service emergency broadcasting (in the event of a nuclear war) and staff of the BBC World Service. In 1990, following the Security Service Act 1989, vetting was further restricted to those responsible for wartime broadcasting and those with access to secret government information. Michael Hodder, who succeeded Stonham, had the MI5 vetting files sent to the BBC Archives in Reading, Berkshire. The BBC has the second largest budget"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_29",
    "chunk": "of any UK-based broadcaster with an operating expenditure of £4.722 billion in 2013/14 compared with £6.471 billion for Sky UK in 2013/14 and £1.843 billion for ITV in the calendar year 2013. The principal means of funding the BBC is through the television licence, costing £169.50 per year per household since April 2024. Such a licence is required to legally receive broadcast television across the UK, the Channel Islands and the Isle of Man. No licence is required to own a television used for other means, or for sound only radio sets (though a separate licence for these was also required for non-TV households until 1971). The cost of a television licence is set by the government and enforced by the criminal law. A 50% discount is offered to people who are registered blind or severely visually impaired, and the licence is completely free for any household containing anyone aged 75 or over. However, from August 2020, the licence fee will only be waived if over 75 and receiving pension credit. The BBC pursues its licence fee collection and enforcement under the trading name \"TV Licensing\". The revenue is collected privately by Capita, an outside agency, and is paid into"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_30",
    "chunk": "the central government Consolidated Fund, a process defined in the Communications Act 2003. Funds are then allocated by the Department for Culture, Media and Sport (DCMS) and the Treasury and approved by Parliament via legislation. Additional revenues are paid by the Department for Work and Pensions to compensate for subsidised licences for eligible over-75-year-olds. The licence fee is classified as a tax, and its evasion is a criminal offence. Since 1991, collection and enforcement of the licence fee has been the responsibility of the BBC in its role as TV Licensing Authority. The BBC carries out surveillance (mostly using subcontractors) on properties (under the auspices of the Regulation of Investigatory Powers Act 2000) and may conduct searches of a property using a search warrant. According to TV Licensing, 216,900 people in the UK were caught watching TV without a licence in 2018/19. Licence fee evasion makes up around one-tenth of all cases prosecuted in magistrates' courts, representing 0.3% of court time. Income from commercial enterprises and from overseas sales of its catalogue of programmes has substantially increased over recent years, with BBC Worldwide contributing some £243 million to the BBC's core public service business. According to the BBC's 2018/19 Annual"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_31",
    "chunk": "Report, its total income was £4.889 billion a decrease from £5.062 billion in 2017/18 – partly owing to a 3.7% phased reduction in government funding for free over-75s TV licences, which can be broken down as follows: The licence fee has, however, attracted criticism. It has been argued that in an age of multi-stream, multi-channel availability, an obligation to pay a licence fee is no longer appropriate. The BBC's use of private sector company Capita to send letters to premises not paying the licence fee has been criticised, especially as there have been cases where such letters have been sent to premises which are up to date with their payments, or do not require a TV licence. The BBC uses advertising campaigns to inform customers of the requirement to pay the licence fee. Past campaigns have been criticised by Conservative MP Boris Johnson and former MP Ann Widdecombe for having a threatening nature and language used to scare evaders into paying. Audio clips and television broadcasts are used to inform listeners of the BBC's comprehensive database. There are a number of pressure groups campaigning on the issue of the licence fee. In 2023, around half a million UK households cancelled"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_32",
    "chunk": "their TV licence, driven by shifting viewing habits and financial pressures. As a result, the BBC saw a decline in revenue, with the number of households paying the licence fee dropping to 23.9 million. The majority of the BBC's commercial output comes from its commercial arm BBC Worldwide which sell programmes abroad and exploit key brands for merchandise. Of their 2012/13 sales, 27% were centred on the five key \"superbrands\" of Doctor Who, Top Gear, Strictly Come Dancing (known as Dancing with the Stars internationally), the BBC's archive of natural history programming (collected under the umbrella of BBC Earth) and the (now sold) travel guide brand Lonely Planet. Broadcasting House in Portland Place, central London, is the official headquarters of the BBC. It is home to six of the ten BBC national radio networks, BBC Radio 1, BBC Radio 1xtra, BBC Asian Network, BBC Radio 3, BBC Radio 4, and BBC Radio 4 Extra. It is also the home of BBC News, which relocated to the building from BBC Television Centre in 2013. On the front of the building are statues of Prospero and Ariel, characters from William Shakespeare's play The Tempest, sculpted by Eric Gill. Renovation of Broadcasting House"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_33",
    "chunk": "began in 2002, and was completed in 2012. Until it closed at the end of March 2013, BBC Television was based at Television Centre, a purpose-built television facility opened in 1960 located in White City, four miles (6 km) west of central London. This facility was host to a number of famous guests and programmes through the years, and its name and image is familiar with many British citizens. Nearby, the White City Place complex contains numerous programme offices, housed in Centre House, the Media Centre and Broadcast Centre. It is in this area around Shepherd's Bush that the majority of BBC employees worked. As part of a major reorganisation of BBC property, the entire BBC News operation relocated from the News Centre at BBC Television Centre to the refurbished Broadcasting House to create what is being described as \"one of the world's largest live broadcast centres\". The BBC News Channel and BBC News International relocated to the premises in early 2013. Broadcasting House is now also home to most of the BBC's national radio stations, and the BBC World Service. The major part of this plan involved the demolition of the two post-war extensions to the building and construction"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_34",
    "chunk": "of an extension designed by Sir Richard MacCormac of MJP Architects. This move concentrated the BBC's London operations, allowing them to sell Television Centre. In addition to the scheme above, the BBC is in the process of making and producing more programmes outside London, involving production centres such as Belfast, Cardiff, Glasgow, Newcastle and, most notably, in Greater Manchester as part of the \"BBC North Project\" scheme where several major departments, including BBC North West, BBC Manchester, BBC Sport, BBC Children's, CBeebies, Radio 5 Live, BBC Radio 5 Sports Extra, BBC Breakfast, BBC Learning and the BBC Philharmonic have all moved from their previous locations in either London or New Broadcasting House, Manchester to the new 200-acre (80ha) MediaCityUK production facilities in Salford, that form part of the large BBC North Group division and will therefore become the biggest staffing operation outside London. As well as the two main sites in London (Broadcasting House and White City), there are seven other important BBC production centres in the UK, mainly specialising in different productions. Cardiff is home to BBC Cymru Wales, which specialises in drama production. Open since 2012, and containing 7 new studios, Roath Lock is notable as the home"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_35",
    "chunk": "of productions such as Doctor Who and Casualty. Broadcasting House Belfast, home to BBC Northern Ireland, specialises in original drama and comedy, and has taken part in many co-productions with independent companies and notably with RTÉ in the Republic of Ireland. BBC Scotland, based in Pacific Quay, Glasgow is a large producer of programmes for the network, including several quiz shows. In England, the larger regions also produce some programming. Previously, the largest hub of BBC programming from the regions is BBC North West. At present they produce all religious and ethical programmes on the BBC, as well as other programmes such as A Question of Sport. However, this is to be merged and expanded under the BBC North project, which involved the region moving from New Broadcasting House, Manchester, to MediaCityUK. BBC Midlands, based at Mailbox Birmingham, also produces drama and contains the headquarters for the English regions and the BBC's daytime output. Other production centres include Broadcasting House Bristol, home of BBC West and famously the BBC Studios Natural History Unit and to a lesser extent, Quarry Hill in Leeds, home of BBC Yorkshire. There are also many smaller local and regional studios throughout the UK, operating the"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_36",
    "chunk": "BBC regional television services and the BBC Local Radio stations. The BBC also operates several news gathering centres in various locations around the world, which provide news coverage of that region to the national and international news operations. In 2004, the BBC contracted out its former BBC Technology division to the German engineering and electronics company Siemens IT Solutions and Services (SIS), outsourcing its IT, telephony and broadcast technology systems. When Atos Origin acquired the SIS division from Siemens in December 2010 for €850 million (£720m), the BBC support contract also passed to Atos, and in July 2011, the BBC announced to staff that its technology support would become an Atos service. Siemens staff working on the BBC contract were transferred to Atos; the BBC's Information Technology systems are now managed by Atos. In 2011, the BBC's chief financial officer Zarin Patel stated to the House of Commons Public Accounts Committee that, following criticism of the BBC's management of major IT projects with Siemens (such as the Digital Media Initiative), the BBC partnership with Atos would be instrumental in achieving cost savings of around £64 million as part of the BBC's \"Delivering Quality First\" programme. In 2012, the BBC's then-chief"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_37",
    "chunk": "technology officer John Linwood, expressed confidence in service improvements to the BBC's technology provision brought about by Atos. He also stated that supplier accountability had been strengthened following some high-profile technology failures which had taken place during the partnership with Siemens. The BBC operates several television channels nationally and internationally. BBC One and BBC Two are the flagship television channels. Others include the youth channel BBC Three, cultural and documentary channel BBC Four, the British and international variations of the BBC News channel, parliamentary channel BBC Parliament, and two children's channels, CBBC and CBeebies. Digital television is now entrenched in the UK, with analogue transmission completely phased out as of December 2012. BBC One is a regionalised TV service which provides opt-outs throughout the day for local news and other local programming. These variations are more pronounced in the BBC \"Nations\", i.e. Northern Ireland, Scotland and Wales, where the presentation is mostly carried out locally on BBC One and Two, and where programme schedules can vary considerably from that of the network. BBC Two variations exist in the Nations; however, English regions today rarely have the option to opt out as regional programming now exists only on BBC One. In"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_38",
    "chunk": "2019, the Scottish variation of BBC Two ceased operation and was replaced with the networked version in favour of a new BBC Scotland channel. BBC Two was the first channel to be transmitted on 625 lines, in 1964. It then carried a small-scale regular colour service from 1967. BBC One followed in November 1969. A new Scottish Gaelic television channel, BBC Alba, was launched in September 2008. It is also the first multi-genre channel to come entirely from Scotland with almost all of its programmes made in Scotland. The service was initially available only via satellite but since June 2011 has been available to viewers in Scotland on Freeview and cable television. The BBC currently operates HD simulcasts of all its nationwide channels with the exception of BBC Parliament. Until 26 March 2013, a separate channel called BBC HD was available, in place of BBC Two HD. It launched on 15 May 2006, following a 12-month trial of the broadcasts. It became a proper channel in 2007, and screened HD programmes as simulcasts of the main network, or as repeats. The corporation has been producing programmes in the format for many years, and stated that it hoped to produce 100%"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_39",
    "chunk": "of new programmes in HDTV by 2010. On 3 November 2010, a high-definition simulcast of BBC One was launched, entitled BBC One HD, and BBC Two HD launched on 26 March 2013, replacing BBC HD. Scotland's new television channel, BBC Scotland, launched in February 2019. In the Republic of Ireland, Belgium, the Netherlands and Switzerland, the BBC channels are available in a number of ways. In these countries digital and cable operators carry a range of BBC channels. These include BBC One, BBC Two, BBC Four and BBC News, although viewers in the Republic of Ireland may receive BBC services via overspill from transmitters in Northern Ireland or Wales, or via \"deflectors\"—transmitters in the Republic which rebroadcast broadcasts from the UK, received off-air, or from digital satellite. Since 1975, the BBC has also provided its TV programmes to the British Forces Broadcasting Service (BFBS), allowing members of UK military serving abroad to watch them on four dedicated TV channels. From 27 March 2013, BFBS will carry versions of BBC One and BBC Two, which will include children's programming from CBBC, as well as carrying programming from BBC Three on a new channel called BFBS Extra. Since 2008, all BBC channels"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_40",
    "chunk": "are available to watch online through the BBC iPlayer service. This online streaming ability came about following experiments with live streaming, involving streaming certain channels in the UK. In February 2014, Director-General Tony Hall announced that the corporation needed to save £100 million. In March 2014, the BBC confirmed plans for BBC Three to become an internet-only channel. In December 2012, the BBC completed a digitisation exercise, scanning the listings of all BBC programmes from an entire run of about 4,500 copies of the Radio Times magazine from the first, 1923, issue to 2009 (later listings already being held electronically), the \"BBC Genome project\", with a view to creating an online database of its programme output. An earlier ten months of listings are to be obtained from other sources. They identified around five million programmes, involving 8.5 million actors, presenters, writers and technical staff. The Genome project was opened to public access on 15 October 2014, with corrections to OCR errors and changes to advertised schedules being crowdsourced. The BBC has ten radio stations serving the whole of the UK, a further seven stations in the \"national regions\" (Wales, Scotland, and Northern Ireland), and 39 other local stations serving defined"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_41",
    "chunk": "areas of England. Of the ten national stations, five are major stations and are available on FM or AM as well as on DAB and online. These are BBC Radio 1, offering new music and popular styles and being notable for its chart show; BBC Radio 2, playing adult contemporary, country and soul music amongst many other genres; BBC Radio 3, presenting classical and jazz music together with some spoken-word programming of a cultural nature in the evenings; BBC Radio 4, focusing on news, factual and other speech-based programming, including drama and comedy; and BBC Radio 5 Live, broadcasting 24-hour news, sport and talk programming. In addition to these five stations, the BBC runs a further five stations that broadcast on DAB and online only. These stations supplement and expand on the big five stations, and were launched in 2002. BBC Radio 1Xtra sisters Radio 1, and broadcasts new black music and urban tracks. BBC Radio 5 Sports Extra sisters 5 Live and offers extra sport analysis, including broadcasting sports that previously were not covered. BBC Radio 6 Music offers alternative music genres and is notable as a platform for new artists. BBC Radio 7, later renamed BBC Radio 4"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_42",
    "chunk": "Extra, provided archive drama, comedy and children's programming. The final station is the BBC Asian Network, providing music, talk and news to this section of the community. This station evolved out of Local radio stations serving certain areas, and as such this station is available on medium wave frequency in some areas of the Midlands. As well as the national stations, the BBC also provides 40 BBC Local Radio stations in England and the Channel Islands, each named for and covering a particular city and its surrounding area (e.g. BBC Radio Bristol), county or region (e.g. BBC Three Counties Radio), or geographical area (e.g. BBC Radio Solent covering the central south coast). A further six stations broadcast in what the BBC terms \"the national regions\": Wales, Scotland, and Northern Ireland. These are BBC Radio Wales (in English), BBC Radio Cymru (in Welsh), BBC Radio Scotland (in English), BBC Radio nan Gàidheal (in Scottish Gaelic), BBC Radio Ulster, and BBC Radio Foyle, the latter being an opt-out station from Radio Ulster for the north-west of Northern Ireland. The BBC's UK national channels are also broadcast in the Channel Islands and the Isle of Man (although these Crown Dependencies are outside the"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_43",
    "chunk": "UK), and in the former there are two local stations – BBC Radio Guernsey and BBC Radio Jersey. There is no BBC local radio station, however, in the Isle of Man, partly because the island has long been served by the popular independent commercial station, Manx Radio, which predates the existence of BBC Local Radio. BBC services in the dependencies are financed from television licence fees which are set at the same level as those payable in the UK, although collected locally. This is the subject of some controversy in the Isle of Man since, as well as having no BBC Local Radio service, the island also lacks a local television news service analogous to that provided by BBC Channel Islands. For a worldwide audience, the BBC World Service provides news, current affairs and information in more than 40 languages, including English, around the world, and is available in over 150 capital cities, making it the world's largest external broadcaster in terms of reception area, language selection and audience reach. It is broadcast worldwide on shortwave radio, DAB and online and has an estimated weekly audience of 192 million, and its websites have an audience of 38 million people per"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_44",
    "chunk": "week. Since 2005, it is also available on DAB in the UK, a step not taken before, due to the way it is funded. Following the Government's spending review in 2011, the service was funded for the first time through the Licence fee. In recent years, some services of the World Service have been reduced: the Thai service ended in 2006, as did the Eastern European languages. Resources were diverted instead into the new BBC News Arabic. Historically, the BBC was the only legal radio broadcaster based in the UK mainland until 1967, when University Radio York, then under the name Radio York, was launched as the first, and now oldest, legal independent radio station in the country. However, the BBC did not enjoy a complete monopoly before this, as several Continental stations, such as Radio Luxembourg, had broadcast programmes in English to Britain since the 1930s and the Isle of Man-based Manx Radio began in 1964. Today, despite the advent of commercial broadcasting, BBC radio stations remain among the most listened-to in the country. Radio 2 has the largest audience share (up to 16.8% in 2011–12) and Radios 1 and 4 ranked second and third in terms of weekly"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_45",
    "chunk": "reach. BBC programming is also available to other services and in other countries. Since 1943, the BBC has provided radio programming to the British Forces Broadcasting Service, which broadcasts in countries where British troops are stationed. BBC Radio 1 is also carried in Canada on Sirius XM (online streaming only). The BBC is a patron of the Radio Academy, a registered UK charity that promotes excellence in broadcasting. BBC News is the largest broadcast news gathering operation in the world, providing services to BBC domestic radio as well as television networks such as the BBC News, BBC Parliament and BBC News International. In addition to this, news stories are available on the BBC Red Button service and BBC News Online. In addition to this, the BBC has been developing new ways to access BBC News and as a result, has launched the service on BBC Mobile, making it accessible to mobile phones and PDAs, as well as developing alerts by email, on digital television, and on computers through a desktop alert. Ratings figures suggest that during major incidents such as the 7 July 2005 London bombings or royal events, the UK audience overwhelmingly turns to the BBC's coverage as opposed"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_46",
    "chunk": "to its commercial rivals. On 7 July 2005, the day that there were a series of coordinated bomb blasts on London's public transport system, the BBC Online website recorded an all time bandwidth peak of 11 Gb/s at 12.00 on 7 July. BBC News received some 1 billion total hits on the day of the event (including all images, text, and HTML), serving some 5.5 terabytes of data. At peak times during the day, there were 40,000-page requests per second for the BBC News website. The previous day's announcement of the 2012 Summer Olympics being awarded to London caused a peak of around 5 Gbit/s. The previous all-time high at BBC Online was caused by the announcement of the Michael Jackson verdict, which used 7.2 Gbit/s. The BBC's online presence includes a comprehensive news website and archive. The BBC's first official online service was the BBC Networking Club, which was launched on 11 May 1994. The service was subsequently relaunched as BBC Online in 1997, before being renamed BBCi, then bbc.co.uk, before it was rebranded back as BBC Online. The website is funded by the Licence fee, but uses GeoIP technology, allowing advertisements to be carried on the site when"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_47",
    "chunk": "viewed outside of the UK. The BBC claims the site to be \"Europe's most popular content-based site\" and states that 13.2 million people in the UK visit the site's more than two million pages each day. The centre of the website is the Homepage, which features a modular layout. Users can choose which modules, and which information, is displayed on their homepage, allowing the user to customise it. This system was first launched in December 2007, becoming permanent in February 2008, and has undergone a few aesthetical changes since then. The home page then has links to other micro-sites, such as BBC News Online, Sport, Weather, TV, and Radio. As part of the site, every programme on BBC Television or Radio is given its own page, with bigger programmes getting their own micro-site, and as a result it is often common for viewers and listeners to be told URLs for the programme website. Another large part of the site also allows users to watch and listen to most Television and Radio output live and for seven days after broadcast using the BBC iPlayer platform, which launched on 27 July 2007, and initially used peer-to-peer and DRM technology to deliver both"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_48",
    "chunk": "radio and TV content of the last seven days for offline use for up to 30 days, since then video is now streamed directly. Also, through participation in the Creative Archive Licence group, bbc.co.uk allowed legal downloads of selected archive material via the internet. The BBC has often included learning as part of its online service, running services such as BBC Jam, Learning Zone Class Clips and also runs services such as BBC WebWise and First Click which are designed to teach people how to use the internet. BBC Jam was a free online service, delivered through broadband and narrowband connections, providing high-quality interactive resources designed to stimulate learning at home and at school. Initial content was made available in January 2006; however, BBC Jam was suspended on 20 March 2007 due to allegations made to the European Commission that it was damaging the interests of the commercial sector of the industry. In recent years, some major on-line companies and politicians have complained that BBC Online receives too much funding from the television licence, meaning that other websites are unable to compete with the vast amount of advertising-free on-line content available on BBC Online. Some have proposed that the amount"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_49",
    "chunk": "of licence fee money spent on BBC Online should be reduced—either being replaced with funding from advertisements or subscriptions, or a reduction in the amount of content available on the site. In response to this the BBC carried out an investigation, and has now set in motion a plan to change the way it provides its online services. BBC Online will now attempt to fill in gaps in the market, and will guide users to other websites for currently existing market provision. (For example, instead of providing local events information and timetables, users will be guided to outside websites already providing that information.) Part of this plan included the BBC closing some of its websites, and rediverting money to redevelop other parts. On 26 February 2010, The Times claimed that Mark Thompson, Director General of the BBC, proposed that the BBC's web output should be cut by 50%, with online staff numbers and budgets reduced by 25% in a bid to scale back BBC operations and allow commercial rivals more room. On 2 March 2010, the BBC reported that it would cut its website spending by 25% and close BBC 6 Music and Asian Network, as part of Mark Thompson's"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_50",
    "chunk": "plans to make \"a smaller, fitter BBC for the digital age\". BBC Red Button is the brand name for the BBC's interactive television services, which are available through Freeview (digital terrestrial), as well as Freesat, Sky UK (satellite), and Virgin Media (cable). Unlike Ceefax, the service's analogue counterpart, BBC Red Button is able to display full-colour graphics, photographs, and video, as well as programmes and can be accessed from any BBC channel. The service carries News, Weather and Sport 24 hours a day, but also provides extra features related to programmes specific at that time. Examples include viewers to play along at home to gameshows, to give, voice and vote on opinions to issues, as used alongside programmes such as Question Time. At some points in the year, when multiple sporting events occur, some coverage of less mainstream sports or games are frequently placed on the Red Button for viewers to watch. Frequently, other features are added unrelated to programmes being broadcast at that time, such as the broadcast of the Doctor Who animated episode Dreamland in November 2009. The BBC employs 5 staff orchestras, a professional choir, and supports two amateur choruses, based in BBC venues across the UK;"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_51",
    "chunk": "the BBC Symphony Orchestra, the BBC Singers and BBC Symphony Chorus based in London, the BBC Scottish Symphony Orchestra in Glasgow, the BBC Philharmonic in Salford, the BBC Concert Orchestra based in Watford, and the BBC National Orchestra of Wales in Cardiff. It also buys a selected number of broadcasts from the Ulster Orchestra in Belfast and the BBC Big Band. The BBC Proms have been produced by the BBC every year since 1927, stepping in to fund the popular eight-week summer classical music festival when music publishers Chappell and Co withdrew their support. In 1930, the newly formed BBC Symphony Orchestra gave all 49 Proms, and have performed at every Last Night of the Proms since then. The Proms have been held at the Royal Albert Hall since 1941, and the BBC's orchestras and choirs are the backbone of the festival, giving around 40% to 50% of all performances each season. Many famous musicians of every genre have played at the BBC, such as The Beatles (Live at the BBC is one of their many albums). The BBC is also responsible for the broadcast of Glastonbury Festival, Reading and Leeds Festivals and United Kingdom coverage of the Eurovision Song"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_52",
    "chunk": "Contest, a show with which the broadcaster has been associated for over 60 years. The BBC also operates the division of BBC Audiobooks sometimes found in association with Chivers Audiobooks. The BBC operates other ventures as well as its broadcasting arm. In addition to broadcasting output on television and radio, some programmes are also displayed on the BBC Big Screens in several central-city locations. The BBC and the Foreign, Commonwealth and Development Office also jointly run BBC Monitoring, which monitors radio, television, the press and the internet worldwide. The BBC also developed several computers throughout the 1980s, most notably the BBC Micro (created as part of the BBC Computer Literacy Project, which foreshadowed the coming microcomputer revolution and its effect on the economy, industry, and society of the United Kingdom), which ran alongside the corporation's educational aims and programming, starting with The Computer Programme broadcast in 1982. The National Museum of Computing at Bletchley Park uses BBC Micros as part of a scheme to educate school children about computer programming. In 1951, in conjunction with Oxford University Press, the BBC published The BBC Hymn Book, intended to be used by radio listeners to follow hymns being broadcast. The book was"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_53",
    "chunk": "published both with and without music, the music edition being entitled The BBC Hymn Book with Music. The book contained 542 popular hymns. The BBC provided the world's first teletext service called Ceefax (near-homophonous with \"See Facts\") from 23 September 1974 until 23 October 2012 on the BBC1 analogue channel, then later on BBC2. It showed informational pages, such as news, sport, and the weather. From New Year's Eve, 1974, ITV's Oracle tried to compete with Ceefax. Oracle closed on New Year's Eve, 1992. During its lifetime, Ceefax attracted millions of viewers, right up until 2012, prior to the digital switchover in the United Kingdom. Since then, the BBC's Red Button Service has provided a digital information system that replaced Ceefax. In 2016 the BBC, in partnership with fellow UK broadcasters ITV and Channel 4 (the latter later withdrew from the project), set up 'project kangaroo' to develop an international online streaming service to rival services such as Netflix and Hulu. During the development stages 'Britflix' was touted as a potential name. However, the service eventually launched as BritBox in March 2017. The online platform shows a catalogue of classic BBC and ITV shows, as well as making a number"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_54",
    "chunk": "of programmes available shortly after their UK broadcast. As of 2021, BritBox is available in the UK, the US, Canada, Australia, and, more recently, South Africa, with the potential availability for new markets in the future. BBC Studios is the wholly owned commercial subsidiary of the BBC, responsible for the commercial exploitation of BBC programmes and other properties, including a number of television stations throughout the world. It was formed in 2018 after the merger of the BBC's commercial production arm and the BBC's commercial international distribution arm, BBC Worldwide, with the latter formed in 1995 following the restructuring of its predecessor, BBC Enterprises. Prior to this, the selling of BBC television programmes was at first handled in 1958 with the establishment of a business manager post. This expanded until the establishment of the Television Promotions (renamed Television Enterprises) department in 1960 under a general manager. The company owns and administers a number of commercial stations around the world operating in a number of territories and on a number of different platforms. These include BBC UKTV for the Australasia region, and formerly BBC America (now fully owned by AMC Networks). The company airs two channels aimed at children, an international"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_55",
    "chunk": "CBeebies channel and BBC Kids, a joint venture with Knowledge Network Corporation. The company also runs BBC Earth, which distributes the BBC's natural history content to countries outside the UK, and BBC Lifestyle, broadcasting programmes based on themes of Food, Style and Wellbeing. In addition to this, BBC Studios ran an international version of the channel BBC HD. BBC Studios also distributes the 24-hour international news channel BBC News. The station is separate from BBC Studios to maintain the station's neutral point of view, but is distributed by BBC Studios. The channel itself is the oldest surviving entity of its kind, and has 50 foreign news bureaus and correspondents in nearly all countries in the world. As officially surveyed, it is available to more than 294 million households, significantly more than CNN's estimated 200 million. In addition to these international channels, BBC Studios also owns the UKTV network of seven channels. These channels contain BBC archive programming to be rebroadcast on their respective channels: Alibi, crime dramas; Dave (slogan: \"The Home of Witty Banter\"); Drama, drama, launched in 2013; Eden, nature; Gold, comedy; W, Entertainment; and Yesterday, history programming. In addition to these channels, many BBC programmes are sold via"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_56",
    "chunk": "BBC Studios to foreign television stations with comedy, documentaries, crime dramas (such as Luther and Peaky Blinders) and historical drama productions being the most popular. The BBC's most successful reality television show format, Strictly Come Dancing—under the title Dancing with the Stars—has been exported to 60 other countries. Shows commissioned and distributed by the BBC include the Wallace & Gromit animated comedy short films The Wrong Trousers and A Close Shave. In addition, BBC television news appears nightly on many PBS stations in the US, as do reruns of BBC programmes such as EastEnders, and in New Zealand on TVNZ 1. In addition to programming, BBC Studios produces material to accompany programmes. The company maintained the publishing arm of the BBC, BBC Magazines, which published the Radio Times; first published by the BBC on 28 September 1923, it is the world's first broadcast listings magazine. Radio Times covers all British television and radio programming schedules, and the 1988 Christmas edition sold 11,220,666 copies, which the Guinness World Records certified as the biggest-selling edition of any British magazine in history. Other magazines that support BBC programming include BBC Top Gear, BBC Good Food, BBC Sky at Night, BBC History, BBC Wildlife"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_57",
    "chunk": "and BBC Music. BBC Magazines was sold to Exponent Private Equity in 2011, which merged it with Origin Publishing (previously owned by BBC Worldwide between 2004 and 2006) to form Immediate Media Company. BBC Studios also publishes books, to accompany programmes such as Sherlock and Doctor Who under the BBC Books brand, a publishing imprint majority owned by Random House. Soundtrack albums, singles (which include two UK number one singles from BBC children's shows, \"Teletubbies say \"Eh-oh!\"\" from Teletubbies and \"Can We Fix It?\" from Bob the Builder), talking books and sections of radio broadcasts are also sold under the brand BBC Records, with DVDs also being sold and licensed in large quantities to consumers both in the UK and abroad under the BBC Studios Home Entertainment brand. Archive programming and classical music recordings are sold under the brand BBC Legends. Until the development, popularisation, and domination of television, radio was the broadcast medium upon which people in the United Kingdom relied. It \"reached into every home in the land, and simultaneously united the nation, an important factor during the Second World War\". The BBC introduced the world's first \"high-definition\" 405-line television service in 1936. It suspended its television service"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_58",
    "chunk": "during the Second World War and until 1946, but remained the only television broadcaster in the UK until 1955, when Independent Television (ITV) began operating. This heralded the transformation of television into a popular and dominant medium. Nevertheless, \"throughout the 1950s radio still remained the dominant source of broadcast comedy\". Further, the BBC was the only legal radio broadcaster until 1968 (when University Radio York obtained its first licence). Despite the advent of commercial television and radio, with competition from ITV, Channel 4 and Sky, the BBC has remained one of the main elements in British popular culture through its obligation to produce TV and radio programmes for mass audiences. However, the arrival of BBC2 allowed the BBC also to make programmes for minority interests in drama, documentaries, current affairs, entertainment, and sport. Examples cited include the television series Civilisation, Doctor Who, I, Claudius, Monty Python's Flying Circus, Pot Black, and Tonight, but other examples can be given in each of these fields as shown by the BBC's entries in the British Film Institute's 2000 list of the BFI TV 100, with the BBC's 1970s sitcom Fawlty Towers (featuring John Cleese as Basil Fawlty) topping the list. Making his BBC"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_59",
    "chunk": "debut in 1949, Benny Hill with The Benny Hill Show became the first British comedian to become famous via television. Popular comedy duos on the BBC include Morecambe and Wise (whose show debuted in 1968) and The Two Ronnies (debuting in 1971). Black comedy sitcom Blackadder, starring Rowan Atkinson as the title character, ran for four series between 1983 and 1989. Top of the Pops, the world's longest-running weekly music show, first aired in January 1964, the Rolling Stones being the first group to perform on it. On air since 22 August 1964, Match of the Day is broadcast on Saturday nights during the Premier League season. Some BBC shows have had a direct impact on society. For example, The Great British Bake Off is credited with reinvigorating interest in baking throughout the UK, with stores reporting sharp rises in sales of baking ingredients and accessories. The export of BBC programmes through services like the BBC World Service and BBC News, as well as through the channels operated by BBC Studios, means that audiences can consume BBC productions worldwide. Long-running BBC shows include: Desert Island Discs, broadcast on radio since 1942, Sports Report, broadcast on radio from 5pm on Saturday"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_60",
    "chunk": "evenings during the football season since January 1948, and featuring the same theme tune by Hubert Bath, is the world's longest-running sports radio programme, The Archers, broadcast on radio since 1951, is the world's longest-running drama, and Panorama, broadcast on BBC television since 1953, is the world's longest-running news television programme. Douglas Adams's 1978 Radio 4 sci-fi comedy series The Hitchhiker's Guide to the Galaxy, which spawned a media franchise, was the first radio comedy programme to be produced in stereo, and was innovative in its use of music and sound effects. The British Academy Film Awards (BAFTAs) was first broadcast on the BBC in 1956, with Vivien Leigh as the host. The television equivalent, the British Academy Television Awards, has been screened exclusively on the BBC since a 2007 awards ceremony that included wins for Jim Broadbent (Best actor) and Ricky Gervais (Best comedy performance). The term \"BBC English\" was used as an alternative name for Received Pronunciation, and the English Pronouncing Dictionary uses the term \"BBC Pronunciation\" to label its recommendations. However, the BBC itself now makes more use of regional accents in order to reflect the diversity of the UK, while continuing to expect clarity and fluency"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_61",
    "chunk": "of its presenters. From its \"starchy\" beginnings, the BBC has also become more inclusive, and now attempts to accommodate the interests of all strata of society and all minorities, because they all pay the licence fee. Older domestic UK audiences often refer to the BBC as \"the Beeb\", a nickname coined by Peter Sellers during a 1972 reunion of the 1950s BBC radio comedy The Goon Show, when he referred to the \"Beeb Beeb Ceeb\". It was then shortened and popularised by radio DJ Kenny Everett. David Bowie's recording sessions at the BBC were released as Bowie at the Beeb, while Queen's BBC recording sessions were released as At the Beeb. Another nickname, now less commonly used, is \"Auntie\", said to originate from the old-fashioned \"Auntie knows best\" attitude, or the idea of aunties and uncles who are present in the background of one's life (but possibly a reference to the \"aunties\" and \"uncles\" who presented children's programmes in the early days) in the days when John Reith, the BBC's first director general, was in charge. The term \"Auntie\" for the BBC is often credited to radio disc-jockey Jack Jackson. To celebrate the fiftieth anniversary of the BBC the song"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_62",
    "chunk": "\"Auntie\" was released in 1972. It also featured in the title of the BBC's blooper show, Auntie's Bloomers, which was presented by Terry Wogan from 1991 to 2001. The two nicknames have also been used together as \"Auntie Beeb\". Throughout its existence, the BBC has faced numerous accusations regarding many topics: the Iraq war, politics, ethics and religion, as well as funding and staffing. It also has been involved in numerous controversies because of its coverage of specific news stories and programming. In October 2014, the BBC Trust issued the \"BBC complaints framework\", outlining complaints and appeals procedures. However, the regulatory oversight of the BBC may be transferred to Ofcom. The British \"House of Commons Select Committee on Culture Media and Sport\" recommended in its report \"The Future of the BBC\", that OFCOM should become the final arbiter of complaints made about the BBC. The BBC has long faced accusations of liberal and left-wing bias. Accusations of a bias against the Premiership of Margaret Thatcher and the Conservative Party were often made against the BBC by members of that government, with Margaret Thatcher herself considering the broadcaster's news coverage to be biased and irresponsible. In 2011, Peter Sissons, a main"
  },
  {
    "source": "BBC.txt",
    "chunk_id": "BBC.txt_63",
    "chunk": "news presenter at the BBC from 1989 to 2009, said that \"at the core of the BBC, in its very DNA, is a way of thinking that is firmly of the Left\". Another BBC presenter, Andrew Marr, commented that \"the BBC is not impartial or neutral. It has a liberal bias, not so much a party-political bias. It is better expressed as a cultural liberal bias.\" Former BBC director Roger Mosey classified it as \"liberal defensive\". In 2022, the BBC chairman, Richard Sharp, acknowledged that \"the BBC does have a liberal bias\", and added that \"the institution is fighting against it\". Writing for The Guardian, the left-wing columnist Owen Jones stated \"the truth is the BBC is stacked full of rightwingers,\" and cited as an example of bias its employment of \"ultra-Thatcherite\" Andrew Neil as a politics presenter. A 2018 opinion poll by BMG Research found that 40% of the British public think that the BBC is politically partisan, with a nearly even split between those that believe it leans to the left or right."
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_0",
    "chunk": "# Big Bang nucleosynthesis In physical cosmology, Big Bang nucleosynthesis (also known as primordial nucleosynthesis, and abbreviated as BBN) is a model for the production of light nuclei, deuterium, He, He, Li, between 0.01s and 200s in the lifetime of the universe. The model uses a combination of thermodynamic arguments and results from equations for the expansion of the universe to define a changing temperature and density, then analyzes the rates of nuclear reactions at these temperatures and densities to predict the nuclear abundance ratios. Refined models agree very well with observations with the exception of the abundance of Li. The model is one of the key concepts in standard cosmology. Elements heavier than lithium are thought to have been created later in the life of the Universe by stellar nucleosynthesis, through the formation, evolution and death of stars. The Big Bang nucleosynthesis (BBN) model assumes a homogeneous plasma, at a temperature corresponding to 1 MeV, consisting of electrons annihilating with positrons to produce photons. In turn, the photons pair to produce electrons and positrons: e + e − ↔ γ γ {\\displaystyle e^{+}e^{-}\\leftrightarrow \\gamma \\gamma } . These particles are in equilibrium. A similar number of neutrinos, also at"
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_1",
    "chunk": "1 MeV, have just dropped out of equilibrium at this density. Finally, there is a very low density of baryons (neutrons and protons). The BBN model follows the nuclear reactions of these baryons as the temperature and pressure drops due to expansion of the universe. These assumptions are based on the intense flux of high energy photons in the plasma. Above 0.1 MeV every nucleus created is blasted apart by a photon. Thus the model first determines the ratio of neutrons to protons and uses this as an input to calculate the hydrogen, deuterium, tritium, and He. The model follows nuclear reaction rates as the temperature and density drops. The evolving density and temperature follow from the Friedmann-Robertson-Walker model. Around k T ≈ 1 {\\displaystyle kT\\approx 1} MeV, the density of neutrinos drops, and reactions like n + e + ↔ p + ν ¯ e {\\displaystyle n+e^{+}\\leftrightarrow p+{\\overline {\\nu }}_{e}} which maintained neutron and proton equilibrium, slow down. The neutron-to-proton ratio decreases to around 1/7. As the temperature and density continue to fall, reactions involving combinations of protons and neutrons shift towards heavier nuclei. These include p + n → D + γ , D + D → n"
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_2",
    "chunk": "+ 3 He , 3 He + D → p + 4 He {\\displaystyle p+n\\rightarrow {\\textrm {D}}+\\gamma ,\\ {\\textrm {D}}+{\\textrm {D}}\\rightarrow n+\\,^{3}{\\textrm {He}},\\ \\,^{3}{\\textrm {He}}+{\\textrm {D}}\\rightarrow p+\\,^{4}{\\textrm {He}}} Due to the higher binding energy of He, the free neutrons and the deuterium nuclei are largely consumed, leaving mostly protons and helium. The fusion of nuclei occurred between roughly 10 seconds to 20 minutes after the Big Bang; this corresponds to the temperature range when the universe was cool enough for deuterium to survive, but hot and dense enough for fusion reactions to occur at a significant rate. The key parameter which allows one to calculate the effects of Big Bang nucleosynthesis is the baryon/photon number ratio, which is a small number of order 6 × 10. This parameter corresponds to the baryon density and controls the rate at which nucleons collide and react; from this it is possible to calculate element abundances after nucleosynthesis ends. Although the baryon per photon ratio is important in determining element abundances, the precise value makes little difference to the overall picture. Without major changes to the Big Bang theory itself, BBN will result in mass abundances of about 75% of hydrogen-1, about 25% helium-4,"
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_3",
    "chunk": "about 0.01% of deuterium and helium-3, trace amounts (on the order of 10) of lithium, and negligible heavier elements. That the observed abundances in the universe are generally consistent with these abundance numbers is considered strong evidence for the Big Bang theory. The history of Big Bang nucleosynthesis research began with a proposal in the 1940s by George Gamow that nuclear reactions during a hot initial phase of the universe produced the observed hydrogen and helium. Calculations by his student Ralph Alpher were published in the famous Alpher–Bethe–Gamow paper outlined a theory of light-element production in the early universe. The first detailed calculations of the primordial isotopic abundances came in 1966 and have been refined over the years using updated estimates of the input nuclear reaction rates. The first systematic Monte Carlo study of how nuclear reaction rate uncertainties impact isotope predictions, over the relevant temperature range, was carried out in 1993. The creation of light elements during BBN was dependent on a number of parameters; among those was the neutron–proton ratio (calculable from Standard Model physics) and the baryon-photon ratio. The neutron–proton ratio was set by Standard Model physics before the nucleosynthesis era, essentially within the first 1-second after"
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_4",
    "chunk": "the Big Bang. Neutrons can react with positrons or electron neutrinos to create protons and other products in one of the following reactions: At times much earlier than 1 sec, these reactions were fast and maintained the n/p ratio close to 1:1. As the temperature dropped, the equilibrium shifted in favour of protons due to their slightly lower mass, and the n/p ratio smoothly decreased. These reactions continued until the decreasing temperature and density caused the reactions to become too slow, which occurred at about T = 0.7 MeV (time around 1 second) and is called the freeze out temperature. At freeze out, the neutron–proton ratio was about 1/6. However, free neutrons are unstable with a mean life of 880 sec; some neutrons decayed in the next few minutes before fusing into any nucleus, so the ratio of total neutrons to protons after nucleosynthesis ends is about 1/7. Almost all neutrons that fused instead of decaying ended up combined into helium-4, due to the fact that helium-4 has the highest binding energy per nucleon among light elements. This predicts that about 8% of all atoms should be helium-4, leading to a mass fraction of helium-4 of about 25%, which is"
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_5",
    "chunk": "in line with observations. Small traces of deuterium and helium-3 remained as there was insufficient time and density for them to react and form helium-4. The baryon–photon ratio, η, is the key parameter determining the abundances of light elements after nucleosynthesis ends. Baryons and light elements can fuse in the following main reactions: along with some other low-probability reactions leading to Li or Be. (An important feature is that there are no stable nuclei with mass 5 or 8, which implies that reactions adding one baryon to He, or fusing two He, do not occur). Most fusion chains during BBN ultimately terminate in He (helium-4), while \"incomplete\" reaction chains lead to small amounts of left-over H or He; the amount of these decreases with increasing baryon-photon ratio. That is, the larger the baryon-photon ratio the more reactions there will be and the more efficiently deuterium will be eventually transformed into helium-4. This result makes deuterium a very useful tool in measuring the baryon-to-photon ratio. Big Bang nucleosynthesis began roughly 20 seconds after the big bang, when the universe had cooled sufficiently to allow deuterium nuclei to survive disruption by high-energy photons. (Note that the neutron–proton freeze-out time was earlier). This"
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_6",
    "chunk": "time is essentially independent of dark matter content, since the universe was highly radiation dominated until much later, and this dominant component controls the temperature/time relation. At this time there were about six protons for every neutron, but a small fraction of the neutrons decay before fusing in the next few hundred seconds, so at the end of nucleosynthesis there are about seven protons to every neutron, and almost all the neutrons are in Helium-4 nuclei. One feature of BBN is that the physical laws and constants that govern the behavior of matter at these energies are very well understood, and hence BBN lacks some of the speculative uncertainties that characterize earlier periods in the life of the universe. Another feature is that the process of nucleosynthesis is determined by conditions at the start of this phase of the life of the universe, and proceeds independently of what happened before. As the universe expands, it cools. Free neutrons are less stable than helium nuclei, and the protons and neutrons have a strong tendency to form helium-4. However, forming helium-4 requires the intermediate step of forming deuterium. Before nucleosynthesis began, the temperature was high enough for many photons to have energy"
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_7",
    "chunk": "greater than the binding energy of deuterium; therefore any deuterium that was formed was immediately destroyed (a situation known as the \"deuterium bottleneck\"). Hence, the formation of helium-4 was delayed until the universe became cool enough for deuterium to survive (at about T = 0.1 MeV); after which there was a sudden burst of element formation. However, very shortly thereafter, around twenty minutes after the Big Bang, the temperature and density became too low for any significant fusion to occur. At this point, the elemental abundances were nearly fixed, and the only changes were the result of the radioactive decay of the two major unstable products of BBN, tritium and beryllium-7. Big Bang nucleosynthesis produced very few nuclei of elements heavier than lithium due to a bottleneck: the absence of a stable nucleus with 8 or 5 nucleons. This deficit of larger atoms also limited the amounts of lithium-7 produced during BBN. In stars, the bottleneck is passed by triple collisions of helium-4 nuclei, producing carbon (the triple-alpha process). However, this process is very slow and requires much higher densities, taking tens of thousands of years to convert a significant amount of helium to carbon in stars, and therefore it"
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_8",
    "chunk": "made a negligible contribution in the minutes following the Big Bang. The predicted abundance of CNO isotopes produced in Big Bang nucleosynthesis is expected to be on the order of 10 that of H, making them essentially undetectable and negligible. Indeed, none of these primordial isotopes of the elements from beryllium to oxygen have yet been detected, although those of beryllium and boron may be able to be detected in the future. So far, the only stable nuclides known experimentally to have been made during Big Bang nucleosynthesis are protium, deuterium, helium-3, helium-4, and lithium-7. Big Bang nucleosynthesis predicts a primordial abundance of about 25% helium-4 by mass, irrespective of the initial conditions of the universe. As long as the universe was hot enough for protons and neutrons to transform into each other easily, their ratio, determined solely by their relative masses, was about 1 neutron to 7 protons (allowing for some decay of neutrons into protons). Once it was cool enough, the neutrons quickly bound with an equal number of protons to form first deuterium, then helium-4. Helium-4 is very stable and is nearly the end of this chain if it runs for only a short time, since helium"
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_9",
    "chunk": "neither decays nor combines easily to form heavier nuclei (since there are no stable nuclei with mass numbers of 5 or 8, helium does not combine easily with either protons, or with itself). Once temperatures are lowered, out of every 16 nucleons (2 neutrons and 14 protons), 4 of these (25% of the total particles and total mass) combine quickly into one helium-4 nucleus. This produces one helium for every 12 hydrogens, resulting in a universe that is a little over 8% helium by number of atoms, and 25% helium by mass. \"One analogy is to think of helium-4 as ash, and the amount of ash that one forms when one completely burns a piece of wood is insensitive to how one burns it.\" The resort to the BBN theory of the helium-4 abundance is necessary as there is far more helium-4 in the universe than can be explained by stellar nucleosynthesis. In addition, it provides an important test for the Big Bang theory. If the observed helium abundance is significantly different from 25%, then this would pose a serious challenge to the theory. This would particularly be the case if the early helium-4 abundance was much smaller than 25%"
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_10",
    "chunk": "because it is hard to destroy helium-4. For a few years during the mid-1990s, observations suggested that this might be the case, causing astrophysicists to talk about a Big Bang nucleosynthetic crisis, but further observations were consistent with the Big Bang theory. Deuterium is in some ways the opposite of helium-4, in that while helium-4 is very stable and difficult to destroy, deuterium is only marginally stable and easy to destroy. The temperatures, time, and densities were sufficient to combine a substantial fraction of the deuterium nuclei to form helium-4 but insufficient to carry the process further using helium-4 in the next fusion step. BBN did not convert all of the deuterium in the universe to helium-4 due to the expansion that cooled the universe and reduced the density, and so cut that conversion short before it could proceed any further. One consequence of this is that, unlike helium-4, the amount of deuterium is very sensitive to initial conditions. The denser the initial universe was, the more deuterium would be converted to helium-4 before time ran out, and the less deuterium would remain. There are no known post-Big Bang processes which can produce significant amounts of deuterium. Hence observations about"
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_11",
    "chunk": "deuterium abundance suggest that the universe is not infinitely old, which is in accordance with the Big Bang theory. During the 1970s, there were major efforts to find processes that could produce deuterium, but those revealed ways of producing isotopes other than deuterium. The problem was that while the concentration of deuterium in the universe is consistent with the Big Bang model as a whole, it is too high to be consistent with a model that presumes that most of the universe is composed of protons and neutrons. If one assumes that all of the universe consists of protons and neutrons, the density of the universe is such that much of the currently observed deuterium would have been burned into helium-4. The standard explanation now used for the abundance of deuterium is that the universe does not consist mostly of baryons, but that non-baryonic matter (also known as dark matter) makes up most of the mass of the universe. This explanation is also consistent with calculations that show that a universe made mostly of protons and neutrons would be far more clumpy than is observed. It is very hard to come up with another process that would produce deuterium other"
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_12",
    "chunk": "than by nuclear fusion. Such a process would require that the temperature be hot enough to produce deuterium, but not hot enough to produce helium-4, and that this process should immediately cool to non-nuclear temperatures after no more than a few minutes. It would also be necessary for the deuterium to be swept away before it reoccurs. Producing deuterium by fission is also difficult. The problem here again is that deuterium is very unlikely due to nuclear processes, and that collisions between atomic nuclei are likely to result either in the fusion of the nuclei, or in the release of free neutrons or alpha particles. During the 1970s, cosmic ray spallation was proposed as a source of deuterium. That theory failed to account for the abundance of deuterium, but led to explanations of the source of other light elements. Lithium-7 and lithium-6 produced in the Big Bang are on the order of: lithium-7 to be 10 of all primordial nuclides; and lithium-6 around 10. The theory of BBN gives a detailed mathematical description of the production of the light \"elements\" deuterium, helium-3, helium-4, and lithium-7. Specifically, the theory yields precise quantitative predictions for the mixture of these elements, that is,"
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_13",
    "chunk": "the primordial abundances at the end of the big-bang. In order to test these predictions, it is necessary to reconstruct the primordial abundances as faithfully as possible, for instance by observing astronomical objects in which very little stellar nucleosynthesis has taken place (such as certain dwarf galaxies) or by observing objects that are very far away, and thus can be seen in a very early stage of their evolution (such as distant quasars). As noted above, in the standard picture of BBN, all of the light element abundances depend on the amount of ordinary matter (baryons) relative to radiation (photons). Since the universe is presumed to be homogeneous, it has one unique value of the baryon-to-photon ratio. For a long time, this meant that to test BBN theory against observations one had to ask: can all of the light element observations be explained with a single value of the baryon-to-photon ratio? Or more precisely, allowing for the finite precision of both the predictions and the observations, one asks: is there some range of baryon-to-photon values which can account for all of the observations? More recently, the question has changed: Precision observations of the cosmic microwave background radiation with the Wilkinson"
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_14",
    "chunk": "Microwave Anisotropy Probe (WMAP) and Planck give an independent value for the baryon-to-photon ratio. The present measurement of helium-4 indicates good agreement, and yet better agreement for helium-3. But for lithium-7, there is a significant discrepancy between BBN and WMAP/Planck, and the abundance derived from Population II stars. The discrepancy, called the \"cosmological lithium problem\", is a factor of 2.4―4.3 below the theoretically predicted value. that have resulted in revised calculations of the standard BBN based on new nuclear data, and to various reevaluation proposals for primordial proton–proton nuclear reactions, especially the abundances of Be + n → Li + p, versus Be + H → Be + p. In addition to the standard BBN scenario there are numerous non-standard BBN scenarios. These should not be confused with non-standard cosmology: a non-standard BBN scenario assumes that the Big Bang occurred, but inserts additional physics in order to see how this affects elemental abundances. These pieces of additional physics include relaxing or removing the assumption of homogeneity, or inserting new particles such as massive neutrinos. There have been, and continue to be, various reasons for researching non-standard BBN. The first, which is largely of historical interest, is to resolve inconsistencies between"
  },
  {
    "source": "Big Bang nucleosynthesis.txt",
    "chunk_id": "Big Bang nucleosynthesis.txt_15",
    "chunk": "BBN predictions and observations. This has proved to be of limited usefulness in that the inconsistencies were resolved by better observations, and in most cases trying to change BBN resulted in abundances that were more inconsistent with observations rather than less. The second reason for researching non-standard BBN, and largely the focus of non-standard BBN in the early 21st century, is to use BBN to place limits on unknown or speculative physics. For example, standard BBN assumes that no exotic hypothetical particles were involved in BBN. One can insert a hypothetical particle (such as a massive neutrino) and see what has to happen before BBN predicts abundances that are very different from observations. This has been done to put limits on the mass of a stable tau neutrino."
  },
  {
    "source": "Bioptics (device).txt",
    "chunk_id": "Bioptics (device).txt_0",
    "chunk": "# Bioptics (device) Bioptics, also known as a bioptic in the singular, and sometimes more formally termed a bioptic telescope, is a term for a pair of vision-enhancement lenses. They magnify between two and six times, and are used to improve distance vision for those with severely impaired eyesight, especially those with albinism. They can either be a combination of head-mounted eyeglasses (termed the \"carrier\") and binoculars, or be designed to attach to existing glasses. Some use monoculars which have small telescopes mounted on, in, or behind their regular lenses, so that they can look through either the regular lens or the telescope. Newer designs use smaller lightweight mini telescopes magnifying up to six times, which can be embedded into the spectacle glass and improve aesthetic appearance. The mini telescopic eyeglasses have been shown to be used in the treatment of nystagmus. In some jurisdictions, those with low vision may be permitted to drive automobiles when using Bioptics. Bioptic driving [sometimes written with uppercase O as biOptic, or hyphenated as bi-optic, to differentiate with other types of bioptic] is a method of driving that utilizes the patient's unmagnified vision in combination with intermittent spotting through a small telescopic system that"
  },
  {
    "source": "Bioptics (device).txt",
    "chunk_id": "Bioptics (device).txt_1",
    "chunk": "improves the sharpness of the patient's far vision. Bioptic patients look through just their carrier lens about 95% of the time. When they want magnification, the patient quickly glances through the binocular portion to see details such as street signs, traffic lights and far distant objects. The brief use of the bioptic telescope is much like the quick look all drivers make into their rear view mirror. Bioptic driving requires careful fitting of the system followed by extensive training in both the use of the bioptic and behind-the-wheel driver's training. Determining whether a visually impaired individual may become a bioptic driver requires a multidisciplinary approach. This may include the low vision specialist, physicians, driving rehabilitators, occupational therapists and orientation and mobility instructors. The process includes a number of checks and balances to rule out those patients who would not be safe, while identifying those with the potential to be a safe bioptic driver."
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_0",
    "chunk": "# Black hole A black hole is a massive, compact astronomical object so dense that its gravity prevents anything from escaping, even light. Albert Einstein's theory of general relativity predicts that a sufficiently compact mass will form a black hole. The boundary of no escape is called the event horizon. A black hole has a great effect on the fate and circumstances of an object crossing it, but has no locally detectable features according to general relativity. In many ways, a black hole acts like an ideal black body, as it reflects no light. Quantum field theory in curved spacetime predicts that event horizons emit Hawking radiation, with the same spectrum as a black body of a temperature inversely proportional to its mass. This temperature is of the order of billionths of a kelvin for stellar black holes, making it essentially impossible to observe directly. Objects whose gravitational fields are too strong for light to escape were first considered in the 18th century by John Michell and Pierre-Simon Laplace. In 1916, Karl Schwarzschild found the first modern solution of general relativity that would characterise a black hole. Due to his influential research, the Schwarzschild metric is named after him. David"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_1",
    "chunk": "Finkelstein, in 1958, first published the interpretation of \"black hole\" as a region of space from which nothing can escape. Black holes were long considered a mathematical curiosity; it was not until the 1960s that theoretical work showed they were a generic prediction of general relativity. The discovery of neutron stars by Jocelyn Bell Burnell in 1967 sparked interest in gravitationally collapsed compact objects as a possible astrophysical reality. The first black hole known was Cygnus X-1, identified by several researchers independently in 1971. Black holes typically form when massive stars collapse at the end of their life cycle. After a black hole has formed, it can grow by absorbing mass from its surroundings. Supermassive black holes of millions of solar masses (M☉) may form by absorbing other stars and merging with other black holes, or via direct collapse of gas clouds. There is consensus that supermassive black holes exist in the centres of most galaxies. The presence of a black hole can be inferred through its interaction with other matter and with electromagnetic radiation such as visible light. Matter falling toward a black hole can form an accretion disk of infalling plasma, heated by friction and emitting light. In"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_2",
    "chunk": "extreme cases, this creates a quasar, some of the brightest objects in the universe. Stars passing too close to a supermassive black hole can be shredded into streamers that shine very brightly before being \"swallowed.\" If other stars are orbiting a black hole, their orbits can be used to determine the black hole's mass and location. Such observations can be used to exclude possible alternatives such as neutron stars. In this way, astronomers have identified numerous stellar black hole candidates in binary systems and established that the radio source known as Sagittarius A*, at the core of the Milky Way galaxy, contains a supermassive black hole of about 4.3 million solar masses. The idea of a body so big that even light could not escape was briefly proposed by English astronomical pioneer and clergyman John Michell and independently by French scientist Pierre-Simon Laplace. Both scholars proposed very large stars rather than the modern model of stars with extraordinary density. Michell's idea, in a short part of a letter published in 1784, calculated that a star with the same density but 500 times the radius of the sun would not let any emitted light escape; the surface escape velocity would exceed"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_3",
    "chunk": "the speed of light. Michell correctly noted that such supermassive but non-radiating bodies might be detectable through their gravitational effects on nearby visible bodies. In 1796, Laplace mentioned that a star could be invisible if it were sufficiently large while speculating on the origin of the Solar System in his book Exposition du Système du Monde. Franz Xaver von Zach asked Laplace for a mathematical analysis, which Laplace provided and published in journal edited by von Zach. Scholars of the time were initially excited by the proposal that giant but invisible 'dark stars' might be hiding in plain view, but enthusiasm dampened when the wavelike nature of light became apparent in the early nineteenth century, since light was understood as a wave rather than a particle, it was unclear what, if any, influence gravity would have on escaping light waves. In 1915, Albert Einstein developed his theory of general relativity, having earlier shown that gravity does influence light's motion. Only a few months later, Karl Schwarzschild found a solution to the Einstein field equations that describes the gravitational field of a point mass and a spherical mass. A few months after Schwarzschild, Johannes Droste, a student of Hendrik Lorentz, independently"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_4",
    "chunk": "gave the same solution for the point mass and wrote more extensively about its properties. This solution had a peculiar behaviour at what is now called the Schwarzschild radius, where it became singular, meaning that some of the terms in the Einstein equations became infinite. The nature of this surface was not quite understood at the time. In 1924, Arthur Eddington showed that the singularity disappeared after a change of coordinates. In 1933, Georges Lemaître realised that this meant the singularity at the Schwarzschild radius was a non-physical coordinate singularity. Arthur Eddington commented on the possibility of a star with mass compressed to the Schwarzschild radius in a 1926 book, noting that Einstein's theory allows us to rule out overly large densities for visible stars like Betelgeuse because \"a star of 250 million km radius could not possibly have so high a density as the Sun. Firstly, the force of gravitation would be so great that light would be unable to escape from it, the rays falling back to the star like a stone to the earth. Secondly, the red shift of the spectral lines would be so great that the spectrum would be shifted out of existence. Thirdly, the"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_5",
    "chunk": "mass would produce so much curvature of the spacetime metric that space would close up around the star, leaving us outside (i.e., nowhere).\" In 1931, Subrahmanyan Chandrasekhar calculated, using special relativity, that a non-rotating body of electron-degenerate matter above a certain limiting mass (now called the Chandrasekhar limit at 1.4 M☉) has no stable solutions. His arguments were opposed by many of his contemporaries like Eddington and Lev Landau, who argued that some yet unknown mechanism would stop the collapse. They were partly correct: a white dwarf slightly more massive than the Chandrasekhar limit will collapse into a neutron star, which is itself stable. In 1939, Robert Oppenheimer and others predicted that neutron stars above another limit, the Tolman–Oppenheimer–Volkoff limit, would collapse further for the reasons presented by Chandrasekhar, and concluded that no law of physics was likely to intervene and stop at least some stars from collapsing to black holes. Their original calculations, based on the Pauli exclusion principle, gave it as 0.7 M☉. Subsequent consideration of neutron-neutron repulsion mediated by the strong force raised the estimate to approximately 1.5 M☉ to 3.0 M☉. Observations of the neutron star merger GW170817, which is thought to have generated a black"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_6",
    "chunk": "hole shortly afterward, have refined the TOV limit estimate to ~2.17 M☉. Oppenheimer and his co-authors interpreted the singularity at the boundary of the Schwarzschild radius as indicating that this was the boundary of a bubble in which time stopped. This is a valid point of view for external observers, but not for infalling observers. The hypothetical collapsed stars were called \"frozen stars\", because an outside observer would see the surface of the star frozen in time at the instant where its collapse takes it to the Schwarzschild radius. Also in 1939, Einstein attempted to prove that black holes were impossible in his publication \"On a Stationary System with Spherical Symmetry Consisting of Many Gravitating Masses\", using his theory of general relativity to defend his argument. Months later, Oppenheimer and his student Hartland Snyder provided the Oppenheimer–Snyder model in their paper \"On Continued Gravitational Contraction\", which predicted the existence of black holes. In the paper, which made no reference to Einstein's recent publication, Oppenheimer and Snyder used Einstein's own theory of general relativity to show the conditions on how a black hole could develop, for the first time in contemporary physics. In 1958, David Finkelstein identified the Schwarzschild surface as"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_7",
    "chunk": "an event horizon, \"a perfect unidirectional membrane: causal influences can cross it in only one direction\". This did not strictly contradict Oppenheimer's results, but extended them to include the point of view of infalling observers. Finkelstein's solution extended the Schwarzschild solution for the future of observers falling into a black hole. A complete extension had already been found by Martin Kruskal, who was urged to publish it. These results came at the beginning of the golden age of general relativity, which was marked by general relativity and black holes becoming mainstream subjects of research. This process was helped by the discovery of pulsars by Jocelyn Bell Burnell in 1967, which, by 1969, were shown to be rapidly rotating neutron stars. Until that time, neutron stars, like black holes, were regarded as just theoretical curiosities; but the discovery of pulsars showed their physical relevance and spurred a further interest in all types of compact objects that might be formed by gravitational collapse. In this period more general black hole solutions were found. In 1963, Roy Kerr found the exact solution for a rotating black hole. Two years later, Ezra Newman found the axisymmetric solution for a black hole that is both"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_8",
    "chunk": "rotating and electrically charged. Through the work of Werner Israel, Brandon Carter, and David Robinson the no-hair theorem emerged, stating that a stationary black hole solution is completely described by the three parameters of the Kerr–Newman metric: mass, angular momentum, and electric charge. At first, it was suspected that the strange features of the black hole solutions were pathological artefacts from the symmetry conditions imposed, and that the singularities would not appear in generic situations. This view was held in particular by Vladimir Belinsky, Isaak Khalatnikov, and Evgeny Lifshitz, who tried to prove that no singularities appear in generic solutions. However, in the late 1960s Roger Penrose and Stephen Hawking used global techniques to prove that singularities appear generically. For this work, Penrose received half of the 2020 Nobel Prize in Physics, Hawking having died in 2018. Based on observations in Greenwich and Toronto in the early 1970s, Cygnus X-1, a galactic X-ray source discovered in 1964, became the first astronomical object commonly accepted to be a black hole. Work by James Bardeen, Jacob Bekenstein, Carter, and Hawking in the early 1970s led to the formulation of black hole thermodynamics. These laws describe the behaviour of a black hole in"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_9",
    "chunk": "close analogy to the laws of thermodynamics by relating mass to energy, area to entropy, and surface gravity to temperature. The analogy was completed when Hawking, in 1974, showed that quantum field theory implies that black holes should radiate like a black body with a temperature proportional to the surface gravity of the black hole, predicting the effect now known as Hawking radiation. On 11 February 2016, the LIGO Scientific Collaboration and the Virgo collaboration announced the first direct detection of gravitational waves, representing the first observation of a black hole merger. On 10 April 2019, the first direct image of a black hole and its vicinity was published, following observations made by the Event Horizon Telescope (EHT) in 2017 of the supermassive black hole in Messier 87's galactic centre. Gaia mission observations have found evidence of a Sun-like star orbiting a black hole named Gaia BH1 around 1,560 light-years (480 parsecs) away; evidence suggests a brown dwarf star orbits Gaia BH2. Though only a couple dozen black holes have been found so far in the Milky Way, there are thought to be hundreds of millions, most of which are solitary and do not cause emission of radiation. Therefore, they"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_10",
    "chunk": "would only be detectable by gravitational lensing. Science writer Marcia Bartusiak traces the term \"black hole\" to physicist Robert H. Dicke, who in the early 1960s reportedly compared the phenomenon to the Black Hole of Calcutta, notorious as a prison where people entered but never left alive. The term \"black hole\" was used in print by Life and Science News magazines in 1963, and by science journalist Ann Ewing in her article \"'Black Holes' in Space\", dated 18 January 1964, which was a report on a meeting of the American Association for the Advancement of Science held in Cleveland, Ohio. In December 1967, a student reportedly suggested the phrase \"black hole\" at a lecture by John Wheeler; Wheeler adopted the term for its brevity and \"advertising value\", and it quickly caught on, leading some to credit Wheeler with coining the phrase. The escape velocity from a black hole exceeds the speed of light. The formula for escape velocity is V = 2 M G / R {\\displaystyle V={\\sqrt {2MG/R}}} for an object at radius R from a spherical mass M, with G being the gravitational constant. When the velocity is the speed of light, c, the radius, R s ="
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_11",
    "chunk": "2 M G / c 2 , {\\displaystyle R_{s}=2MG/c^{2},} is called the Schwarzschild radius. A technical definition of a black hole is any object whose mass is contained in a radius smaller than its Schwarzschild radius, a limit derived from one solution to the equations of general relativity. The no-hair theorem postulates that, once it achieves a stable condition after formation, a black hole has only three independent physical properties: mass, electric charge, and angular momentum; the black hole is otherwise featureless. If the conjecture is true, any two black holes that share the same values for these properties, or parameters, are indistinguishable from one another. The degree to which the conjecture is true for real black holes under the laws of modern physics is currently an unsolved problem. These properties are special because they are visible from outside a black hole. For example, a charged black hole repels other like charges just like any other charged object. Similarly, the total mass inside a sphere containing a black hole can be found by using the gravitational analogue of Gauss's law (through the ADM mass), far away from the black hole. Likewise, the angular momentum (or spin) can be measured from"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_12",
    "chunk": "far away using frame dragging by the gravitomagnetic field, through for example the Lense–Thirring effect. When an object falls into a black hole, any information about the shape of the object or distribution of charge on it is evenly distributed along the horizon of the black hole, and is lost to outside observers. The behaviour of the horizon in this situation is a dissipative system that is closely analogous to that of a conductive stretchy membrane with friction and electrical resistance—the membrane paradigm. This is different from other field theories such as electromagnetism, which do not have any friction or resistivity at the microscopic level, because they are time-reversible. Because a black hole eventually achieves a stable state with only three parameters, there is no way to avoid losing information about the initial conditions: the gravitational and electric fields of a black hole give very little information about what went in. The information that is lost includes every quantity that cannot be measured far away from the black hole horizon, including approximately conserved quantum numbers such as the total baryon number and lepton number. This behaviour is so puzzling that it has been called the black hole information loss paradox."
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_13",
    "chunk": "The simplest static black holes have mass but neither electric charge nor angular momentum. These black holes are often referred to as Schwarzschild black holes after Karl Schwarzschild who discovered this solution in 1916. According to Birkhoff's theorem, it is the only vacuum solution that is spherically symmetric. This means there is no observable difference at a distance between the gravitational field of such a black hole and that of any other spherical object of the same mass. The popular notion of a black hole \"sucking in everything\" in its surroundings is therefore correct only near a black hole's horizon; far away, the external gravitational field is identical to that of any other body of the same mass. Solutions describing more general black holes also exist. Non-rotating charged black holes are described by the Reissner–Nordström metric, while the Kerr metric describes a non-charged rotating black hole. The most general stationary black hole solution known is the Kerr–Newman metric, which describes a black hole with both charge and angular momentum. While the mass of a black hole can take any positive value, the charge and angular momentum are constrained by the mass. The total electric charge Q and the total angular"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_14",
    "chunk": "momentum J are expected to satisfy the inequality Q 2 4 π ϵ 0 + c 2 J 2 G M 2 ≤ G M 2 {\\displaystyle {\\frac {Q^{2}}{4\\pi \\epsilon _{0}}}+{\\frac {c^{2}J^{2}}{GM^{2}}}\\leq GM^{2}} for a black hole of mass M. Black holes with the minimum possible mass satisfying this inequality are called extremal. Solutions of Einstein's equations that violate this inequality exist, but they do not possess an event horizon. These solutions have so-called naked singularities that can be observed from the outside, and hence are deemed unphysical. The cosmic censorship hypothesis rules out the formation of such singularities, when they are created through the gravitational collapse of realistic matter. This is supported by numerical simulations. Due to the relatively large strength of the electromagnetic force, black holes forming from the collapse of stars are expected to retain the nearly neutral charge of the star. Rotation, however, is expected to be a universal feature of compact astrophysical objects. The black-hole candidate binary X-ray source GRS 1915+105 appears to have an angular momentum near the maximum allowed value. That uncharged limit is J ≤ G M 2 c , {\\displaystyle J\\leq {\\frac {GM^{2}}{c}},} allowing definition of a dimensionless spin parameter such"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_15",
    "chunk": "that 0 ≤ c J G M 2 ≤ 1. {\\displaystyle 0\\leq {\\frac {cJ}{GM^{2}}}\\leq 1.} Black holes are commonly classified according to their mass, independent of angular momentum, J. The size of a black hole, as determined by the radius of the event horizon, or Schwarzschild radius, is proportional to the mass, M, through r s = 2 G M c 2 ≈ 2.95 M M ⊙ k m , {\\displaystyle r_{\\mathrm {s} }={\\frac {2GM}{c^{2}}}\\approx 2.95\\,{\\frac {M}{M_{\\odot }}}~\\mathrm {km,} } where rs is the Schwarzschild radius and M☉ is the mass of the Sun. For a black hole with nonzero spin or electric charge, the radius is smaller, until an extremal black hole could have an event horizon close to r + = G M c 2 . {\\displaystyle r_{\\mathrm {+} }={\\frac {GM}{c^{2}}}.} The defining feature of a black hole is the appearance of an event horizon—a boundary in spacetime through which matter and light can pass only inward towards the mass of the black hole. Nothing, not even light, can escape from inside the event horizon. The event horizon is referred to as such because if an event occurs within the boundary, information from that event cannot reach an"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_16",
    "chunk": "outside observer, making it impossible to determine whether such an event occurred. As predicted by general relativity, the presence of a mass deforms spacetime in such a way that the paths taken by particles bend towards the mass. At the event horizon of a black hole, this deformation becomes so strong that there are no paths that lead away from the black hole. In a thought experiment, a distant observer can imagine clocks near a black hole which would appear to tick more slowly than those farther away from the black hole. This effect, known as gravitational time dilation, would also cause an object falling into a black hole to appear to slow as it approaches the event horizon, taking an infinite amount of time to reach it. All processes on this object would appear to slow down, from the viewpoint of a fixed outside observer, and any light emitted by the object to appear redder and dimmer, an effect known as gravitational redshift. Eventually, the falling object fades away until it can no longer be seen. Typically this process happens very rapidly with an object disappearing from view within less than a second. On the other hand, imaginary, indestructible"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_17",
    "chunk": "observers falling into a black hole would not notice any of these effects as they cross the event horizon. Their own clocks appear to them to tick normally, they cross the event horizon after a finite time without noting any singular behaviour. In general relativity, it is impossible to determine the location of the event horizon from local observations, due to Einstein's equivalence principle. The topology of the event horizon of a black hole at equilibrium is always spherical. For non-rotating (static) black holes the geometry of the event horizon is precisely spherical, while for rotating black holes the event horizon is oblate. At the centre of the unrealistically simple Schwarzschild model of a black hole is a gravitational singularity a region where the spacetime curvature becomes infinite. For a non-rotating black hole, this region takes the shape of a single point; for a rotating black hole it is smeared out to form a ring singularity that lies in the plane of rotation. In both cases, the singular region has zero volume. It can also be shown that the singular region contains all the mass of the black hole solution. The singular region can thus be thought of as having"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_18",
    "chunk": "infinite density. Even though the Schwarzschild is not valid at the singularity, it is fun to think about a trip to towards it. Observers falling into a Schwarzschild black hole (i.e., non-rotating, not charged) cannot avoid being carried into the singularity once they cross the event horizon. They can prolong the experience by accelerating away to slow their descent, but only up to a limit. When they reach the singularity, they are crushed to infinite density and their mass is added to the total of the black hole. Before that happens, they will have been torn apart by the growing tidal forces in a process sometimes referred to as spaghettification or the \"noodle effect\". In the case of a charged (Reissner–Nordström) or rotating (Kerr) black hole, it is possible to avoid the singularity. Extending these solutions as far as possible reveals the hypothetical possibility of exiting the black hole into a different spacetime with the black hole acting as a wormhole. The possibility of travelling to another universe is, however, only theoretical since any perturbation would destroy this possibility. It also appears to be possible to follow closed timelike curves (returning to one's own past) around the Kerr singularity, which"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_19",
    "chunk": "leads to problems with causality like the grandfather paradox. It is expected that none of these peculiar effects would survive in a proper quantum treatment of rotating and charged black holes. The appearance of singularities in general relativity is signalling the breakdown of the theory. This breakdown occurs where quantum effects should describe these actions, due to the extremely high density and therefore particle interactions. To date, it has not been possible to combine quantum and gravitational effects into a single theory, although there exist attempts to formulate such a theory of quantum gravity. It is generally expected that such a theory will not feature singularities. The photon sphere is a spherical boundary where photons that move on tangents to that sphere would be trapped in a non-stable but circular orbit around the black hole. For non-rotating black holes, the photon sphere has a radius 1.5 times the Schwarzschild radius. Their orbits would be dynamically unstable, hence any small perturbation, such as a particle of infalling matter, would cause an instability that would grow over time, either setting the photon on an outward trajectory causing it to escape the black hole, or on an inward spiral where it would eventually"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_20",
    "chunk": "cross the event horizon. While light can still escape from the photon sphere, any light that crosses the photon sphere on an inbound trajectory will be captured by the black hole. Hence any light that reaches an outside observer from the photon sphere must have been emitted by objects between the photon sphere and the event horizon. For a Kerr black hole the radius of the photon sphere depends on the spin parameter and on the details of the photon orbit, which can be prograde (the photon rotates in the same sense of the black hole spin) or retrograde. Rotating black holes are surrounded by a region of spacetime in which it is impossible to stand still, called the ergosphere. This is the result of a process known as frame-dragging; general relativity predicts that any rotating mass will tend to slightly \"drag\" along the spacetime immediately surrounding it. Any object near the rotating mass will tend to start moving in the direction of rotation. For a rotating black hole, this effect is so strong near the event horizon that an object would have to move faster than the speed of light in the opposite direction to just stand still. The"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_21",
    "chunk": "ergosphere of a black hole is a volume bounded by the black hole's event horizon and the ergosurface, which coincides with the event horizon at the poles but is at a much greater distance around the equator. Objects and radiation can escape normally from the ergosphere. Through the Penrose process, objects can emerge from the ergosphere with more energy than they entered with. The extra energy is taken from the rotational energy of the black hole. Thereby the rotation of the black hole slows down. A variation of the Penrose process in the presence of strong magnetic fields, the Blandford–Znajek process is considered a likely mechanism for the enormous luminosity and relativistic jets of quasars and other active galactic nuclei. In Newtonian gravity, test particles can stably orbit at arbitrary distances from a central object. In general relativity, however, there exists an innermost stable circular orbit (often called the ISCO), for which any infinitesimal inward perturbations to a circular orbit will lead to spiraling into the black hole, and any outward perturbations will, depending on the energy, result in spiraling in, stably orbiting between apastron and periastron, or escaping to infinity. The location of the ISCO depends on the spin"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_22",
    "chunk": "of the black hole, in the case of a Schwarzschild black hole (spin zero) is: r I S C O = 3 r s = 6 G M c 2 , {\\displaystyle r_{\\rm {ISCO}}=3\\,r_{s}={\\frac {6\\,GM}{c^{2}}},} and decreases with increasing black hole spin for particles orbiting in the same direction as the spin. The final observable region of spacetime around a black hole is called the plunging region. In this area it is no longer possible for matter to follow circular orbits or to stop a final descent into the black hole. Instead it will rapidly plunge toward the black hole close to the speed of light. Given the bizarre character of black holes, it was long questioned whether such objects could actually exist in nature or whether they were merely pathological solutions to Einstein's equations. Einstein himself wrongly thought black holes would not form, because he held that the angular momentum of collapsing particles would stabilise their motion at some radius. This led the general relativity community to dismiss all results to the contrary for many years. However, a minority of relativists continued to contend that black holes were physical objects, and by the end of the 1960s, they had"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_23",
    "chunk": "persuaded the majority of researchers in the field that there is no obstacle to the formation of an event horizon. Penrose demonstrated that once an event horizon forms, general relativity without quantum mechanics requires that a singularity will form within. Shortly afterwards, Hawking showed that many cosmological solutions that describe the Big Bang have singularities without scalar fields or other exotic matter. The Kerr solution, the no-hair theorem, and the laws of black hole thermodynamics showed that the physical properties of black holes were simple and comprehensible, making them respectable subjects for research. Conventional black holes are formed by gravitational collapse of heavy objects such as stars, but they can also in theory be formed by other processes. Gravitational collapse occurs when an object's internal pressure is insufficient to resist the object's own gravity. For stars this usually occurs either because a star has too little \"fuel\" left to maintain its temperature through stellar nucleosynthesis, or because a star that would have been stable receives extra matter in a way that does not raise its core temperature. In either case the star's temperature is no longer high enough to prevent it from collapsing under its own weight. The collapse may"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_24",
    "chunk": "be stopped by the degeneracy pressure of the star's constituents, allowing the condensation of matter into an exotic denser state. The result is one of the various types of compact star. Which type forms depends on the mass of the remnant of the original star left if the outer layers have been blown away (for example, in a Type II supernova). The mass of the remnant, the collapsed object that survives the explosion, can be substantially less than that of the original star. Remnants exceeding 5 M☉ are produced by stars that were over 20 M☉ before the collapse. If the mass of the remnant exceeds about 3–4 M☉ (the Tolman–Oppenheimer–Volkoff limit), either because the original star was very heavy or because the remnant collected additional mass through accretion of matter, even the degeneracy pressure of neutrons is insufficient to stop the collapse. No known mechanism (except possibly quark degeneracy pressure) is powerful enough to stop the implosion and the object will inevitably collapse to form a black hole. The gravitational collapse of heavy stars is assumed to be responsible for the formation of stellar mass black holes. Star formation in the early universe may have resulted in very massive"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_25",
    "chunk": "stars, which upon their collapse would have produced black holes of up to 10 M☉. These black holes could be the seeds of the supermassive black holes found in the centres of most galaxies. It has further been suggested that massive black holes with typical masses of ~10 M☉ could have formed from the direct collapse of gas clouds in the young universe. These massive objects have been proposed as the seeds that eventually formed the earliest quasars observed already at redshift z ∼ 7 {\\displaystyle z\\sim 7} . Some candidates for such objects have been found in observations of the young universe. While most of the energy released during gravitational collapse is emitted very quickly, an outside observer does not actually see the end of this process. Even though the collapse takes a finite amount of time from the reference frame of infalling matter, a distant observer would see the infalling material slow and halt just above the event horizon, due to gravitational time dilation. Light from the collapsing material takes longer and longer to reach the observer, with the light emitted just before the event horizon forms delayed an infinite amount of time. Thus the external observer never"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_26",
    "chunk": "sees the formation of the event horizon; instead, the collapsing material seems to become dimmer and increasingly red-shifted, eventually fading away. Gravitational collapse requires great density. In the current epoch of the universe these high densities are found only in stars, but in the early universe shortly after the Big Bang densities were much greater, possibly allowing for the creation of black holes. High density alone is not enough to allow black hole formation since a uniform mass distribution will not allow the mass to bunch up. In order for primordial black holes to have formed in such a dense medium, there must have been initial density perturbations that could then grow under their own gravity. Different models for the early universe vary widely in their predictions of the scale of these fluctuations. Various models predict the creation of primordial black holes ranging in size from a Planck mass ( m P = ℏ c / G {\\displaystyle m_{P}={\\sqrt {\\hbar c/G}}} ≈ 1.2×10 GeV/c ≈ 2.2×10 kg) to hundreds of thousands of solar masses. Despite the early universe being extremely dense, it did not re-collapse into a black hole during the Big Bang, since the expansion rate was greater than"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_27",
    "chunk": "the attraction. Following inflation theory there was a net repulsive gravitation in the beginning until the end of inflation. Since then the Hubble flow was slowed by the energy density of the universe. Models for the gravitational collapse of objects of relatively constant size, such as stars, do not necessarily apply in the same way to rapidly expanding space such as the Big Bang. Gravitational collapse is not the only process that could create black holes. In principle, black holes could be formed in high-energy collisions that achieve sufficient density. As of 2002, no such events have been detected, either directly or indirectly as a deficiency of the mass balance in particle accelerator experiments. This suggests that there must be a lower limit for the mass of black holes. Theoretically, this boundary is expected to lie around the Planck mass, where quantum effects are expected to invalidate the predictions of general relativity. This would put the creation of black holes firmly out of reach of any high-energy process occurring on or near the Earth. However, certain developments in quantum gravity suggest that the minimum black hole mass could be much lower: some braneworld scenarios for example put the boundary as"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_28",
    "chunk": "low as 1 TeV/c. This would make it conceivable for micro black holes to be created in the high-energy collisions that occur when cosmic rays hit the Earth's atmosphere, or possibly in the Large Hadron Collider at CERN. These theories are very speculative, and the creation of black holes in these processes is deemed unlikely by many specialists. Even if micro black holes could be formed, it is expected that they would evaporate in about 10 seconds, posing no threat to the Earth. Once a black hole has formed, it can continue to grow by absorbing additional matter. Any black hole will continually absorb gas and interstellar dust from its surroundings. This growth process is one possible way through which some supermassive black holes may have been formed, although the formation of supermassive black holes is still an open field of research. A similar process has been suggested for the formation of intermediate-mass black holes found in globular clusters. Black holes can also merge with other objects such as stars or even other black holes. This is thought to have been important, especially in the early growth of supermassive black holes, which could have formed from the aggregation of many"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_29",
    "chunk": "smaller objects. The process has also been proposed as the origin of some intermediate-mass black holes. In 1974, Hawking predicted that black holes are not entirely black but emit small amounts of thermal radiation at a temperature ħc/(8πGMkB); this effect has become known as Hawking radiation. By applying quantum field theory to a static black hole background, he determined that a black hole should emit particles that display a perfect black body spectrum. Since Hawking's publication, many others have verified the result through various approaches. If Hawking's theory of black hole radiation is correct, then black holes are expected to shrink and evaporate over time as they lose mass by the emission of photons and other particles. The temperature of this thermal spectrum (Hawking temperature) is proportional to the surface gravity of the black hole, which, for a Schwarzschild black hole, is inversely proportional to the mass. Hence, large black holes emit less radiation than small black holes. A stellar black hole of 1 M☉ has a Hawking temperature of 62 nanokelvins. This is far less than the 2.7 K temperature of the cosmic microwave background radiation. Stellar-mass or larger black holes receive more mass from the cosmic microwave background"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_30",
    "chunk": "than they emit through Hawking radiation and thus will grow instead of shrinking. To have a Hawking temperature larger than 2.7 K (and be able to evaporate), a black hole would need a mass less than the Moon. Such a black hole would have a diameter of less than a tenth of a millimetre. If a black hole is very small, the radiation effects are expected to become very strong. A black hole with the mass of a car would have a diameter of about 10 m and take a nanosecond to evaporate, during which time it would briefly have a luminosity of more than 200 times that of the Sun. Lower-mass black holes are expected to evaporate even faster; for example, a black hole of mass 1 TeV/c would take less than 10 seconds to evaporate completely. For such a small black hole, quantum gravity effects are expected to play an important role and could hypothetically make such a small black hole stable, although current developments in quantum gravity do not indicate this is the case. The Hawking radiation for an astrophysical black hole is predicted to be very weak and would thus be exceedingly difficult to detect from"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_31",
    "chunk": "Earth. A possible exception, however, is the burst of gamma rays emitted in the last stage of the evaporation of primordial black holes. Searches for such flashes have proven unsuccessful and provide stringent limits on the possibility of existence of low mass primordial black holes. NASA's Fermi Gamma-ray Space Telescope launched in 2008 will continue the search for these flashes. If black holes evaporate via Hawking radiation, a solar mass black hole will evaporate (beginning once the temperature of the cosmic microwave background drops below that of the black hole) over a period of 10 years. A supermassive black hole with a mass of 10 M☉ will evaporate in around 2×10 years. During the collapse of a supercluster of galaxies, supermassive black holes are predicted to grow to perhaps 10 M☉. Even these would evaporate over a timescale of up to 10 years. By nature, black holes do not themselves emit any electromagnetic radiation other than the hypothetical Hawking radiation, so astrophysicists searching for black holes must generally rely on indirect observations. For example, a black hole's existence can sometimes be inferred by observing its gravitational influence on its surroundings. The Event Horizon Telescope (EHT) is an active program that"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_32",
    "chunk": "directly observes the immediate environment of black holes' event horizons, such as the black hole at the centre of the Milky Way. In April 2017, EHT began observing the black hole at the centre of Messier 87. \"In all, eight radio observatories on six mountains and four continents observed the galaxy in Virgo on and off for 10 days in April 2017\" to provide the data yielding the image in April 2019. After two years of data processing, EHT released its first image of a black hole, at the center of the Messier 87 galaxy. What is visible is not the black hole—which shows as black because of the loss of all light within this dark region. Instead, it is the gases at the edge of the event horizon, displayed as orange or red, that define the black hole. On 12 May 2022, the EHT released the first image of Sagittarius A*, the supermassive black hole at the centre of the Milky Way galaxy. The published image displayed the same ring-like structure and \"shadow\" seen in the M87* black hole. The boundary of the shadow or area of less brightness matches the predicted gravitationally lensed photon orbits. The image was"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_33",
    "chunk": "created using the same techniques as for the M87 black hole. The imaging process for Sagittarius A*, which is more than a thousand times smaller and less massive than M87*, was significantly more complex because of the instability of its surroundings. The image of Sagittarius A* was partially blurred by turbulent plasma on the way to the galactic centre, an effect which prevents resolution of the image at longer wavelengths. The brightening of this material in the 'bottom' half of the processed EHT image is thought to be caused by Doppler beaming, whereby material approaching the viewer at relativistic speeds is perceived as brighter than material moving away. In the case of a black hole, this phenomenon implies that the visible material is rotating at relativistic speeds (>1,000 km/s [2,200,000 mph]), the only speeds at which it is possible to centrifugally balance the immense gravitational attraction of the singularity, and thereby remain in orbit above the event horizon. This configuration of bright material implies that the EHT observed M87* from a perspective catching the black hole's accretion disc nearly edge-on, as the whole system rotated clockwise. The extreme gravitational lensing associated with black holes produces the illusion of a perspective"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_34",
    "chunk": "that sees the accretion disc from above. In reality, most of the ring in the EHT image was created when the light emitted by the far side of the accretion disc bent around the black hole's gravity well and escaped, meaning that most of the possible perspectives on M87* can see the entire disc, even that directly behind the \"shadow\". In 2015, the EHT detected magnetic fields just outside the event horizon of Sagittarius A* and even discerned some of their properties. The field lines that pass through the accretion disc were a complex mixture of ordered and tangled. Theoretical studies of black holes had predicted the existence of magnetic fields. In April 2023, an image of the shadow of the Messier 87 black hole and the related high-energy jet, viewed together for the first time, was presented. On 14 September 2015, the LIGO gravitational wave observatory made the first-ever successful direct observation of gravitational waves. The signal was consistent with theoretical predictions for the gravitational waves produced by the merger of two black holes: one with about 36 solar masses, and the other around 29 solar masses. This observation provides the most concrete evidence for the existence of black"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_35",
    "chunk": "holes to date. For instance, the gravitational wave signal suggests that the separation of the two objects before the merger was just 350 km, or roughly four times the Schwarzschild radius corresponding to the inferred masses. The objects must therefore have been extremely compact, leaving black holes as the most plausible interpretation. More importantly, the signal observed by LIGO also included the start of the post-merger ringdown, the signal produced as the newly formed compact object settles down to a stationary state. Arguably, the ringdown is the most direct way of observing a black hole. From the LIGO signal, it is possible to extract the frequency and damping time of the dominant mode of the ringdown. From these, it is possible to infer the mass and angular momentum of the final object, which match independent predictions from numerical simulations of the merger. The frequency and decay time of the dominant mode are determined by the geometry of the photon sphere. Hence, observation of this mode confirms the presence of a photon sphere; however, it cannot exclude possible exotic alternatives to black holes that are compact enough to have a photon sphere. The observation also provides the first observational evidence for"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_36",
    "chunk": "the existence of stellar-mass black hole binaries. Furthermore, it is the first observational evidence of stellar-mass black holes weighing 25 solar masses or more. The proper motions of stars near the centre of our own Milky Way provide strong observational evidence that these stars are orbiting a supermassive black hole. Since 1995, astronomers have tracked the motions of 90 stars orbiting an invisible object coincident with the radio source Sagittarius A*. By fitting their motions to Keplerian orbits, the astronomers were able to infer, in 1998, that a 2.6×10 M☉ object must be contained in a volume with a radius of 0.02 light-years to cause the motions of those stars. Since then, one of the stars—called S2—has completed a full orbit. From the orbital data, astronomers were able to refine the calculations of the mass to 4.3×10 M☉ and a radius of less than 0.002 light-years for the object causing the orbital motion of those stars. The upper limit on the object's size is still too large to test whether it is smaller than its Schwarzschild radius. Nevertheless, these observations strongly suggest that the central object is a supermassive black hole as there are no other plausible scenarios for confining"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_37",
    "chunk": "so much invisible mass into such a small volume. Additionally, there is some observational evidence that this object might possess an event horizon, a feature unique to black holes. Due to conservation of angular momentum, gas falling into the gravitational well created by a massive object will typically form a disk-like structure around the object. Artists' impressions such as the accompanying representation of a black hole with corona commonly depict the black hole as if it were a flat-space body hiding the part of the disk just behind it, but in reality gravitational lensing would greatly distort the image of the accretion disk. Within such a disk, friction would cause angular momentum to be transported outward, allowing matter to fall farther inward, thus releasing potential energy and increasing the temperature of the gas. When the accreting object is a neutron star or a black hole, the gas in the inner accretion disk orbits at very high speeds because of its proximity to the compact object. The resulting friction is so significant that it heats the inner disk to temperatures at which it emits vast amounts of electromagnetic radiation (mainly X-rays). These bright X-ray sources may be detected by telescopes. This"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_38",
    "chunk": "process of accretion is one of the most efficient energy-producing processes known. Up to 40% of the rest mass of the accreted material can be emitted as radiation. In nuclear fusion only about 0.7% of the rest mass will be emitted as energy. In many cases, accretion disks are accompanied by relativistic jets that are emitted along the poles, which carry away much of the energy. The mechanism for the creation of these jets is currently not well understood, in part due to insufficient data. As such, many of the universe's more energetic phenomena have been attributed to the accretion of matter on black holes. In particular, active galactic nuclei and quasars are believed to be the accretion disks of supermassive black holes. Similarly, X-ray binaries are generally accepted to be binary star systems in which one of the two stars is a compact object accreting matter from its companion. It has also been suggested that some ultraluminous X-ray sources may be the accretion disks of intermediate-mass black holes. Stars have been observed to get torn apart by tidal forces in the immediate vicinity of supermassive black holes in galaxy nuclei, in what is known as a tidal disruption event"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_39",
    "chunk": "(TDE). Some of the material from the disrupted star forms an accretion disk around the black hole, which emits observable electromagnetic radiation. In November 2011 the first direct observation of a quasar accretion disk around a supermassive black hole was reported. X-ray binaries are binary star systems that emit a majority of their radiation in the X-ray part of the spectrum. These X-ray emissions are generally thought to result when one of the stars (compact object) accretes matter from another (regular) star. The presence of an ordinary star in such a system provides an opportunity for studying the central object and to determine if it might be a black hole. If such a system emits signals that can be directly traced back to the compact object, it cannot be a black hole. The absence of such a signal does, however, not exclude the possibility that the compact object is a neutron star. By studying the companion star it is often possible to obtain the orbital parameters of the system and to obtain an estimate for the mass of the compact object. If this is much larger than the Tolman–Oppenheimer–Volkoff limit (the maximum mass a star can have without collapsing) then"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_40",
    "chunk": "the object cannot be a neutron star and is generally expected to be a black hole. The first strong candidate for a black hole, Cygnus X-1, was discovered in this way by Charles Thomas Bolton, Louise Webster, and Paul Murdin in 1972. Some doubt remained, due to the uncertainties that result from the companion star being much heavier than the candidate black hole. Currently, better candidates for black holes are found in a class of X-ray binaries called soft X-ray transients. In this class of system, the companion star is of relatively low mass allowing for more accurate estimates of the black hole mass. These systems actively emit X-rays for only several months once every 10–50 years. During the period of low X-ray emission, called quiescence, the accretion disk is extremely faint, allowing detailed observation of the companion star during this period. One of the best such candidates is V404 Cygni. The X-ray emissions from accretion disks sometimes flicker at certain frequencies. These signals are called quasi-periodic oscillations and are thought to be caused by material moving along the inner edge of the accretion disk (the innermost stable circular orbit). As such their frequency is linked to the mass of"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_41",
    "chunk": "the compact object. They can thus be used as an alternative way to determine the mass of candidate black holes. Astronomers use the term \"active galaxy\" to describe galaxies with unusual characteristics, such as unusual spectral line emission and very strong radio emission. Theoretical and observational studies have shown that the activity in these active galactic nuclei (AGN) may be explained by the presence of supermassive black holes, which can be millions of times more massive than stellar ones. The models of these AGN consist of a central black hole that may be millions or billions of times more massive than the Sun; a disk of interstellar gas and dust called an accretion disk; and two jets perpendicular to the accretion disk. Although supermassive black holes are expected to be found in most AGN, only some galaxies' nuclei have been more carefully studied in attempts to both identify and measure the actual masses of the central supermassive black hole candidates. Some of the most notable galaxies with supermassive black hole candidates include the Andromeda Galaxy, M32, M87, NGC 3115, NGC 3377, NGC 4258, NGC 4889, NGC 1277, OJ 287, APM 08279+5255 and the Sombrero Galaxy. It is now widely accepted"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_42",
    "chunk": "that the centre of nearly every galaxy, not just active ones, contains a supermassive black hole. The close observational correlation between the mass of this hole and the velocity dispersion of the host galaxy's bulge, known as the M–sigma relation, strongly suggests a connection between the formation of the black hole and that of the galaxy itself. Another way the black hole nature of an object may be tested is through observation of effects caused by a strong gravitational field in their vicinity. One such effect is gravitational lensing: The deformation of spacetime around a massive object causes light rays to be deflected, such as light passing through an optic lens. Observations have been made of weak gravitational lensing, in which light rays are deflected by only a few arcseconds. Microlensing occurs when the sources are unresolved and the observer sees a small brightening. The turn of the millennium saw the first 3 candidate detections of black holes in this way, and in January 2022, astronomers reported the first confirmed detection of a microlensing event from an isolated black hole. Another possibility for observing gravitational lensing by a black hole would be to observe stars orbiting the black hole. There"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_43",
    "chunk": "are several candidates for such an observation in orbit around Sagittarius A*. The evidence for stellar black holes strongly relies on the existence of an upper limit for the mass of a neutron star. The size of this limit heavily depends on the assumptions made about the properties of dense matter. New exotic phases of matter could push up this bound. A phase of free quarks at high density might allow the existence of dense quark stars, and some supersymmetric models predict the existence of Q stars. Some extensions of the standard model posit the existence of preons as fundamental building blocks of quarks and leptons, which could hypothetically form preon stars. These hypothetical models could potentially explain a number of observations of stellar black hole candidates. However, it can be shown from arguments in general relativity that any such object will have a maximum mass. Since the average density of a black hole inside its Schwarzschild radius is inversely proportional to the square of its mass, supermassive black holes are much less dense than stellar black holes. The average density of a 10 M☉ black hole is comparable to that of water. Consequently, the physics of matter forming a"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_44",
    "chunk": "supermassive black hole is much better understood and the possible alternative explanations for supermassive black hole observations are much more mundane. For example, a supermassive black hole could be modelled by a large cluster of very dark objects. However, such alternatives are typically not stable enough to explain the supermassive black hole candidates. The evidence for the existence of stellar and supermassive black holes implies that in order for black holes not to form, general relativity must fail as a theory of gravity, perhaps due to the onset of quantum mechanical corrections. A much anticipated feature of a theory of quantum gravity is that it will not feature singularities or event horizons and thus black holes would not be real artefacts. For example, in the fuzzball model based on string theory, the individual states of a black hole solution do not generally have an event horizon or singularity, but for a classical/semiclassical observer the statistical average of such states appears just as an ordinary black hole as deduced from general relativity. A few theoretical objects have been conjectured to match observations of astronomical black hole candidates identically or near-identically, but which function via a different mechanism. These include the gravastar,"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_45",
    "chunk": "the black star, related nestar and the dark-energy star. In 1971, Hawking showed under general conditions that the total area of the event horizons of any collection of classical black holes can never decrease, even if they collide and merge. This result, now known as the second law of black hole mechanics, is remarkably similar to the second law of thermodynamics, which states that the total entropy of an isolated system can never decrease. As with classical objects at absolute zero temperature, it was assumed that black holes had zero entropy. If this were the case, the second law of thermodynamics would be violated by entropy-laden matter entering a black hole, resulting in a decrease in the total entropy of the universe. Therefore, Bekenstein proposed that a black hole should have an entropy, and that it should be proportional to its horizon area. The link with the laws of thermodynamics was further strengthened by Hawking's discovery in 1974 that quantum field theory predicts that a black hole radiates blackbody radiation at a constant temperature. This seemingly causes a violation of the second law of black hole mechanics, since the radiation will carry away energy from the black hole causing it"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_46",
    "chunk": "to shrink. The radiation also carries away entropy, and it can be proven under general assumptions that the sum of the entropy of the matter surrounding a black hole and one quarter of the area of the horizon as measured in Planck units is in fact always increasing. This allows the formulation of the first law of black hole mechanics as an analogue of the first law of thermodynamics, with the mass acting as energy, the surface gravity as temperature and the area as entropy. One puzzling feature is that the entropy of a black hole scales with its area rather than with its volume, since entropy is normally an extensive quantity that scales linearly with the volume of the system. This odd property led Gerard 't Hooft and Leonard Susskind to propose the holographic principle, which suggests that anything that happens in a volume of spacetime can be described by data on the boundary of that volume. Although general relativity can be used to perform a semiclassical calculation of black hole entropy, this situation is theoretically unsatisfying. In statistical mechanics, entropy is understood as counting the number of microscopic configurations of a system that have the same macroscopic qualities,"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_47",
    "chunk": "such as mass, charge, pressure, etc. Without a satisfactory theory of quantum gravity, one cannot perform such a computation for black holes. Some progress has been made in various approaches to quantum gravity. In 1995, Andrew Strominger and Cumrun Vafa showed that counting the microstates of a specific supersymmetric black hole in string theory reproduced the Bekenstein–Hawking entropy. Since then, similar results have been reported for different black holes both in string theory and in other approaches to quantum gravity like loop quantum gravity. Because a black hole has only a few internal parameters, most of the information about the matter that went into forming the black hole is lost. Regardless of the type of matter which goes into a black hole, it appears that only information concerning the total mass, charge, and angular momentum are conserved. As long as black holes were thought to persist forever this information loss is not that problematic, as the information can be thought of as existing inside the black hole, inaccessible from the outside, but represented on the event horizon in accordance with the holographic principle. However, black holes slowly evaporate by emitting Hawking radiation. This radiation does not appear to carry any"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_48",
    "chunk": "additional information about the matter that formed the black hole, meaning that this information appears to be gone forever. The question whether information is truly lost in black holes (the black hole information paradox) has divided the theoretical physics community. In quantum mechanics, loss of information corresponds to the violation of a property called unitarity, and it has been argued that loss of unitarity would also imply violation of conservation of energy, though this has also been disputed. Over recent years evidence has been building that indeed information and unitarity are preserved in a full quantum gravitational treatment of the problem. One attempt to resolve the black hole information paradox is known as black hole complementarity. In 2012, the \"firewall paradox\" was introduced with the goal of demonstrating that black hole complementarity fails to solve the information paradox. According to quantum field theory in curved spacetime, a single emission of Hawking radiation involves two mutually entangled particles. The outgoing particle escapes and is emitted as a quantum of Hawking radiation; the infalling particle is swallowed by the black hole. Assume a black hole formed a finite time in the past and will fully evaporate away in some finite time in"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_49",
    "chunk": "the future. Then, it will emit only a finite amount of information encoded within its Hawking radiation. According to research by physicists like Don Page and Leonard Susskind, there will eventually be a time by which an outgoing particle must be entangled with all the Hawking radiation the black hole has previously emitted. This seemingly creates a paradox: a principle called \"monogamy of entanglement\" requires that, like any quantum system, the outgoing particle cannot be fully entangled with two other systems at the same time; yet here the outgoing particle appears to be entangled both with the infalling particle and, independently, with past Hawking radiation. In order to resolve this contradiction, physicists may eventually be forced to give up one of three time-tested principles: Einstein's equivalence principle, unitarity, or local quantum field theory. One possible solution, which violates the equivalence principle, is that a \"firewall\" destroys incoming particles at the event horizon. In general, which—if any—of these assumptions should be abandoned remains a topic of debate. Christopher Nolan's 2014 science fiction epic Interstellar features a black hole known as Gargantua, which is the central object of a planetary system in a distant galaxy. Humanity accessed this system via a wormhole"
  },
  {
    "source": "Black hole.txt",
    "chunk_id": "Black hole.txt_50",
    "chunk": "in the outer solar system, near Saturn."
  },
  {
    "source": "Camden House Publishing.txt",
    "chunk_id": "Camden House Publishing.txt_0",
    "chunk": "# Camden House Publishing Camden House, Inc. was founded in 1979 by professors James Hardin and Gunther Holst with the purpose of publishing scholarly books in the field of German literature, Austrian Literature, and German language culture. Camden House books were published in Columbia, South Carolina, until 1998. When the company became an imprint in that year, place of publication moved to Rochester, New York. The series Studies in German Literature, Language, and Culture was established in that same year and continues to the present; over 350 books in this series have appeared as of 2011 The Camden House areas of interest expanded over the following years under the direction of James Hardin, emeritus professor at the University of South Carolina. German language literature in Austria and Switzerland were added to the purview of Camden House early in its history. The new series Literary Criticism in Perspective was established in the following decade, and in time broadened to include American and British literature. The aim of this more specialized series was, and is, to elucidate the role of literary criticism over the years, to show how it is subject to varying vogues and philosophical or critical viewpoints, and how criticism"
  },
  {
    "source": "Camden House Publishing.txt",
    "chunk_id": "Camden House Publishing.txt_1",
    "chunk": "itself is a mirror of changing taste and critical bias. In the late 1980s, Camden House increasingly sought out highly qualified scholars to write or edit commissioned works, especially in its Companion series. It was fortunate in locating and working with prominent Germanists who brought out companions to the works of such canonical writers as Hartmann von Aue, Friedrich Schiller, Heinrich von Kleist, Heinrich Heine, Thomas Mann, Rainer Maria Rilke, Franz Kafka, and many others over the next two decades. In addition, companions to major works were commissioned and published, including books focused on Goethe's Faust (I and II), the Nibelungenlied, Gottfried von Strassburg's Tristan, and Mann's Magic Mountain. Distinct periods in the history of German literature were treated in companions to German Realism and German Expressionism. Additionally, Camden House established in its first decade of operation a series developed by James Hardin titled Literary Criticism in Perspective which not only provides the reception history of a given German or Austrian literary work, but records the changing nature of literary criticism itself over the years. Another important aspect of the German program included translations of such key works as Hans Jakob Christoffel von Grimmelshausen's Simplicius Simplicissimus, the first great German"
  },
  {
    "source": "Camden House Publishing.txt",
    "chunk_id": "Camden House Publishing.txt_2",
    "chunk": "novel; Johann Beer's early comic novel Teutsche Winternaechte (German Winter Nights); and Goethe's Wilhelm Meisters theatralische Sendung (Wilhelm Meister's Theatrical Calling), the neglected forerunner to the prototype of Wilhelm Meister's Apprenticeship. Publication of individually authored monographs on numerous prominent German writer have appeared in the Camden House list, as well as numerous works on German film, theater, song, satire, holocaust literature, literature of the German Democratic Republic, and Austrian literature. The ten-volume Camden House History of German Literature, which appeared over a period of six years with completion in 2007, treats German literature in extended essays beginning with the earliest Germanic literature, including Gothic, and discusses in detail not only the literary highpoints of German literature—the High Middle Ages, the Age of Goethe, early twentieth-century works—but also generally neglected periods such as the Early Modern period. About this volume a reviewer wrote 'The editor and the contributors are to be praised for having accomplished a truly Herculean task through which this period finally receives the recognition it deserves. There is nothing comparable on the German, or any other, scholarly book market.' The CH history is prima facie the most voluminous recent analysis of German literature in English; it engaged the"
  },
  {
    "source": "Camden House Publishing.txt",
    "chunk_id": "Camden House Publishing.txt_3",
    "chunk": "collaboration of Germanists from the U.S., the U.K., Germany, Austria, and Australia. This is perhaps the most significant achievement to date of Camden House. In recent years Camden House has branched out into North American literature, extending its series \"Literary Criticism in Perspective\" (series editors: Scott Peeples and James Walker) in that area and having launched a \"European Studies in North American Literature and Culture\" (ESNALC; series editor: Reingard M. Nischik) in 1996. Part of the latter series is the 605-page History of Literature in Canada: English-Canadian and French-Canadian (ed. R.M. Nischik, 2008), Camden House's second major literary history and one of the extremely few histories of Canadian literature to discuss both Canadian literature written in English and Canadian literature written in French in a balanced way. In 1998 Camden House became an imprint of its long-time distributor, Boydell & Brewer, and has continued to publish books in all the areas described above under the editorship of James Walker, who has been associated with the firm since 1994. James Hardin remains a consulting editor with Camden House."
  },
  {
    "source": "Cassini's laws.txt",
    "chunk_id": "Cassini's laws.txt_0",
    "chunk": "# Cassini's laws Cassini's laws provide a compact description of the motion of the Moon. They were established in 1693 by Giovanni Domenico Cassini, a prominent scientist of his time. Refinements of these laws to include physical librations have been made, and they have been generalized to treat other satellites and planets. In the case of the Moon, its rotational axis always points some 1.5 degrees away from the North ecliptic pole. The normal to the Moon's orbital plane and its rotational axis are always on opposite sides of the normal to the ecliptic. Therefore, both the normal to the orbital plane and the Moon's rotational axis precess around the ecliptic pole with the same period. The period is about 18.6 years and the motion is retrograde. A system obeying these laws is said to be in a Cassini state, that is: an evolved rotational state where the spin axis, orbit normal, and normal to the Laplace plane are coplanar while the obliquity remains constant. The Laplace plane is defined as the plane about which a planet or satellite orbit precesses with constant inclination. The normal to the Laplace plane for a moon is between the planet's spin axis and"
  },
  {
    "source": "Cassini's laws.txt",
    "chunk_id": "Cassini's laws.txt_1",
    "chunk": "the planet's orbit normal, being closer to the latter if the moon is distant from the planet. If a planet itself is in a Cassini state, the Laplace plane is the invariable plane of the stellar system. Cassini state 1 is defined as the situation in which both the spin axis and the orbit normal axis are on the same side of the normal to the Laplace plane. Cassini state 2 is defined as the case in which the spin axis and the orbit normal axis are on opposite sides of the normal to the Laplace plane. Earth's Moon is in Cassini state 2. In general, the spin axis moves in the direction perpendicular to both itself and the orbit normal, due to the tidal force exerted by the object being orbited (planet or star) and other objects in the system. (In the case of the Moon, its spin axis moves mostly under the influence of the Earth, whereas the smaller tidal influence of the Sun works in the same direction at full moon and in the opposite direction at new moon and is thus negligible.) The rate of movement of the spin axis goes to zero if the spin"
  },
  {
    "source": "Cassini's laws.txt",
    "chunk_id": "Cassini's laws.txt_2",
    "chunk": "axis coincides with the orbit normal. If the orbit normal precesses in a regular circular motion (due to tidal influences from other objects, such as the Sun in the case of the Moon), it is possible to characterize the solutions to the differential equation for the motion of the spin axis. It turns out that the spin axis traces out loops on the unit sphere that rotates at the speed of the orbital precession (so that the orbit normal and the normal to the Laplace plane are fixed points in the sphere). With certain values of the parameters, there are three areas on the sphere in each of which we have circulation around a point inside the area where the spin axis doesn't move (in this rotating frame of reference). These points are Cassini states 1 and 2 and a third Cassini state in which the rotation is retrograde (which would not apply to a moon like ours that is tidally locked). The three areas are separated by a separatrix that crosses itself, and the point where it crosses itself is the unstable Cassini state 4. (Under other parameter values only states 2 and 3 exist, and there is no"
  },
  {
    "source": "Cassini's laws.txt",
    "chunk_id": "Cassini's laws.txt_3",
    "chunk": "separatrix.) If an object flexes and dissipates kinetic energy, then these solutions are not exact and the system will slowly evolve and approach a stable Cassini state. This has happened with the Moon. It has reached a state with a constant obliquity of 6.7°, at which the precession of the spin axis takes the same 18.6 years as taken by the precession of the orbit normal, and is thus in a Cassini state."
  },
  {
    "source": "Catadioptric system.txt",
    "chunk_id": "Catadioptric system.txt_0",
    "chunk": "# Catadioptric system A catadioptric optical system is one where refraction and reflection are combined in an optical system, usually via lenses (dioptrics) and curved mirrors (catoptrics). Catadioptric combinations are used in focusing systems such as searchlights, headlamps, early lighthouse focusing systems, optical telescopes, microscopes, and telephoto lenses. Other optical systems that use lenses and mirrors are also referred to as \"catadioptric\", such as surveillance catadioptric sensors. Catadioptric combinations have been used for many early optical systems. In the 1820s, Augustin-Jean Fresnel developed several catadioptric lighthouse reflector versions of his Fresnel lens. Léon Foucault developed a catadioptric microscope in 1859 to counteract aberrations of using a lens to image objects at high power. In 1876 a French engineer, A. Mangin, invented what has come to be called the Mangin mirror, a concave glass reflector with the silver surface on the rear side of the glass. The two surfaces of the reflector have different radii to correct the aberration of the spherical mirror. Light passes through the glass twice, making the overall system act like a triplet lens. Mangin mirrors were used in searchlights, where they produced a nearly true parallel beam. Many Catadioptric telescopes use negative lenses with a reflective"
  },
  {
    "source": "Catadioptric system.txt",
    "chunk_id": "Catadioptric system.txt_1",
    "chunk": "coating on the backside that are referred to as “Mangin mirrors”, although they are not single-element objectives like the original Mangin, and some even predate Mangin's invention. Catadioptric telescopes are optical telescopes that combine specifically shaped mirrors and lenses to form an image. This is usually done so that the telescope can have an overall greater degree of error correction than their all-lens or all-mirror counterparts, with a consequently wider aberration-free field of view. Their designs can have simple all-spherical surfaces and can take advantage of a folded optical path that reduces the mass of the telescope, making them easier to manufacture. Many types employ “correctors”, a lens or curved mirror in a combined image-forming optical system so that the reflective or refractive element can correct the aberrations produced by its counterpart. Catadioptric dialytes are the earliest type of catadioptric telescope. They consist of a single-element refracting telescope objective combined with a silver-backed negative lens (similar to a Mangin mirror). The first of these was the Hamiltonian telescope patented by W. F. Hamilton in 1814. The Schupmann medial telescope designed by German optician Ludwig Schupmann near the end of the 19th century placed the catadioptric mirror beyond the focus of"
  },
  {
    "source": "Catadioptric system.txt",
    "chunk_id": "Catadioptric system.txt_2",
    "chunk": "the refractor primary and added a third correcting/focusing lens to the system. There are several telescope designs that take advantage of placing one or more full-diameter lenses (commonly called a \"corrector plate\") in front of a spherical primary mirror. These designs take advantage of all the surfaces being \"spherically symmetrical\" and were originally invented as modifications of mirror based optical systems (reflecting telescopes) to allow them to have an image plane relatively free of coma or astigmatism so they could be used as astrographic cameras. They work by combining a spherical mirror's ability to reflect light back to the same point with a large lens at the front of the system (a corrector) that slightly bends the incoming light, allowing the spherical mirror to image objects at infinity. Some of these designs have been adapted to create compact, long-focal-length catadioptric cassegrains. The Schmidt corrector, the first full-diameter corrector plate, was used in Bernhard Schmidt's 1931 Schmidt camera. The Schmidt camera is a wide-field photographic telescope, with the corrector plate at the center of curvature of the primary mirror, producing an image at a focus inside the tube assembly at the prime focus where a curved film plate or detector is"
  },
  {
    "source": "Catadioptric system.txt",
    "chunk_id": "Catadioptric system.txt_3",
    "chunk": "mounted. The relatively thin and lightweight corrector allows Schmidt cameras to be constructed in diameters up to 1.3 m. The corrector's complex shape takes several processes to make, starting with a flat piece of optical glass, placing a vacuum on one side of it to curve the whole piece, then grinding and polishing the other side flat to achieve the exact shape required to correct the spherical aberration caused by the primary mirror. The design has lent itself to many Schmidt variants. The idea of replacing the complicated Schmidt corrector plate with an easy-to-manufacture full-aperture spherical meniscus lens (a meniscus corrector shell) to create a wide-field telescope occurred to at least four optical designers in early 1940s war-torn Europe, including Albert Bouwers (1940), Dmitri Dmitrievich Maksutov (1941), K. Penning, and Dennis Gabor (1941). Wartime secrecy kept these inventors from knowing about each other's designs, leading to each being an independent invention. Albert Bouwers built a prototype meniscus telescope in August 1940 and patented it in February 1941. It used a spherically concentric meniscus and was only suitable as a monochromatic astronomical camera. In a later design he added a cemented doublet to correct chromatic aberration. Dmitri Maksutov built a prototype"
  },
  {
    "source": "Catadioptric system.txt",
    "chunk_id": "Catadioptric system.txt_4",
    "chunk": "for a similar type of meniscus telescope, the Maksutov telescope, in October 1941 and patented it in November of that same year. His design corrected spherical and chromatic aberrations by placing a weak negative-shaped meniscus corrector closer to the primary mirror. The Houghton telescope or Lurie–Houghton telescope is a design that uses a wide compound positive-negative lens over the entire front aperture to correct spherical aberration of the main mirror. If desired, the two corrector elements can be made with the same type of glass, since the Houghton corrector's chromatic aberration is minimal. The corrector is thicker than a Schmidt-Cassegrain's front corrector, but much thinner than a Maksutov meniscus corrector. All the lens surfaces and the mirror's surface are spheroidal, greatly easing amateur construction. In sub-aperture corrector designs, the corrector elements are usually at the focus of a much larger objective. These elements can be both lenses and mirrors, but since multiple surfaces are involved, achieving good aberration correction in these systems can be very complex. Examples of sub-aperture corrector catadioptric telescopes include the Argunov–Cassegrain telescope, the Klevtsov–Cassegrain telescope and sub-aperture corrector Maksutovs, which use as a \"secondary mirror\" an optical group consisting of lens elements and sometimes mirrors designed"
  },
  {
    "source": "Catadioptric system.txt",
    "chunk_id": "Catadioptric system.txt_5",
    "chunk": "to correct aberration, as well as Jones-Bird Newtonian telescopes, which use a spherical primary mirror combined with a small corrector lens mounted near the focus. Various types of catadioptric systems are also used in camera lenses known alternatively as catadioptric lenses (CATs), reflex lenses, or mirror lenses. These lenses use some form of the cassegrain design which greatly reduces the physical length of the optical assembly, partly by folding the optical path, but mostly through the telephoto effect of the convex secondary mirror which multiplies the focal length many times (up to 4 to 5 times). This creates lenses with focal lengths from 250 mm up to and beyond 1000 mm that are much shorter and compact than their long-focus or telephoto counterparts. Moreover, chromatic aberration, a major problem with long refractive lenses, and off-axis aberration, a major problem with reflective telescopes, is almost completely eliminated by the catadioptric system, making the image they produce suitable to fill the large focal plane of a camera. Catadioptric lenses do, however, have several drawbacks. The fact that they have a central obstruction means they cannot use an adjustable diaphragm to control light transmission. This means the lens's F-number value is fixed to"
  },
  {
    "source": "Catadioptric system.txt",
    "chunk_id": "Catadioptric system.txt_6",
    "chunk": "the overall designed focal ratio of the optical system (the diameter of the primary mirror divided into the focal length). The inability to stop down the lens results in the catadioptric lens having a short depth of field. Exposure is usually adjusted by the placement of neutral density filters on the front or rear of the lens. Their modulation transfer function shows low contrast at low spatial frequencies. Finally, their most salient characteristic is the annular shape of defocused areas of the image, giving a doughnut-shaped 'iris blur' or bokeh, caused by the shape of the entrance pupil. Several companies made catadioptric lenses throughout the later part of the 20th century. Nikon (under the Mirror-Nikkor and later Reflex-Nikkor names) and Canon both offered several designs, such as 500 mm 1:8 and 1000 mm 1:11. Smaller companies such as Tamron, Samyang, Vivitar, and Opteka also offered several versions, with the three latter of these brands still actively producing a number of catadioptric lenses for use in modern system cameras. Minolta (later Sony) offered a 500 mm catadioptric lens for their Alpha range of cameras. The Minolta lens had the distinction of being the only reflex lens manufactured by a major brand"
  },
  {
    "source": "Catadioptric system.txt",
    "chunk_id": "Catadioptric system.txt_7",
    "chunk": "to feature auto-focus."
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_0",
    "chunk": "# Cave painting In archaeology, cave paintings are a type of parietal art (which category also includes petroglyphs, or engravings), found on the wall or ceilings of caves. The term usually implies prehistoric origin. These paintings were often created by Homo sapiens, but also Denisovans and Neanderthals; other species in the same Homo genus. Discussion around prehistoric art is important in understanding the history of the Homo sapiens species and how Homo sapiens have come to have unique abstract thoughts. Some point to these prehistoric paintings as possible examples of creativity, spirituality, and sentimental thinking in prehistoric humans. The oldest known are more than 40,000 years old (art of the Upper Paleolithic) and found in the caves in the district of Maros (Sulawesi, Indonesia). The oldest are often constructed from hand stencils and simple geometric shapes. More recently, in 2021, cave art of a pig found in Sulawesi, Indonesia, and dated to over 45,500 years ago, has been reported. A 2018 study claimed an age of 64,000 years for the oldest examples of non-figurative cave art in the Iberian Peninsula. Represented by three red non-figurative symbols found in the caves of Maltravieso, Ardales and La Pasiega, Spain, these predate the"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_1",
    "chunk": "appearance of modern humans in Europe by at least 20,000 years and thus must have been made by Neanderthals rather than modern humans. In November 2018, scientists reported the discovery of the then-oldest known figurative art painting, over 40,000 (perhaps as old as 52,000) years old, of an unknown animal, in the cave of Lubang Jeriji Saléh on the Indonesian island of Borneo. In December 2019, cave paintings portraying pig hunting within the Maros-Pangkep karst region in Sulawesi were discovered to be even older, with an estimated age of at least 51,200 years. This finding was recognized as \"the oldest known depiction of storytelling and the earliest instance of figurative art in human history.\" On July 3, 2024, the journal Nature published research findings indicating that the cave paintings which depict anthropomorphic figures interacting with a pig and measure 36 by 15 inches (91 by 38 cm) in Leang Karampuang are approximately 51,200 years old, establishing them as the oldest known figurative art paintings in the world. Nearly 350 caves have now been discovered in France and Spain that contain art from prehistoric times. Initially, the age of the paintings had been a contentious issue, since methods like radiocarbon dating"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_2",
    "chunk": "can produce misleading results if contaminated by other samples, and caves and rocky overhangs (where parietal art is found) are typically littered with debris from many time periods. But subsequent technology has made it possible to date the paintings by sampling the pigment itself, torch marks on the walls, or the formation of carbonate deposits on top of the paintings. The subject matter can also indicate chronology: for instance, the reindeer depicted in the Spanish cave of Cueva de las Monedas places the drawings in the last Ice Age. The oldest known cave painting is a red hand stencil in Maltravieso cave, Cáceres, Spain. It has been dated using the uranium-thorium method to older than 64,000 years and was made by a Neanderthal. The oldest date given to an animal cave painting is now a depiction of several human figures hunting pigs in the caves in the Maros-Pangkep karst of South Sulawesi, Indonesia, dated to be over 43,900 years old. Before this, the oldest known figurative cave paintings were that of a bull dated to 40,000 years, at Lubang Jeriji Saléh cave, East Kalimantan, Indonesian Borneo, and a depiction of a pig with a minimum age of 35,400 years at"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_3",
    "chunk": "Timpuseng cave in Sulawesi. The earliest known European figurative cave paintings are those of the Cave of El Castillo in Spain, which a 2012 study using uranium-thorium dated back to at least 40,000 BC. Prior to this announcement, it was believed that the oldest figurative cave paintings were those of the Chauvet Cave in France, dating to earlier than 30,000 BC in the Upper Paleolithic according to radiocarbon dating. Some researchers believe the drawings are too advanced for this era and question this age. More than 80 radiocarbon dates had been obtained by 2011, with samples taken from torch marks and from the paintings themselves, as well as from animal bones and charcoal found on the cave floor. The radiocarbon dates from these samples show that there were two periods of creation in Chauvet: 35,000 years ago and 30,000 years ago. One of the surprises was that many of the paintings were modified repeatedly over thousands of years, possibly explaining the confusion about finer paintings that seemed to date earlier than cruder ones. In 2009, cavers discovered drawings in Coliboaia Cave in Romania, stylistically comparable to those at Chauvet. An initial dating puts the age of an image in the"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_4",
    "chunk": "same range as Chauvet: about 32,000 years old. In Australia, cave paintings have been found on the Arnhem Land plateau showing megafauna which are thought to have been extinct for over 40,000 years, making this site another candidate for oldest known painting; however, the proposed age is dependent on the estimate of the extinction of the species seemingly depicted. Another Australian site, Nawarla Gabarnmang, has charcoal drawings that have been radiocarbon-dated to 28,000 years, making it the oldest site in Australia and among the oldest in the world for which reliable date evidence has been obtained. Other examples may date as late as the Early Bronze Age, but the well-known Magdalenian style seen at Lascaux in France (c. 15,000 BC) and Altamira in Spain died out about 10,000 BC, coinciding with the advent of the Neolithic period. Some caves probably continued to be painted over a period of several thousands of years. The next phase of surviving European prehistoric painting, the rock art of the Iberian Mediterranean Basin, was very different, concentrating on large assemblies of smaller and much less detailed figures, with at least as many humans as animals. This was created roughly between 10,000 and 5,500 years ago,"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_5",
    "chunk": "and painted in rock shelters under cliffs or shallow caves, in contrast to the recesses of deep caves used in the earlier (and much colder) period. Although individual figures are less naturalistic, they are grouped in coherent grouped compositions to a much greater degree. Over a long period of time, the cave art has become less naturalistic and has graduated from beautiful, naturalistic animal drawings to simple ones, and then to abstract shapes. Cave artists use a variety of techniques such as finger tracing, modeling in clay, engravings, bas-relief sculpture, hand stencils, and paintings done in two or three colors. Scholars classify cave art as \"Signs\" or abstract marks. The most common subjects in cave paintings are large wild animals, such as bison, horses, aurochs, and deer, and tracings of human hands as well as abstract patterns, called finger flutings. The species found most often were suitable for hunting by humans, but were not necessarily the actual typical prey found in associated deposits of bones; for example, the painters of Lascaux have mainly left reindeer bones, but this species does not appear at all in the cave paintings, where equine species are the most common. Drawings of humans were rare"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_6",
    "chunk": "and are usually schematic as opposed to the more detailed and naturalistic images of animal subjects. Kieran D. O'Hara, geologist, suggests in his book Cave Art and Climate Change that climate controlled the themes depicted. Pigments used include red and yellow ochre, hematite, manganese oxide and charcoal. Sometimes the silhouette of the animal was incised in the rock first, and in some caves all or many of the images are only engraved in this fashion, taking them somewhat out of a strict definition of \"cave painting\". Similarly, large animals are also the most common subjects in the many small carved and engraved bone or ivory (less often stone) pieces dating from the same periods. But these include the group of Venus figurines, which with a few incomplete exceptions have no real equivalent in Paleolithic cave paintings. One counterexample is a feminine figure in the Chauvet Cave, as described in an interview with Dominique Baffier in Cave of Forgotten Dreams. Hand stencils, formed by placing a hand against the wall and covering the surrounding area in pigment result in the characteristic image of a roughly round area of solid pigment with the negative shape of the hand in the centre, these"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_7",
    "chunk": "may then be decorated with dots, dashes, and patterns. Often, these are found in the same caves as other paintings, or may be the only form of painting in a location. Some walls contain many hand stencils. Similar hands are also painted in the usual fashion. A number of hands show a finger wholly or partly missing, for which a number of explanations have been given. Hand images are found in similar forms in Europe, Eastern Asia, Australia, and South America. One site in Baja California features handprints as a prominent motif in its rock art. Archaeological study of this site revealed that, based on the size of the handprints, they most likely belonged to the women of the community. In addition to this, they were likely used during initiation rituals in Chinigchinich religious practices, which were commonly practiced in the Luiseño territory where this site is located. In the early 20th century, following the work of Walter Baldwin Spencer and Francis James Gillen, scholars such as Salomon Reinach, Henri Breuil and Count Bégouën [fr] interpreted the paintings as 'utilitarian' hunting magic to increase the abundance of prey. Jacob Bronowski states, \"I think that the power that we see expressed"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_8",
    "chunk": "here for the first time is the power of anticipation: the forward-looking imagination. In these paintings the hunter was made familiar with dangers which he knew he had to face but to which he had not yet come.\" Another theory, developed by David Lewis-Williams and broadly based on ethnographic studies of contemporary hunter-gatherer societies, is that the paintings were made by paleolithic shamans. The shaman would retreat into the darkness of the caves, enter into a trance state, then paint images of their visions, perhaps with some notion of drawing out power from the cave walls themselves. R. Dale Guthrie, who has studied both highly artistic and lower quality art and figurines, identifies a wide range of skill and age among the artists. He hypothesizes that the main themes in the paintings and other artifacts (powerful beasts, risky hunting scenes and the representation of nude women) are the work of adolescent males, who constituted a large portion of cave painters, based on surrounding hand print analysis. However, in analyzing hand prints and stencils in French and Spanish caves, Dean Snow of Pennsylvania State University has proposed that a proportion of them, including those around the spotted horses in Pech Merle,"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_9",
    "chunk": "were of female hands. Analysis in 2022, led by Bennett Bacon, an amateur archaeologist, along with a team of professional archeologists and psychologists at the University of Durham, including Paul Pettitt and Robert William Kentridge, suggested that lines and dots (and a commonly seen, if curious, \"Y\" symbol, which was proposed to mean \"to give birth\") on upper palaeolithic cave paintings correlated with the mating cycle of animals in a lunar calendar, potentially making them the earliest known evidence of a proto-writing system and explaining one object of many cave paintings. Other sites include Creswell Crags, Nottinghamshire, England (~14,500 ys old cave etchings and bas-reliefs discovered in 2003), Peștera Coliboaia in Romania (~29,000 y.o. art?). Rock painting was also performed on cliff faces; but fewer of those have survived because of erosion. One example is the rock paintings of Astuvansalmi (3,000–2,500 BC) in the Saimaa area of Finland. When Marcelino Sanz de Sautuola first encountered the Magdalenian paintings of the Cave of Altamira in Cantabria, Spain in 1879, the academics of the time considered them hoaxes. Recent reappraisals and numerous additional discoveries have since demonstrated their authenticity, while at the same time stimulating interest in the artistry and symbolism of"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_10",
    "chunk": "Upper Palaeolithic peoples. In Indonesia the caves in the district of Maros in Sulawesi are famous for their hand prints. About 1,500 negative handprints have also been found in 30 painted caves in the Sangkulirang area of Kalimantan; preliminary dating analysis as of 2005 put their age in the range of 10,000 years old. A 2014 study based on uranium–thorium dating dated a Maros hand stencil to a minimum age of 39,900 years. A painting of a babirusa was dated to at least 35.4 ka, placing it among the oldest known figurative depictions worldwide. In November 2018, scientists reported the discovery of the oldest known figurative art painting, over 40,000 (perhaps as old as 52,000) years old, of an unknown animal, in the cave of Lubang Jeriji Saléh on the Indonesian island of Borneo. And more recently, in 2021, archaeologists announced the discovery of cave art at least 45,500 years old in Leang Tedongnge cave, Indonesia. According to the journal Science Advances, the cave painting of a warty pig is the earliest evidence of human settlement of the region. It has been reported that it is rapidly deteriorating as a result of climate change in the region. Originating in the"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_11",
    "chunk": "Paleolithic period, the rock art found in Khoit Tsenkher Cave, Mongolia, includes symbols and animal forms painted from the walls up to the ceiling. Stags, buffalo, oxen, ibex, lions, Argali sheep, antelopes, camels, elephants, ostriches, and other animal pictorials are present, often forming a palimpsest of overlapping images. The paintings appear brown or red in color, and are stylistically similar to other Paleolithic rock art from around the world but are unlike any other examples in Mongolia. The Padah-Lin Caves of Burma contain 11,000-year-old paintings and many rock tools. The Ambadevi rock shelters have the oldest cave paintings in India, dating back to 25,000 years. The Bhimbetka rock shelters are dated to about 8,000 BC. Similar paintings are found in other parts of India as well. In Tamil Nadu, ancient Paleolithic Cave paintings are found in Kombaikadu, Kilvalai, Settavarai and Nehanurpatti. In Odisha they are found in Yogimatha and Gudahandi. In Karnataka, these paintings are found in Hiregudda near Badami. The most recent painting, consisting of geometric figures, date to the medieval period. Executed mainly in red and white with the occasional use of green and yellow, the paintings depict the lives and times of the people who lived in"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_12",
    "chunk": "the caves, including scenes of childbirth, communal dancing and drinking, religious rites and burials, as well as indigenous animals. Cave paintings found at the Apollo 11 Cave in Namibia are estimated to date from approximately 25,500–27,500 years ago. In 2011, archaeologists found a small rock fragment at Blombos Cave, about 300 km (190 mi) east of Cape Town on the southern cape coastline in South Africa, among spear points and other excavated material. After extensive testing for seven years, it was revealed that the lines drawn on the rock were handmade and from an ochre crayon dating back 73,000 years. This makes it the oldest known rock painting. Significant early cave paintings, executed in ochre, have been found in Kimberley and Kakadu, Australia. Ochre is not an organic material, so carbon dating of these pictures is often impossible. The oldest so far dated at 17,300 years is an ochre painting of a kangaroo in the Kimberley region, which was dated by carbon dating wasp nest material underlying and overlying the painting. Sometimes the approximate date, or at least, an epoch, can be surmised from the painting content, contextual artifacts, or organic material intentionally or inadvertently mixed with the inorganic ochre"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_13",
    "chunk": "paint, including torch soot. A red ochre painting, discovered at the centre of the Arnhem Land Plateau, depicts two emu-like birds with their necks outstretched. They have been identified by a palaeontologist as depicting the megafauna species Genyornis, giant birds thought to have become extinct more than 40,000 years ago; however, this evidence is inconclusive for dating. It may suggest that Genyornis became extinct at a later date than previously determined. Hook Island in the Whitsunday Islands is also home to a number of cave paintings created by the seafaring Ngaro people. In the Philippines at Tabon Caves the oldest artwork may be a relief of a shark above the cave entrance. It was partially disfigured by a later jar burial scene. The Edakkal Caves of Kerala, India, contain drawings that range over periods from the Neolithic as early as 5,000 BC to 1,000 BC. Rock art near Qohaito appears to indicate habitation in the area since the fifth millennium BC, while the town is known to have survived to the sixth century AD. Mount Emba Soira, Eritrea's highest mountain, lies near the site, as does a small successor village. Much of the rock art sites are found together with"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_14",
    "chunk": "evidence of prehistoric stone tools, suggesting that the art could predate the widely presumed pastoralist and domestication events that occurred 5,000– 4,000 years ago. In 2002, a French archaeological team discovered the Laas Geel cave paintings on the outskirts of Hargeisa in Somaliland. Dating back around 5,000 years, the paintings depict both wild animals and decorated cows. They also feature herders, who are believed to be the creators of the rock art. In 2008, Somali archaeologists announced the discovery of other cave paintings in Dhambalin region, which the researchers suggest includes one of the earliest known depictions of a hunter on horseback. The rock art is dated to 1000 to 3000 BC. Additionally, between the towns of Las Khorey and El Ayo in Karinhegane is a site of numerous cave paintings of real and mythical animals. Each painting has an inscription below it, which collectively have been estimated to be around 2,500 years old. Karihegane's rock art is in the same distinctive style as the Laas Geel and Dhambalin cave paintings. Around 25 miles from Las Khorey is found Gelweita, another key rock art site. In Djibouti, rock art of what appear to be antelopes and a giraffe are also"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_15",
    "chunk": "found at Dorra and Balho. Many cave paintings are found in the Tassili n'Ajjer mountains in southeast Algeria. A UNESCO World Heritage Site, the rock art was first discovered in 1933 and has since yielded 15,000 engravings and drawings that keep a record of the various animal migrations, climatic shifts, and change in human inhabitation patterns in this part of the Sahara from 6000 BC to the late classical period. Other cave paintings are also found at the Akakus, Mesak Settafet and Tadrart in Libya and other Sahara regions including: Ayr mountains, Niger and Tibesti, Chad. The Cave of Swimmers and the Cave of Beasts in southwest Egypt, near the border with Libya, in the mountainous Gilf Kebir region of the Sahara Desert. The Cave of Swimmers was discovered in October 1933 by the Hungarian explorer László Almásy. The site contains rock painting images of people swimming, which are estimated to have been created 10,000 years ago during the time of the most recent Ice Age. In 2020, limestone cave decorated with scenes of animals such as donkeys, camels, deer, mule and mountain goats was uncovered in the area of Wadi Al-Zulma by the archaeological mission from the Tourism and"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_16",
    "chunk": "Antiquities Ministry. Rock art cave is 15 meters deep and 20 meters high. At uKhahlamba / Drakensberg Park, South Africa, now thought to be some 3,000 years old, the paintings by the San people who settled in the area some 8,000 years ago depict animals and humans, and are thought to represent religious beliefs. Human figures are much more common in the rock art of Africa than in Europe. Distinctive monochrome and polychrome cave paintings and murals exist in the mid-peninsula regions of southern Baja California and northern Baja California Sur, consisting of Pre-Columbian paintings of humans, land animals, sea creatures, and abstract designs. These paintings are mostly confined to the sierras of this region, but can also be found in outlying mesas and rock shelters. According to recent radiocarbon studies of the area, of materials recovered from archaeological deposits in the rock shelters and on materials in the paintings themselves, suggest that the Great Murals may have a time range extending as far back as 7,500 years ago. Native artists in the Chumash tribes created cave paintings that are located in present-day Santa Barbara, Ventura, and San Luis Obispo Counties in Southern California in the United States. They include"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_17",
    "chunk": "examples at Burro Flats Painted Cave and Chumash Painted Cave State Historic Park. There are also Native American pictogram examples in caves of the Southwestern United States. Cave art that is 6,000 years old was found in the Cumberland Plateau region of Tennessee. Native American tribes have contributed to the makings of Californian cave art, whether it be in Northern or Baja California. The Chumash people of Southern and Baja California made paintings in Swordfish Cave. It was given its name after the swordfish that are painted on its walls and is a sacred site for religious and cultural practices of the Chumash tribe. It was under attack of demolition, which prompted the start of its conservation with cooperation between the Vandenberg Air Force Base and the Tribal Elders Council of the Santa Ynez Band of Chumash. These two parties were able to stabilize and conserve the cave and its art. When previously studied, there were many conclusions about how the paintings were made but not a lot of conclusions about the symbolic value of the rock art and what its meaning to the Chumash tribe. The excavation of the inside of the cave became a viewing area for archaeologists"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_18",
    "chunk": "and anthropologists, specifically Clayton Lebow, Douglas Harrow, and Rebecca McKim, to find out the symbolic meaning of the art. Some of the tools that were used to make the pictographs were found in the site and were connected to the two early occupations that were in the area. This pushed back the general knowledge of understood antiquity of rock art on California's Central Coast by more than 2,000 years. The National Institution of Anthropology and History (INAH) established in Mexico recorded over 1,500 rock art related archaeological monuments in Baja California. A little under 300 of the sites were connected to Native American Tribes. Throughout these 300 sites, 65% have paintings, 24% have petroglyphs, 10% have both paintings and petroglyphs, and 1% have geoglyphs. Five of these sites located in Baja California show hand designs or paintings, and they all spread out in that area. These sites include Milagro de Guadalupe (23 imprints), Corral de Queno (6 imprints), Rancho Viejo (1 drawing), Piedras Gordas (5 imprints), and finally Valle Seco (3 imprints). Serra da Capivara National Park is a national park in the north east of Brazil with many prehistoric paintings; the park was created to protect the prehistoric artifacts"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_19",
    "chunk": "and paintings found there. It became a World Heritage Site in 1991. Its best known archaeological site is Pedra Furada. It is located in northeast state of Piauí, between latitudes 8° 26' 50\" and 8° 54' 23\" south and longitudes 42° 19' 47\" and 42° 45' 51\" west. It falls within the municipal areas of São Raimundo Nonato, São João do Piauí, Coronel José Dias and Canto do Buriti. It has an area of 1291.4 square kilometres (319,000 acres). The area has the largest concentration of prehistoric small farms on the American continents. Scientific studies confirm that the Capivara mountain range was densely populated in prehistoric periods. Cueva de las Manos (Spanish for \"Cave of the Hands\") is a cave located in the province of Santa Cruz, Argentina, 163 km (101 mi) south of the town of Perito Moreno, within the borders of the Francisco P. Moreno National Park, which includes many sites of archaeological and paleontological importance. The hand images are often negative (stencilled). Besides these there are also depictions of human beings, guanacos, rheas, felines and other animals, as well as geometric shapes, zigzag patterns, representations of the sun, and hunting scenes. Similar paintings, though in smaller numbers,"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_20",
    "chunk": "can be found in nearby caves. There are also red dots on the ceilings, probably made by submerging their hunting bolas in ink, and then throwing them up. The colours of the paintings vary from red (made from hematite) to white, black or yellow. The negative hand impressions date to around 550 BC, the positive impressions from 180 BC, while the hunting drawings are calculated to more than 10,000 years old. Most of the hands are \"left hands\" (that is, with thumb on the right, even though this pattern can be obtained as easily with both right and left hands, depending on whether the back or front is used) which has been used as an argument to suggest that painters held the spraying pipe with their right hand. There are rock paintings in caves in Thailand, Malaysia, Indonesia, and Burma. In Thailand, caves and scarps along the Thai-Burmese border, in the Petchabun Range of Central Thailand, and overlooking the Mekong River in Nakorn Sawan Province, all contain galleries of rock paintings. In Malaysia, the Tambun rock art is dated at 2000 years, and those in the Painted Cave at Niah Caves National Park are 1200 years old. The anthropologist Ivor"
  },
  {
    "source": "Cave painting.txt",
    "chunk_id": "Cave painting.txt_21",
    "chunk": "Hugh Norman Evans visited Malaysia in the early 1920s and found that some of the tribes (especially Negritos) were still producing cave paintings and had added depictions of modern objects including what are believed to be automobiles. (See prehistoric Malaysia.) In Indonesia, rock paintings can be found in Sumatra, Kalimantan, Sulawesi, Flores, Timor, Maluku and Papua."
  },
  {
    "source": "Chandrasekhar limit.txt",
    "chunk_id": "Chandrasekhar limit.txt_0",
    "chunk": "# Chandrasekhar limit The Chandrasekhar limit (/ˌtʃəndrəˈʃeɪkər/) is the maximum mass of a stable white dwarf star. The currently accepted value of the Chandrasekhar limit is about 1.4 M☉ (2.765×10 kg). The limit was named after Subrahmanyan Chandrasekhar. White dwarfs resist gravitational collapse primarily through electron degeneracy pressure, compared to main sequence stars, which resist collapse through thermal pressure. The Chandrasekhar limit is the mass above which electron degeneracy pressure in the star's core is insufficient to balance the star's own gravitational self-attraction. Normal stars fuse gravitationally compressed hydrogen into helium, generating vast amounts of heat. As the hydrogen is consumed, the stars' core compresses further allowing the helium and heavier nuclei to fuse ultimately resulting in stable iron nuclei, a process called stellar evolution. The next step depends upon the mass of the star. Stars below the Chandrasekhar limit become stable white dwarf stars, remaining that way throughout the rest of the history of the universe (assuming the absence of external forces). Stars above the limit can become neutron stars or black holes. The Chandrasekhar limit is a consequence of competition between gravity and electron degeneracy pressure. Electron degeneracy pressure is a quantum-mechanical effect arising from the Pauli exclusion"
  },
  {
    "source": "Chandrasekhar limit.txt",
    "chunk_id": "Chandrasekhar limit.txt_1",
    "chunk": "principle. Since electrons are fermions, no two electrons can be in the same state, so not all electrons can be in the minimum-energy level. Rather, electrons must occupy a band of energy levels. Compression of the electron gas increases the number of electrons in a given volume and raises the maximum energy level in the occupied band. Therefore, the energy of the electrons increases on compression, so pressure must be exerted on the electron gas to compress it, producing electron degeneracy pressure. With sufficient compression, electrons are forced into nuclei in the process of electron capture, relieving the pressure. In the nonrelativistic case, electron degeneracy pressure gives rise to an equation of state of the form P = K1ρ, where P is the pressure, ρ is the mass density, and K1 is a constant. Solving the hydrostatic equation leads to a model white dwarf that is a polytrope of index ⁠3/2⁠ – and therefore has radius inversely proportional to the cube root of its mass, and volume inversely proportional to its mass. As the mass of a model white dwarf increases, the typical energies to which degeneracy pressure forces the electrons are no longer negligible relative to their rest masses."
  },
  {
    "source": "Chandrasekhar limit.txt",
    "chunk_id": "Chandrasekhar limit.txt_2",
    "chunk": "The velocities of the electrons approach the speed of light, and special relativity must be taken into account. In the strongly relativistic limit, the equation of state takes the form P = K2ρ. This yields a polytrope of index 3, which has a total mass, Mlimit, depending only on K2. For a fully relativistic treatment, the equation of state used interpolates between the equations P = K1ρ for small ρ and P = K2ρ for large ρ. When this is done, the model radius still decreases with mass, but becomes zero at Mlimit. This is the Chandrasekhar limit. The curves of radius against mass for the non-relativistic and relativistic models are shown in the graph. They are colored blue and green, respectively. μe has been set equal to 2. Radius is measured in standard solar radii or kilometers, and mass in standard solar masses. Calculated values for the limit vary depending on the nuclear composition of the mass. Chandrasekhar gives the following expression, based on the equation of state for an ideal Fermi gas: M limit = ω 3 0 3 π 2 ( ℏ c G ) 3 2 1 ( μ e m H ) 2 {\\displaystyle M_{\\text{limit}}={\\frac"
  },
  {
    "source": "Chandrasekhar limit.txt",
    "chunk_id": "Chandrasekhar limit.txt_3",
    "chunk": "{\\omega _{3}^{0}{\\sqrt {3\\pi }}}{2}}\\left({\\frac {\\hbar c}{G}}\\right)^{\\frac {3}{2}}{\\frac {1}{(\\mu _{\\text{e}}m_{\\text{H}})^{2}}}} where: As √ħc/G is the Planck mass, the limit is of the order of M Pl 3 m H 2 {\\displaystyle {\\frac {M_{\\text{Pl}}^{3}}{m_{\\text{H}}^{2}}}} The limiting mass can be obtained formally from the Chandrasekhar's white dwarf equation by taking the limit of large central density. A more accurate value of the limit than that given by this simple model requires adjusting for various factors, including electrostatic interactions between the electrons and nuclei and effects caused by nonzero temperature. Lieb and Yau have given a rigorous derivation of the limit from a relativistic many-particle Schrödinger equation. In 1926, the British physicist Ralph H. Fowler observed that the relationship between the density, energy, and temperature of white dwarfs could be explained by viewing them as a gas of nonrelativistic, non-interacting electrons and nuclei that obey Fermi–Dirac statistics. This Fermi gas model was then used by the British physicist Edmund Clifton Stoner in 1929 to calculate the relationship among the mass, radius, and density of white dwarfs, assuming they were homogeneous spheres. Wilhelm Anderson applied a relativistic correction to this model, giving rise to a maximum possible mass of approximately 1.37×10 kg. In 1930, Stoner"
  },
  {
    "source": "Chandrasekhar limit.txt",
    "chunk_id": "Chandrasekhar limit.txt_4",
    "chunk": "derived the internal energy–density equation of state for a Fermi gas, and was then able to treat the mass–radius relationship in a fully relativistic manner, giving a limiting mass of approximately 2.19×10 kg (for μe = 2.5). Stoner went on to derive the pressure–density equation of state, which he published in 1932. These equations of state were also previously published by the Soviet physicist Yakov Frenkel in 1928, together with some other remarks on the physics of degenerate matter. Frenkel's work, however, was ignored by the astronomical and astrophysical community. A series of papers published between 1931 and 1935 had its beginning on a trip from India to England in 1930, where the Indian physicist Subrahmanyan Chandrasekhar worked on the calculation of the statistics of a degenerate Fermi gas. In these papers, Chandrasekhar solved the hydrostatic equation together with the nonrelativistic Fermi gas equation of state, and also treated the case of a relativistic Fermi gas, giving rise to the value of the limit shown above. Chandrasekhar reviews this work in his Nobel Prize lecture. The existence of a related limit, based on the conceptual breakthrough of combining relativity with Fermi degeneracy, was first established in separate papers published by"
  },
  {
    "source": "Chandrasekhar limit.txt",
    "chunk_id": "Chandrasekhar limit.txt_5",
    "chunk": "Wilhelm Anderson and E. C. Stoner for a uniform density star in 1929. Eric G. Blackman wrote that the roles of Stoner and Anderson in the discovery of mass limits were overlooked when Freeman Dyson wrote a biography of Chandrasekhar. Michael Nauenberg claims that Stoner established the mass limit first. The priority dispute has also been discussed at length by Virginia Trimble who writes that: \"Chandrasekhar famously, perhaps even notoriously did his critical calculation on board ship in 1930, and ... was not aware of either Stoner's or Anderson's work at the time. His work was therefore independent, but, more to the point, he adopted Eddington's polytropes for his models which could, therefore, be in hydrostatic equilibrium, which constant density stars cannot, and real ones must be.\" This value was also computed in 1932 by the Soviet physicist Lev Landau, who, however, did not apply it to white dwarfs and concluded that quantum laws might be invalid for stars heavier than 1.5 solar mass. Chandrasekhar's work on the limit aroused controversy, owing to the opposition of the British astrophysicist Arthur Eddington. Eddington was aware that the existence of black holes was theoretically possible, and also realized that the existence of"
  },
  {
    "source": "Chandrasekhar limit.txt",
    "chunk_id": "Chandrasekhar limit.txt_6",
    "chunk": "the limit made their formation possible. However, he was unwilling to accept that this could happen. After a talk by Chandrasekhar on the limit in 1935, he replied: The star has to go on radiating and radiating and contracting and contracting until, I suppose, it gets down to a few km radius, when gravity becomes strong enough to hold in the radiation, and the star can at last find peace. ... I think there should be a law of Nature to prevent a star from behaving in this absurd way! Eddington's proposed solution to the perceived problem was to modify relativistic mechanics so as to make the law P = K1ρ universally applicable, even for large ρ. Although Niels Bohr, Fowler, Wolfgang Pauli, and other physicists agreed with Chandrasekhar's analysis, at the time, owing to Eddington's status, they were unwilling to publicly support Chandrasekhar. Through the rest of his life, Eddington held to his position in his writings, including his work on his fundamental theory. The drama associated with this disagreement is one of the main themes of Empire of the Stars, Arthur I. Miller's biography of Chandrasekhar. In Miller's view: Chandra's discovery might well have transformed and accelerated developments"
  },
  {
    "source": "Chandrasekhar limit.txt",
    "chunk_id": "Chandrasekhar limit.txt_7",
    "chunk": "in both physics and astrophysics in the 1930s. Instead, Eddington's heavy-handed intervention lent weighty support to the conservative community astrophysicists, who steadfastly refused even to consider the idea that stars might collapse to nothing. As a result, Chandra's work was almost forgotten. However, Chandrasekhar chose to move on, leaving the study of stellar structure to focus on stellar dynamics. In 1983 in recognition for his work, Chandrasekhar shared a Nobel prize \"for his theoretical studies of the physical processes of importance to the structure and evolution of the stars\" with William Alfred Fowler. The core of a star is kept from collapsing by the heat generated by the fusion of nuclei of lighter elements into heavier ones. At various stages of stellar evolution, the nuclei required for this process are exhausted, and the core collapses, causing it to become denser and hotter. A critical situation arises when iron accumulates in the core, since iron nuclei are incapable of generating further energy through fusion. If the core becomes sufficiently dense, electron degeneracy pressure will play a significant part in stabilizing it against gravitational collapse. If a main-sequence star is not too massive (less than approximately 8 solar masses), it eventually sheds"
  },
  {
    "source": "Chandrasekhar limit.txt",
    "chunk_id": "Chandrasekhar limit.txt_8",
    "chunk": "enough mass to form a white dwarf having mass below the Chandrasekhar limit, which consists of the former core of the star. For more-massive stars, electron degeneracy pressure does not keep the iron core from collapsing to very great density, leading to formation of a neutron star, black hole, or, speculatively, a quark star. (For very massive, low-metallicity stars, it is also possible that instabilities destroy the star completely.) During the collapse, neutrons are formed by the capture of electrons by protons in the process of electron capture, leading to the emission of neutrinos. The decrease in gravitational potential energy of the collapsing core releases a large amount of energy on the order of 10 J (100 foes). Most of this energy is carried away by the emitted neutrinos and the kinetic energy of the expanding shell of gas; only about 1% is emitted as optical light. This process is believed responsible for supernovae of types Ib, Ic, and II. Type Ia supernovae derive their energy from runaway fusion of the nuclei in the interior of a white dwarf. This fate may befall carbon–oxygen white dwarfs that accrete matter from a companion giant star, leading to a steadily increasing mass."
  },
  {
    "source": "Chandrasekhar limit.txt",
    "chunk_id": "Chandrasekhar limit.txt_9",
    "chunk": "As the white dwarf's mass approaches the Chandrasekhar limit, its central density increases, and, as a result of compressional heating, its temperature also increases. This eventually ignites nuclear fusion reactions, leading to an immediate carbon detonation, which disrupts the star and causes the supernova. A strong indication of the reliability of Chandrasekhar's formula is that the absolute magnitudes of supernovae of Type Ia are all approximately the same; at maximum luminosity, MV is approximately −19.3, with a standard deviation of no more than 0.3. A 1-sigma interval therefore represents a factor of less than 2 in luminosity. This seems to indicate that all type Ia supernovae convert approximately the same amount of mass to energy. In April 2003, the Supernova Legacy Survey observed a type Ia supernova, designated SNLS-03D3bb, in a galaxy approximately 4 billion light years away. According to a group of astronomers at the University of Toronto and elsewhere, the observations of this supernova are best explained by assuming that it arose from a white dwarf that had grown to twice the mass of the Sun before exploding. They believe that the star, dubbed the \"Champagne Supernova\" may have been spinning so fast that a centrifugal tendency allowed"
  },
  {
    "source": "Chandrasekhar limit.txt",
    "chunk_id": "Chandrasekhar limit.txt_10",
    "chunk": "it to exceed the limit. Alternatively, the supernova may have resulted from the merger of two white dwarfs, so that the limit was only violated momentarily. Nevertheless, they point out that this observation poses a challenge to the use of type Ia supernovae as standard candles. Since the observation of the Champagne Supernova in 2003, several more type Ia supernovae have been observed that are very bright, and thought to have originated from white dwarfs whose masses exceeded the Chandrasekhar limit. These include SN 2006gz, SN 2007if, and SN 2009dc. The super-Chandrasekhar mass white dwarfs that gave rise to these supernovae are believed to have had masses up to 2.4–2.8 solar masses. One way to potentially explain the problem of the Champagne Supernova was considering it the result of an aspherical explosion of a white dwarf. However, spectropolarimetric observations of SN 2009dc showed it had a polarization smaller than 0.3, making the large asphericity theory unlikely. Stars sufficiently massive to pass the Chandrasekhar limit provided by electron degeneracy pressure do not become white dwarf stars. Instead they explode as supernovae. If the final mass is below the Tolman–Oppenheimer–Volkoff limit, then neutron degeneracy pressure contributes to the balance against gravity and"
  },
  {
    "source": "Chandrasekhar limit.txt",
    "chunk_id": "Chandrasekhar limit.txt_11",
    "chunk": "the result will be a neutron star; but if the total mass is above the Tolman-Oppenheimer-Volkhoff limit, the result will be a black hole."
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_0",
    "chunk": "# Chemical element A chemical element is a chemical substance whose atoms all have the same number of protons. The number of protons is called the atomic number of that element. For example, oxygen has an atomic number of 8: each oxygen atom has 8 protons in its nucleus. Atoms of the same element can have different numbers of neutrons in their nuclei, known as isotopes of the element. Two or more atoms can combine to form molecules. Some elements form molecules of atoms of said element only: e.g. atoms of hydrogen (H) form diatomic molecules (H2). Chemical compounds are substances made of atoms of different elements; they can have molecular or non-molecular structure. Mixtures are materials containing different chemical substances; that means (in case of molecular substances) that they contain different types of molecules. Atoms of one element can be transformed into atoms of a different element in nuclear reactions, which change an atom's atomic number. Historically, the term \"chemical element\" meant a substance that cannot be broken down into constituent substances by chemical reactions, and for most practical purposes this definition still has validity. There was some controversy in the 1920s over whether isotopes deserved to be recognised"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_1",
    "chunk": "as separate elements if they could be separated by chemical means. The term \"(chemical) element\" is used in two different but closely related meanings: it can mean a chemical substance consisting of a single kind of atom (a free element), or it can mean that kind of atom as a component of various chemical substances. For example, water (H2O) consists of the elements hydrogen (H) and oxygen (O) even though it does not contain the chemical substances (di)hydrogen (H2) and (di)oxygen (O2), as H2O molecules are different from H2 and O2 molecules. For the meaning \"chemical substance consisting of a single kind of atom\", the terms \"elementary substance\" and \"simple substance\" have been suggested, but they have not gained much acceptance in English chemical literature, whereas in some other languages their equivalent is widely used. For example, French distinguishes élément chimique (kind of atoms) and corps simple (chemical substance consisting of one kind of atom); Russian distinguishes химический элемент and простое вещество. Almost all baryonic matter in the universe is composed of elements (among rare exceptions are neutron stars). When different elements undergo chemical reactions, atoms are rearranged into new compounds held together by chemical bonds. Only a few elements,"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_2",
    "chunk": "such as silver and gold, are found uncombined as relatively pure native element minerals. Nearly all other naturally occurring elements occur in the Earth as compounds or mixtures. Air is mostly a mixture of molecular nitrogen and oxygen, though it does contain compounds including carbon dioxide and water, as well as atomic argon, a noble gas which is chemically inert and therefore does not undergo chemical reactions. The history of the discovery and use of elements began with early human societies that discovered native minerals like carbon, sulfur, copper and gold (though the modern concept of an element was not yet understood). Attempts to classify materials such as these resulted in the concepts of classical elements, alchemy, and similar theories throughout history. Much of the modern understanding of elements developed from the work of Dmitri Mendeleev, a Russian chemist who published the first recognizable periodic table in 1869. This table organizes the elements by increasing atomic number into rows (\"periods\") in which the columns (\"groups\") share recurring (\"periodic\") physical and chemical properties. The periodic table summarizes various properties of the elements, allowing chemists to derive relationships between them and to make predictions about elements not yet discovered, and potential new"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_3",
    "chunk": "compounds. By November 2016, the International Union of Pure and Applied Chemistry (IUPAC) recognized a total of 118 elements. The first 94 occur naturally on Earth, and the remaining 24 are synthetic elements produced in nuclear reactions. Save for unstable radioactive elements (radioelements) which decay quickly, nearly all elements are available industrially in varying amounts. The discovery and synthesis of further new elements is an ongoing area of scientific study. The lightest elements are hydrogen and helium, both created by Big Bang nucleosynthesis in the first 20 minutes of the universe in a ratio of around 3:1 by mass (or 12:1 by number of atoms), along with tiny traces of the next two elements, lithium and beryllium. Almost all other elements found in nature were made by various natural methods of nucleosynthesis. On Earth, small amounts of new atoms are naturally produced in nucleogenic reactions, or in cosmogenic processes, such as cosmic ray spallation. New atoms are also naturally produced on Earth as radiogenic daughter isotopes of ongoing radioactive decay processes such as alpha decay, beta decay, spontaneous fission, cluster decay, and other rarer modes of decay. Of the 94 naturally occurring elements, those with atomic numbers 1 through 82"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_4",
    "chunk": "each have at least one stable isotope (except for technetium, element 43 and promethium, element 61, which have no stable isotopes). Isotopes considered stable are those for which no radioactive decay has yet been observed. Elements with atomic numbers 83 through 94 are unstable enough that radioactive decay of all isotopes can be detected. Some of these elements, notably bismuth (atomic number 83), thorium (atomic number 90), and uranium (atomic number 92), have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy metals before our Solar System formed. At 2×10 years, over 10 times the estimated age of the universe, bismuth-209 has the longest known alpha decay half-life of any nuclide, and is almost always considered on par with the 80 stable elements. The heaviest elements (those beyond plutonium, element 94) are radioactive, with half-lives so short that they are not found in nature and must be synthesized. There are now 118 known elements. \"Known\" here means observed well enough, even from just a few decay products, to have been differentiated from other elements. Most recently, the synthesis of element 118 (since named oganesson) was reported in October"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_5",
    "chunk": "2006, and the synthesis of element 117 (tennessine) was reported in April 2010. Of these 118 elements, 94 occur naturally on Earth. Six of these occur in extreme trace amounts: technetium, atomic number 43; promethium, number 61; astatine, number 85; francium, number 87; neptunium, number 93; and plutonium, number 94. These 94 elements have been detected in the universe at large, in the spectra of stars and also supernovae, where short-lived radioactive elements are newly being made. The first 94 elements have been detected directly on Earth as primordial nuclides present from the formation of the Solar System, or as naturally occurring fission or transmutation products of uranium and thorium. The remaining 24 heavier elements, not found today either on Earth or in astronomical spectra, have been produced artificially: all are radioactive, with short half-lives; if any of these elements were present when the Earth formed, they are certain to have completely decayed, and if present in novae, are in quantities too small to have been noted. Technetium was the first purportedly non-naturally occurring element synthesized, in 1937, though traces of technetium have since been found in nature (and also the element may have been discovered naturally in 1925). This"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_6",
    "chunk": "pattern of artificial production and later natural discovery has been repeated with several other radioactive naturally occurring rare elements. Lists of elements are available by name, atomic number, density, melting point, boiling point and chemical symbol, as well as ionization energy. The nuclides of stable and radioactive elements are also available as a list of nuclides, sorted by length of half-life for those that are unstable. One of the most convenient, and certainly the most traditional presentation of the elements, is in the form of the periodic table, which groups together elements with similar chemical properties (and usually also similar electronic structures). The atomic number of an element is equal to the number of protons in each atom, and defines the element. For example, all carbon atoms contain 6 protons in their atomic nucleus; so the atomic number of carbon is 6. Carbon atoms may have different numbers of neutrons; atoms of the same element having different numbers of neutrons are known as isotopes of the element. The number of protons in the nucleus also determines its electric charge, which in turn determines the number of electrons of the atom in its non-ionized state. The electrons are placed into atomic"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_7",
    "chunk": "orbitals that determine the atom's chemical properties. The number of neutrons in a nucleus usually has very little effect on an element's chemical properties; except for hydrogen (for which the kinetic isotope effect is significant). Thus, all carbon isotopes have nearly identical chemical properties because they all have six electrons, even though they may have 6 to 8 neutrons. That is why atomic number, rather than mass number or atomic weight, is considered the identifying characteristic of an element. Isotopes are atoms of the same element (that is, with the same number of protons in their nucleus), but having different numbers of neutrons. Thus, for example, there are three main isotopes of carbon. All carbon atoms have 6 protons, but they can have either 6, 7, or 8 neutrons. Since the mass numbers of these are 12, 13 and 14 respectively, said three isotopes are known as carbon-12, carbon-13, and carbon-14 (C, C, and C). Natural carbon is a mixture of C (about 98.9%), C (about 1.1%) and about 1 atom per trillion of C. Most (54 of 94) naturally occurring elements have more than one stable isotope. Except for the isotopes of hydrogen (which differ greatly from each other"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_8",
    "chunk": "in relative mass—enough to cause chemical effects), the isotopes of a given element are chemically nearly indistinguishable. All elements have radioactive isotopes (radioisotopes); most of these radioisotopes do not occur naturally. Radioisotopes typically decay into other elements via alpha decay, beta decay, or inverse beta decay; some isotopes of the heaviest elements also undergo spontaneous fission. Isotopes that are not radioactive, are termed \"stable\" isotopes. All known stable isotopes occur naturally (see primordial nuclide). The many radioisotopes that are not found in nature have been characterized after being artificially produced. Certain elements have no stable isotopes and are composed only of radioisotopes: specifically the elements without any stable isotopes are technetium (atomic number 43), promethium (atomic number 61), and all observed elements with atomic number greater than 82. Of the 80 elements with at least one stable isotope, 26 have only one stable isotope. The mean number of stable isotopes for the 80 stable elements is 3.1 stable isotopes per element. The largest number of stable isotopes for a single element is 10 (for tin, element 50). The mass number of an element, A, is the number of nucleons (protons and neutrons) in the atomic nucleus. Different isotopes of a"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_9",
    "chunk": "given element are distinguished by their mass number, which is written as a superscript on the left hand side of the chemical symbol (e.g., U). The mass number is always an integer and has units of \"nucleons\". Thus, magnesium-24 (24 is the mass number) is an atom with 24 nucleons (12 protons and 12 neutrons). Whereas the mass number simply counts the total number of neutrons and protons and is thus an integer, the atomic mass of a particular isotope (or \"nuclide\") of the element is the mass of a single atom of that isotope, and is typically expressed in daltons (symbol: Da), aka universal atomic mass units (symbol: u). Its relative atomic mass is a dimensionless number equal to the atomic mass divided by the atomic mass constant, which equals 1 Da. In general, the mass number of a given nuclide differs in value slightly from its relative atomic mass, since the mass of each proton and neutron is not exactly 1 Da; since the electrons contribute a lesser share to the atomic mass as neutron number exceeds proton number; and because of the nuclear binding energy and electron binding energy. For example, the atomic mass of chlorine-35 to"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_10",
    "chunk": "five significant digits is 34.969 Da and that of chlorine-37 is 36.966 Da. However, the relative atomic mass of each isotope is quite close to its mass number (always within 1%). The only isotope whose atomic mass is exactly a natural number is C, which has a mass of 12 Da; because the dalton is defined as 1/12 of the mass of a free neutral carbon-12 atom in the ground state. The standard atomic weight (commonly called \"atomic weight\") of an element is the average of the atomic masses of all the chemical element's isotopes as found in a particular environment, weighted by isotopic abundance, relative to the atomic mass unit. This number may be a fraction that is not close to a whole number. For example, the relative atomic mass of chlorine is 35.453 u, which differs greatly from a whole number as it is an average of about 76% chlorine-35 and 24% chlorine-37. Whenever a relative atomic mass value differs by more than ~1% from a whole number, it is due to this averaging effect, as significant amounts of more than one isotope are naturally present in a sample of that element. Chemists and nuclear scientists have different"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_11",
    "chunk": "definitions of a pure element. In chemistry, a pure element means a substance whose atoms all (or in practice almost all) have the same atomic number, or number of protons. Nuclear scientists, however, define a pure element as one that consists of only one isotope. For example, a copper wire is 99.99% chemically pure if 99.99% of its atoms are copper, with 29 protons each. However it is not isotopically pure since natural copper consists of two stable isotopes, 69% Cu and 31% Cu, with different numbers of neutrons. (See Isotopes of copper.) However, pure gold would be both chemically and isotopically pure, since ordinary gold consists only of one isotope, Au. Atoms of chemically pure elements may bond to each other chemically in more than one way, allowing the pure element to exist in multiple chemical structures (spatial arrangements of atoms), known as allotropes, which differ in their properties. For example, carbon can be found as diamond, which has a tetrahedral structure around each carbon atom; graphite, which has layers of carbon atoms with a hexagonal structure stacked on top of each other; graphene, which is a single layer of graphite that is very strong; fullerenes, which have nearly"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_12",
    "chunk": "spherical shapes; and carbon nanotubes, which are tubes with a hexagonal structure (even these may differ from each other in electrical properties). The ability of an element to exist in one of many structural forms is known as 'allotropy'. The reference state of an element is defined by convention, usually as the thermodynamically most stable allotrope and physical state at a pressure of 1 bar and a given temperature (typically 298.15K). However, for phosphorus, the reference state is white phosphorus even though it is not the most stable allotrope, and the reference state for carbon is graphite, because the structure of graphite is more stable than that of the other allotropes. In thermochemistry, an element is defined to have an enthalpy of formation of zero in its reference state. Several kinds of descriptive categorisations can be applied broadly to the elements, including consideration of their general physical and chemical properties, their states of matter under familiar conditions, their melting and boiling points, their densities, their crystal structures as solids, and their origins. Several terms are commonly used to characterise the general physical and chemical properties of the chemical elements. A first distinction is between metals, which readily conduct electricity, nonmetals,"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_13",
    "chunk": "which do not, and a small group, (the metalloids), having intermediate properties and often behaving as semiconductors. A more refined classification is often shown in coloured presentations of the periodic table. This system restricts the terms \"metal\" and \"nonmetal\" to only certain of the more broadly defined metals and nonmetals, adding additional terms for certain sets of the more broadly viewed metals and nonmetals. The version of this classification used in the periodic tables presented here includes: actinides, alkali metals, alkaline earth metals, halogens, lanthanides, transition metals, post-transition metals, metalloids, reactive nonmetals, and noble gases. In this system, the alkali metals, alkaline earth metals, and transition metals, as well as the lanthanides and the actinides, are special groups of the metals viewed in a broader sense. Similarly, the reactive nonmetals and the noble gases are nonmetals viewed in the broader sense. In some presentations, the halogens are not distinguished, with astatine identified as a metalloid and the others identified as nonmetals. Another commonly used basic distinction among the elements is their state of matter (phase), whether solid, liquid, or gas, at standard temperature and pressure (STP). Most elements are solids at STP, while several are gases. Only bromine and mercury"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_14",
    "chunk": "are liquid at 0 degrees Celsius (32 degrees Fahrenheit) and 1 atmosphere pressure; caesium and gallium are solid at that temperature, but melt at 28.4°C (83.2°F) and 29.8°C (85.6°F), respectively. Melting and boiling points, typically expressed in degrees Celsius at a pressure of one atmosphere, are commonly used in characterizing the various elements. While known for most elements, either or both of these measurements is still undetermined for some of the radioactive elements available in only tiny quantities. Since helium remains a liquid even at absolute zero at atmospheric pressure, it has only a boiling point, and not a melting point, in conventional presentations. The density at selected standard temperature and pressure (STP) is often used in characterizing the elements. Density is often expressed in grams per cubic centimetre (g/cm). Since several elements are gases at commonly encountered temperatures, their densities are usually stated for their gaseous forms; when liquefied or solidified, the gaseous elements have densities similar to those of the other elements. When an element has allotropes with different densities, one representative allotrope is typically selected in summary presentations, while densities for each allotrope can be stated where more detail is provided. For example, the three familiar allotropes"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_15",
    "chunk": "of carbon (amorphous carbon, graphite, and diamond) have densities of 1.8–2.1, 2.267, and 3.515 g/cm, respectively. The elements studied to date as solid samples have eight kinds of crystal structures: cubic, body-centered cubic, face-centered cubic, hexagonal, monoclinic, orthorhombic, rhombohedral, and tetragonal. For some of the synthetically produced transuranic elements, available samples have been too small to determine crystal structures. Chemical elements may also be categorised by their origin on Earth, with the first 94 considered naturally occurring, while those with atomic numbers beyond 94 have only been produced artificially via human-made nuclear reactions. Of the 94 naturally occurring elements, 83 are considered primordial and either stable or weakly radioactive. The longest-lived isotopes of the remaining 11 elements have half lives too short for them to have been present at the beginning of the Solar System, and are therefore \"transient elements\". Of these 11 transient elements, five (polonium, radon, radium, actinium, and protactinium) are relatively common decay products of thorium and uranium. The remaining six transient elements (technetium, promethium, astatine, francium, neptunium, and plutonium) occur only rarely, as products of rare decay modes or nuclear reaction processes involving uranium or other heavy elements. Elements with atomic numbers 1 through 82, except"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_16",
    "chunk": "43 (technetium) and 61 (promethium), each have at least one isotope for which no radioactive decay has been observed. Observationally stable isotopes of some elements (such as tungsten and lead), however, are predicted to be slightly radioactive with very long half-lives: for example, the half-lives predicted for the observationally stable lead isotopes range from 10 to 10 years. Elements with atomic numbers 43, 61, and 83 through 94 are unstable enough that their radioactive decay can be detected. Three of these elements, bismuth (element 83), thorium (90), and uranium (92) have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy elements before the formation of the Solar System. For example, at over 1.9×10 years, over a billion times longer than the estimated age of the universe, bismuth-209 has the longest known alpha decay half-life of any isotope. The last 24 elements (those beyond plutonium, element 94) undergo radioactive decay with short half-lives and cannot be produced as daughters of longer-lived elements, and thus are not known to occur in nature at all. The properties of the elements are often summarized using the periodic table, which powerfully and elegantly"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_17",
    "chunk": "organizes the elements by increasing atomic number into rows (\"periods\") in which the columns (\"groups\") share recurring (\"periodic\") physical and chemical properties. The table contains 118 confirmed elements as of 2021. Though earlier precursors to this presentation exist, its invention is generally credited to Russian chemist Dmitri Mendeleev in 1869, who intended the table to illustrate recurring trends in the properties of the elements. The layout of the table has been refined and extended over time as new elements have been discovered and new theoretical models have been developed to explain chemical behavior. Use of the periodic table is now ubiquitous in chemistry, providing an extremely useful framework to classify, systematize and compare all the many different forms of chemical behavior. The table has also found wide application in physics, geology, biology, materials science, engineering, agriculture, medicine, nutrition, environmental health, and astronomy. Its principles are especially important in chemical engineering. The various chemical elements are formally identified by their unique atomic numbers, their accepted names, and their chemical symbols. The known elements have atomic numbers from 1 to 118, conventionally presented as Arabic numerals. Since the elements can be uniquely sequenced by atomic number, conventionally from lowest to highest (as"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_18",
    "chunk": "in a periodic table), sets of elements are sometimes specified by such notation as \"through\", \"beyond\", or \"from ... through\", as in \"through iron\", \"beyond uranium\", or \"from lanthanum through lutetium\". The terms \"light\" and \"heavy\" are sometimes also used informally to indicate relative atomic numbers (not densities), as in \"lighter than carbon\" or \"heavier than lead\", though the atomic masses of the elements (their atomic weights or atomic masses) do not always increase monotonically with their atomic numbers. The naming of various substances now known as elements precedes the atomic theory of matter, as names were given locally by various cultures to various minerals, metals, compounds, alloys, mixtures, and other materials, though at the time it was not known which chemicals were elements and which compounds. As they were identified as elements, the existing names for anciently known elements (e.g., gold, mercury, iron) were kept in most countries. National differences emerged over the element names either for convenience, linguistic niceties, or nationalism. For example, German speakers use \"Wasserstoff\" (water stuff) for \"hydrogen\", \"Sauerstoff\" (acid stuff) for \"oxygen\", and \"Stickstoff\" (smothering stuff) for \"nitrogen\"; English and some other languages use \"sodium\" for \"natrium\", and \"potassium\" for \"kalium\"; and the French,"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_19",
    "chunk": "Italians, Greeks, Portuguese and Poles prefer \"azote/azot/azoto\" (from roots meaning \"no life\") for \"nitrogen\". For purposes of international communication and trade, the official names of the chemical elements both ancient and more recently recognised are decided by the International Union of Pure and Applied Chemistry (IUPAC), which has decided on a sort of international English language, drawing on traditional English names even when an element's chemical symbol is based on a Latin or other traditional word, for example adopting \"gold\" rather than \"aurum\" as the name for the 79th element (Au). IUPAC prefers the British spellings \"aluminium\" and \"caesium\" over the U.S. spellings \"aluminum\" and \"cesium\", and the U.S. \"sulfur\" over British \"sulphur\". However, elements that are practical to sell in bulk in many countries often still have locally used national names, and countries whose national language does not use the Latin alphabet are likely to use the IUPAC element names. According to IUPAC, element names are not proper nouns; therefore, the full name of an element is not capitalised in English, even if derived from a proper noun, as in californium and einsteinium. Isotope names are also uncapitalised if written out, e.g., carbon-12 or uranium-235. Chemical element symbols (such"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_20",
    "chunk": "as Cf for californium and Es for einsteinium), are always capitalised (see below). In the second half of the 20th century, physics laboratories became able to produce elements with half-lives too short for an appreciable amount of them to exist at any time. These are also named by IUPAC, which generally adopts the name chosen by the discoverer. This practice can lead to the controversial question of which research group actually discovered an element, a question that delayed the naming of elements with atomic number of 104 and higher for a considerable amount of time. (See element naming controversy). Precursors of such controversies involved the nationalistic namings of elements in the late 19th century. For example, lutetium was named after Paris, France. The Germans were reluctant to relinquish naming rights to the French, often calling it cassiopeium. Similarly, the British discoverer of niobium originally named it columbium, in reference to the New World. It was used extensively as such by American publications before the international standardisation (in 1950). Before chemistry became a science, alchemists designed arcane symbols for both metals and common compounds. These were however used as abbreviations in diagrams or procedures; there was no concept of atoms combining"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_21",
    "chunk": "to form molecules. With his advances in the atomic theory of matter, John Dalton devised his own simpler symbols, based on circles, to depict molecules. The current system of chemical notation was invented by Jöns Jacob Berzelius in 1814. In this system, chemical symbols are not mere abbreviations—though each consists of letters of the Latin alphabet. They are intended as universal symbols for people of all languages and alphabets. Since Latin was the common language of science at Berzelius' time, his symbols were abbreviations based on the Latin names of elements (they may be Classical Latin names of elements known since antiquity or Neo-Latin coinages for later elements). The symbols are not followed by a period (full stop) as with abbreviations. In most cases, Latin names of elements as used by Berzelius have the same roots as the modern English name. For example, hydrogen has the symbol \"H\" from Neo-Latin hydrogenium, which has the same Greek roots as English hydrogen. However, in eleven cases Latin (as used by Berzelius) and English names of elements have different roots. Eight of them are the seven metals of antiquity and a metalloid also known since antiquity: \"Fe\" (Latin ferrum) for iron, \"Hg\" (Latin"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_22",
    "chunk": "hydrargyrum) for mercury, \"Sn\" (Latin stannum) for tin, \"Au\" (Latin aurum) for gold, \"Ag\" (Latin argentum) for silver, \"Pb\" (Latin plumbum) for lead, \"Cu\" (Latin cuprum) for copper, and \"Sb\" (Latin stibium) for antimony. The three other mismatches between Neo-Latin (as used by Berzelius) and English names are \"Na\" (Neo-Latin natrium) for sodium, \"K\" (Neo-Latin kalium) for potassium, and \"W\" (Neo-Latin wolframium) for tungsten. These mismatches came from different suggestings of naming the elements in the Modern era. Initially Berzelius had suggested \"So\" and \"Po\" for sodium and potassium, but he changed the symbols to \"Na\" and \"K\" later in the same year. Elements discovered after 1814 were also assigned unique chemical symbols, based on the name of the element. The use of Latin as the universal language of science was fading, but chemical names of newly discovered elements came to be borrowed from language to language with little or no modification. Symbols of elements discovered after 1814 match their names in English, French (ignoring the acute accent on ⟨é⟩), and German (though German often allows alternate spellings with ⟨k⟩ or ⟨z⟩ instead of ⟨c⟩: e.g., the name of calcium may be spelled Calcium or Kalzium in German, but its"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_23",
    "chunk": "symbol is always \"Ca\"). Other languages sometimes modify element name spellings: Spanish iterbio (ytterbium), Italian afnio (hafnium), Swedish moskovium (moscovium); but those modifications do not affect chemical symbols: Yb, Hf, Mc. Chemical symbols are understood internationally when element names might require translation. There have been some differences in the past. For example, Germans in the past have used \"J\" (for the name Jod) for iodine, but now use \"I\" and Iod. The first letter of a chemical symbol is always capitalised, and the subsequent letters, if any, are always lowercase; see the preceding examples. There are also symbols in chemical equations for groups of elements, for example in comparative formulas. These are often a single capital letter, and the letters are reserved and not used for names of specific elements. For example, \"X\" indicates a variable group (usually a halogen) in a class of compounds, while \"R\" is a radical, meaning a compound structure such as a hydrocarbon chain. The letter \"Q\" is reserved for \"heat\" in a chemical reaction. \"Y\" is also often used as a general chemical symbol, though it is also the symbol of yttrium. \"Z\" is also often used as a general variable group. \"E\" is"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_24",
    "chunk": "used in organic chemistry to denote an electron-withdrawing group or an electrophile; similarly \"Nu\" denotes a nucleophile. \"L\" is used to represent a general ligand in inorganic and organometallic chemistry. \"M\" is also often used in place of a general metal. At least two other, two-letter generic chemical symbols are also in informal use, \"Ln\" for any lanthanide and \"An\" for any actinide. \"Rg\" was formerly used for any rare gas element, but the group of rare gases has now been renamed noble gases and \"Rg\" now refers to roentgenium. Isotopes of an element are distinguished by mass number (total protons and neutrons), with this number combined with the element's symbol. IUPAC prefers that isotope symbols be written in superscript notation when practical, for example C and U. However, other notations, such as carbon-12 and uranium-235, or C-12 and U-235, are also used. As a special case, the three naturally occurring isotopes of hydrogen are often specified as H for H (protium), D for H (deuterium), and T for H (tritium). This convention is easier to use in chemical equations, replacing the need to write out the mass number each time. Thus, the formula for heavy water may be written"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_25",
    "chunk": "D2O instead of H2O. Only about 4% of the total mass of the universe is made of atoms or ions, and thus represented by elements. This fraction is about 15% of the total matter, with the remainder of the matter (85%) being dark matter. The nature of dark matter is unknown, but it is not composed of atoms of elements because it contains no protons, neutrons, or electrons. (The remaining non-matter part of the mass of the universe is composed of the even less well understood dark energy). The 94 naturally occurring elements were produced by at least four classes of astrophysical process. Most of the hydrogen, helium and a very small quantity of lithium were produced in the first few minutes of the Big Bang. This Big Bang nucleosynthesis happened only once; the other processes are ongoing. Nuclear fusion inside stars produces elements through stellar nucleosynthesis, including all elements from carbon to iron in atomic number. Elements higher in atomic number than iron, including heavy elements like uranium and plutonium, are produced by various forms of explosive nucleosynthesis in supernovae and neutron star mergers. The light elements lithium, beryllium and boron are produced mostly through cosmic ray spallation (fragmentation"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_26",
    "chunk": "induced by cosmic rays) of carbon, nitrogen, and oxygen. In the early phases of the Big Bang, nucleosynthesis of hydrogen resulted in the production of hydrogen-1 (protium, H) and helium-4 (He), as well as a smaller amount of deuterium (H) and tiny amounts (on the order of 10) of lithium and beryllium. Even smaller amounts of boron may have been produced in the Big Bang, since it has been observed in some very old stars, while carbon has not. No elements heavier than boron were produced in the Big Bang. As a result, the primordial abundance of atoms (or ions) consisted of ~75% H, 25% He, and 0.01% deuterium, with only tiny traces of lithium, beryllium, and perhaps boron. Subsequent enrichment of galactic halos occurred due to stellar nucleosynthesis and supernova nucleosynthesis. However, the element abundance in intergalactic space can still closely resemble primordial conditions, unless it has been enriched by some means. On Earth (and elsewhere), trace amounts of various elements continue to be produced from other elements as products of nuclear transmutation processes. These include some produced by cosmic rays or other nuclear reactions (see cosmogenic and nucleogenic nuclides), and others produced as decay products of long-lived primordial"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_27",
    "chunk": "nuclides. For example, trace (but detectable) amounts of carbon-14 (C) are continually produced in the air by cosmic rays impacting nitrogen atoms, and argon-40 (Ar) is continually produced by the decay of primordially occurring but unstable potassium-40 (K). Also, three primordially occurring but radioactive actinides, thorium, uranium, and plutonium, decay through a series of recurrently produced but unstable elements such as radium and radon, which are transiently present in any sample of containing these metals. Three other radioactive elements, technetium, promethium, and neptunium, occur only incidentally in natural materials, produced as individual atoms by nuclear fission of the nuclei of various heavy elements or in other rare nuclear processes. Besides the 94 naturally occurring elements, several artificial elements have been produced by nuclear physics technology. By 2016, these experiments had produced all elements up to atomic number 118. The following graph (note log scale) shows the abundance of elements in our Solar System. The table shows the 12 most common elements in our galaxy (estimated spectroscopically), as measured in parts per million by mass. Nearby galaxies that have evolved along similar lines have a corresponding enrichment of elements heavier than hydrogen and helium. The more distant galaxies are being viewed"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_28",
    "chunk": "as they appeared in the past, so their abundances of elements appear closer to the primordial mixture. As physical laws and processes appear common throughout the visible universe, however, scientists expect that these galaxies evolved elements in similar abundance. The abundance of elements in the Solar System is in keeping with their origin from nucleosynthesis in the Big Bang and a number of progenitor supernova stars. Very abundant hydrogen and helium are products of the Big Bang, but the next three elements are rare since they had little time to form in the Big Bang and are not made in stars (they are, however, produced in small quantities by the breakup of heavier elements in interstellar dust, as a result of impact by cosmic rays). Beginning with carbon, elements are produced in stars by buildup from alpha particles (helium nuclei), resulting in an alternatingly larger abundance of elements with even atomic numbers (these are also more stable). In general, such elements up to iron are made in large stars in the process of becoming supernovas. Iron-56 is particularly common, since it is the most stable nuclide that can easily be made from alpha particles (being a product of decay of"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_29",
    "chunk": "radioactive nickel-56, ultimately made from 14 helium nuclei). Elements heavier than iron are made in energy-absorbing processes in large stars, and their abundance in the universe (and on Earth) generally decreases with their atomic number. The abundance of the chemical elements on Earth varies from air to crust to ocean, and in various types of life. The abundance of elements in Earth's crust differs from that in the Solar System (as seen in the Sun and massive planets like Jupiter) mainly in selective loss of the very lightest elements (hydrogen and helium) and also volatile neon, carbon (as hydrocarbons), nitrogen and sulfur, as a result of solar heating in the early formation of the Solar System. Oxygen, the most abundant Earth element by mass, is retained on Earth by combination with silicon. Aluminium at 8% by mass is more common in the Earth's crust than in the universe and solar system, but the composition of the far more bulky mantle, which has magnesium and iron in place of aluminium (which occurs there only at 2% of mass) more closely mirrors the elemental composition of the solar system, save for the noted loss of volatile elements to space, and loss of"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_30",
    "chunk": "iron which has migrated to the Earth's core. The composition of the human body, by contrast, more closely follows the composition of seawater—save that the human body has additional stores of carbon and nitrogen necessary to form the proteins and nucleic acids, together with phosphorus in the nucleic acids and energy transfer molecule adenosine triphosphate (ATP) that occurs in the cells of all living organisms. Certain kinds of organisms require particular additional elements, for example the magnesium in chlorophyll in green plants, the calcium in mollusc shells, or the iron in the hemoglobin in vertebrates' red blood cells. The concept of an \"element\" as an indivisible substance has developed through three major historical phases: Classical definitions (such as those of the ancient Greeks), chemical definitions, and atomic definitions. Ancient philosophy posited a set of classical elements to explain observed patterns in nature. These elements originally referred to earth, water, air and fire rather than the chemical elements of modern science. The term 'elements' (stoicheia) was first used by Greek philosopher Plato around 360 BCE in his dialogue Timaeus, which includes a discussion of the composition of inorganic and organic bodies and is a speculative treatise on chemistry. Plato believed the"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_31",
    "chunk": "elements introduced a century earlier by Empedocles were composed of small polyhedral forms: tetrahedron (fire), octahedron (air), icosahedron (water), and cube (earth). Aristotle, c. 350 BCE, also used the term stoicheia and added a fifth element, aether, which formed the heavens. Aristotle defined an element as: Element – one of those bodies into which other bodies can decompose, and that itself is not capable of being divided into other. In 1661, in The Sceptical Chymist, Robert Boyle proposed his theory of corpuscularism which favoured the analysis of matter as constituted of irreducible units of matter (atoms); and, choosing to side with neither Aristotle's view of the four elements nor Paracelsus' view of three fundamental elements, left open the question of the number of elements. Boyle argued against a pre-determined number of elements—directly against Paracelsus' three principles (sulfur, mercury, and salt), indirectly against the \"Aristotelian\" elements (earth, water, air, and fire), for Boyle felt that the arguments against the former were at least as valid against the latter. Much of what I am to deliver ... may be indifferently apply'd to the four Peripatetick Elements, and the three Chymical Principles ... the Chymical Hypothesis seeming to be much more countenanc'd by"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_32",
    "chunk": "Experience then the other, it will be expedient to insist chiefly upon the disproving of that; especially since most of the Arguments that are imploy'd against it, may, by a little variation, be made ... at least as strongly against the less plausible, Aristotelian Doctrine. Then Boyle stated his view in four propositions. In the first and second, he suggests that matter consists of particles, but that these particles may be difficult to separate. Boyle used the concept of \"corpuscles\"—or \"atomes\", as he also called them—to explain how a limited number of elements could combine into a vast number of compounds. Propos. I. ... At the first Production of mixt Bodies, the Universal Matter whereof they ... consisted, was actually divided into little Particles. ... The Generation ... and wasting of Bodies ... and ... the Chymical Resolutions of mixt Bodies, and ... Operations of ... Fires upon them ... manifest their consisting of parts very minute... Epicurus ... as you well know, supposes ... all ... Bodies ... to be produc'd by ... Atomes, moving themselves to and fro ... in the ... Infinite Vacuum. ... Propos. II. ... These minute Particles ... were ... associated into minute ..."
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_33",
    "chunk": "Clusters ... not easily dissipable into such Particles as compos'd them. ... If we assigne to the Corpuscles, whereof each Element consists, a peculiar size and shape ... such ... Corpuscles may be mingled in such various Proportions, and ... connected so many ... wayes, that an almost incredible number of ... Concretes may be compos'd of them. Boyle explained that gold reacts with aqua regia, and mercury with nitric acid, sulfuric acid, and sulfur to produce various \"compounds\", and that they could be recovered from those compounds, just as would be expected of elements. Yet, Boyle did not consider gold, mercury, or lead elements, but rather—together with wine—\"perfectly mixt bodies\". Quicksilver ... with Aqua fortis will be brought into a ... white Powder ... with Sulphur it will compose a blood-red ... Cinaber. And yet out of all these exotick Compounds, we may recover the very same running Mercury. ... Propos. III. ... From most of such mixt Bodies ... there may by the Help of the Fire, be actually obtain'd a determinate number (whether Three, Four or Five, or fewer or more) of Substances ... The Chymists are wont to call the Ingredients of mixt Bodies, Principles, as"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_34",
    "chunk": "the Aristotelians name them Elements. ... Principles ... as not being compounded of any more primary Bodies: and Elements, in regard that all mix'd Bodies are compounded of them. Even though Boyle is primarily regarded as the first modern chemist, The Sceptical Chymist still contains old ideas about the elements, alien to a contemporary viewpoint. Sulfur, for example, is not only the familiar yellow non-metal but also an inflammable \"spirit\". In 1724, in his book Logick, the English minister and logician Isaac Watts enumerated the elements then recognised by chemists. Watts' list of elements included two of Paracelsus' principles (sulfur and salt) and two classical elements (earth and water) as well as \"spirit\". Watts did, however, note a lack of consensus among chemists. Elements are such Substances as cannot be resolved, or reduced, into two or more Substances of different Kinds. ... Followers of Aristotle made Fire, Air, Earth and Water to be the four Elements, of which all earthly Things were compounded; and they suppos'd the Heavens to be a Quintessence, or fifth sort of Body, distinct from all these : But, since experimental Philosophy ... have been better understood, this Doctrine has been abundantly refuted. The Chymists make"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_35",
    "chunk": "Spirit, Salt, Sulphur, Water and Earth to be their five Elements, because they can reduce all terrestrial Things to these five :.. tho' they are not all agreed. The first modern list of elements was given in Antoine Lavoisier's 1789 Elements of Chemistry, which contained 33 elements, including light and caloric. By 1818, Jöns Jacob Berzelius had determined atomic weights for 45 of the 49 then-accepted elements. Dmitri Mendeleev had 63 elements in his 1869 periodic table. From Boyle until the early 20th century, an element was defined as a pure substance that cannot be decomposed into any simpler substance and cannot be transformed into other elements by chemical processes. Elements at the time were generally distinguished by their atomic weights, a property measurable with fair accuracy by available analytical techniques. The 1913 discovery by English physicist Henry Moseley that the nuclear charge is the physical basis for the atomic number, further refined when the nature of protons and neutrons became appreciated, eventually led to the current definition of an element based on atomic number (number of protons). The use of atomic numbers, rather than atomic weights, to distinguish elements has greater predictive value (since these numbers are integers) and"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_36",
    "chunk": "also resolves some ambiguities in the chemistry-based view due to varying properties of isotopes and allotropes within the same element. Currently, IUPAC defines an element to exist if it has isotopes with a lifetime longer than the 10 seconds it takes the nucleus to form an electronic cloud. By 1914, eighty-seven elements were known, all naturally occurring (see Discovery of chemical elements). The remaining naturally occurring elements were discovered or isolated in subsequent decades, and various additional elements have also been produced synthetically, with much of that work pioneered by Glenn T. Seaborg. In 1955, element 101 was discovered and named mendelevium in honor of D. I. Mendeleev, the first to arrange the elements periodically. Ten materials familiar to various prehistoric cultures are now known to be elements: Carbon, copper, gold, iron, lead, mercury, silver, sulfur, tin, and zinc. Three additional materials now accepted as elements, arsenic, antimony, and bismuth, were recognised as distinct substances before 1500 AD. Phosphorus, cobalt, and platinum were isolated before 1750. Most of the remaining naturally occurring elements were identified and characterised by 1900, including: The first transuranium element (element with an atomic number greater than 92) discovered was neptunium in 1940. Since 1999, the"
  },
  {
    "source": "Chemical element.txt",
    "chunk_id": "Chemical element.txt_37",
    "chunk": "IUPAC/IUPAP Joint Working Party has considered claims for the discovery of new elements. As of January 2016, all 118 elements have been confirmed by IUPAC as being discovered. The discovery of element 112 was acknowledged in 2009, and the name copernicium and the chemical symbol Cn were suggested for it. The name and symbol were officially endorsed by IUPAC on 19 February 2010. The heaviest element that is believed to have been synthesised to date is element 118, oganesson, on 9 October 2006, by the Flerov Laboratory of Nuclear Reactions in Dubna, Russia. Tennessine, element 117 was the latest element claimed to be discovered, in 2009. On 28 November 2016, scientists at the IUPAC officially recognised the names for the four newest elements, with atomic numbers 113, 115, 117, and 118."
  },
  {
    "source": "Comet Hale–Bopp.txt",
    "chunk_id": "Comet Hale–Bopp.txt_0",
    "chunk": "# Comet Hale–Bopp Comet Hale–Bopp (formally designated C/1995 O1) is a long-period comet that was one of the most widely observed of the 20th century and one of the brightest seen for many decades. Alan Hale and Thomas Bopp discovered Comet Hale–Bopp separately on July 23, 1995, before it became visible to the naked eye. It is difficult to predict the maximum brightness of new comets with any degree of certainty, but Hale–Bopp exceeded most predictions when it passed perihelion on April 1, 1997, reaching about magnitude −1.8. Its massive nucleus size made it visible to the naked eye for a record 18 months. This is twice as long as the Great Comet of 1811, the previous record holder. Accordingly, Hale–Bopp was dubbed the Great Comet of 1997. The comet was discovered independently on July 23, 1995, by two observers, Alan Hale and Thomas Bopp, both in the United States. Hale had spent many hundreds of hours searching for comets without success, and was tracking known comets from his driveway in New Mexico when he chanced upon Hale–Bopp just after midnight. The comet had an apparent magnitude of 10.5 and lay near the globular cluster M70 in the constellation of"
  },
  {
    "source": "Comet Hale–Bopp.txt",
    "chunk_id": "Comet Hale–Bopp.txt_1",
    "chunk": "Sagittarius. Hale first established that there was no other deep-sky object near M70, and then consulted a directory of known comets, finding that none were known to be in this area of the sky. Once he had established that the object was moving relative to the background stars, he emailed the Central Bureau for Astronomical Telegrams, the clearing house for astronomical discoveries. Bopp did not own a telescope. He was out with friends near Stanfield, Arizona, observing star clusters and galaxies when he chanced across the comet while at the eyepiece of his friend's telescope. He realized he might have spotted something new when, like Hale, he checked his star maps to determine if any other deep-sky objects were known to be near M70, and found none. He alerted the Central Bureau for Astronomical Telegrams through a Western Union telegram. Brian G. Marsden, who had run the bureau since 1968, laughed, \"Nobody sends telegrams anymore. I mean, by the time that telegram got here, Alan Hale had already e-mailed us three times with updated coordinates.\" The following morning, it was confirmed that this was a new comet, and it was given the designation C/1995 O1. The discovery was announced in"
  },
  {
    "source": "Comet Hale–Bopp.txt",
    "chunk_id": "Comet Hale–Bopp.txt_2",
    "chunk": "International Astronomical Union circular 6187. Hale–Bopp's orbital position was calculated as 7.2 astronomical units (au) from the Sun, placing it between Jupiter and Saturn and by far the greatest distance from Earth at which a comet had been discovered by amateurs. Most comets at this distance are extremely faint, and show no discernible activity, but Hale–Bopp already had an observable coma. A precovery image taken at the UK Schmidt Telescope in 1993 was found to show the then-unnoticed comet some 13 AU from the Sun, a distance at which most comets are essentially unobservable. (Halley's Comet was more than 100 times fainter at the same distance from the Sun.) Analysis indicated later that its comet nucleus was 60±20 kilometres in diameter, approximately six times the size of Halley's Comet. Its great distance and surprising activity indicated that comet Hale–Bopp might become very bright when it reached perihelion in 1997. However, comet scientists were wary – comets can be extremely unpredictable, and many have large outbursts at great distances only to diminish in brightness later. Comet Kohoutek in 1973 had been touted as a \"comet of the century\" and turned out to be unspectacular. Hale–Bopp became visible to the naked eye"
  },
  {
    "source": "Comet Hale–Bopp.txt",
    "chunk_id": "Comet Hale–Bopp.txt_3",
    "chunk": "in May 1996, and although its rate of brightening slowed considerably during the latter half of that year, scientists were still cautiously optimistic that it would become very bright. It was too closely aligned with the Sun to be observable during December 1996, but when it reappeared in January 1997 it was already bright enough to be seen by anyone who looked for it, even from large cities with light-polluted skies. The Internet was a growing phenomenon at the time, and numerous websites that tracked the comet's progress and provided daily images from around the world became extremely popular. The Internet played a large role in encouraging the unprecedented public interest in comet Hale–Bopp. As the comet approached the Sun, it continued to brighten, shining at 2nd magnitude in February, and showing a growing pair of tails, the blue gas tail pointing straight away from the Sun and the yellowish dust tail curving away along its orbit. On March 9, a solar eclipse in China, Mongolia and eastern Siberia allowed observers there to see the comet in the daytime. Hale–Bopp had its closest approach to Earth on March 22, 1997, at a distance of 1.315 au. As it passed perihelion"
  },
  {
    "source": "Comet Hale–Bopp.txt",
    "chunk_id": "Comet Hale–Bopp.txt_4",
    "chunk": "on April 1, 1997, the comet developed into a spectacular sight. It shone brighter than any star in the sky except Sirius, and its dust tail stretched 40–45 degrees across the sky. The comet was visible well before the sky got fully dark each night, and while many great comets are very close to the Sun as they pass perihelion, comet Hale–Bopp was visible all night to Northern Hemisphere observers. After its perihelion passage, the comet moved into the southern celestial hemisphere. The comet was much less impressive to southern hemisphere observers than it had been in the northern hemisphere, but southerners could see the comet gradually fade from view during the second half of 1997. The last naked-eye observations were reported in December 1997, which meant that the comet had remained visible without aid for 569 days, or about 18+1⁄2 months. The previous record had been set by the Great Comet of 1811, which was visible to the naked eye for about 9 months. The comet continued to fade as it receded, but was still tracked by astronomers. In October 2007, 10 years after the perihelion and at a distance of 25.7 au from the Sun, the comet was"
  },
  {
    "source": "Comet Hale–Bopp.txt",
    "chunk_id": "Comet Hale–Bopp.txt_5",
    "chunk": "still active as indicated by the detection of the CO-driven coma. Herschel Space Observatory images taken in 2010 suggest comet Hale–Bopp is covered in a fresh frost layer. Hale–Bopp was again detected in December 2010 when it was 30.7 AU away from the Sun, and in 2012, at 33.2 AU from the Sun. The James Webb Space Telescope observed Hale–Bopp in 2022, when it was 46.2 AU from the Sun. The comet likely made its previous perihelion approximately 4,200 years ago, roughly the year 2215 BC. The estimated closest approach to Earth was 1.4 au, and it may have been observed in ancient Egypt during the 6th dynasty reign of the Pharaoh Pepi II (Reign: 2247 – c. 2216 BC). Pepi's pyramid at Saqqara contains a text referring to an \"nhh-star\" as a companion of the pharaoh in the heavens, where \"nhh\" is the hieroglyph for long hair. Hale–Bopp may have had a near collision with Jupiter in 2215 BC, which probably caused a dramatic change in its orbit, and 2215 BC may have been its first passage through the inner Solar System from the Oort cloud. The comet's current orbit is almost perpendicular to the plane of the ecliptic,"
  },
  {
    "source": "Comet Hale–Bopp.txt",
    "chunk_id": "Comet Hale–Bopp.txt_6",
    "chunk": "so further close approaches to planets will be rare. However, in April 1996 the comet passed within 0.77 AU (115 million km) of Jupiter, close enough for its orbit to be measurably affected by the planet's gravity. The comet's orbit was shortened considerably to a period of roughly 2,399 years, and it will next return to the inner Solar System around the year 4385. Its greatest distance from the Sun (aphelion) will be about 354 AU, reduced from about 525 AU. The estimated probability of Hale–Bopp's striking Earth in future passages through the inner Solar System is remote, about 2.5×10 per orbit. However, given that the comet nucleus is around 60 km (37 mi) in diameter, the consequences of such an impact would be apocalyptic. Weissman conservatively estimates the diameter at 35 km (22 mi); an estimated density of 0.6 g/cm then gives a cometary mass of 1.3×10 g. At a probable impact velocity of 52.5 km/s, impact energy can be calculated as 1.9×10 ergs, or 4.4×10 megatons, about 44 times the estimated energy of the K-T impact event. Over many orbits, the cumulative effect of gravitational perturbations on comets with high orbital inclinations and small perihelion distances is generally"
  },
  {
    "source": "Comet Hale–Bopp.txt",
    "chunk_id": "Comet Hale–Bopp.txt_7",
    "chunk": "to reduce the perihelion distance to very small values. Hale–Bopp has about a 15% chance of eventually becoming a sungrazing comet through this process. If such is the case, it could undergo huge mass loss, or break up into smaller pieces like the Kreutz sungrazers. It would also be extremely bright, due to a combination of closeness to the Sun and nuclei size, potentially exceeding Halley's Comet in 837 AD. Due to the massive size of its nucleus, Comet Hale–Bopp was observed intensively by astronomers during its perihelion passage, and several important advances in cometary science resulted from these observations. The dust production rate of the comet was very high (up to 2.0×10 kg/s), which may have made the inner coma optically thick. Based on the properties of the dust grains – high temperature, high albedo and strong 10 μm silicate emission feature – the astronomers concluded the dust grains are smaller than observed in any other comet. Hale–Bopp showed the highest ever linear polarization detected for any comet. Such polarization is the result of solar radiation getting scattered by the dust particles in the coma of the comet and depends on the nature of the grains. It further confirms"
  },
  {
    "source": "Comet Hale–Bopp.txt",
    "chunk_id": "Comet Hale–Bopp.txt_8",
    "chunk": "that the dust grains in the coma of comet Hale–Bopp were smaller than inferred in any other comet. One of the most remarkable discoveries was that the comet had a third type of tail. In addition to the well-known gas and dust tails, Hale–Bopp also exhibited a faint sodium tail, only visible with powerful instruments with dedicated filters. Sodium emission had been previously observed in other comets, but had not been shown to come from a tail. Hale–Bopp's sodium tail consisted of neutral atoms (not ions), and extended to some 50 million km (31 million mi) in length. The source of the sodium appeared to be the inner coma, although not necessarily the nucleus. There are several possible mechanisms for generating a source of sodium atoms, including collisions between dust grains surrounding the nucleus, and \"sputtering\" of sodium from dust grains by ultraviolet light. It is not yet established which mechanism is primarily responsible for creating Hale–Bopp's sodium tail, and the narrow and diffuse components of the tail may have different origins. While the comet's dust tail roughly followed the path of the comet's orbit and the gas tail pointed almost directly away from the Sun, the sodium tail appeared"
  },
  {
    "source": "Comet Hale–Bopp.txt",
    "chunk_id": "Comet Hale–Bopp.txt_9",
    "chunk": "to lie between the two. This implies that the sodium atoms are driven away from the comet's head by radiation pressure. The abundance of deuterium in comet Hale–Bopp in the form of heavy water was found to be about twice that of Earth's oceans. If Hale–Bopp's deuterium abundance is typical of all comets, this implies that although cometary impacts are thought to be the source of a significant amount of the water on Earth, they cannot be the only source. Deuterium was also detected in many other hydrogen compounds in the comet. The ratio of deuterium to normal hydrogen was found to vary from compound to compound, which astronomers believe suggests that cometary ices were formed in interstellar clouds, rather than in the solar nebula. Theoretical modelling of ice formation in interstellar clouds suggests that comet Hale–Bopp formed at temperatures of around 25–45 kelvin. Spectroscopic observations of Hale–Bopp revealed the presence of many organic chemicals, several of which had never been detected in comets before. These complex molecules may exist within the cometary nucleus, or might be synthesised by reactions in the comet. Hale–Bopp was the first comet where the noble gas argon was detected. Noble gases are chemically inert"
  },
  {
    "source": "Comet Hale–Bopp.txt",
    "chunk_id": "Comet Hale–Bopp.txt_10",
    "chunk": "and vary from low to high volatility. Since different noble elements have different sublimation temperatures, and don't interact with other elements, they can be used for probing the temperature histories of the cometary ices. Krypton has a sublimation temperature of 16–20 K and was found to be depleted more than 25 times relative to the solar abundance, while argon with its higher sublimation temperature was enriched relative to the solar abundance. Together these observations indicate that the interior of Hale–Bopp has always been colder than 35–40 K, but has at some point been warmer than 20 K. Unless the solar nebula was much colder and richer in argon than generally believed, this suggests that the comet formed beyond Neptune in the Kuiper belt region and then migrated outward to the Oort cloud. Comet Hale–Bopp's activity and outgassing were not spread uniformly over its nucleus, but instead came from several specific jets. Observations of the material streaming away from these jets allowed astronomers to measure the rotation period of the comet, which was found to be about 11 hours 46 minutes. In 1997 a paper was published that hypothesised the existence of a binary nucleus to fully explain the observed pattern"
  },
  {
    "source": "Comet Hale–Bopp.txt",
    "chunk_id": "Comet Hale–Bopp.txt_11",
    "chunk": "of comet Hale–Bopp's dust emission observed in October 1995. The paper was based on theoretical analysis, and did not claim an observational detection of the proposed satellite nucleus, but estimated that it would have a diameter of about 30 km (19 mi), with the main nucleus being about 70 km (43 mi) across, and would orbit in about three days at a distance of about 180 km (110 mi). This analysis was confirmed by observations in 1996 using Wide-Field Planetary Camera 2 of the Hubble Space Telescope which had taken images of the comet that revealed the satellite. Although observations using adaptive optics in late 1997 and early 1998 showed a double peak in the brightness of the nucleus, controversy still exists over whether such observations can only be explained by a binary nucleus. The discovery of the satellite was not confirmed by other observations. Also, while comets have been observed to break up before, no case had been found of a stable binary nucleus until the subsequent discovery of P/2006 VW139. In November 1996, amateur astronomer Chuck Shramek of Houston, Texas took a CCD image of the comet which showed a fuzzy, slightly elongated object nearby. His computer sky-viewing"
  },
  {
    "source": "Comet Hale–Bopp.txt",
    "chunk_id": "Comet Hale–Bopp.txt_12",
    "chunk": "program did not identify the star, so Shramek called the Art Bell radio program Coast to Coast AM to announce that he had discovered a \"Saturn-like object\" following Hale–Bopp. UFO enthusiasts, such as remote viewing proponent and Emory University political science professor Courtney Brown, soon concluded that there was an alien spacecraft following the comet. Several astronomers, including Alan Hale, stated that the object was simply the 8.5-magnitude star SAO141894. They noted that the star did not appear on Shramek's computer program because the user preferences were set incorrectly. Art Bell claimed to have obtained an image of the object from an anonymous astrophysicist who was about to confirm its discovery. However, astronomers Olivier Hainaut and David Tholen of the University of Hawaii stated that the alleged photo was an altered copy of one of their own comet images. Thirty-nine members of the Heaven's Gate cult died in a mass suicide, in March 1997 with the intention of teleporting to a spaceship which they believed was flying behind the comet. Nancy Lieder, who claims to receive messages from aliens through an implant in her brain, stated that Hale–Bopp was a fiction designed to distract the population from the coming arrival"
  },
  {
    "source": "Comet Hale–Bopp.txt",
    "chunk_id": "Comet Hale–Bopp.txt_13",
    "chunk": "of \"Nibiru\" or \"Planet X\", a giant planet whose close passage would disrupt the Earth's rotation, causing global cataclysm. Her original date for the apocalypse was May 2003, which passed without incident, but various conspiracy websites continued to predict the coming of Nibiru, most of whom tied it to the 2012 phenomenon. Lieder and others' claims of the planet Nibiru have been repeatedly debunked by scientists. Its lengthy period of visibility and extensive coverage in the media meant that Hale–Bopp was probably the most-observed comet in history, making a far greater impact on the general public than the return of Halley's Comet in 1986, and certainly seen by a greater number of people than witnessed any of Halley's previous appearances. For instance, 69% of Americans had seen Hale–Bopp by April 9, 1997. Hale–Bopp was a record-breaking comet – the farthest comet from the Sun discovered by amateurs, with the largest well-measured cometary nucleus known after 95P/Chiron, and it was visible to the naked eye for twice as long as the previous record-holder. It was also brighter than magnitude 0 for eight weeks, longer than any other recorded comet. Carolyn Shoemaker and her husband Gene, co-discoverers of comet Shoemaker–Levy 9, were"
  },
  {
    "source": "Comet Hale–Bopp.txt",
    "chunk_id": "Comet Hale–Bopp.txt_14",
    "chunk": "involved in a car crash after photographing the comet. Gene died in the crash and his ashes were sent to the Moon aboard NASA's Lunar Prospector mission along with an image of Hale–Bopp, \"the last comet that the Shoemakers observed together\". Composer Dmitry Kayukin created the music album Comet 97 based on his memories of observing Comet Hale–Bopp."
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_0",
    "chunk": "# Communications satellite A communications satellite is an artificial satellite that relays and amplifies radio telecommunication signals via a transponder; it creates a communication channel between a source transmitter and a receiver at different locations on Earth. Communications satellites are used for television, telephone, radio, internet, and military applications. Many communications satellites are in geostationary orbit 22,236 miles (35,785 km) above the equator, so that the satellite appears stationary at the same point in the sky; therefore the satellite dish antennas of ground stations can be aimed permanently at that spot and do not have to move to track the satellite. Others form satellite constellations in low Earth orbit, where antennas on the ground have to follow the position of the satellites and switch between satellites frequently. The radio waves used for telecommunications links travel by line of sight and so are obstructed by the curve of the Earth. The purpose of communications satellites is to relay the signal around the curve of the Earth allowing communication between widely separated geographical points. Communications satellites use a wide range of radio and microwave frequencies. To avoid signal interference, international organizations have regulations for which frequency ranges or \"bands\" certain organizations are"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_1",
    "chunk": "allowed to use. This allocation of bands minimizes the risk of signal interference. In October 1945, Arthur C. Clarke published an article titled \"Extraterrestrial Relays\" in the British magazine Wireless World. The article described the fundamentals behind the deployment of artificial satellites in geostationary orbits to relay radio signals. Because of this, Arthur C. Clarke is often quoted as being the inventor of the concept of the communications satellite, and the term 'Clarke Belt' is employed as a description of the orbit. The first artificial Earth satellite was Sputnik 1, which was put into orbit by the Soviet Union on 4 October 1957. It was developed by Mikhail Tikhonravov and Sergey Korolev, building on work by Konstantin Tsiolkovsky. Sputnik 1 was equipped with an on-board radio transmitter that worked on two frequencies of 20.005 and 40.002 MHz, or 7 and 15 meters wavelength. The satellite was not placed in orbit to send data from one point on Earth to another, but the radio transmitter was meant to study the properties of radio wave distribution throughout the ionosphere. The launch of Sputnik 1 was a major step in the exploration of space and rocket development, and marks the beginning of the"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_2",
    "chunk": "Space Age. There are two major classes of communications satellites, passive and active. Passive satellites only reflect the signal coming from the source, toward the direction of the receiver. With passive satellites, the reflected signal is not amplified at the satellite, and only a small amount of the transmitted energy actually reaches the receiver. Since the satellite is so far above Earth, the radio signal is attenuated due to free-space path loss, so the signal received on Earth is very weak. Active satellites, on the other hand, amplify the received signal before retransmitting it to the receiver on the ground. Passive satellites were the first communications satellites, but are little used now. Work that was begun in the field of electrical intelligence gathering at the United States Naval Research Laboratory in 1951 led to a project named Communication Moon Relay. Military planners had long shown considerable interest in secure and reliable communications lines as a tactical necessity, and the ultimate goal of this project was the creation of the longest communications circuit in human history, with the Moon, Earth's natural satellite, acting as a passive relay. After achieving the first transoceanic communication between Washington, D.C., and Hawaii on 23 January"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_3",
    "chunk": "1956, this system was publicly inaugurated and put into formal production in January 1960. The first satellite purpose-built to actively relay communications was Project SCORE, led by Advanced Research Projects Agency (ARPA) and launched on 18 December 1958, which used a tape recorder to carry a stored voice message, as well as to receive, store, and retransmit messages. It was used to send a Christmas greeting to the world from U.S. President Dwight D. Eisenhower. The satellite also executed several realtime transmissions before the non-rechargeable batteries failed on 30 December 1958 after eight hours of actual operation. The direct successor to SCORE was another ARPA-led project called Courier. Courier 1B was launched on 4 October 1960 to explore whether it would be possible to establish a global military communications network by using \"delayed repeater\" satellites, which receive and store information until commanded to rebroadcast them. After 17 days, a command system failure ended communications from the satellite. NASA's satellite applications program launched the first artificial satellite used for passive relay communications in Echo 1 on 12 August 1960. Echo 1 was an aluminized balloon satellite acting as a passive reflector of microwave signals. Communication signals were bounced off the satellite"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_4",
    "chunk": "from one point on Earth to another. This experiment sought to establish the feasibility of worldwide broadcasts of telephone, radio, and television signals. Telstar was the first active, direct relay communications commercial satellite and marked the first transatlantic transmission of television signals. Belonging to AT&T as part of a multi-national agreement between AT&T, Bell Telephone Laboratories, NASA, the British General Post Office, and the French National PTT (Post Office) to develop satellite communications, it was launched by NASA from Cape Canaveral on 10 July 1962, in the first privately sponsored space launch. Another passive relay experiment primarily intended for military communications purposes was Project West Ford, which was led by Massachusetts Institute of Technology's Lincoln Laboratory. After an initial failure in 1961, a launch on 9 May 1963 dispersed 350 million copper needle dipoles to create a passive reflecting belt. Even though only about half of the dipoles properly separated from each other, the project was able to successfully experiment and communicate using frequencies in the SHF X band spectrum. An immediate antecedent of the geostationary satellites was the Hughes Aircraft Company's Syncom 2, launched on 26 July 1963. Syncom 2 was the first communications satellite in a geosynchronous orbit."
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_5",
    "chunk": "It revolved around the Earth once per day at constant speed, but because it still had north–south motion, special equipment was needed to track it. Its successor, Syncom 3, launched on 19 July 1964, was the first geostationary communications satellite. Syncom 3 obtained a geosynchronous orbit, without a north–south motion, making it appear from the ground as a stationary object in the sky. A direct extension of the passive experiments of Project West Ford was the Lincoln Experimental Satellite program, also conducted by the Lincoln Laboratory on behalf of the United States Department of Defense. The LES-1 active communications satellite was launched on 11 February 1965 to explore the feasibility of active solid-state X band long-range military communications. A total of nine satellites were launched between 1965 and 1976 as part of this series. In the United States, 1962 saw the creation of the Communications Satellite Corporation (COMSAT) private corporation, which was subject to instruction by the US Government on matters of national policy. Over the next two years, international negotiations led to the Intelsat Agreements, which in turn led to the launch of Intelsat 1, also known as Early Bird, on 6 April 1965, and which was the first"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_6",
    "chunk": "commercial communications satellite to be placed in geosynchronous orbit. Subsequent Intelsat launches in the 1960s provided multi-destination service and video, audio, and data service to ships at sea (Intelsat 2 in 1966–67), and the completion of a fully global network with Intelsat 3 in 1969–70. By the 1980s, with significant expansions in commercial satellite capacity, Intelsat was on its way to become part of the competitive private telecommunications industry, and had started to get competition from the likes of PanAmSat in the United States, which, ironically, was then bought by its archrival in 2005. When Intelsat was launched, the United States was the only launch source outside of the Soviet Union, who did not participate in the Intelsat agreements. The Soviet Union launched its first communications satellite on 23 April 1965 as part of the Molniya program. This program was also unique at the time for its use of what then became known as the Molniya orbit, which describes a highly elliptical orbit, with two high apogees daily over the northern hemisphere. This orbit provides a long dwell time over Russian territory as well as over Canada at higher latitudes than geostationary orbits over the equator. In the 2020s, the"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_7",
    "chunk": "popularity of low Earth orbit satellite internet constellations providing relatively low-cost internet services led to reducing demand for new geostationary orbit communications satellites. Communications satellites usually have one of three primary types of orbit, while other orbital classifications are used to further specify orbital details. MEO and LEO are non-geostationary orbit (NGSO). As satellites in MEO and LEO orbit the Earth faster, they do not remain visible in the sky to a fixed point on Earth continually like a geostationary satellite, but appear to a ground observer to cross the sky and \"set\" when they go behind the Earth beyond the visible horizon. Therefore, to provide continuous communications capability with these lower orbits requires a larger number of satellites, so that one of these satellites will always be visible in the sky for transmission of communication signals. However, due to their closer distance to the Earth, LEO or MEO satellites can communicate to ground with reduced latency and at lower power than would be required from a geosynchronous orbit. A low Earth orbit (LEO) typically is a circular orbit about 160 to 2,000 kilometres (99 to 1,243 mi) above the Earth's surface and, correspondingly, a period (time to revolve around"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_8",
    "chunk": "the Earth) of about 90 minutes. Because of their low altitude, these satellites are only visible from within a radius of roughly 1,000 kilometres (620 mi) from the sub-satellite point. In addition, satellites in low Earth orbit change their position relative to the ground position quickly. So even for local applications, many satellites are needed if the mission requires uninterrupted connectivity. Low-Earth-orbiting satellites are less expensive to launch into orbit than geostationary satellites and, due to proximity to the ground, do not require as high signal strength (signal strength falls off as the square of the distance from the source, so the effect is considerable). Thus there is a trade off between the number of satellites and their cost. In addition, there are important differences in the onboard and ground equipment needed to support the two types of missions. A group of satellites working in concert is known as a satellite constellation. Two such constellations, intended to provide satellite phone and low-speed data services, primarily to remote areas, are the Iridium and Globalstar systems. The Iridium system has 66 satellites, which orbital inclination of 86.4° and inter-satellite links provide service availability over the entire surface of Earth. Starlink is a"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_9",
    "chunk": "satellite internet constellation operated by SpaceX, that aims for global satellite Internet access coverage. It is also possible to offer discontinuous coverage using a low-Earth-orbit satellite capable of storing data received while passing over one part of Earth and transmitting it later while passing over another part. This will be the case with the CASCADE system of Canada's CASSIOPE communications satellite. Another system using this store and forward method is Orbcomm. A medium Earth orbit is a satellite in orbit somewhere between 2,000 and 35,786 kilometres (1,243 and 22,236 mi) above the Earth's surface. MEO satellites are similar to LEO satellites in functionality. MEO satellites are visible for much longer periods of time than LEO satellites, usually between 2 and 8 hours. MEO satellites have a larger coverage area than LEO satellites. A MEO satellite's longer duration of visibility and wider footprint means fewer satellites are needed in a MEO network than a LEO network. One disadvantage is that a MEO satellite's distance gives it a longer time delay and weaker signal than a LEO satellite, although these limitations are not as severe as those of a GEO satellite. Like LEOs, these satellites do not maintain a stationary distance from"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_10",
    "chunk": "the Earth. This is in contrast to the geostationary orbit, where satellites are always 35,786 kilometres (22,236 mi) from Earth. Typically the orbit of a medium Earth orbit satellite is about 16,000 kilometres (10,000 mi) above Earth. In various patterns, these satellites make the trip around Earth in anywhere from 2 to 8 hours. To an observer on Earth, a satellite in a gestationary orbit appears motionless, in a fixed position in the sky. This is because it revolves around the Earth at Earth's own angular velocity (one revolution per sidereal day, in an equatorial orbit). A geostationary orbit is useful for communications because ground antennas can be aimed at the satellite without their having to track the satellite's motion. This is relatively inexpensive. In applications that require many ground antennas, such as DirecTV distribution, the savings in ground equipment can more than outweigh the cost and complexity of placing a satellite into orbit. By 2000, Hughes Space and Communications (now Boeing Satellite Development Center) had built nearly 40 percent of the more than one hundred satellites in service worldwide. Other major satellite manufacturers include Space Systems/Loral, Orbital Sciences Corporation with the Star Bus series, Indian Space Research Organisation, Lockheed"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_11",
    "chunk": "Martin (owns the former RCA Astro Electronics/GE Astro Space business), Northrop Grumman, Alcatel Space, now Thales Alenia Space, with the Spacebus series, and Astrium. Geostationary satellites must operate above the equator and therefore appear lower on the horizon as the receiver gets farther from the equator. This will cause problems for extreme northerly latitudes, affecting connectivity and causing multipath interference (caused by signals reflecting off the ground and into the ground antenna). Thus, for areas close to the North (and South) Pole, a geostationary satellite may appear below the horizon. Therefore, Molniya orbit satellites have been launched, mainly in Russia, to alleviate this problem. Molniya orbits can be an appealing alternative in such cases. The Molniya orbit is highly inclined, guaranteeing good elevation over selected positions during the northern portion of the orbit. (Elevation is the extent of the satellite's position above the horizon. Thus, a satellite at the horizon has zero elevation and a satellite directly overhead has elevation of 90 degrees.) The Molniya orbit is designed so that the satellite spends the great majority of its time over the far northern latitudes, during which its ground footprint moves only slightly. Its period is one half day, so that"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_12",
    "chunk": "the satellite is available for operation over the targeted region for six to nine hours every second revolution. In this way a constellation of three Molniya satellites (plus in-orbit spares) can provide uninterrupted coverage. The first satellite of the Molniya series was launched on 23 April 1965 and was used for experimental transmission of TV signals from a Moscow uplink station to downlink stations located in Siberia and the Russian Far East, in Norilsk, Khabarovsk, Magadan and Vladivostok. In November 1967 Soviet engineers created a unique system of national TV network of satellite television, called Orbita, that was based on Molniya satellites. In the United States, the National Polar-orbiting Operational Environmental Satellite System (NPOESS) was established in 1994 to consolidate the polar satellite operations of NASA (National Aeronautics and Space Administration) NOAA (National Oceanic and Atmospheric Administration). NPOESS manages a number of satellites for various purposes; for example, METSAT for meteorological satellite, EUMETSAT for the European branch of the program, and METOP for meteorological operations. These orbits are Sun synchronous, meaning that they cross the equator at the same local time each day. For example, the satellites in the NPOESS (civilian) orbit will cross the equator, going from south to"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_13",
    "chunk": "north, at times 1:30 P.M., 5:30 P.M., and 9:30 P.M. There are plans and initiatives to bring dedicated communications satellite beyond geostationary orbits. NASA proposed LunaNet as a data network aiming to provide a \"Lunar Internet\" for cis-lunar spacecraft and Installations. The Moonlight Initiative is an equivalent ESA project that is stated to be compatible and providing navigational services for the lunar surface. Both programmes are satellite constellations of several satellites in various orbits around the Moon. Other orbits are also planned to be used. Positions in the Earth-Moon-Libration points are also proposed for communication satellites covering the Moon alike communication satellites in geosynchronous orbit cover the Earth. Also, dedicated communication satellites in orbits around Mars supporting different missions on surface and other orbits are considered, such as the Mars Telecommunications Orbiter. The bandwidth available from a satellite depends upon the number of transponders provided by the satellite. Each service (TV, Voice, Internet, radio) requires a different amount of bandwidth for transmission. This is typically known as link budgeting and a network simulator can be used to arrive at the exact value. Allocating frequencies to satellite services is a complicated process which requires international coordination and planning. This is carried"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_14",
    "chunk": "out under the auspices of the International Telecommunication Union (ITU). To facilitate frequency planning, the world is divided into three regions: Within these regions, frequency bands are allocated to various satellite services, although a given service may be allocated different frequency bands in different regions. Some of the services provided by satellites are: The first and historically most important application for communication satellites was in intercontinental long distance telephony. The fixed Public Switched Telephone Network relays telephone calls from land line telephones to an Earth station, where they are then transmitted to a geostationary satellite. The downlink follows an analogous path. Improvements in submarine communications cables through the use of fiber-optics caused some decline in the use of satellites for fixed telephony in the late 20th century. Satellite communications are still used in many applications today. Remote islands such as Ascension Island, Saint Helena, Diego Garcia, and Easter Island, where no submarine cables are in service, need satellite telephones. There are also regions of some continents and countries where landline telecommunications are rare to non existent, for example large regions of South America, Africa, Canada, China, Russia, and Australia. Satellite communications also provide connection to the edges of Antarctica and"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_15",
    "chunk": "Greenland. Other land use for satellite phones are rigs at sea, a backup for hospitals, military, and recreation. Ships at sea, as well as planes, often use satellite phones. Satellite phone systems can be accomplished by a number of means. On a large scale, often there will be a local telephone system in an isolated area with a link to the telephone system in a main land area. There are also services that will patch a radio signal to a telephone system. In this example, almost any type of satellite can be used. Satellite phones connect directly to a constellation of either geostationary or low-Earth-orbit satellites. Calls are then forwarded to a satellite teleport connected to the Public Switched Telephone Network . As television became the main market, its demand for simultaneous delivery of relatively few signals of large bandwidth to many receivers being a more precise match for the capabilities of geosynchronous comsats. Two satellite types are used for North American television and radio: Direct broadcast satellite (DBS), and Fixed Service Satellite (FSS). The definitions of FSS and DBS satellites outside of North America, especially in Europe, are a bit more ambiguous. Most satellites used for direct-to-home television in"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_16",
    "chunk": "Europe have the same high power output as DBS-class satellites in North America, but use the same linear polarization as FSS-class satellites. Examples of these are the Astra, Eutelsat, and Hotbird spacecraft in orbit over the European continent. Because of this, the terms FSS and DBS are more so used throughout the North American continent, and are uncommon in Europe. Fixed Service Satellites use the C band, and the lower portions of the Ku band. They are normally used for broadcast feeds to and from television networks and local affiliate stations (such as program feeds for network and syndicated programming, live shots, and backhauls), as well as being used for distance learning by schools and universities, business television (BTV), Videoconferencing, and general commercial telecommunications. FSS satellites are also used to distribute national cable channels to cable television headends. Free-to-air satellite TV channels are also usually distributed on FSS satellites in the Ku band. The Intelsat Americas 5, Galaxy 10R and AMC 3 satellites over North America provide a quite large amount of FTA channels on their Ku band transponders. The American Dish Network DBS service has also recently used FSS technology as well for their programming packages requiring their SuperDish"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_17",
    "chunk": "antenna, due to Dish Network needing more capacity to carry local television stations per the FCC's \"must-carry\" regulations, and for more bandwidth to carry HDTV channels. A direct broadcast satellite is a communications satellite that transmits to small DBS satellite dishes (usually 18 to 24 inches or 45 to 60 cm in diameter). Direct broadcast satellites generally operate in the upper portion of the microwave Ku band. DBS technology is used for DTH-oriented (Direct-To-Home) satellite TV services, such as DirecTV, DISH Network and Orby TV in the United States, Bell Satellite TV and Shaw Direct in Canada, Freesat and Sky in the UK, Ireland, and New Zealand and DSTV in South Africa. Operating at lower frequency and lower power than DBS, FSS satellites require a much larger dish for reception (3 to 8 feet (1 to 2.5 m) in diameter for Ku band, and 12 feet (3.6 m) or larger for C band). They use linear polarization for each of the transponders' RF input and output (as opposed to circular polarization used by DBS satellites), but this is a minor technical difference that users do not notice. FSS satellite technology was also originally used for DTH satellite TV from the"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_18",
    "chunk": "late 1970s to the early 1990s in the United States in the form of TVRO (Television Receive Only) receivers and dishes. It was also used in its Ku band form for the now-defunct Primestar satellite TV service. Some satellites have been launched that have transponders in the Ka band, such as DirecTV's SPACEWAY-1 satellite, and Anik F2. NASA and ISRO have also launched experimental satellites carrying Ka band beacons recently. Some manufacturers have also introduced special antennas for mobile reception of DBS television. Using Global Positioning System (GPS) technology as a reference, these antennas automatically re-aim to the satellite no matter where or how the vehicle (on which the antenna is mounted) is situated. These mobile satellite antennas are popular with some recreational vehicle owners. Such mobile DBS antennas are also used by JetBlue Airways for DirecTV (supplied by LiveTV, a subsidiary of JetBlue), which passengers can view on-board on LCD screens mounted in the seats. Satellite radio offers audio broadcast services in some countries, notably the United States. Mobile services allow listeners to roam a continent, listening to the same audio programming anywhere. A satellite radio or subscription radio (SR) is a digital radio signal that is broadcast by"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_19",
    "chunk": "a communications satellite, which covers a much wider geographical range than terrestrial radio signals. Amateur radio operators have access to amateur satellites, which have been designed specifically to carry amateur radio traffic. Most such satellites operate as spaceborne repeaters, and are generally accessed by amateurs equipped with UHF or VHF radio equipment and highly directional antennas such as Yagis or dish antennas. Due to launch costs, most current amateur satellites are launched into fairly low Earth orbits, and are designed to deal with only a limited number of brief contacts at any given time. Some satellites also provide data-forwarding services using the X.25 or similar protocols. After the 1990s, satellite communication technology has been used as a means to connect to the Internet via broadband data connections. This can be very useful for users who are located in remote areas, and cannot access a broadband connection, or require high availability of services. Communications satellites are used for military communications applications, such as Global Command and Control Systems. Examples of military systems that use communication satellites are the MILSTAR, the DSCS, and the FLTSATCOM of the United States, NATO satellites, United Kingdom satellites (for instance Skynet), and satellites of the former"
  },
  {
    "source": "Communications satellite.txt",
    "chunk_id": "Communications satellite.txt_20",
    "chunk": "Soviet Union. India has launched its first Military Communication satellite GSAT-7, its transponders operate in UHF, F, C and Ku band bands. Typically military satellites operate in the UHF, SHF (also known as X-band) or EHF (also known as Ka band) frequency bands. Near-ground in situ environmental monitoring equipment (such as tide gauges, weather stations, weather buoys, and radiosondes), may use satellites for one-way data transmission or two-way telemetry and telecontrol. It may be based on a secondary payload of a weather satellite (as in the case of GOES and METEOSAT and others in the Argos system) or in dedicated satellites (such as SCD). The data rate is typically much lower than in satellite Internet access."
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_0",
    "chunk": "# Cosmic microwave background The cosmic microwave background (CMB, CMBR), or relic radiation, is microwave radiation that fills all space in the observable universe. With a standard optical telescope, the background space between stars and galaxies is almost completely dark. However, a sufficiently sensitive radio telescope detects a faint background glow that is almost uniform and is not associated with any star, galaxy, or other object. This glow is strongest in the microwave region of the electromagnetic spectrum. The accidental discovery of the CMB in 1965 by American radio astronomers Arno Penzias and Robert Wilson was the culmination of work initiated in the 1940s. The CMB is landmark evidence of the Big Bang theory for the origin of the universe. In the Big Bang cosmological models, during the earliest periods, the universe was filled with an opaque fog of dense, hot plasma of sub-atomic particles. As the universe expanded, this plasma cooled to the point where protons and electrons combined to form neutral atoms of mostly hydrogen. Unlike the plasma, these atoms could not scatter thermal radiation by Thomson scattering, and so the universe became transparent. Known as the recombination epoch, this decoupling event released photons to travel freely through"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_1",
    "chunk": "space. However, the photons have grown less energetic due to the cosmological redshift associated with the expansion of the universe. The surface of last scattering refers to a shell at the right distance in space so photons are now received that were originally emitted at the time of decoupling. The CMB is not completely smooth and uniform, showing a faint anisotropy that can be mapped by sensitive detectors. Ground and space-based experiments such as COBE, WMAP and Planck have been used to measure these temperature inhomogeneities. The anisotropy structure is determined by various interactions of matter and photons up to the point of decoupling, which results in a characteristic lumpy pattern that varies with angular scale. The distribution of the anisotropy across the sky has frequency components that can be represented by a power spectrum displaying a sequence of peaks and valleys. The peak values of this spectrum hold important information about the physical properties of the early universe: the first peak determines the overall curvature of the universe, while the second and third peak detail the density of normal matter and so-called dark matter, respectively. Extracting fine details from the CMB data can be challenging, since the emission has"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_2",
    "chunk": "undergone modification by foreground features such as galaxy clusters. The cosmic microwave background radiation is an emission of uniform black body thermal energy coming from all directions. Intensity of the CMB is expressed in kelvin (K), the SI unit of temperature. The CMB has a thermal black body spectrum at a temperature of 2.72548±0.00057 K. Variations in intensity are expressed as variations in temperature. The blackbody temperature uniquely characterizes the intensity of the radiation at all wavelengths; a measured brightness temperature at any wavelength can be converted to a blackbody temperature. The radiation is remarkably uniform across the sky, very unlike the almost point-like structure of stars or clumps of stars in galaxies. The radiation is isotropic to roughly one part in 25,000: the root mean square variations are just over 100 μK, after subtracting a dipole anisotropy from the Doppler shift of the background radiation. The latter is caused by the peculiar velocity of the Sun relative to the comoving cosmic rest frame as it moves at 369.82 ± 0.11 km/s towards the constellation Crater near its boundary with the constellation Leo The CMB dipole and aberration at higher multipoles have been measured, consistent with galactic motion. Despite the"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_3",
    "chunk": "very small degree of anisotropy in the CMB, many aspects can be measured with high precision and such measurements are critical for cosmological theories. In addition to temperature anisotropy, the CMB should have an angular variation in polarization. The polarisation at each direction in the sky has an orientation described in terms of E-mode and B-mode polarization. The E-mode signal is a factor of 10 less strong than the temperature anisotropy; it supplements the temperature data as they are correlated. The B-mode signal is even weaker but may contain additional cosmological data. The anisotropy is related to physical origin of the polarisation. Excitation of an electron by linear polarised light generates polarized light at 90 degrees to the incident direction. If the incoming radiation is isotropic, different incoming directions create polarizations that cancel out. If the incoming radiation has quadrupole anisotropy, residual polarization will be seen. Other than the temperature and polarization anisotropy, the CMB frequency spectrum is expected to feature tiny departures from the black-body law known as spectral distortions. These are also at the focus of an active research effort with the hope of a first measurement within the forthcoming decades, as they contain a wealth of information"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_4",
    "chunk": "about the primordial universe and the formation of structures at late time. The CMB contains the vast majority of photons in the universe by a factor of 400 to 1; the number density of photons in the CMB is one billion times (10) the number density of matter in the universe. Without the expansion of the universe to cause the cooling of the CMB, the night sky would shine as brightly as the Sun. The energy density of the CMB is 0.260 eV/cm (4.17×10 J/m), about 411 photons/cm. In 1931, Georges Lemaître speculated that remnants of the early universe may be observable as radiation, but his candidate was cosmic rays. Richard C. Tolman showed in 1934 that expansion of the universe would cool blackbody radiation while maintaining a thermal spectrum. The cosmic microwave background was first predicted in 1948 by Ralph Alpher and Robert Herman, in a correction they prepared for a paper by Alpher's PhD advisor George Gamow. Alpher and Herman were able to estimate the temperature of the cosmic microwave background to be 5 K. The first published recognition of the CMB radiation as a detectable phenomenon appeared in a brief paper by Soviet astrophysicists A. G. Doroshkevich"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_5",
    "chunk": "and Igor Novikov, in the spring of 1964. In 1964, David Todd Wilkinson and Peter Roll, Robert H. Dicke's colleagues at Princeton University, began constructing a Dicke radiometer to measure the cosmic microwave background. In 1964, Arno Penzias and Robert Woodrow Wilson at the Crawford Hill location of Bell Telephone Laboratories in nearby Holmdel Township, New Jersey had built a Dicke radiometer that they intended to use for radio astronomy and satellite communication experiments. The antenna was constructed in 1959 to support Project Echo—the National Aeronautics and Space Administration's passive communications satellites, which used large Earth orbiting aluminized plastic balloons as reflectors to bounce radio signals from one point on the Earth to another. On 20 May 1964 they made their first measurement clearly showing the presence of the microwave background, with their instrument having an excess 4.2K antenna temperature which they could not account for. After receiving a telephone call from Crawford Hill, Dicke said \"Boys, we've been scooped.\" A meeting between the Princeton and Crawford Hill groups determined that the antenna temperature was indeed due to the microwave background. Penzias and Wilson received the 1978 Nobel Prize in Physics for their discovery. The interpretation of the cosmic microwave"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_6",
    "chunk": "background was a controversial issue in the late 1960s. Alternative explanations included energy from within the Solar System, from galaxies, from intergalactic plasma and from multiple extragalactic radio sources. Two requirements would show that the microwave radiation was truly \"cosmic\". First, the intensity vs frequency or spectrum needed to be shown to match a thermal or blackbody source. This was accomplished by 1968 in a series of measurements of the radiation temperature at higher and lower wavelengths. Second, the radiation needed be shown to be isotropic, the same from all directions. This was also accomplished by 1970, demonstrating that this radiation was truly cosmic in origin. In the 1970s numerous studies showed that tiny deviations from isotropy in the CMB could result from events in the early universe. Harrison, Peebles and Yu, and Zel'dovich realized that the early universe would require quantum inhomogeneities that would result in temperature anisotropy at the level of 10 or 10. Rashid Sunyaev, using the alternative name relic radiation, calculated the observable imprint that these inhomogeneities would have on the cosmic microwave background. After a lull in the 1970s caused in part by the many experimental difficulties in measuring CMB at high precision, increasingly stringent"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_7",
    "chunk": "limits on the anisotropy of the cosmic microwave background were set by ground-based experiments during the 1980s. RELIKT-1, a Soviet cosmic microwave background anisotropy experiment on board the Prognoz 9 satellite (launched 1 July 1983), gave the first upper limits on the large-scale anisotropy. The other key event in the 1980s was the proposal by Alan Guth for cosmic inflation. This theory of rapid spatial expansion gave an explanation for large-scale isotropy by allowing causal connection just before the epoch of last scattering. With this and similar theories, detailed prediction encouraged larger and more ambitious experiments. The NASA Cosmic Background Explorer (COBE) satellite orbited Earth in 1989–1996 detected and quantified the large-scale anisotropies at the limit of its detection capabilities. The NASA COBE mission clearly confirmed the primary anisotropy with the Differential Microwave Radiometer instrument, publishing their findings in 1992. The team received the Nobel Prize in physics for 2006 for this discovery. Inspired by the COBE results, a series of ground and balloon-based experiments measured cosmic microwave background anisotropies on smaller angular scales over the two decades. The sensitivity of the new experiments improved dramatically, with a reduction in internal noise by three orders of magnitude. The primary goal"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_8",
    "chunk": "of these experiments was to measure the scale of the first acoustic peak, which COBE did not have sufficient resolution to resolve. This peak corresponds to large scale density variations in the early universe that are created by gravitational instabilities, resulting in acoustical oscillations in the plasma. The first peak in the anisotropy was tentatively detected by the MAT/TOCO experiment and the result was confirmed by the BOOMERanG and MAXIMA experiments. These measurements demonstrated that the geometry of the universe is approximately flat, rather than curved. They ruled out cosmic strings as a major component of cosmic structure formation and suggested cosmic inflation was the right theory of structure formation. Inspired by the initial COBE results of an extremely isotropic and homogeneous background, a series of ground- and balloon-based experiments quantified CMB anisotropies on smaller angular scales over the next decade. The primary goal of these experiments was to measure the angular scale of the first acoustic peak, for which COBE did not have sufficient resolution. These measurements were able to rule out cosmic strings as the leading theory of cosmic structure formation, and suggested cosmic inflation was the right theory. During the 1990s, the first peak was measured with"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_9",
    "chunk": "increasing sensitivity and by 2000 the BOOMERanG experiment reported that the highest power fluctuations occur at scales of approximately one degree. Together with other cosmological data, these results implied that the geometry of the universe is flat. A number of ground-based interferometers provided measurements of the fluctuations with higher accuracy over the next three years, including the Very Small Array, Degree Angular Scale Interferometer (DASI), and the Cosmic Background Imager (CBI). DASI made the first detection of the polarization of the CMB and the CBI provided the first E-mode polarization spectrum with compelling evidence that it is out of phase with the T-mode spectrum. In June 2001, NASA launched a second CMB space mission, WMAP, to make much more precise measurements of the large-scale anisotropies over the full sky. WMAP used symmetric, rapid-multi-modulated scanning, rapid switching radiometers at five frequencies to minimize non-sky signal noise. The data from the mission was released in five installments, the last being the nine-year summary. The results are broadly consistent Lambda CDM models based on 6 free parameters and fitting in to Big Bang cosmology with cosmic inflation. The Degree Angular Scale Interferometer (DASI) was a telescope installed at the U.S. National Science Foundation's"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_10",
    "chunk": "Amundsen–Scott South Pole Station in Antarctica. It was a 13-element interferometer operating between 26 and 36 GHz (Ka band) in ten bands. The instrument is similar in design to the Cosmic Background Imager (CBI) and the Very Small Array (VSA). A third space mission, the ESA (European Space Agency) Planck Surveyor, was launched in May 2009 and performed an even more detailed investigation until it was shut down in October 2013. Planck employed both HEMT radiometers and bolometer technology and measured the CMB at a smaller scale than WMAP. Its detectors were trialled in the Antarctic Viper telescope as ACBAR (Arcminute Cosmology Bolometer Array Receiver) experiment—which has produced the most precise measurements at small angular scales to date—and in the Archeops balloon telescope. On 21 March 2013, the European-led research team behind the Planck cosmology probe released the mission's all-sky map (565x318 jpeg, 3600x1800 jpeg) of the cosmic microwave background. The map suggests the universe is slightly older than researchers expected. According to the map, subtle fluctuations in temperature were imprinted on the deep sky when the cosmos was about 370000 years old. The imprint reflects ripples that arose as early, in the existence of the universe, as the first"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_11",
    "chunk": "nonillionth (10) of a second. Apparently, these ripples gave rise to the present vast cosmic web of galaxy clusters and dark matter. Based on the 2013 data, the universe contains 4.9% ordinary matter, 26.8% dark matter and 68.3% dark energy. On 5 February 2015, new data was released by the Planck mission, according to which the age of the universe is 13.799±0.021 billion years old and the Hubble constant was measured to be 67.74±0.46 (km/s)/Mpc. The cosmic microwave background radiation and the cosmological redshift-distance relation are together regarded as the best available evidence for the Big Bang event. Measurements of the CMB have made the inflationary Big Bang model the Standard Cosmological Model. The discovery of the CMB in the mid-1960s curtailed interest in alternatives such as the steady state theory. In the Big Bang model for the formation of the universe, inflationary cosmology predicts that after about 10 seconds the nascent universe underwent exponential growth that smoothed out nearly all irregularities. The remaining irregularities were caused by quantum fluctuations in the inflaton field that caused the inflation event. Long before the formation of stars and planets, the early universe was more compact, much hotter and, starting 10 seconds after"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_12",
    "chunk": "the Big Bang, filled with a uniform glow from its white-hot fog of interacting plasma of photons, electrons, and baryons. As the universe expanded, adiabatic cooling caused the energy density of the plasma to decrease until it became favorable for electrons to combine with protons, forming hydrogen atoms. This recombination event happened when the temperature was around 3000 K or when the universe was approximately 379,000 years old. As photons did not interact with these electrically neutral atoms, the former began to travel freely through space, resulting in the decoupling of matter and radiation. The color temperature of the ensemble of decoupled photons has continued to diminish ever since; now down to 2.7260±0.0013 K, it will continue to drop as the universe expands. The intensity of the radiation corresponds to black-body radiation at 2.726 K because red-shifted black-body radiation is just like black-body radiation at a lower temperature. According to the Big Bang model, the radiation from the sky we measure today comes from a spherical surface called the surface of last scattering. This represents the set of locations in space at which the decoupling event is estimated to have occurred and at a point in time such that the"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_13",
    "chunk": "photons from that distance have just reached observers. Most of the radiation energy in the universe is in the cosmic microwave background, making up a fraction of roughly 6×10 of the total density of the universe. Two of the greatest successes of the Big Bang theory are its prediction of the almost perfect black body spectrum and its detailed prediction of the anisotropies in the cosmic microwave background. The CMB spectrum has become the most precisely measured black body spectrum in nature. In the late 1940s Alpher and Herman reasoned that if there was a Big Bang, the expansion of the universe would have stretched the high-energy radiation of the very early universe into the microwave region of the electromagnetic spectrum, and down to a temperature of about 5 K. They were slightly off with their estimate, but they had the right idea. They predicted the CMB. It took another 15 years for Penzias and Wilson to discover that the microwave background was actually there. According to standard cosmology, the CMB gives a snapshot of the hot early universe at the point in time when the temperature dropped enough to allow electrons and protons to form hydrogen atoms. This event"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_14",
    "chunk": "made the universe nearly transparent to radiation because light was no longer being scattered off free electrons. When this occurred some 380,000 years after the Big Bang, the temperature of the universe was about 3,000 K. This corresponds to an ambient energy of about 0.26 eV, which is much less than the 13.6 eV ionization energy of hydrogen. This epoch is generally known as the \"time of last scattering\" or the period of recombination or decoupling. Since decoupling, the color temperature of the background radiation has dropped by an average factor of 1,089 due to the expansion of the universe. As the universe expands, the CMB photons are redshifted, causing them to decrease in energy. The color temperature of this radiation stays inversely proportional to a parameter that describes the relative expansion of the universe over time, known as the scale length. The color temperature Tr of the CMB as a function of redshift, z, can be shown to be proportional to the color temperature of the CMB as observed in the present day (2.725 K or 0.2348 meV): The high degree of uniformity throughout the observable universe and its faint but measured anisotropy lend strong support for the Big"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_15",
    "chunk": "Bang model in general and the ΛCDM (\"Lambda Cold Dark Matter\") model in particular. Moreover, the fluctuations are coherent on angular scales that are larger than the apparent cosmological horizon at recombination. Either such coherence is acausally fine-tuned, or cosmic inflation occurred. The anisotropy, or directional dependency, of the cosmic microwave background is divided into two types: primary anisotropy, due to effects that occur at the surface of last scattering and before; and secondary anisotropy, due to effects such as interactions of the background radiation with intervening hot gas or gravitational potentials, which occur between the last scattering surface and the observer. The structure of the cosmic microwave background anisotropies is principally determined by two effects: acoustic oscillations and diffusion damping (also called collisionless damping or Silk damping). The acoustic oscillations arise because of a conflict in the photon–baryon plasma in the early universe. The pressure of the photons tends to erase anisotropies, whereas the gravitational attraction of the baryons, moving at speeds much slower than light, makes them tend to collapse to form overdensities. These two effects compete to create acoustic oscillations, which give the microwave background its characteristic peak structure. The peaks correspond, roughly, to resonances in which"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_16",
    "chunk": "the photons decouple when a particular mode is at its peak amplitude. The peaks contain interesting physical signatures. The angular scale of the first peak determines the curvature of the universe (but not the topology of the universe). The next peak—ratio of the odd peaks to the even peaks—determines the reduced baryon density. The third peak can be used to get information about the dark-matter density. The locations of the peaks give important information about the nature of the primordial density perturbations. There are two fundamental types of density perturbations called adiabatic and isocurvature. A general density perturbation is a mixture of both, and different theories that purport to explain the primordial density perturbation spectrum predict different mixtures. The CMB spectrum can distinguish between these two because these two types of perturbations produce different peak locations. Isocurvature density perturbations produce a series of peaks whose angular scales (ℓ values of the peaks) are roughly in the ratio 1 : 3 : 5 : ..., while adiabatic density perturbations produce peaks whose locations are in the ratio 1 : 2 : 3 : ... Observations are consistent with the primordial density perturbations being entirely adiabatic, providing key support for inflation, and"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_17",
    "chunk": "ruling out many models of structure formation involving, for example, cosmic strings. Collisionless damping is caused by two effects, when the treatment of the primordial plasma as fluid begins to break down: These effects contribute about equally to the suppression of anisotropies at small scales and give rise to the characteristic exponential damping tail seen in the very small angular scale anisotropies. The depth of the LSS refers to the fact that the decoupling of the photons and baryons does not happen instantaneously, but instead requires an appreciable fraction of the age of the universe up to that era. One method of quantifying how long this process took uses the photon visibility function (PVF). This function is defined so that, denoting the PVF by P(t), the probability that a CMB photon last scattered between time t and t + dt is given by P(t) dt. The maximum of the PVF (the time when it is most likely that a given CMB photon last scattered) is known quite precisely. The first-year WMAP results put the time at which P(t) has a maximum as 372,000 years. This is often taken as the \"time\" at which the CMB formed. However, to figure out"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_18",
    "chunk": "how long it took the photons and baryons to decouple, we need a measure of the width of the PVF. The WMAP team finds that the PVF is greater than half of its maximal value (the \"full width at half maximum\", or FWHM) over an interval of 115,000 years. By this measure, decoupling took place over roughly 115,000 years, and thus when it was complete, the universe was roughly 487,000 years old. Since the CMB came into existence, it has apparently been modified by several subsequent physical processes, which are collectively referred to as late-time anisotropy, or secondary anisotropy. When the CMB photons became free to travel unimpeded, ordinary matter in the universe was mostly in the form of neutral hydrogen and helium atoms. However, observations of galaxies today seem to indicate that most of the volume of the intergalactic medium (IGM) consists of ionized material (since there are few absorption lines due to hydrogen atoms). This implies a period of reionization during which some of the material of the universe was broken into hydrogen ions. The CMB photons are scattered by free charges such as electrons that are not bound in atoms. In an ionized universe, such charged particles"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_19",
    "chunk": "have been liberated from neutral atoms by ionizing (ultraviolet) radiation. Today these free charges are at sufficiently low density in most of the volume of the universe that they do not measurably affect the CMB. However, if the IGM was ionized at very early times when the universe was still denser, then there are two main effects on the CMB: Both of these effects have been observed by the WMAP spacecraft, providing evidence that the universe was ionized at very early times, at a redshift around 10. The detailed provenance of this early ionizing radiation is still a matter of scientific debate. It may have included starlight from the very first population of stars (population III stars), supernovae when these first stars reached the end of their lives, or the ionizing radiation produced by the accretion disks of massive black holes. The time following the emission of the cosmic microwave background—and before the observation of the first stars—is semi-humorously referred to by cosmologists as the Dark Age, and is a period which is under intense study by astronomers (see 21 centimeter radiation). Two other effects which occurred between reionization and our observations of the cosmic microwave background, and which appear"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_20",
    "chunk": "to cause anisotropies, are the Sunyaev–Zeldovich effect, where a cloud of high-energy electrons scatters the radiation, transferring some of its energy to the CMB photons, and the Sachs–Wolfe effect, which causes photons from the Cosmic Microwave Background to be gravitationally redshifted or blueshifted due to changing gravitational fields. The standard cosmology that includes the Big Bang \"enjoys considerable popularity among the practicing cosmologists\" However, there are challenges to the standard big bang framework for explaining CMB data. In particular standard cosmology requires fine-tuning of some free parameters, with different values supported by different experimental data. As an example of the fine-tuning issue, standard cosmology cannot predict the present temperature of the relic radiation, T 0 {\\displaystyle T_{0}} . This value of T 0 {\\displaystyle T_{0}} is one of the best results of experimental cosmology and the steady state model can predict it. However, alternative models have their own set of problems and they have only made post-facto explanations of existing observations. Nevertheless, these alternatives have played an important historic role in providing ideas for and challenges to the standard explanation. The cosmic microwave background is polarized at the level of a few microkelvin. There are two types of polarization, called"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_21",
    "chunk": "E-mode (or gradient-mode) and B-mode (or curl mode). This is in analogy to electrostatics, in which the electric field (E-field) has a vanishing curl and the magnetic field (B-field) has a vanishing divergence. The E-modes arise from Thomson scattering in a heterogeneous plasma. E-modes were first seen in 2002 by the Degree Angular Scale Interferometer (DASI). B-modes are expected to be an order of magnitude weaker than the E-modes. The former are not produced by standard scalar type perturbations, but are generated by gravitational waves during cosmic inflation shortly after the big bang. However, gravitational lensing of the stronger E-modes can also produce B-mode polarization. Detecting the original B-modes signal requires analysis of the contamination caused by lensing of the relatively strong E-mode signal. Models of \"slow-roll\" cosmic inflation in the early universe predicts primordial gravitational waves that would impact the polarisation of the cosmic microwave background, creating a specific pattern of B-mode polarization. Detection of this pattern would support the theory of inflation and their strength can confirm and exclude different models of inflation. Claims that this characteristic pattern of B-mode polarization had been measured by BICEP2 instrument were later attributed to cosmic dust due to new results of"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_22",
    "chunk": "the Planck experiment. The second type of B-modes was discovered in 2013 using the South Pole Telescope with help from the Herschel Space Observatory. In October 2014, a measurement of the B-mode polarization at 150 GHz was published by the POLARBEAR experiment. Compared to BICEP2, POLARBEAR focuses on a smaller patch of the sky and is less susceptible to dust effects. The team reported that POLARBEAR's measured B-mode polarization was of cosmological origin (and not just due to dust) at a 97.2% confidence level. The CMB angular anisotropies are usually presented in terms of power per multipole. The map of temperature across the sky, T ( θ , φ ) , {\\displaystyle T(\\theta ,\\varphi ),} is written as coefficients of spherical harmonics, T ( θ , φ ) = ∑ ℓ m a ℓ m Y ℓ m ( θ , φ ) {\\displaystyle T(\\theta ,\\varphi )=\\sum _{\\ell m}a_{\\ell m}Y_{\\ell m}(\\theta ,\\varphi )} where the a ℓ m {\\displaystyle a_{\\ell m}} term measures the strength of the angular oscillation in Y ℓ m ( θ , φ ) {\\displaystyle Y_{\\ell m}(\\theta ,\\varphi )} , and ℓ is the multipole number while m is the azimuthal number. The azimuthal variation is"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_23",
    "chunk": "not significant and is removed by applying the angular correlation function, giving power spectrum term C ℓ ≡ ⟨ | a ℓ m | 2 ⟩ . {\\displaystyle C_{\\ell }\\equiv \\langle |a_{\\ell m}|^{2}\\rangle .} Increasing values of ℓ correspond to higher multipole moments of CMB, meaning more rapid variation with angle. The monopole term, ℓ = 0, is the constant isotropic mean temperature of the CMB, Tγ = 2.7255±0.0006 K with one standard deviation confidence. This term must be measured with absolute temperature devices, such as the FIRAS instrument on the COBE satellite. CMB dipole represents the largest anisotropy, which is in the first spherical harmonic (ℓ = 1), a cosine function. The amplitude of CMB dipole is around 3.3621±0.0010 mK. The CMB dipole moment is interpreted as the peculiar motion of the Earth relative to the CMB. Its amplitude depends on the time due to the Earth's orbit about the barycenter of the solar system. This enables us to add a time-dependent term to the dipole expression. The modulation of this term is 1 year, which fits the observation done by COBE FIRAS. The dipole moment does not encode any primordial information. From the CMB data, it is seen"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_24",
    "chunk": "that the Sun appears to be moving at 369.82±0.11 km/s relative to the reference frame of the CMB (also called the CMB rest frame, or the frame of reference in which there is no motion through the CMB). The Local Group — the galaxy group that includes our own Milky Way galaxy — appears to be moving at 620±15 km/s in the direction of galactic longitude ℓ = 271.9°±2°, b = 30°±3°. The dipole is now used to calibrate mapping studies. The temperature variation in the CMB temperature maps at higher multipoles, or ℓ ≥ 2, is considered to be the result of perturbations of the density in the early Universe, before the recombination epoch at a redshift of around z ⋍ 1100. Before recombination, the Universe consisted of a hot, dense plasma of electrons and baryons. In such a hot dense environment, electrons and protons could not form any neutral atoms. The baryons in such early Universe remained highly ionized and so were tightly coupled with photons through the effect of Thompson scattering. These phenomena caused the pressure and gravitational effects to act against each other, and triggered fluctuations in the photon-baryon plasma. Quickly after the recombination epoch, the"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_25",
    "chunk": "rapid expansion of the universe caused the plasma to cool down and these fluctuations are \"frozen into\" the CMB maps we observe today. Raw CMBR data, even from space vehicles such as WMAP or Planck, contain foreground effects that completely obscure the fine-scale structure of the cosmic microwave background. The fine-scale structure is superimposed on the raw CMBR data but is too small to be seen at the scale of the raw data. The most prominent of the foreground effects is the dipole anisotropy caused by the Sun's motion relative to the CMBR background. The dipole anisotropy and others due to Earth's annual motion relative to the Sun and numerous microwave sources in the galactic plane and elsewhere must be subtracted out to reveal the extremely tiny variations characterizing the fine-scale structure of the CMBR background. The detailed analysis of CMBR data to produce maps, an angular power spectrum, and ultimately cosmological parameters is a complicated, computationally difficult problem. In practice it is hard to take the effects of noise and foreground sources into account. In particular, these foregrounds are dominated by galactic emissions such as bremsstrahlung, synchrotron, and dust that emit in the microwave band; in practice, the galaxy"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_26",
    "chunk": "has to be removed, resulting in a CMB map that is not a full-sky map. In addition, point sources like galaxies and clusters represent foreground sources which must be removed so as not to distort the short scale structure of the CMB power spectrum. Constraints on many cosmological parameters can be obtained from their effects on the power spectrum, and results are often calculated using Markov chain Monte Carlo sampling techniques. With the increasingly precise data provided by WMAP, there have been a number of claims that the CMB exhibits anomalies, such as very large scale anisotropies, anomalous alignments, and non-Gaussian distributions. The most longstanding of these is the low-ℓ multipole controversy. Even in the COBE map, it was observed that the quadrupole (ℓ = 2, spherical harmonic) has a low amplitude compared to the predictions of the Big Bang. In particular, the quadrupole and octupole (ℓ = 3) modes appear to have an unexplained alignment with each other and with both the ecliptic plane and equinoxes. A number of groups have suggested that this could be the signature of quantum corrections or new physics at the greatest observable scales; other groups suspect systematic errors in the data. Ultimately, due"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_27",
    "chunk": "to the foregrounds and the cosmic variance problem, the greatest modes will never be as well measured as the small angular scale modes. The analyses were performed on two maps that have had the foregrounds removed as far as possible: the \"internal linear combination\" map of the WMAP collaboration and a similar map prepared by Max Tegmark and others. Later analyses have pointed out that these are the modes most susceptible to foreground contamination from synchrotron, dust, and bremsstrahlung emission, and from experimental uncertainty in the monopole and dipole. A full Bayesian analysis of the WMAP power spectrum demonstrates that the quadrupole prediction of Lambda-CDM cosmology is consistent with the data at the 10% level and that the observed octupole is not remarkable. Carefully accounting for the procedure used to remove the foregrounds from the full sky map further reduces the significance of the alignment by ~5%. Recent observations with the Planck telescope, which is very much more sensitive than WMAP and has a larger angular resolution, record the same anomaly, and so instrumental error (but not foreground contamination) appears to be ruled out. Coincidence is a possible explanation, chief scientist from WMAP, Charles L. Bennett suggested coincidence and human"
  },
  {
    "source": "Cosmic microwave background.txt",
    "chunk_id": "Cosmic microwave background.txt_28",
    "chunk": "psychology were involved, \"I do think there is a bit of a psychological effect; people want to find unusual things.\" Measurements of the density of quasars based on Wide-field Infrared Survey Explorer data finds a dipole significantly different from the one extracted from the CMB anisotropy. This difference is conflict with the cosmological principle. Assuming the universe keeps expanding and it does not suffer a Big Crunch, a Big Rip, or another similar fate, the cosmic microwave background will continue redshifting until it will no longer be detectable, and will be superseded first by the one produced by starlight, and perhaps, later by the background radiation fields of processes that may take place in the far future of the universe such as proton decay, evaporation of black holes, and positronium decay."
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_0",
    "chunk": "# Dark energy In physical cosmology and astronomy, dark energy is a proposed form of energy that affects the universe on the largest scales. Its primary effect is to drive the accelerating expansion of the universe. It also slows the rate of structure formation. Assuming that the lambda-CDM model of cosmology is correct, dark energy dominates the universe, contributing 68% of the total energy in the present-day observable universe while dark matter and ordinary (baryonic) matter contribute 26% and 5%, respectively, and other components such as neutrinos and photons are nearly negligible. Dark energy's density is very low: 7×10 g/cm (6×10 J/m in mass-energy), much less than the density of ordinary matter or dark matter within galaxies. However, it dominates the universe's mass–energy content because it is uniform across space. The first observational evidence for dark energy's existence came from measurements of supernovae. Type Ia supernovae have constant luminosity, which means that they can be used as accurate distance measures. Comparing this distance to the redshift (which measures the speed at which the supernova is receding) shows that the universe's expansion is accelerating. Prior to this observation, scientists thought that the gravitational attraction of matter and energy in the universe"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_1",
    "chunk": "would cause the universe's expansion to slow over time. Since the discovery of accelerating expansion, several independent lines of evidence have been discovered that support the existence of dark energy. The exact nature of dark energy remains a mystery, and many possible explanations have been theorized. The main candidates are a cosmological constant (representing a constant energy density filling space homogeneously) and scalar fields (dynamic quantities having energy densities that vary in time and space) such as quintessence or moduli. A cosmological constant would remain constant across time and space, while scalar fields can vary. Yet other possibilities are interacting dark energy (see the section Dark energy § Theories of dark energy) an observational effect, cosmological coupling and shockwave cosmology (see the section § Alternatives to dark energy). The \"cosmological constant\" is a constant term that can be added to Einstein field equations of general relativity. If considered as a \"source term\" in the field equation, it can be viewed as equivalent to the mass of empty space (which conceptually could be either positive or negative), or \"vacuum energy\". The cosmological constant was first proposed by Einstein as a mechanism to obtain a solution to the gravitational field equation that"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_2",
    "chunk": "would lead to a static universe, effectively using dark energy to balance gravity. Einstein gave the cosmological constant the symbol Λ (capital lambda). Einstein stated that the cosmological constant required that 'empty space takes the role of gravitating negative masses that are distributed all over the interstellar space'. The mechanism was an example of fine-tuning, and it was later realized that Einstein's static universe would not be stable: local inhomogeneities would ultimately lead to either the runaway expansion or contraction of the universe. The equilibrium is unstable: if the universe expands slightly, then the expansion releases vacuum energy, which causes yet more expansion. Likewise, a universe that contracts slightly will continue contracting. According to Einstein, \"empty space\" can possess its own energy. Because this energy is a property of space itself, it would not be diluted as space expands. As more space comes into existence, more of this energy-of-space would appear, thereby causing accelerated expansion. These sorts of disturbances are inevitable, due to the uneven distribution of matter throughout the universe. Further, observations made by Edwin Hubble in 1929 showed that the universe appears to be expanding and is not static. Einstein reportedly referred to his failure to predict the"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_3",
    "chunk": "idea of a dynamic universe, in contrast to a static universe, as his greatest blunder. Alan Guth and Alexei Starobinsky proposed in 1980 that a negative pressure field, similar in concept to dark energy, could drive cosmic inflation in the very early universe. Inflation postulates that some repulsive force, qualitatively similar to dark energy, resulted in an enormous and exponential expansion of the universe during its earliest stages. Such expansion is an essential feature of most current models of the Big Bang. However, inflation must have occurred at a much higher energy density than the dark energy we observe today, and inflation is thought to have completely ended when the universe was just a fraction of a second old. It is unclear what relation, if any, exists between dark energy and inflation. Even after inflationary models became accepted, the cosmological constant was thought to be irrelevant to the current universe. Nearly all inflation models predict that the total (matter+energy) density of the universe should be very close to the critical density. During the 1980s, most cosmological research focused on models with critical density in matter only, usually 95% cold dark matter (CDM) and 5% ordinary matter (baryons). These models were"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_4",
    "chunk": "found to be successful at forming realistic galaxies and clusters, but some problems appeared in the late 1980s: in particular, the model required a value for the Hubble constant lower than preferred by observations, and the model under-predicted observations of large-scale galaxy clustering. These difficulties became stronger after the discovery of anisotropy in the cosmic microwave background by the COBE spacecraft in 1992, and several modified CDM models came under active study through the mid-1990s: these included the Lambda-CDM model and a mixed cold/hot dark matter model. The first direct evidence for dark energy came from supernova observations in 1998 of accelerated expansion in Riess et al. and in Perlmutter et al., and the Lambda-CDM model then became the leading model. Soon after, dark energy was supported by independent observations: in 2000, the BOOMERanG and Maxima cosmic microwave background experiments observed the first acoustic peak in the cosmic microwave background, showing that the total (matter+energy) density is close to 100% of critical density. Then in 2001, the 2dF Galaxy Redshift Survey gave strong evidence that the matter density is around 30% of critical. The large difference between these two supports a smooth component of dark energy making up the difference."
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_5",
    "chunk": "Much more precise measurements from WMAP in 2003–2010 have continued to support the standard model and give more accurate measurements of the key parameters. The term \"dark energy\", echoing Fritz Zwicky's \"dark matter\" from the 1930s, was coined by Michael S. Turner in 1998. The nature of dark energy is more hypothetical than that of dark matter, and many things about it remain in the realm of speculation. Dark energy is thought to be very homogeneous and not dense, and is not known to interact through any of the fundamental forces other than gravity. Since it is rarefied and un-massive—roughly 10 kg/m—it is unlikely to be detectable in laboratory experiments. The reason dark energy can have such a profound effect on the universe, making up 68% of universal density in spite of being so dilute, is that it is believed to uniformly fill otherwise empty space. The vacuum energy, that is, the particle-antiparticle pairs generated and mutually annihilated within a time frame in accord with Heisenberg's uncertainty principle in the energy-time formulation, has been often invoked as the main contribution to dark energy. The mass–energy equivalence postulated by general relativity implies that the vacuum energy should exert a gravitational force."
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_6",
    "chunk": "Hence, the vacuum energy is expected to contribute to the cosmological constant, which in turn impinges on the accelerated expansion of the universe. However, the cosmological constant problem asserts that there is a huge disagreement between the observed values of vacuum energy density and the theoretical large value of zero-point energy obtained by quantum field theory; the problem remains unresolved. Independently of its actual nature, dark energy would need to have a strong negative pressure to explain the observed acceleration of the expansion of the universe. According to general relativity, the pressure within a substance contributes to its gravitational attraction for other objects just as its mass density does. This happens because the physical quantity that causes matter to generate gravitational effects is the stress–energy tensor, which contains both the energy (or matter) density of a substance and its pressure. In the Friedmann–Lemaître–Robertson–Walker metric, it can be shown that a strong constant negative pressure (i.e., tension) in all the universe causes an acceleration in the expansion if the universe is already expanding, or a deceleration in contraction if the universe is already contracting. This accelerating expansion effect is sometimes labeled \"gravitational repulsion\". In standard cosmology, there are three components of"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_7",
    "chunk": "the universe: matter, radiation, and dark energy. This matter is anything whose energy density scales with the inverse cube of the scale factor, i.e., ρ ∝ a, while radiation is anything whose energy density scales to the inverse fourth power of the scale factor (ρ ∝ a). This can be understood intuitively: for an ordinary particle in a cube-shaped box, doubling the length of an edge of the box decreases the density (and hence energy density) by a factor of eight (2). For radiation, the decrease in energy density is greater, because an increase in spatial distance also causes a redshift. The final component is dark energy: it is an intrinsic property of space and has a constant energy density, regardless of the dimensions of the volume under consideration (ρ ∝ a). Thus, unlike ordinary matter, it is not diluted by the expansion of space. High-precision measurements of the expansion of the universe are required to understand how the expansion rate changes over time and space. In general relativity, the evolution of the expansion rate is estimated from the curvature of the universe and the cosmological equation of state (the relationship between temperature, pressure, and combined matter, energy, and vacuum"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_8",
    "chunk": "energy density for any region of space). Measuring the equation of state for dark energy is one of the biggest efforts in observational cosmology today. Adding the cosmological constant to cosmology's standard FLRW metric leads to the Lambda-CDM model, which has been referred to as the \"standard model of cosmology\" because of its precise agreement with observations. As of 2013, the Lambda-CDM model is consistent with a series of increasingly rigorous cosmological observations, including the Planck spacecraft and the Supernova Legacy Survey. First results from the SNLS reveal that the average behavior (i.e., equation of state) of dark energy behaves like Einstein's cosmological constant to a precision of 10%. Recent results from the Hubble Space Telescope Higher-Z Team indicate that dark energy has been present for at least 9 billion years and during the period preceding cosmic acceleration. In Mar 2025, the Dark Energy Spectroscopic Instrument (DESI) collaboration announce that evidence for evolving dark energy has been discovered in analysis combining DESI data on baryon acoustic oscillations (BAO) with the CMB, weak lensing and supernovae dataset, with significance ranging from 2.8 to 4.2σ. Results suggest that the density of dark energy is slowly decreasing with time. The evidence for dark"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_9",
    "chunk": "energy is indirect but comes from three independent sources: In 1998, the High-Z Supernova Search Team published observations of Type Ia (\"one-A\") supernovae. In 1999, the Supernova Cosmology Project followed by suggesting that the expansion of the universe is accelerating. The 2011 Nobel Prize in Physics was awarded to Saul Perlmutter, Brian P. Schmidt, and Adam G. Riess for their leadership in the discovery. Since then, these observations have been corroborated by several independent sources. Measurements of the cosmic microwave background, gravitational lensing, and the large-scale structure of the cosmos, as well as improved measurements of supernovae, have been consistent with the Lambda-CDM model. Some people argue that the only indications for the existence of dark energy are observations of distance measurements and their associated redshifts. Cosmic microwave background anisotropies and baryon acoustic oscillations serve only to demonstrate that distances to a given redshift are larger than would be expected from a \"dusty\" Friedmann–Lemaître universe and the local measured Hubble constant. Supernovae are useful for cosmology because they are excellent standard candles across cosmological distances. They allow researchers to measure the expansion history of the universe by looking at the relationship between the distance to an object and its redshift,"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_10",
    "chunk": "which gives how fast it is receding from us. The relationship is roughly linear, according to Hubble's law. It is relatively easy to measure redshift, but finding the distance to an object is more difficult. Usually, astronomers use standard candles: objects for which the intrinsic brightness, or absolute magnitude, is known. This allows the object's distance to be measured from its actual observed brightness, or apparent magnitude. Type Ia supernovae are the most accurate known standard candles across cosmological distances because of their extreme and consistent luminosity. Recent observations of supernovae are consistent with a universe made up 66.6% of dark energy and 33.4% of a combination of dark matter and baryonic matter assuming a flat Lambda-CDM model. The theory of large-scale structure, which governs the formation of structures in the universe (stars, quasars, galaxies and galaxy groups and clusters), also suggests that the density of matter in the universe is only 30% of the critical density. A 2011 survey, the WiggleZ galaxy survey of more than 200,000 galaxies, provided further evidence towards the existence of dark energy, although the exact physics behind it remains unknown. The WiggleZ survey from the Australian Astronomical Observatory scanned the galaxies to determine their"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_11",
    "chunk": "redshift. Then, by exploiting the fact that baryon acoustic oscillations have left voids regularly of ≈150 Mpc diameter, surrounded by the galaxies, the voids were used as standard rulers to estimate distances to galaxies as far as 2,000 Mpc (redshift 0.6), allowing for accurate estimate of the speeds of galaxies from their redshift and distance. The data confirmed cosmic acceleration up to half of the age of the universe (7 billion years) and constrain its inhomogeneity to 1 part in 10. This provides a confirmation to cosmic acceleration independent of supernovae. The existence of dark energy, in whatever form, is needed to reconcile the measured geometry of space with the total amount of matter in the universe. Measurements of cosmic microwave background anisotropies indicate that the universe is close to flat. For the shape of the universe to be flat, the mass–energy density of the universe must be equal to the critical density. The total amount of matter in the universe (including baryons and dark matter), as measured from the cosmic microwave background spectrum, accounts for only about 30% of the critical density. This implies the existence of an additional form of energy to account for the remaining 70%. The"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_12",
    "chunk": "Wilkinson Microwave Anisotropy Probe (WMAP) spacecraft seven-year analysis estimated a universe made up of 72.8% dark energy, 22.7% dark matter, and 4.5% ordinary matter. Work done in 2013 based on the Planck spacecraft observations of the cosmic microwave background gave a more accurate estimate of 68.3% dark energy, 26.8% dark matter, and 4.9% ordinary matter. Accelerated cosmic expansion causes gravitational potential wells and hills to flatten as photons pass through them, producing cold spots and hot spots on the cosmic microwave background aligned with vast supervoids and superclusters. This so-called late-time Integrated Sachs–Wolfe effect (ISW) is a direct signal of dark energy in a flat universe. It was reported at high significance in 2008 by Ho et al. and Giannantonio et al. A new approach to test evidence of dark energy through observational Hubble constant data (OHD), also known as cosmic chronometers, has gained significant attention in recent years. The Hubble constant, H(z), is measured as a function of cosmological redshift. OHD directly tracks the expansion history of the universe by taking passively evolving early-type galaxies as \"cosmic chronometers\". From this point, this approach provides standard clocks in the universe. The core of this idea is the measurement of the"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_13",
    "chunk": "differential age evolution as a function of redshift of these cosmic chronometers. Thus, it provides a direct estimate of the Hubble parameter The reliance on a differential quantity, ⁠Δz/Δt⁠, brings more information and is appealing for computation: It can minimize many common issues and systematic effects. Analyses of supernovae and baryon acoustic oscillations (BAO) are based on integrals of the Hubble parameter, whereas ⁠Δz/Δt⁠ measures it directly. For these reasons, this method has been widely used to examine the accelerated cosmic expansion and study properties of dark energy. Dark energy's status as a hypothetical force with unknown properties makes it an active target of research. The problem is attacked from a variety of angles, such as modifying the prevailing theory of gravity (general relativity), attempting to pin down the properties of dark energy, and finding alternative ways to explain the observational data. The simplest explanation for dark energy is that it is an intrinsic, fundamental energy of space. This is the cosmological constant, usually represented by the Greek letter Λ (Lambda, hence the name Lambda-CDM model). Since energy and mass are related according to the equation E = mc, Einstein's theory of general relativity predicts that this energy will have"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_14",
    "chunk": "a gravitational effect. It is sometimes called vacuum energy because it is the energy density of empty space – of vacuum. A major outstanding problem is that the same quantum field theories predict a huge cosmological constant, about 120 orders of magnitude too large. This would need to be almost, but not exactly, cancelled by an equally large term of the opposite sign. Some supersymmetric theories require a cosmological constant that is exactly zero. Also, it is unknown whether there is a metastable vacuum state in string theory with a positive cosmological constant, and it has been conjectured by Ulf Danielsson et al. that no such state exists. This conjecture would not rule out other models of dark energy, such as quintessence, that could be compatible with string theory. In quintessence models of dark energy, the observed acceleration of the scale factor is caused by the potential energy of a dynamical field, referred to as quintessence field. Quintessence differs from the cosmological constant in that it can vary in space and time. In order for it not to clump and form structure like matter, the field must be very light so that it has a large Compton wavelength. In the"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_15",
    "chunk": "simplest scenarios, the quintessence field has a canonical kinetic term, is minimally coupled to gravity, and does not feature higher order operations in its Lagrangian. No evidence of quintessence is yet available, nor has it been ruled out. It generally predicts a slightly slower acceleration of the expansion of the universe than the cosmological constant. Some scientists think that the best evidence for quintessence would come from violations of Einstein's equivalence principle and variation of the fundamental constants in space or time. Scalar fields are predicted by the Standard Model of particle physics and string theory, but an analogous problem to the cosmological constant problem (or the problem of constructing models of cosmological inflation) occurs: renormalization theory predicts that scalar fields should acquire large masses. The coincidence problem asks why the acceleration of the Universe began when it did. If acceleration began earlier in the universe, structures such as galaxies would never have had time to form, and life, at least as we know it, would never have had a chance to exist. Proponents of the anthropic principle view this as support for their arguments. However, many models of quintessence have a so-called \"tracker\" behavior, which solves this problem. In"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_16",
    "chunk": "these models, the quintessence field has a density which closely tracks (but is less than) the radiation density until matter–radiation equality, which triggers quintessence to start behaving as dark energy, eventually dominating the universe. This naturally sets the low energy scale of the dark energy. In 2004, when scientists fit the evolution of dark energy with the cosmological data, they found that the equation of state had possibly crossed the cosmological constant boundary (w = −1) from above to below. A no-go theorem has been proved that this scenario requires models with at least two types of quintessence. This scenario is the so-called Quintom scenario. Some special cases of quintessence are phantom energy, in which the energy density of quintessence actually increases with time, and k-essence (short for kinetic quintessence) which has a non-standard form of kinetic energy such as a negative kinetic energy. They can have unusual properties: phantom energy, for example, can cause a Big Rip. A group of researchers argued in 2021 that observations of the Hubble tension may imply that only quintessence models with a nonzero coupling constant are viable. This class of theories attempts to come up with an all-encompassing theory of both dark matter"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_17",
    "chunk": "and dark energy as a single phenomenon that modifies the laws of gravity at various scales. This could, for example, treat dark energy and dark matter as different facets of the same unknown substance, or postulate that cold dark matter decays into dark energy. Another class of theories that unifies dark matter and dark energy are suggested to be covariant theories of modified gravities. These theories alter the dynamics of spacetime such that the modified dynamics stems to what have been assigned to the presence of dark energy and dark matter. Dark energy could in principle interact not only with the rest of the dark sector, but also with ordinary matter. However, cosmology alone is not sufficient to effectively constrain the strength of the coupling between dark energy and baryons, so that other indirect techniques or laboratory searches have to be adopted. It was briefly theorized in the early 2020s that excess observed in the XENON1T detector in Italy may have been caused by a chameleon model of dark energy, but further experiments disproved this possibility. The density of dark energy might have varied in time during the history of the universe. Modern observational data allows us to estimate the"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_18",
    "chunk": "present density of dark energy. Using baryon acoustic oscillations, it is possible to investigate the effect of dark energy in the history of the universe, and constrain parameters of the equation of state of dark energy. To that end, several models have been proposed. One of the most popular models is the Chevallier–Polarski–Linder model (CPL). Some other common models are Barboza & Alcaniz (2008), Jassal et al. (2005), Wetterich. (2004), and Oztas et al. (2018). There is some observational evidence that dark energy is indeed decreasing with time. Data from the Dark Energy Spectroscopic Instrument (DESI), tracking the size of baryon acoustic oscillations over the universe's expansion history, suggests that the amount of dark energy is 10% lower than it was 4.5 billion years ago. However, there is not yet sufficient data to rule out dark energy being the cosmological constant. The evidence for dark energy is heavily dependent on the theory of general relativity. Therefore, it is conceivable that a modification to general relativity also eliminates the need for dark energy. There are many such theories, and research is ongoing. The measurement of the speed of gravity in the first gravitational wave measured by non-gravitational means (GW170817) ruled out"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_19",
    "chunk": "many modified gravity theories as explanations to dark energy. Astrophysicist Ethan Siegel states that, while such alternatives gain mainstream press coverage, almost all professional astrophysicists are confident that dark energy exists and that none of the competing theories successfully explain observations to the same level of precision as standard dark energy. Some alternatives to dark energy, such as inhomogeneous cosmology, aim to explain the observational data by a more refined use of established theories. In this scenario, dark energy does not actually exist, and is merely a measurement artifact. For example, if we are located in an emptier-than-average region of space, the observed cosmic expansion rate could be mistaken for a variation in time, or acceleration. A different approach uses a cosmological extension of the equivalence principle to show how space might appear to be expanding more rapidly in the voids surrounding our local cluster. While weak, such effects considered cumulatively over billions of years could become significant, creating the illusion of cosmic acceleration, and making it appear as if we live in a Hubble bubble. Yet other possibilities are that the accelerated expansion of the universe is an illusion caused by the relative motion of us to the rest"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_20",
    "chunk": "of the universe, or that the statistical methods employed were flawed. A laboratory direct detection attempt failed to detect any force associated with dark energy. Observational skepticism explanations of dark energy have generally not gained much traction among cosmologists. For example, a paper that suggested the anisotropy of the local Universe has been misrepresented as dark energy was quickly countered by another paper claiming errors in the original paper. Another study questioning the essential assumption that the luminosity of Type Ia supernovae does not vary with stellar population age was also swiftly rebutted by other cosmologists. This theory was formulated by researchers of the University of Hawaiʻi at Mānoa in February 2023. The idea is that if one requires the Kerr metric (which describes rotating black holes) to asymptote to the Friedmann-Robertson-Walker metric (which describes the isotropic and homogeneous universe that is the basic assumption of modern cosmology), then one finds that black holes gain mass as the universe expands. The rate is measured to be ∝a, where a is the scale factor. This particular rate means that the energy density of black holes remains constant over time, mimicking dark energy (see Dark energy#Technical definition). The theory is called \"cosmological"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_21",
    "chunk": "coupling\" because the black holes couple to a cosmological requirement. Other astrophysicists are skeptical, with a variety of papers claiming that the theory fails to explain other observations. Shockwave cosmology, proposed by Joel Smoller and Blake Temple in 2003, has the “big bang” as an explosion inside a black hole, producing the expanding volume of space and matter that includes the observable universe. A related theory by Smoller, Temple, and Vogler proposes that this shockwave may have resulted in our part of the universe having a lower density than that surrounding it, causing the accelerated expansion normally attributed to dark energy. They also propose that this related theory could be tested: a universe with dark energy should give a figure for the cubic correction to redshift versus luminosity C = −0.180 at a = a whereas for Smoller, Temple, and Vogler's alternative C should be positive rather than negative. They give a more precise calculation for their shockwave model alternative as: the cubic correction to redshift versus luminosity at a = a is C = 0.359. Although shockwave cosmology produces a universe that \"looks essentially identical to the aftermath of the big bang\", cosmologists consider that it needs further development"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_22",
    "chunk": "before it could be considered as a more advantageous model than the big bang theory (or standard model) in explaining the universe. In particular, and especially for the proposed alternative to dark energy, it would need to explain big bang nucleosynthesis, the quantitative details of the microwave background anisotropies, the Lyman-alpha forest, and galaxy surveys. Cosmologists estimate that the acceleration began roughly 5 billion years ago. Before that, it is thought that the expansion was decelerating, due to the attractive influence of matter. The density of dark matter in an expanding universe decreases more quickly than dark energy, and eventually the dark energy dominates. Specifically, when the volume of the universe doubles, the density of dark matter is halved, but the density of dark energy is nearly unchanged (it is exactly constant in the case of a cosmological constant). Projections into the future can differ radically for different models of dark energy. For a cosmological constant, or any other model that predicts that the acceleration will continue indefinitely, the ultimate result will be that galaxies outside the Local Group will have a line-of-sight velocity that continually increases with time, eventually far exceeding the speed of light. This is not a"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_23",
    "chunk": "violation of special relativity because the notion of \"velocity\" used here is different from that of velocity in a local inertial frame of reference, which is still constrained to be less than the speed of light for any massive object (see Uses of the proper distance for a discussion of the subtleties of defining any notion of relative velocity in cosmology). Because the Hubble parameter is decreasing with time, there can actually be cases where a galaxy that is receding from us faster than light does manage to emit a signal which reaches us eventually. However, because of the accelerating expansion, it is projected that most galaxies will eventually cross a type of cosmological event horizon where any light they emit past that point will never be able to reach us at any time in the infinite future because the light never reaches a point where its \"peculiar velocity\" toward us exceeds the expansion velocity away from us (these two notions of velocity are also discussed in Uses of the proper distance). Assuming the dark energy is constant (a cosmological constant), the current distance to this cosmological event horizon is about 16 billion light years, meaning that a signal from"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_24",
    "chunk": "an event happening at present would eventually be able to reach us in the future if the event were less than 16 billion light years away, but the signal would never reach us if the event were more than 16 billion light years away. As galaxies approach the point of crossing this cosmological event horizon, the light from them will become more and more redshifted, to the point where the wavelength becomes too large to detect in practice and the galaxies appear to vanish completely (see Future of an expanding universe). Planet Earth, the Milky Way, and the Local Group of galaxies of which the Milky Way is a part, would all remain virtually undisturbed as the rest of the universe recedes and disappears from view. In this scenario, the Local Group would ultimately suffer heat death, just as was hypothesized for the flat, matter-dominated universe before measurements of cosmic acceleration. There are other, more speculative ideas about the future of the universe. The phantom energy model of dark energy results in divergent expansion, which would imply that the effective force of dark energy continues growing until it dominates all other forces in the universe. Under this scenario, dark energy"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_25",
    "chunk": "would ultimately tear apart all gravitationally bound structures, including galaxies and solar systems, and eventually overcome the electrical and nuclear forces to tear apart atoms themselves, ending the universe in a \"Big Rip\". On the other hand, dark energy might dissipate with time or even become attractive. Such uncertainties leave open the possibility of gravity eventually prevailing and lead to a universe that contracts in on itself in a \"Big Crunch\", or that there may even be a dark energy cycle, which implies a cyclic model of the universe in which every iteration (Big Bang then eventually a Big Crunch) takes about a trillion (10) years. While none of these are supported by observations, they are not ruled out. The astrophysicist David Merritt identifies dark energy as an example of an \"auxiliary hypothesis\", an ad hoc postulate that is added to a theory in response to observations that falsify it. He argues that the dark energy hypothesis is a conventionalist hypothesis, that is, a hypothesis that adds no empirical content and hence is unfalsifiable in the sense defined by Karl Popper. However, his opinion is not shared by all scientists. Taken together, all the current data provide strong evidence for"
  },
  {
    "source": "Dark energy.txt",
    "chunk_id": "Dark energy.txt_26",
    "chunk": "the existence of dark energy; they constrain the fraction of critical density contributed by dark energy, 0.76 ± 0.02 , and the equation-of-state parameter: assuming that w is constant. This implies that the Universe began accelerating at redshift z ~ 0.4 and age t ~ 10 Ga . These results are robust – data from any one method can be removed without compromising the constraints – and they are not substantially weakened by dropping the assumption of spatial flatness."
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_0",
    "chunk": "# Dark matter In astronomy, dark matter is an invisible and hypothetical form of matter that does not interact with light or other electromagnetic radiation. Dark matter is implied by gravitational effects that cannot be explained by general relativity unless more matter is present than can be observed. Such effects occur in the context of formation and evolution of galaxies, gravitational lensing, the observable universe's current structure, mass position in galactic collisions, the motion of galaxies within galaxy clusters, and cosmic microwave background anisotropies. Dark matter is thought to serve as gravitational scaffolding for cosmic structures. After the Big Bang, dark matter clumped into blobs along narrow filaments with superclusters of galaxies forming a cosmic web at scales on which entire galaxies appear like tiny particles. In the standard Lambda-CDM model of cosmology, the mass–energy content of the universe is 5% ordinary matter, 26.8% dark matter, and 68.2% a form of energy known as dark energy. Thus, dark matter constitutes 85% of the total mass, while dark energy and dark matter constitute 95% of the total mass–energy content. While the density of dark matter is significant in the halo around a galaxy, its local density in the Solar System is"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_1",
    "chunk": "much less than normal matter. The total of all the dark matter out to the orbit of Neptune would add up about 10^17 kg, the same as a large asteroid. Dark matter is not known to interact with ordinary baryonic matter and radiation except through gravity, making it difficult to detect in the laboratory. The most prevalent explanation is that dark matter is some as-yet-undiscovered subatomic particle, such as either weakly interacting massive particles (WIMPs) or axions. The other main possibility is that dark matter is composed of primordial black holes. Dark matter is classified as \"cold\", \"warm\", or \"hot\" according to velocity (more precisely, its free streaming length). Recent models have favored a cold dark matter scenario, in which structures emerge by the gradual accumulation of particles. Although the astrophysics community generally accepts the existence of dark matter, a minority of astrophysicists, intrigued by specific observations that are not well explained by ordinary dark matter, argue for various modifications of the standard laws of general relativity. These include modified Newtonian dynamics, tensor–vector–scalar gravity, or entropic gravity. So far none of the proposed modified gravity theories can describe every piece of observational evidence at the same time, suggesting that even"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_2",
    "chunk": "if gravity has to be modified, some form of dark matter will still be required. The hypothesis of dark matter has an elaborate history. Wm. Thomson, Lord Kelvin, discussed the potential number of stars around the Sun in the appendices of a book based on a series of lectures given in 1884 in Baltimore. He inferred their density using the observed velocity dispersion of the stars near the Sun, assuming that the Sun was 20–100 million years old. He posed what would happen if there were a thousand million stars within 1 kiloparsec of the Sun (at which distance their parallax would be 1 milli-arcsecond). Kelvin concluded Many of our supposed thousand million stars – perhaps a great majority of them – may be dark bodies. In 1906, Henri Poincaré used the French term [matière obscure] (\"dark matter\") in discussing Kelvin's work. He found that the amount of dark matter would need to be less than that of visible matter, incorrectly, it turns out. The second to suggest the existence of dark matter using stellar velocities was Dutch astronomer Jacobus Kapteyn in 1922. A publication from 1930 by Swedish astronomer Knut Lundmark points to him being the first to realise"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_3",
    "chunk": "that the universe must contain much more mass than can be observed. Dutch radio astronomy pioneer Jan Oort also hypothesized the existence of dark matter in 1932. Oort was studying stellar motions in the galactic neighborhood and found the mass in the galactic plane must be greater than what was observed, but this measurement was later determined to be incorrect. In 1933, Swiss astrophysicist Fritz Zwicky studied galaxy clusters while working at Caltech and made a similar inference. Zwicky applied the virial theorem to the Coma Cluster and obtained evidence of unseen mass he called dunkle Materie ('dark matter'). Zwicky estimated its mass based on the motions of galaxies near its edge and compared that to an estimate based on its brightness and number of galaxies. He estimated the cluster had about 400 times more mass than was visually observable. The gravity effect of the visible galaxies was far too small for such fast orbits, thus mass must be hidden from view. Based on these conclusions, Zwicky inferred some unseen matter provided the mass and associated gravitational attraction to hold the cluster together. Zwicky's estimates were off by more than an order of magnitude, mainly due to an obsolete value"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_4",
    "chunk": "of the Hubble constant; the same calculation today shows a smaller fraction, using greater values for luminous mass. Nonetheless, Zwicky did correctly conclude from his calculation that most of the gravitational matter present was dark. However unlike modern theories, Zwicky considered \"dark matter\" to be non-luminous ordinary matter. Further indications of mass-to-light ratio anomalies came from measurements of galaxy rotation curves. In 1939, H.W. Babcock reported the rotation curve for the Andromeda nebula (now called the Andromeda Galaxy), which suggested the mass-to-luminosity ratio increases radially. He attributed it to either light absorption within the galaxy or modified dynamics in the outer portions of the spiral, rather than to unseen matter. Following Babcock's 1939 report of unexpectedly rapid rotation in the outskirts of the Andromeda Galaxy and a mass-to-light ratio of 50; in 1940, Oort discovered and wrote about the large non-visible halo of NGC 3115. The hypothesis of dark matter largely took root in the 1970s. Several different observations were synthesized to argue that galaxies should be surrounded by halos of unseen matter. In two papers that appeared in 1974, this conclusion was drawn in tandem by independent groups: in Princeton, New Jersey, by Jeremiah Ostriker, Jim Peebles, and Amos"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_5",
    "chunk": "Yahil, and in Tartu, Estonia, by Jaan Einasto, Enn Saar, and Ants Kaasik. One of the observations that served as evidence for the existence of galactic halos of dark matter was the shape of galaxy rotation curves. These observations were done in optical and radio astronomy. In optical astronomy, Vera Rubin and Kent Ford worked with a new spectrograph to measure the velocity curve of edge-on spiral galaxies with greater accuracy. At the same time, radio astronomers were making use of new radio telescopes to map the 21 cm line of atomic hydrogen in nearby galaxies. The radial distribution of interstellar atomic hydrogen (H) often extends to much greater galactic distances than can be observed as collective starlight, expanding the sampled distances for rotation curves – and thus of the total mass distribution – to a new dynamical regime. Early mapping of Andromeda with the 300-foot (91 m) telescope at Green Bank and the 250-foot (76 m) dish at Jodrell Bank already showed the H rotation curve did not trace the decline expected from Keplerian orbits. As more sensitive receivers became available, Roberts & Whitehurst (1975) were able to trace the rotational velocity of Andromeda to 30 kpc, much beyond"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_6",
    "chunk": "the optical measurements. Illustrating the advantage of tracing the gas disk at large radii; that paper's Figure 16 combines the optical data (the cluster of points at radii of less than 15 kpc with a single point further out) with the H data between 20 and 30 kpc, exhibiting the flatness of the outer galaxy rotation curve; the solid curve peaking at the center is the optical surface density, while the other curve shows the cumulative mass, still rising linearly at the outermost measurement. In parallel, the use of interferometric arrays for extragalactic H spectroscopy was being developed. Rogstad & Shostak (1972) published H rotation curves of five spirals mapped with the Owens Valley interferometer; the rotation curves of all five were very flat, suggesting very large values of mass-to-light ratio in the outer parts of their extended H disks. In 1978, Albert Bosma showed further evidence of flat rotation curves using data from the Westerbork Synthesis Radio Telescope. In 1978, Steigman et al. presented a study that extended earlier cosmological relic-density calculations to any hypothetical stable, electrically neutral, weak-scale lepton, showing how such a particle's abundance would \"freeze out\" in the early Universe and providing analytic expressions that linked"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_7",
    "chunk": "its mass and weak interaction cross-section to the present-day matter density. By decoupling the analysis from specific neutrino properties and treating the candidate generically, the authors set out a framework that later became the standard template for Weakly Interacting Massive Particles (WIMPs) and for comparing particle-physics models with cosmological constraints. Though subsequent work has refined the methodology and explored many alternative candidates, this paper marked the first explicit, systematic treatment of dark matter as a new particle species beyond the Standard Model. By the late 1970s the existence of dark matter halos around galaxies was widely recognized as real, and became a major unsolved problem in astronomy. A stream of observations in the 1980–1990s supported the presence of dark matter. Persic, Salucci & Stel (1996) is notable for the investigation of 967 spirals. The evidence for dark matter also included gravitational lensing of background objects by galaxy clusters, the temperature distribution of hot gas in galaxies and clusters, and the pattern of anisotropies in the cosmic microwave background. According to the current consensus among cosmologists, dark matter is composed primarily of some type of not-yet-characterized subatomic particle. The search for this particle, by a variety of means, is one of"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_8",
    "chunk": "the major efforts in particle physics. In standard cosmological calculations, \"matter\" means any constituent of the universe whose energy density scales with the inverse cube of the scale factor, i.e., ρ ∝ a . This is in contrast to \"radiation\", which scales as the inverse fourth power of the scale factor ρ ∝ a , and a cosmological constant, which does not change with respect to a (ρ ∝ a). The different scaling factors for matter and radiation are a consequence of radiation redshift. For example, after doubling the diameter of the observable Universe via cosmic expansion, the scale, a, has doubled. The energy of the cosmic microwave background radiation has been halved (because the wavelength of each photon has doubled); the energy of ultra-relativistic particles, such as early-era standard-model neutrinos, is similarly halved. The cosmological constant, as an intrinsic property of space, has a constant energy density regardless of the volume under consideration. In principle, \"dark matter\" means all components of the universe which are not visible but still obey ρ ∝ a . In practice, the term \"dark matter\" is often used to mean only the non-baryonic component of dark matter, i.e., excluding \"missing baryons\". Context will usually"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_9",
    "chunk": "indicate which meaning is intended. This section presents the observational evidence for dark matter from the smallest to the largest scales. The arms of spiral galaxies rotate around their galactic center. The luminous mass density of a spiral galaxy decreases as one goes from the center to the outskirts. If luminous mass were all the matter, then the galaxy can be modelled as a point mass in the centre and test masses orbiting around it, similar to the Solar System. From Kepler's Third Law, it is expected that the rotation velocities will decrease with distance from the center, similar to the Solar System. This is not observed. Instead, the galaxy rotation curve remains flat or even increases as distance from the center increases. If Kepler's laws are correct, then the obvious way to resolve this discrepancy is to conclude the mass distribution in spiral galaxies is not similar to that of the Solar System. In particular, there may be a lot of non-luminous matter (dark matter) in the outskirts of the galaxy. Stars in bound systems must obey the virial theorem. The theorem, together with the measured velocity distribution, can be used to measure the mass distribution in a bound"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_10",
    "chunk": "system, such as elliptical galaxies or globular clusters. With some exceptions, velocity dispersion estimates of elliptical galaxies do not match the predicted velocity dispersion from the observed mass distribution, even assuming complicated distributions of stellar orbits. As with galaxy rotation curves, the obvious way to resolve the discrepancy is to postulate the existence of non-luminous matter. Galaxy clusters are particularly important for dark matter studies since their masses can be estimated in three independent ways: Generally, these three methods are in reasonable agreement that dark matter outweighs visible matter by approximately 5 to 1. The Bullet Cluster is the result of a recent collision of two galaxy clusters. It is of particular note because the location of the center of mass as measured by gravitational lensing is different from the location of the center of mass of visible matter. This is difficult for modified gravity theories, which generally predict lensing around visible matter, to explain. Standard dark matter theory however has no issue: the hot, visible gas in each cluster would be cooled and slowed down by electromagnetic interactions, while dark matter (which does not interact electromagnetically) would not. This leads to the dark matter separating from the visible gas,"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_11",
    "chunk": "producing the separate lensing peak as observed. One of the consequences of general relativity is the gravitational lens. Gravitational lensing occurs when massive objects between a source of light and the observer act as a lens to bend light from this source. Lensing does not depend on the properties of the mass; it only requires there to be a mass. The more massive an object, the more lensing is observed. An example is a cluster of galaxies lying between a more distant source such as a quasar and an observer. In this case, the galaxy cluster will lens the quasar. Strong lensing is the observed distortion of background galaxies into arcs when their light passes through such a gravitational lens. It has been observed around many distant clusters including Abell 1689. By measuring the distortion geometry, the mass of the intervening cluster can be obtained. In the weak regime, lensing does not distort background galaxies into arcs, causing minute distortions instead. By examining the apparent shear deformation of the adjacent background galaxies, the mean distribution of dark matter can be characterized. The measured mass-to-light ratios correspond to dark matter densities predicted by other large-scale structure measurements. Type Ia supernovae can"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_12",
    "chunk": "be used as standard candles to measure extragalactic distances, which can in turn be used to measure how fast the universe has expanded in the past. Data indicates the universe is expanding at an accelerating rate, the cause of which is usually ascribed to dark energy. Since observations indicate the universe is almost flat, it is expected the total energy density of everything in the universe should sum to 1 (Ωtot ≈ 1). The measured dark energy density is ΩΛ ≈ 0.690; the observed ordinary (baryonic) matter energy density is Ωb ≈ 0.0482 and the energy density of radiation is negligible. This leaves a missing Ωdm ≈ 0.258 which nonetheless behaves like matter (see technical definition section above) – dark matter. Large galaxy redshift surveys may be used to make a three-dimensional map of the galaxy distribution. These maps are slightly distorted because distances are estimated from observed redshifts; the redshift contains a contribution from the galaxy's so-called peculiar velocity in addition to the dominant Hubble expansion term. On average, superclusters are expanding more slowly than the cosmic mean due to their gravity, while voids are expanding faster than average. In a redshift map, galaxies in front of a supercluster"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_13",
    "chunk": "have excess radial velocities towards it and have redshifts slightly higher than their distance would imply, while galaxies behind the supercluster have redshifts slightly low for their distance. This effect causes superclusters to appear squashed in the radial direction, and likewise voids are stretched. Their angular positions are unaffected. This effect is not detectable for any one structure since the true shape is not known, but can be measured by averaging over many structures. It was predicted quantitatively by Nick Kaiser in 1987, and first decisively measured in 2001 by the 2dF Galaxy Redshift Survey. Results are in agreement with the Lambda-CDM model. In astronomical spectroscopy, the Lyman-alpha forest is the sum of the absorption lines arising from the Lyman-alpha transition of neutral hydrogen in the spectra of distant galaxies and quasars. Lyman-alpha forest observations can also constrain cosmological models. These constraints agree with those obtained from WMAP data. Although both dark matter and ordinary matter are matter, they do not behave in the same way. In particular, in the early universe, ordinary matter was ionized and interacted strongly with radiation via Thomson scattering. Dark matter does not interact directly with radiation, but it does affect the cosmic microwave background"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_14",
    "chunk": "(CMB) by its gravitational potential (mainly on large scales) and by its effects on the density and velocity of ordinary matter. Ordinary and dark matter perturbations, therefore, evolve differently with time and leave different imprints on the CMB. The CMB is very close to a perfect blackbody but contains very small temperature anisotropies of a few parts in 100,000. A sky map of anisotropies can be decomposed into an angular power spectrum, which is observed to contain a series of acoustic peaks at near-equal spacing but different heights. The locations of these peaks depend on cosmological parameters. Matching theory to data, therefore, constrains cosmological parameters. The CMB anisotropy was first discovered by COBE in 1992, though this had too coarse resolution to detect the acoustic peaks. After the discovery of the first acoustic peak by the balloon-borne BOOMERanG experiment in 2000, the power spectrum was precisely observed by WMAP in 2003–2012, and even more precisely by the Planck spacecraft in 2013–2015. The results support the Lambda-CDM model. The observed CMB angular power spectrum provides powerful evidence in support of dark matter, as its precise structure is well fitted by the Lambda-CDM model, but difficult to reproduce with any competing model"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_15",
    "chunk": "such as modified Newtonian dynamics (MOND). Structure formation refers to the period after the Big Bang when density perturbations collapsed to form stars, galaxies, and clusters. Prior to structure formation, the Friedmann solutions to general relativity describe a homogeneous universe. Later, small anisotropies gradually grew and condensed the homogeneous universe into stars, galaxies and larger structures. Ordinary matter is affected by radiation, which is the dominant element of the universe at very early times. As a result, its density perturbations are washed out and unable to condense into structure. If there were only ordinary matter in the universe, there would not have been enough time for density perturbations to grow into the galaxies and clusters currently seen. Dark matter provides a solution to this problem because it is unaffected by radiation. Therefore, its density perturbations can grow first. The resulting gravitational potential acts as an attractive potential well for ordinary matter collapsing later, speeding up the structure formation process. Baryon acoustic oscillations (BAO) are fluctuations in the density of the visible baryonic matter (normal matter) of the universe on large scales. These are predicted to arise in the Lambda-CDM model due to acoustic oscillations in the photon–baryon fluid of the"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_16",
    "chunk": "early universe and can be observed in the cosmic microwave background angular power spectrum. BAOs set up a preferred length scale for baryons. As the dark matter and baryons clumped together after recombination, the effect is much weaker in the galaxy distribution in the nearby universe, but is detectable as a subtle (≈1 percent) preference for pairs of galaxies to be separated by 147 Mpc, compared to those separated by 130–160 Mpc. This feature was predicted theoretically in the 1990s and then discovered in 2005, in two large galaxy redshift surveys, the Sloan Digital Sky Survey and the 2dF Galaxy Redshift Survey. Combining the CMB observations with BAO measurements from galaxy redshift surveys provides a precise estimate of the Hubble constant and the average matter density in the Universe. The results support the Lambda-CDM model. Dark matter can be divided into cold, warm, and hot categories. These categories refer to velocity rather than an actual temperature, and indicate how far corresponding objects moved due to random motions in the early universe, before they slowed due to cosmic expansion. This distance is called the free streaming length. The categories of dark matter are set with respect to the size of the"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_17",
    "chunk": "collection of mass prior to structure formation that later collapses to form a dwarf galaxy. This collection of mass is sometimes called a protogalaxy. Dark matter particles are classified as cold, warm, or hot if their free streaming length is much smaller (cold), similar to (warm), or much larger (hot) than the protogalaxy of a dwarf galaxy. Mixtures of the above are also possible: a theory of mixed dark matter was popular in the mid-1990s, but was rejected following the discovery of dark energy. The significance of the free streaming length is that the universe began with some primordial density fluctuations from the Big Bang (in turn arising from quantum fluctuations at the microscale). Particles from overdense regions will naturally spread to underdense regions, but because the universe is expanding quickly, there is a time limit for them to do so. Faster particles (hot dark matter) can beat the time limit while slower particles cannot. The particles travel a free streaming length's worth of distance within the time limit; therefore this length sets a minimum scale for later structure formation. Because galaxy-size density fluctuations get washed out by free-streaming, hot dark matter implies the first objects that can form are"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_18",
    "chunk": "huge supercluster-size pancakes, which then fragment into galaxies, while the reverse is true for cold dark matter. Deep-field observations show that galaxies formed first, followed by clusters and superclusters as galaxies clump together, and therefore that most dark matter is cold. This is also the reason why neutrinos, which move at nearly the speed of light and therefore would fall under hot dark matter, cannot make up the bulk of dark matter. The identity of dark matter is unknown, but there are many hypotheses about what dark matter could consist of, as set out in the table below. Dark matter can refer to any substance which interacts predominantly via gravity with visible matter (e.g., stars and planets). Hence in principle it need not be composed of a new type of fundamental particle but could, at least in part, be made up of standard baryonic matter, such as protons or neutrons. Most of the ordinary matter familiar to astronomers, including planets, brown dwarfs, red dwarfs, visible stars, white dwarfs, neutron stars, and black holes, fall into this category. A black hole would ingest both baryonic and non-baryonic matter that comes close enough to its event horizon; afterwards, the distinction between the"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_19",
    "chunk": "two is lost. These massive objects that are hard to detect are collectively known as MACHOs. Some scientists initially hoped that baryonic MACHOs could account for and explain all the dark matter. However, multiple lines of evidence suggest the majority of dark matter is not baryonic: If baryonic matter cannot make up most of dark matter, then dark matter must be non-baryonic. There are two main candidates for non-baryonic dark matter: new particles and primordial black holes. Unlike baryonic matter, nonbaryonic particles do not contribute to the formation of the elements in the early universe (Big Bang nucleosynthesis) and so its presence is felt only via its gravitational effects (such as weak lensing). In addition, some dark matter candidates can interact with themselves (self-interacting dark matter) or with ordinary particles (e.g. WIMPs or Weakly Interacting Massive Particles), possibly resulting in observable by-products such as gamma rays and neutrinos (indirect detection). Candidates abound (see the table above), each with their own strengths and weaknesses. There exists no formal definition of a Weakly Interacting Massive Particle, but broadly, it is an elementary particle which interacts via gravity and any other force (or forces) which is as weak as or weaker than the"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_20",
    "chunk": "weak nuclear force, but also non-vanishing in strength. Many WIMP candidates are expected to have been produced thermally in the early Universe, similarly to the particles of the Standard Model according to Big Bang cosmology, and usually will constitute cold dark matter. Obtaining the correct abundance of dark matter today via thermal production requires a self-annihilation cross section of ⟨ σ v ⟩ ≃ 3 × 10 − 26 c m 3 s − 1 {\\displaystyle \\langle \\sigma v\\rangle \\simeq 3\\times 10^{-26}\\mathrm {cm} ^{3}\\;\\mathrm {s} ^{-1}} , which is roughly what is expected for a new particle in the 100 GeV mass range that interacts via the electroweak force. Because supersymmetric extensions of the Standard Model of particle physics readily predict a new particle with these properties, this apparent coincidence is known as the \"WIMP miracle\", and a stable supersymmetric partner has long been a prime explanation for dark matter. Experimental efforts to detect WIMPs include the search for products of WIMP annihilation, including gamma rays, neutrinos and cosmic rays in nearby galaxies and galaxy clusters; direct detection experiments designed to measure the collision of WIMPs with nuclei in the laboratory, as well as attempts to directly produce WIMPs in"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_21",
    "chunk": "colliders, such as the Large Hadron Collider at CERN. In the early 2010s, results from direct-detection experiments along with the lack of evidence for supersymmetry at the Large Hadron Collider (LHC) experiment have cast doubt on the simplest WIMP hypothesis. Axions are hypothetical elementary particles originally theorized in 1978 independently by Frank Wilczek and Steven Weinberg as the Goldstone boson of Peccei–Quinn theory, which had been proposed in 1977 to solve the strong CP problem in quantum chromodynamics (QCD). QCD effects produce an effective periodic potential in which the axion field moves. Expanding the potential about one of its minima, one finds that the product of the axion mass with the axion decay constant is determined by the topological susceptibility of the QCD vacuum. An axion with mass much less than 60 keV is long-lived and weakly interacting: A perfect dark matter candidate. The oscillations of the axion field about the minimum of the effective potential, the so-called misalignment mechanism, generate a cosmological population of cold axions with an abundance depending on the mass of the axion. With a mass above 5 μeV/c (10 times the electron mass) axions could account for dark matter, and thus be both a dark-matter"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_22",
    "chunk": "candidate and a solution to the strong CP problem. If inflation occurs at a low scale and lasts sufficiently long, the axion mass can be as low as 1 peV/c. Because axions have extremely low mass, their de Broglie wavelength is very large, in turn meaning that quantum effects could help resolve the small-scale problems of the Lambda-CDM model. A single ultralight axion with a decay constant at the grand unified theory scale provides the correct relic density without fine-tuning. Axions as a dark matter candidate have gained in popularity in recent years, because of the non-detection of WIMPS. Primordial black holes are hypothetical black holes that formed soon after the Big Bang. In the inflationary era and early radiation-dominated universe, extremely dense pockets of subatomic matter may have been tightly packed to the point of gravitational collapse, creating primordial black holes without the supernova compression typically needed to make black holes today. Because the creation of primordial black holes would pre-date the first stars, they are not limited to the narrow mass range of stellar black holes and also not classified as baryonic dark matter. The idea that black holes could form in the early universe was first suggested"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_23",
    "chunk": "by Yakov Zeldovich and Igor Dmitriyevich Novikov in 1967, and independently by Stephen Hawking in 1971. It quickly became clear that such black holes might account for at least part of dark matter. Primordial black holes as a dark matter candidate has the major advantage that it is based on a well-understood theory (General Relativity) and objects (black holes) that are already known to exist. However, producing primordial black holes requires exotic cosmic inflation or physics beyond the standard model of particle physics, and might also require fine-tuning. Primordial black holes can also span nearly the entire possible mass range, from atom-sized to supermassive. The idea that primordial black holes make up dark matter gained prominence in 2015 following results of gravitational wave measurements which detected the merger of intermediate-mass black holes. Black holes with about 30 solar masses are not predicted to form by either stellar collapse (typically less than 15 solar masses) or by the merger of black holes in galactic centers (millions or billions of solar masses), which suggests that the detected black holes might be primordial. A later survey of about a thousand supernovae detected no gravitational lensing events, when about eight would be expected if"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_24",
    "chunk": "intermediate-mass primordial black holes above a certain mass range accounted for over 60% of dark matter. However, that study assumed that all black holes have the same or similar mass to the LIGO/Virgo mass range, which might not be the case (as suggested by subsequent James Webb Space Telescope observations). The possibility that atom-sized primordial black holes account for a significant fraction of dark matter was ruled out by measurements of positron and electron fluxes outside the Sun's heliosphere by the Voyager 1 spacecraft. Tiny black holes are theorized to emit Hawking radiation. However the detected fluxes were too low and did not have the expected energy spectrum, suggesting that tiny primordial black holes are not widespread enough to account for dark matter. Nonetheless, research and theories proposing dense dark matter accounts for dark matter continue as of 2018, including approaches to dark matter cooling, and the question remains unsettled. In 2019, the lack of microlensing effects in the observation of Andromeda suggests that tiny black holes do not exist. Nonetheless, there still exists a largely unconstrained mass range smaller than that which can be limited by optical microlensing observations, where primordial black holes may account for all dark matter."
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_25",
    "chunk": "The last major possibility is that general relativity, the theory underpinning modern cosmology, is incorrect. General relativity is well-tested on Solar System scales, but its validity on galactic or cosmological scales has not been well proven. A suitable modification to general relativity can conceivably eliminate the need for dark matter. The best-known theories of this class are MOND and its relativistic generalization tensor–vector–scalar gravity (TeVeS), f(R) gravity, negative mass, dark fluid, and entropic gravity. Alternative theories abound. A problem with modifying gravity is that observational evidence for dark matter – let alone general relativity – comes from so many independent approaches (see the \"observational evidence\" section above). Explaining any individual observation is possible but explaining all of them in the absence of dark matter is very difficult. Nonetheless, there have been some scattered successes for alternative hypotheses, such as a 2016 test of gravitational lensing in entropic gravity and a 2020 measurement of a unique MOND effect. The prevailing opinion among most astrophysicists is that while modifications to general relativity can conceivably explain part of the observational evidence, there is probably enough data to conclude there must be some form of dark matter present in the universe. If dark matter"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_26",
    "chunk": "is composed of weakly interacting particles, then an obvious question is whether it can form objects equivalent to planets, stars, or black holes. Historically, the answer has been it cannot, because of two factors: If dark matter is made up of subatomic particles, then millions, possibly billions, of such particles must pass through every square centimeter of the Earth each second. Many experiments aim to test this hypothesis. Although WIMPs have been the main search candidates, axions have drawn renewed attention, with the Axion Dark Matter Experiment (ADMX) searches for axions and many more planned in the future. Another candidate is heavy hidden sector particles which only interact with ordinary matter via gravity. These experiments can be divided into two classes: direct detection experiments, which search for the scattering of dark matter particles off atomic nuclei within a detector; and indirect detection, which look for the products of dark matter particle annihilations or decays. Direct detection experiments aim to observe low-energy recoils (typically a few keVs) of nuclei induced by interactions with particles of dark matter, which (in theory) are passing through the Earth. After such a recoil, the nucleus will emit energy in the form of scintillation light or"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_27",
    "chunk": "phonons as they pass through sensitive detection apparatus. To do so effectively, it is crucial to maintain an extremely low background, which is the reason why such experiments typically operate deep underground, where interference from cosmic rays is minimized. Examples of underground laboratories with direct detection experiments include the Stawell mine, the Soudan mine, the SNOLAB underground laboratory at Sudbury, the Gran Sasso National Laboratory, the Canfranc Underground Laboratory, the Boulby Underground Laboratory, the Deep Underground Science and Engineering Laboratory and the China Jinping Underground Laboratory. These experiments mostly use either cryogenic or noble liquid detector technologies. Cryogenic detectors operating at temperatures below 100 mK, detect the heat produced when a particle hits an atom in a crystal absorber such as germanium. Noble liquid detectors detect scintillation produced by a particle collision in liquid xenon or argon. Cryogenic detector experiments include such projects as CDMS, CRESST, EDELWEISS, and EURECA, while noble liquid experiments include LZ, XENON, DEAP, ArDM, WARP, DarkSide, PandaX, and LUX, the Large Underground Xenon experiment. Both of these techniques focus strongly on their ability to distinguish background particles (which predominantly scatter off electrons) from dark matter particles (that scatter off nuclei). Other experiments include SIMPLE and PICASSO,"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_28",
    "chunk": "which use alternative methods in their attempts to detect dark matter. Currently there has been no well-established claim of dark matter detection from a direct detection experiment, leading instead to strong upper limits on the mass and interaction cross section with nucleons of such dark matter particles. The DAMA/NaI and more recent DAMA/LIBRA experimental collaborations have detected an annual modulation in the rate of events in their detectors, which they claim is due to dark matter. This results from the expectation that as the Earth orbits the Sun, the velocity of the detector relative to the dark matter halo will vary by a small amount. This claim is so far unconfirmed and in contradiction with negative results from other experiments such as LUX, SuperCDMS and XENON100. A special case of direct detection experiments covers those with directional sensitivity. This is a search strategy based on the motion of the Solar System around the Galactic Center. A low-pressure time projection chamber makes it possible to access information on recoiling tracks and constrain WIMP-nucleus kinematics. WIMPs coming from the direction in which the Sun travels (approximately towards Cygnus) may then be separated from background, which should be isotropic. Directional dark matter experiments"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_29",
    "chunk": "include DMTPC, DRIFT, Newage and MIMAC. Indirect detection experiments search for the products of the self-annihilation or decay of dark matter particles in outer space. For example, in regions of high dark matter density (e.g., the centre of the Milky Way) two dark matter particles could annihilate to produce gamma rays or Standard Model particle–antiparticle pairs. Alternatively, if a dark matter particle is unstable, it could decay into Standard Model (or other) particles. These processes could be detected indirectly through an excess of gamma rays, antiprotons or positrons emanating from high density regions in the Milky Way and other galaxies. A major difficulty inherent in such searches is that various astrophysical sources can mimic the signal expected from dark matter, and so multiple signals are likely required for a conclusive discovery. A few of the dark matter particles passing through the Sun or Earth may scatter off atoms and lose energy. Thus dark matter may accumulate at the center of these bodies, increasing the chance of collision/annihilation. This could produce a distinctive signal in the form of high-energy neutrinos. Such a signal would be strong indirect proof of WIMP dark matter. High-energy neutrino telescopes such as AMANDA, IceCube and ANTARES"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_30",
    "chunk": "are searching for this signal. The detection by LIGO in September 2015 of gravitational waves opens the possibility of observing dark matter in a new way, particularly if it is in the form of primordial black holes. Many experimental searches have been undertaken to look for such emission from dark matter annihilation or decay, examples of which follow. The Energetic Gamma Ray Experiment Telescope observed more gamma rays in 2008 than expected from the Milky Way, but scientists concluded this was most likely due to incorrect estimation of the telescope's sensitivity. The Fermi Gamma-ray Space Telescope is searching for similar gamma rays. In 2009, an as yet unexplained surplus of gamma rays from the Milky Way's galactic center was found in Fermi data. This Galactic Center GeV excess might be due to dark matter annihilation or to a population of pulsars. In April 2012, an analysis of previously available data from Fermi's Large Area Telescope instrument produced statistical evidence of a 130 GeV signal in the gamma radiation coming from the center of the Milky Way. WIMP annihilation was seen as the most probable explanation. At higher energies, ground-based gamma-ray telescopes have set limits on the annihilation of dark matter"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_31",
    "chunk": "in dwarf spheroidal galaxies and in clusters of galaxies. The PAMELA experiment (launched in 2006) detected excess positrons. They could be from dark matter annihilation or from pulsars. No excess antiprotons were observed. In 2013, results from the Alpha Magnetic Spectrometer on the International Space Station indicated excess high-energy cosmic rays which could be due to dark matter annihilation. An alternative approach to the detection of dark matter particles in nature is to produce them in a laboratory. Experiments with the Large Hadron Collider (LHC) may be able to detect dark matter particles produced in collisions of the LHC proton beams. Because a dark matter particle should have negligible interactions with normal visible matter, it may be detected indirectly as (large amounts of) missing energy and momentum that escape the detectors, provided other (non-negligible) collision products are detected. Constraints on dark matter also exist from the LEP experiment using a similar principle, but probing the interaction of dark matter particles with electrons rather than quarks. Any discovery from collider searches must be corroborated by discoveries in the indirect or direct detection sectors to prove that the particle discovered is, in fact, dark matter. Dark matter regularly appears as a topic"
  },
  {
    "source": "Dark matter.txt",
    "chunk_id": "Dark matter.txt_32",
    "chunk": "in hybrid periodicals that cover both factual scientific topics and science fiction, and dark matter itself has been referred to as \"the stuff of science fiction\". Mention of dark matter is made in works of fiction. In such cases, it is usually attributed extraordinary physical or magical properties, thus becoming inconsistent with the hypothesized properties of dark matter in physics and cosmology. For example: More broadly, the phrase \"dark matter\" is used metaphorically in fiction to evoke the unseen or invisible."
  },
  {
    "source": "Dark-energy star.txt",
    "chunk_id": "Dark-energy star.txt_0",
    "chunk": "# Dark-energy star A dark-energy star is a hypothetical compact astrophysical object, which a minority of physicists think might constitute an alternative explanation for observations of astronomical black hole candidates. The concept was proposed by physicist George Chapline. The theory states that infalling matter is converted into vacuum energy or dark energy, as the matter falls through the event horizon. The space within the event horizon would end up with a large value for the cosmological constant and have negative pressure to exert against gravity. There would be no information-destroying singularity. In March 2005, physicist George Chapline claimed that quantum mechanics makes it a \"near certainty\" that black holes do not exist and are instead dark-energy stars. The dark-energy star is a different concept from that of a gravastar. Dark-energy stars were first proposed because in quantum physics, absolute time is required; however, in general relativity, an object falling towards a black hole would, to an outside observer, seem to have time pass infinitely slowly at the event horizon. The object itself would feel as if time flowed normally. In order to reconcile quantum mechanics with black holes, Chapline theorized that a phase transition in the phase of space occurs"
  },
  {
    "source": "Dark-energy star.txt",
    "chunk_id": "Dark-energy star.txt_1",
    "chunk": "at the event horizon. He based his ideas on the physics of superfluids. As a column of superfluid grows taller, at some point, density increases, slowing down the speed of sound, so that it approaches zero. However, at that point, quantum physics makes sound waves dissipate their energy into the superfluid, so that the zero sound speed condition is never encountered. In the dark-energy star hypothesis, infalling matter approaching the event horizon decays into successively lighter particles. Nearing the event horizon, environmental effects accelerate proton decay. This may account for high-energy cosmic-ray sources and positron sources in the sky. When the matter falls through the event horizon, the energy equivalent of some or all of that matter is converted into dark energy. This negative pressure counteracts the mass the star gains, avoiding a singularity. The negative pressure also gives a very high number for the cosmological constant. Furthermore, 'primordial' dark-energy stars could form by fluctuations of spacetime itself, which is analogous to \"blobs of liquid condensing spontaneously out of a cooling gas\". This not only alters the understanding of black holes, but has the potential to explain the dark energy and dark matter that are indirectly observed."
  },
  {
    "source": "Democritus.txt",
    "chunk_id": "Democritus.txt_0",
    "chunk": "# Democritus Democritus (/dɪˈmɒkrɪtəs/, dim-OCK-rit-əs; Greek: Δημόκριτος, Dēmókritos, meaning \"chosen of the people\"; c. 460 – c. 370 BC) was an Ancient Greek pre-Socratic philosopher from Abdera, primarily remembered today for his formulation of an atomic theory of the universe. Democritus wrote extensively on a wide variety of topics. None of Democritus' original work has survived, except through second-hand references. Many of these references come from Aristotle, who viewed him as an important rival in the field of natural philosophy. He was known in antiquity as the ‘laughing philosopher’ because of his emphasis on the value of cheerfulness. Democritus was born in Abdera, on the coast of Thrace. He was a polymath and prolific writer, producing nearly eighty treatises on subjects such as poetry, harmony, military tactics, and Babylonian theology. He traveled extensively, visiting Egypt and Persia, but was not particularly impressed by these countries. He once remarked that he would rather uncover a single scientific explanation than become the king of Persia. Although many anecdotes about Democritus' life survive, their authenticity cannot be verified and modern scholars doubt their accuracy. Ancient accounts of his life have claimed that he lived to a very old age, with some writers claiming"
  },
  {
    "source": "Democritus.txt",
    "chunk_id": "Democritus.txt_1",
    "chunk": "that he was over a hundred years old at the time of his death. Democritus wrote on ethics as well as physics. Democritus was a student of Leucippus. Early sources such as Aristotle and Theophrastus credit Leucippus with creating atomism and sharing its ideas with Democritus, but later sources credit only Democritus, making it hard to distinguish their individual contributions. He concluded that divisibility of matter comes to an end, and the smallest possible fragments must be bodies with sizes and shapes, although the exact argument for this conclusion of his is not known. The smallest and indivisible bodies he called \"atoms\". Atoms, Democritus believed, are too small to be detected by the senses; they are infinite in numbers and come in infinitely many varieties, and they have existed forever and that these atoms are in constant motion in the void or vacuum. The middle-sized objects of everyday life are complexes of atoms that are brought together by random collisions, differing in kind based on the variations among their constituent atoms. For Democritus, the only true realities are atoms and the void. What we perceive as water, fire, plants, or humans are merely combinations of atoms in the void. The"
  },
  {
    "source": "Democritus.txt",
    "chunk_id": "Democritus.txt_2",
    "chunk": "sensory qualities we experience are not real; they exist only by convention. Of the mass of atoms, Democritus said, \"The more any indivisible exceeds, the heavier it is.\" However, his exact position on atomic weight is disputed. His exact contributions are difficult to disentangle from those of his mentor Leucippus, as they are often mentioned together in texts. Their speculation on atoms, taken from Leucippus, bears a passing and partial resemblance to the 19th-century understanding of atomic structure that has led some to regard Democritus as more of a scientist than other Greek philosophers; however, their ideas rested on very different bases. Democritus, along with Leucippus and Epicurus, proposed the earliest views on the shapes and connectivity of atoms. They reasoned that the solidness of the material corresponded to the shape of the atoms involved. Using analogies from humans' sense experiences, he gave a picture or an image of an atom that distinguished them from each other by their shape, their size, and the arrangement of their parts. Moreover, connections were explained by material links in which single atoms were supplied with attachments: some with hooks and eyes, others with balls and sockets. The Democritean atom is an inert solid"
  },
  {
    "source": "Democritus.txt",
    "chunk_id": "Democritus.txt_3",
    "chunk": "that excludes other bodies from its volume and interacts with other atoms mechanically. Quantum-mechanical atoms are similar in that their motion can be described by mechanics in addition to their electric, magnetic and quantum interactions. They are different in that they can be split into protons, neutrons, and electrons. The elementary particles are similar to Democritean atoms in that they are indivisible but their collisions are governed purely by quantum physics. Fermions observe the Pauli exclusion principle, which is similar to the Democritean principle that atoms exclude other bodies from their volume. However, bosons do not, with the prime example being the elementary particle photon. The theory of the atomists appears to be more nearly aligned with that of modern science than any other theory of antiquity. However, the similarity with modern concepts of science can be confusing when trying to understand where the hypothesis came from. Classical atomists could not have had an empirical basis for modern concepts of atoms and molecules. The atomistic void hypothesis was a response to the paradoxes of Parmenides and Zeno, the founders of metaphysical logic, who put forth difficult-to-answer arguments in favor of the idea that there can be no movement. They held"
  },
  {
    "source": "Democritus.txt",
    "chunk_id": "Democritus.txt_4",
    "chunk": "that any movement would require a void—which is nothing—but a nothing cannot exist. The Parmenidean position was \"You say there is a void; therefore the void is not nothing; therefore there is not the void.\" The position of Parmenides appeared validated by the observation that where there seems to be nothing there is air, and indeed even where there is not matter there is something, for instance light waves. The atomists agreed that motion required a void, but simply rejected the argument of Parmenides on the grounds that motion was an observable fact. Therefore, they asserted, there must be a void. Democritus held that originally the universe was composed of nothing but tiny atoms churning in chaos, until they collided together to form larger units—including the earth and everything on it. He surmised that there are many worlds, some growing, some decaying; some with no sun or moon, some with several. He held that every world has a beginning and an end and that a world could be destroyed by collision with another world. Democritus was also a pioneer of mathematics and geometry in particular. According to Archimedes, Democritus was among the first to observe that a cone and pyramid"
  },
  {
    "source": "Democritus.txt",
    "chunk_id": "Democritus.txt_5",
    "chunk": "with the same base area and height has one-third the volume of a cylinder or prism respectively, a result which Archimedes states was later proved by Eudoxus of Cnidus. Plutarch also reports that Democritus worked on a problem involving the cross-section of a cone that Thomas Heath suggests may be an early version of infinitesimal calculus. Democritus thought that the first humans lived an anarchic and animal sort of life, foraging individually and living off the most palatable herbs and the fruit which grew wild on the trees, until fear of wild animals drove them together into societies. He believed that these early people had no language, but that they gradually began to articulate their expressions, establishing symbols for every sort of object, and in this manner came to understand each other. He says that the earliest men lived laboriously, having none of the utilities of life; clothing, houses, fire, domestication, and farming were unknown to them. Democritus presents the early period of mankind as one of learning by trial and error, and says that each step slowly led to more discoveries; they took refuge in the caves in winter, stored fruits that could be preserved, and through reason and"
  },
  {
    "source": "Democritus.txt",
    "chunk_id": "Democritus.txt_6",
    "chunk": "keenness of mind came to build upon each new idea. Democritus was eloquent on ethical topics. Some sixty pages of his fragments, as recorded in Diels–Kranz, are devoted to moral counsel. The ethics and politics of Democritus come to us mostly in the form of maxims. In placing the quest for happiness at the center of moral philosophy, he was followed by almost every moralist of antiquity. The most common maxims associated with him are \"Accept favours only if you plan to do greater favours in return\", and he is also believed to impart some controversial advice such as \"It is better not to have any children, for to bring them up well takes great trouble and care, and seeing them grow up badly is the cruellest of all pains\". He also wrote a treatise on the purpose of life and the nature of happiness. He held that \"happiness was not to be found in riches but in the goods of the soul and one should not take pleasure in mortal things\". Another saying that is often attributed to him is \"The hopes of the educated were better than the riches of the ignorant\". He also stated that \"the cause"
  },
  {
    "source": "Democritus.txt",
    "chunk_id": "Democritus.txt_7",
    "chunk": "of sin is ignorance of what is better\", which become a central notion later in the Socratic moral thought. Another idea he propounded which was later echoed in the Socratic moral thought was the maxim that \"you are better off being wronged than doing wrong\". His other moral notions went contrary to the then prevalent views such as his idea that \"A good person not only refrains from wrongdoing but does not even want to do wrong\", for the generally held notion back then was that virtue reaches it apex when it triumphs over conflicting human passions. Later Greek historians consider Democritus to have established aesthetics as a subject of investigation and study, as he wrote theoretically on poetry and fine art long before authors such as Aristotle. Specifically, Thrasyllus identified six works in the philosopher's oeuvre which had belonged to aesthetics as a discipline, but only fragments of the relevant works are extant; hence of all Democritus writings on these matters, only a small percentage of his thoughts and ideas can be known. Diogenes Laertius attributes several works to Democritus, but none of them have survived in a complete form. A collections of sayings credited to Democritus have been"
  },
  {
    "source": "Democritus.txt",
    "chunk_id": "Democritus.txt_8",
    "chunk": "preserved by Stobaeus, as well as a collection of sayings ascribed to Democrates which some scholars including Diels and Kranz have also ascribed to Democritus. Diogenes Laertius claims that Plato disliked Democritus so much that he wished to have all of his books burned. He was nevertheless well known to his fellow northern-born philosopher Aristotle, and was the teacher of Protagoras. Democritus is evoked by English writer Samuel Johnson in his poem, The Vanity of Human Wishes (1749), ll. 49–68, and summoned to \"arise on earth, /With chearful wisdom and instructive mirth, /See motley life in modern trappings dress'd, /And feed with varied fools th'eternal jest.\""
  },
  {
    "source": "Equatorial bulge.txt",
    "chunk_id": "Equatorial bulge.txt_0",
    "chunk": "# Equatorial bulge An equatorial bulge is a difference between the equatorial and polar diameters of a planet, due to the centrifugal force exerted by the rotation about the body's axis. A rotating body tends to form an oblate spheroid rather than a sphere. The planet Earth has a rather slight equatorial bulge; its equatorial diameter is about 43 km (27 mi) greater than its polar diameter, with a difference of about 1⁄298 of the equatorial diameter. If Earth were scaled down to a globe with an equatorial diameter of 1 metre (3.3 ft), that difference would be only 3 mm (0.12 in). While too small to notice visually, that difference is still more than twice the largest deviations of the actual surface from the ellipsoid, including the tallest mountains and deepest oceanic trenches. Earth's rotation also affects the sea level, the imaginary surface used as a reference frame from which to measure altitudes. This surface coincides with the mean water surface level in oceans, and is extrapolated over land by taking into account the local gravitational potential and the centrifugal force. The difference of the radii is thus about 21 km (13 mi). An observer standing at sea level"
  },
  {
    "source": "Equatorial bulge.txt",
    "chunk_id": "Equatorial bulge.txt_1",
    "chunk": "on either pole, therefore, is 21 km (13 mi) closer to Earth's center than if standing at sea level on the Equator. As a result, the highest point on Earth, measured from the center and outwards, is the peak of Mount Chimborazo in Ecuador rather than Mount Everest. But since the ocean also bulges, like Earth and its atmosphere, Chimborazo is not as high above sea level as Everest is. Similarly the lowest point on Earth, measured from the center and outwards, is the Litke Deep in the Arctic Ocean rather than Challenger Deep in the Pacific Ocean. But since the ocean also flattens, like Earth and its atmosphere, Litke Deep is not as low below sea level as Challenger Deep is. More precisely, Earth's surface is usually approximated by an ideal oblate ellipsoid, for the purposes of defining precisely the latitude and longitude grid for cartography, as well as the \"center of the Earth\". In the WGS-84 standard Earth ellipsoid, widely used for map-making and the GPS system, Earth's radius is assumed to be 6378.137 km (3963.191 mi) to the Equator and 6356.7523142 km (3949.9027642 mi) to either pole, meaning a difference of 21.3846858 km (13.2878277 mi) between the"
  },
  {
    "source": "Equatorial bulge.txt",
    "chunk_id": "Equatorial bulge.txt_2",
    "chunk": "radii or 42.7693716 km (26.5756554 mi) between the diameters, and a relative flattening of 1/298.257223563. The ocean surface is much closer to this standard ellipsoid than the solid surface of Earth is. Gravity tends to contract a celestial body into a sphere, the shape for which all the mass is as close to the center of gravity as possible. Rotation causes a distortion from this spherical shape; a common measure of the distortion is the flattening (sometimes called ellipticity or oblateness), which can depend on a variety of factors including the size, angular velocity, density, and elasticity. A way for one to get a feel for the type of equilibrium involved is to imagine someone seated in a spinning swivel chair and holding a weight in each hand; if the individual pulls the weights inward towards them, work is being done and their rotational kinetic energy increases. The increase in rotation rate is so strong that at the faster rotation rate the required centripetal force is larger than with the starting rotation rate. Something analogous to this occurs in planet formation. Matter first coalesces into a slowly rotating disk-shaped distribution, and collisions and friction convert kinetic energy to heat, which"
  },
  {
    "source": "Equatorial bulge.txt",
    "chunk_id": "Equatorial bulge.txt_3",
    "chunk": "allows the disk to self-gravitate into a very oblate spheroid. As long as the proto-planet is still too oblate to be in equilibrium, the release of gravitational potential energy on contraction keeps driving the increase in rotational kinetic energy. As the contraction proceeds, the rotation rate keeps going up, hence the required force for further contraction keeps going up. There is a point where the increase of rotational kinetic energy on further contraction would be larger than the release of gravitational potential energy. The contraction process can only proceed up to that point, so it halts there. As long as there is no equilibrium there can be violent convection, and as long as there is violent convection friction can convert kinetic energy to heat, draining rotational kinetic energy from the system. When the equilibrium state has been reached then large scale conversion of kinetic energy to heat ceases. In that sense the equilibrium state is the lowest state of energy that can be reached. The Earth's rotation rate is still slowing down, though gradually, by about two thousandths of a second per rotation every 100 years. Estimates of how fast the Earth was rotating in the past vary, because it"
  },
  {
    "source": "Equatorial bulge.txt",
    "chunk_id": "Equatorial bulge.txt_4",
    "chunk": "is not known exactly how the moon was formed. Estimates of the Earth's rotation 500 million years ago are around 20 modern hours per \"day\". The Earth's rate of rotation is slowing down mainly because of tidal interactions with the Moon and the Sun. Since the solid parts of the Earth are ductile, the Earth's equatorial bulge has been decreasing in step with the decrease in the rate of rotation. Because of a planet's rotation around its own axis, the gravitational acceleration is less at the equator than at the poles. In the 17th century, following the invention of the pendulum clock, French scientists found that clocks sent to French Guiana, on the northern coast of South America, ran slower than their exact counterparts in Paris. Measurements of the acceleration due to gravity at the equator must also take into account the planet's rotation. Any object that is stationary with respect to the surface of the Earth is actually following a circular trajectory, circumnavigating the Earth's axis. Pulling an object into such a circular trajectory requires a force. The acceleration that is required to circumnavigate the Earth's axis along the equator at one revolution per sidereal day is 0.0339 m/s."
  },
  {
    "source": "Equatorial bulge.txt",
    "chunk_id": "Equatorial bulge.txt_5",
    "chunk": "Providing this acceleration decreases the effective gravitational acceleration. At the Equator, the effective gravitational acceleration is 9.7805 m/s. This means that the true gravitational acceleration at the Equator must be 9.8144 m/s (9.7805 + 0.0339 = 9.8144). At the poles, the gravitational acceleration is 9.8322 m/s. The difference of 0.0178 m/s between the gravitational acceleration at the poles and the true gravitational acceleration at the Equator is because objects located on the Equator are about 21 km (13 mi) further away from the center of mass of the Earth than at the poles, which corresponds to a smaller gravitational acceleration. In summary, there are two contributions to the fact that the effective gravitational acceleration is less strong at the equator than at the poles. About 70% of the difference is contributed by the fact that objects circumnavigate the Earth's axis, and about 30% is due to the non-spherical shape of the Earth. The diagram illustrates that on all latitudes the effective gravitational acceleration is decreased by the requirement of providing a centripetal force; the decreasing effect is strongest on the Equator. The fact that the Earth's gravitational field slightly deviates from being spherically symmetrical also affects the orbits of satellites"
  },
  {
    "source": "Equatorial bulge.txt",
    "chunk_id": "Equatorial bulge.txt_6",
    "chunk": "through secular orbital precessions. They depend on the orientation of the Earth's symmetry axis in the inertial space, and, in the general case, affect all the Keplerian orbital elements with the exception of the semimajor axis. If the reference z axis of the coordinate system adopted is aligned along the Earth's symmetry axis, then only the longitude of the ascending node Ω, the argument of pericenter ω and the mean anomaly M undergo secular precessions. Such perturbations, which were earlier used to map the Earth's gravitational field from space, may play a relevant disturbing role when satellites are used to make tests of general relativity because the much smaller relativistic effects are qualitatively indistinguishable from the oblateness-driven disturbances. The flattening f {\\displaystyle f} for the equilibrium configuration of a self-gravitating spheroid, composed of uniform density incompressible fluid, rotating steadily about some fixed axis, for a small amount of flattening, is approximated by: f = a e − a p a = 5 4 ω 2 a 3 G M = 15 π 4 1 G T 2 ρ {\\displaystyle f={\\frac {a_{e}-a_{p}}{a}}={\\frac {5}{4}}{\\frac {\\omega ^{2}a^{3}}{GM}}={\\frac {15\\pi }{4}}{\\frac {1}{GT^{2}\\rho }}} Real flattening is smaller due to mass concentration in the center of"
  },
  {
    "source": "Equatorial bulge.txt",
    "chunk_id": "Equatorial bulge.txt_7",
    "chunk": "celestial bodies."
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_0",
    "chunk": "# Europa (moon) Europa /jʊˈroʊpə/ , or Jupiter II, is the smallest of the four Galilean moons orbiting Jupiter, and the sixth-closest to the planet of all the 97 known moons of Jupiter. It is also the sixth-largest moon in the Solar System. Europa was discovered independently by Simon Marius and Galileo Galilei and was named (by Marius) after Europa, the Phoenician mother of King Minos of Crete and lover of Zeus (the Greek equivalent of the Roman god Jupiter). Slightly smaller than Earth's Moon, Europa is made of silicate rock and has a water-ice crust and probably an iron–nickel core. It has a very thin atmosphere, composed primarily of oxygen. Its geologically young white-beige surface is striated by light tan cracks and streaks, with very few impact craters. In addition to Earth-bound telescope observations, Europa has been examined by a succession of space-probe flybys, the first occurring in the early 1970s. In September 2022, the Juno spacecraft flew within about 320 km (200 miles) of Europa for a more recent close-up view. Europa has the smoothest surface of any known solid object in the Solar System. The apparent youth and smoothness of the surface is due to a water"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_1",
    "chunk": "ocean beneath the surface, which could conceivably harbor extraterrestrial life, although such life would most likely be that of single celled organisms and bacteria-like creatures. The predominant model suggests that heat from tidal flexing causes the ocean to remain liquid and drives ice movement similar to plate tectonics, absorbing chemicals from the surface into the ocean below. Sea salt from a subsurface ocean may be coating some geological features on Europa, suggesting that the ocean is interacting with the sea floor. This may be important in determining whether Europa could be habitable. In addition, the Hubble Space Telescope detected water vapor plumes similar to those observed on Saturn's moon Enceladus, which are thought to be caused by erupting cryogeysers. In May 2018, astronomers provided supporting evidence of water plume activity on Europa, based on an updated analysis of data obtained from the Galileo space probe, which orbited Jupiter from 1995 to 2003. Such plume activity could help researchers in a search for life from the subsurface Europan ocean without having to land on the moon. In March 2024, astronomers reported that the surface of Europa may have much less oxygen than previously inferred. The Galileo mission, launched in 1989, provides"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_2",
    "chunk": "the bulk of current data on Europa. No spacecraft has yet landed on Europa, although there have been several proposed exploration missions. The European Space Agency's Jupiter Icy Moon Explorer (JUICE) is a mission to Ganymede launched on 14 April 2023, that will include two flybys of Europa. NASA's Europa Clipper was launched on 14 October 2024. Europa, along with Jupiter's three other large moons, Io, Ganymede, and Callisto, was discovered by Galileo Galilei on 8 January 1610, and possibly independently by Simon Marius. On 7 January, Galileo had observed Io and Europa together using a 20×-magnification refracting telescope at the University of Padua, but the low resolution could not separate the two objects. The following night, he saw Io and Europa for the first time as separate bodies. The moon is the namesake of Europa, in Greek mythology the daughter of the Phoenician king of Tyre. Like all the Galilean satellites, Europa is named after a lover of Zeus, the Greek counterpart of Jupiter. Europa was courted by Zeus and became the queen of Crete. The naming scheme was suggested by Simon Marius, who attributed the proposal to Johannes Kepler: Jupiter is much blamed by the poets on account"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_3",
    "chunk": "of his irregular loves. Three maidens are especially mentioned as having been clandestinely courted by Jupiter with success. Io, daughter of the River Inachus, Callisto of Lycaon, Europa of Agenor. Then there was Ganymede, the handsome son of King Tros, whom Jupiter, having taken the form of an eagle, transported to heaven on his back, as poets fabulously tell... I think, therefore, that I shall not have done amiss if the First is called by me Io, the Second Europa, the Third, on account of its majesty of light, Ganymede, the Fourth Callisto... The names fell out of favor for a considerable time and were not revived in general use until the mid-20th century. In much of the earlier astronomical literature, Europa is simply referred to by its Roman numeral designation as Jupiter II (a system also introduced by Galileo) or as the \"second satellite of Jupiter\". In 1892, the discovery of Amalthea, whose orbit lay closer to Jupiter than those of the Galilean moons, pushed Europa to the third position. The Voyager probes discovered three more inner satellites in 1979, so Europa is now counted as Jupiter's sixth satellite, though it is still referred to as Jupiter II. The"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_4",
    "chunk": "adjectival form has stabilized as Europan. Planetary moons other than Earth's were never given symbols in the astronomical literature. Denis Moskowitz, a software engineer who designed most of the dwarf planet symbols, proposed a Greek epsilon (the initial of Europa) combined with the cross-bar of the Jupiter symbol as the symbol of Europa (). This symbol is not widely used. Europa orbits Jupiter in roughly 3.55 days, with an orbital radius of about 670,900 km. With an orbital eccentricity of only 0.009, the orbit itself is nearly circular, and the orbital inclination relative to Jupiter's equatorial plane is small, at 0.470°. Like its fellow Galilean satellites, Europa is tidally locked to Jupiter, with one hemisphere of Europa constantly facing Jupiter. Because of this, there is a sub-Jovian point on Europa's surface, from which Jupiter would appear to hang directly overhead. Europa's prime meridian is a line passing through this point. Research suggests that tidal locking may not be full, as a non-synchronous rotation has been proposed: Europa spins faster than it orbits, or at least did so in the past. This suggests an asymmetry in internal mass distribution and that a layer of subsurface liquid separates the icy crust from"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_5",
    "chunk": "the rocky interior. The slight eccentricity of Europa's orbit, maintained by gravitational disturbances from the other Galileans, causes Europa's sub-Jovian point to oscillate around a mean position. As Europa comes slightly nearer to Jupiter, Jupiter's gravitational attraction increases, causing Europa to elongate towards and away from it. As Europa moves slightly away from Jupiter, Jupiter's gravitational force decreases, causing Europa to relax back into a more spherical shape, and creating tides in its ocean. The orbital eccentricity of Europa is continuously pumped by its mean-motion resonance with Io. Thus, the tidal flexing kneads Europa's interior and gives it a source of heat, possibly allowing its ocean to stay liquid while driving subsurface geological processes. The ultimate source of this energy is Jupiter's rotation, which is tapped by Io through the tides it raises on Jupiter and is transferred to Europa and Ganymede by the orbital resonance. Analysis of the unique cracks lining Europa yielded evidence that it likely spun around a tilted axis at some point in time. If correct, this would explain many of Europa's features. Europa's immense network of crisscrossing cracks serves as a record of the stresses caused by massive tides in its global ocean. Europa's tilt"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_6",
    "chunk": "could influence calculations of how much of its history is recorded in its frozen shell, how much heat is generated by tides in its ocean, and even how long the ocean has been liquid. Its ice layer must stretch to accommodate these changes. When there is too much stress, it cracks. A tilt in Europa's axis could suggest that its cracks may be much more recent than previously thought. The reason for this is that the direction of the spin pole may change by as much as a few degrees per day, completing one precession period over several months. A tilt could also affect estimates of the age of Europa's ocean. Tidal forces are thought to generate the heat that keeps Europa's ocean liquid, and a tilt in the spin axis would cause more heat to be generated by tidal forces. Such additional heat would have allowed the ocean to remain liquid for a longer time. However, it has not yet been determined when this hypothesized shift in the spin axis might have occurred. Europa is slightly smaller than the Earth's moon. At just over 3,100 kilometres (1,900 mi) in diameter, it is the sixth-largest moon and fifteenth-largest object in"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_7",
    "chunk": "the Solar System. Though by a wide margin the least massive of the Galilean satellites, it is nonetheless more massive than all known moons in the Solar System smaller than itself combined. Its bulk density suggests that it is similar in composition to terrestrial planets, being primarily composed of silicate rock. It is estimated that Europa has an outer layer of water around 100 km (62 mi) thick – a part frozen as its crust and a part as a liquid ocean underneath the ice. Recent magnetic-field data from the Galileo orbiter showed that Europa has an induced magnetic field through interaction with Jupiter's, which suggests the presence of a subsurface conductive layer. This layer is likely to be a salty liquid-water ocean. Portions of the crust are estimated to have undergone a rotation of nearly 80°, nearly flipping over (see true polar wander), which would be unlikely if the ice were solidly attached to the mantle. Europa probably contains a metallic iron core. Europa is the smoothest known object in the Solar System, lacking large-scale features such as mountains and craters. The prominent markings crisscrossing Europa appear to be mainly albedo features that emphasize low topography. There are few"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_8",
    "chunk": "craters on Europa, because its surface is tectonically too active and therefore young. Its icy crust has an albedo (light reflectivity) of 0.64, one of the highest of any moon. This indicates a young and active surface: based on estimates of the frequency of cometary bombardment that Europa experiences, the surface is about 20 to 180 million years old. There is no scientific consensus about the explanation for Europa's surface features. It has been postulated Europa's equator may be covered in icy spikes called penitentes, which may be up to 15 meters high. Their formation is due to direct overhead sunlight near the equator causing the ice to sublime, forming vertical cracks. Although the imaging available from the Galileo orbiter does not have the resolution for confirmation, radar and thermal data are consistent with this speculation. The ionizing radiation level at Europa's surface is equivalent to a daily dose of about 5.4 Sv (540 rem), an amount that would cause severe illness or death in human beings exposed for a single Earth day (24 hours). A Europan day is about 3.5 times as long as an Earth day. Europa's most striking surface features are a series of dark streaks crisscrossing"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_9",
    "chunk": "the entire globe, called lineae (English: lines). Close examination shows that the edges of Europa's crust on either side of the cracks have moved relative to each other. The larger bands are more than 20 km (12 mi) across, often with dark, diffuse outer edges, regular striations, and a central band of lighter material. The most likely hypothesis is that the lineae on Europa were produced by a series of eruptions of warm ice as Europa's crust slowly spreads open to expose warmer layers beneath. The effect would have been similar to that seen on Earth's oceanic ridges. These various fractures are thought to have been caused in large part by the tidal flexing exerted by Jupiter. Because Europa is tidally locked to Jupiter, and therefore always maintains approximately the same orientation towards Jupiter, the stress patterns should form a distinctive and predictable pattern. However, only the youngest of Europa's fractures conform to the predicted pattern; other fractures appear to occur at increasingly different orientations the older they are. This could be explained if Europa's surface rotates slightly faster than its interior, an effect that is possible due to the subsurface ocean mechanically decoupling Europa's surface from its rocky mantle"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_10",
    "chunk": "and the effects of Jupiter's gravity tugging on Europa's outer ice crust. Comparisons of Voyager and Galileo spacecraft photos serve to put an upper limit on this hypothetical slippage. A full revolution of the outer rigid shell relative to the interior of Europa takes at least 12,000 years. Studies of Voyager and Galileo images have revealed evidence of subduction on Europa's surface, suggesting that, just as the cracks are analogous to ocean ridges, so plates of icy crust analogous to tectonic plates on Earth are recycled into the molten interior. This evidence of both crustal spreading at bands and convergence at other sites suggests that Europa may have active plate tectonics, similar to Earth. However, the physics driving these plate tectonics are not likely to resemble those driving terrestrial plate tectonics, as the forces resisting potential Earth-like plate motions in Europa's crust are significantly stronger than the forces that could drive them. Other features present on Europa are circular and elliptical lenticulae (Latin for \"freckles\"). Many are domes, some are pits and some are smooth, dark spots. Others have a jumbled or rough texture. The dome tops look like pieces of the older plains around them, suggesting that the domes"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_11",
    "chunk": "formed when the plains were pushed up from below. One hypothesis states that these lenticulae were formed by diapirs of warm ice rising up through the colder ice of the outer crust, much like magma chambers in Earth's crust. The smooth, dark spots could be formed by meltwater released when the warm ice breaks through the surface. The rough, jumbled lenticulae (called regions of \"chaos\"; for example, Conamara Chaos) would then be formed from many small fragments of crust, embedded in hummocky, dark material, appearing like icebergs in a frozen sea. An alternative hypothesis suggests that lenticulae are actually small areas of chaos and that the claimed pits, spots and domes are artefacts resulting from the over-interpretation of early, low-resolution Galileo images. The implication is that the ice is too thin to support the convective diapir model of feature formation. In November 2011, a team of researchers, including researchers at University of Texas at Austin, presented evidence suggesting that many \"chaos terrain\" features on Europa sit atop vast lakes of liquid water. These lakes would be entirely encased in Europa's icy outer shell and distinct from a liquid ocean thought to exist farther down beneath the ice shell. Full confirmation"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_12",
    "chunk": "of the lakes' existence will require a space mission designed to probe the ice shell either physically or indirectly, e.g. using radar. Chaos features may also be a result of increased melting of the ice shell and deposition of marine ice at low latitudes as a result of heterogeneous heating. Work published by researchers from Williams College suggests that chaos terrain may represent sites where impacting comets penetrated through the ice crust and into an underlying ocean. The scientific consensus is that a layer of liquid water exists beneath Europa's surface, and that heat from tidal flexing allows the subsurface ocean to remain liquid. Europa's surface temperature averages about 110 K (−160 °C; −260 °F) at the equator and only 50 K (−220 °C; −370 °F) at the poles, keeping Europa's icy crust as hard as granite. The first hints of a subsurface ocean came from theoretical considerations of tidal heating (a consequence of Europa's slightly eccentric orbit and orbital resonance with the other Galilean moons). Galileo imaging team members argue for the existence of a subsurface ocean from analysis of Voyager and Galileo images. The most dramatic example is \"chaos terrain\", a common feature on Europa's surface that some"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_13",
    "chunk": "interpret as a region where the subsurface ocean has melted through the icy crust. This interpretation is controversial. Most geologists who have studied Europa favor what is commonly called the \"thick ice\" model, in which the ocean has rarely, if ever, directly interacted with the present surface. The best evidence for the thick-ice model is a study of Europa's large craters. The largest impact structures are surrounded by concentric rings and appear to be filled with relatively flat, fresh ice; based on this and on the calculated amount of heat generated by Europan tides, it is estimated that the outer crust of solid ice is approximately 10 to 30 km (6 to 20 mi) thick, including a ductile \"warm ice\" layer, which could mean that the liquid ocean underneath may be about 100 km (60 mi) deep. This leads to a volume of Europa's oceans of 3×10m, between two or three times the volume of Earth's oceans. The thin-ice model suggests that Europa's ice shell may be only a few kilometers thick. However, most planetary scientists conclude that this model considers only those topmost layers of Europa's crust that behave elastically when affected by Jupiter's tides. One example is flexure"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_14",
    "chunk": "analysis, in which Europa's crust is modeled as a plane or sphere weighted and flexed by a heavy load. Models such as this suggest the outer elastic portion of the ice crust could be as thin as 200 metres (660 ft). If the ice shell of Europa is really only a few kilometers thick, this \"thin ice\" model would mean that regular contact of the liquid interior with the surface could occur through open ridges, causing the formation of areas of chaotic terrain. Large impacts going fully through the ice crust would also be a way that the subsurface ocean could be exposed. The Galileo orbiter found that Europa has a weak magnetic moment, which is induced by the varying part of the Jovian magnetic field. The field strength at the magnetic equator (about 120 nT) created by this magnetic moment is about one-sixth the strength of Ganymede's field and six times the value of Callisto's. The existence of the induced moment requires a layer of a highly electrically conductive material in Europa's interior. The most plausible candidate for this role is a large subsurface ocean of liquid saltwater. Since the Voyager spacecraft flew past Europa in 1979, scientists have"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_15",
    "chunk": "worked to understand the composition of the reddish-brown material that coats fractures and other geologically youthful features on Europa's surface. Spectrographic evidence suggests that the darker, reddish streaks and features on Europa's surface may be rich in salts such as magnesium sulfate, deposited by evaporating water that emerged from within. Sulfuric acid hydrate is another possible explanation for the contaminant observed spectroscopically. In either case, because these materials are colorless or white when pure, some other material must also be present to account for the reddish color, and sulfur compounds are suspected. Another hypothesis for the colored regions is that they are composed of abiotic organic compounds collectively called tholins. The morphology of Europa's impact craters and ridges is suggestive of fluidized material welling up from the fractures where pyrolysis and radiolysis take place. In order to generate colored tholins on Europa, there must be a source of materials (carbon, nitrogen, and water) and a source of energy to make the reactions occur. Impurities in the water ice crust of Europa are presumed both to emerge from the interior as cryovolcanic events that resurface the body, and to accumulate from space as interplanetary dust. Tholins bring important astrobiological implications, as"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_16",
    "chunk": "they may play a role in prebiotic chemistry and abiogenesis. The presence of sodium chloride in the internal ocean has been suggested by a 450 nm absorption feature, characteristic of irradiated NaCl crystals, that has been spotted in HST observations of the chaos regions, presumed to be areas of recent subsurface upwelling. The subterranean ocean of Europa contains carbon and was observed on the surface ice as a concentration of carbon dioxide within Tara Regio, a geologically recently resurfaced terrain. JWST NIRSpec observations show that the northern hemisphere show crystalline water ice beneath the surface and amorphous ice dominating the surface. In the southern hemisphere Regiones Tara and Powys crystalline water ice dominates both the surface and the deeper layers. These two regiones likely experience ongoing thermal (re)crystallization, as the radiation near Jupiter cause particle amorphization at the top 10 microns over a period of less than 15 days. Europa receives thermal energy from tidal heating, which occurs through the tidal friction and tidal flexing processes caused by tidal acceleration: orbital and rotational energy are dissipated as heat in the core of the moon, the internal ocean, and the ice crust. Ocean tides are converted to heat by frictional losses"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_17",
    "chunk": "in the oceans and their interaction with the solid bottom and with the top ice crust. In late 2008, it was suggested Jupiter may keep Europa's oceans warm by generating large planetary tidal waves on Europa because of its small but non-zero obliquity. This generates so-called Rossby waves that travel quite slowly, at just a few kilometers per day, but can generate significant kinetic energy. For the current axial tilt estimate of 0.1 degree, the resonance from Rossby waves would contain 7.3×10 J of kinetic energy, which is two thousand times larger than that of the flow excited by the dominant tidal forces. Dissipation of this energy could be the principal heat source of Europa's ocean. Tidal flexing kneads Europa's interior and ice shell, which becomes a source of heat. Depending on the amount of tilt, the heat generated by the ocean flow could be 100 to thousands of times greater than the heat generated by the flexing of Europa's rocky core in response to the gravitational pull from Jupiter and the other moons circling that planet. Europa's seafloor could be heated by the moon's constant flexing, driving hydrothermal activity similar to undersea volcanoes in Earth's oceans. Experiments and ice"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_18",
    "chunk": "modeling published in 2016, indicate that tidal flexing dissipation can generate one order of magnitude more heat in Europa's ice than scientists had previously assumed. Their results indicate that most of the heat generated by the ice actually comes from the ice's crystalline structure (lattice) as a result of deformation, and not friction between the ice grains. The greater the deformation of the ice sheet, the more heat is generated. In addition to tidal heating, the interior of Europa could also be heated by the decay of radioactive material (radiogenic heating) within the rocky mantle. But the models and values observed are one hundred times higher than those that could be produced by radiogenic heating alone, thus implying that tidal heating has a leading role in Europa. The Hubble Space Telescope acquired an image of Europa in 2012 that was interpreted to be a plume of water vapour erupting from near its south pole. The image suggests the plume may be 200 km (120 mi) high, or more than 20 times the height of Mt. Everest., though recent observations and modeling suggest that typical Europan plumes may be much smaller. It has been suggested that if plumes exist, they are"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_19",
    "chunk": "episodic and likely to appear when Europa is at its farthest point from Jupiter, in agreement with tidal force modeling predictions. Additional imaging evidence from the Hubble Space Telescope was presented in September 2016. In May 2018, astronomers provided supporting evidence of water plume activity on Europa, based on an updated critical analysis of data obtained from the Galileo space probe, which orbited Jupiter between 1995 and 2003. Galileo flew by Europa in 1997 within 206 km (128 mi) of the moon's surface and the researchers suggest it may have flown through a water plume. Such plume activity could help researchers in a search for life from the subsurface Europan ocean without having to land on the moon. The tidal forces are about 1,000 times stronger than the Moon's effect on Earth. The only other moon in the Solar System exhibiting water vapor plumes is Enceladus. The estimated eruption rate at Europa is about 7000 kg/s compared to about 200 kg/s for the plumes of Enceladus. If confirmed, it would open the possibility of a flyby through the plume and obtain a sample to analyze in situ without having to use a lander and drill through kilometres of ice. In"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_20",
    "chunk": "November 2020, a study was published in the peer-reviewed scientific journal Geophysical Research Letters suggesting that the plumes may originate from water within the crust of Europa as opposed to its subsurface ocean. The study's model, using images from the Galileo space probe, proposed that a combination of freezing and pressurization may result in at least some of the cryovolcanic activity. The pressure generated by migrating briny water pockets would thus, eventually, burst through the crust, thereby creating these plumes. The hypothesis that cryovolcanism on Europa could be triggered by freezing and pressurization of liquid pockets in the icy crust was first proposed by Sarah Fagents at the University of Hawai'i at Mānoa, who in 2003, was the first to model and publish work on this process. A press release from NASA's Jet Propulsion Laboratory referencing the November 2020 study suggested that plumes sourced from migrating liquid pockets could potentially be less hospitable to life. This is due to a lack of substantial energy for organisms to thrive off, unlike proposed hydrothermal vents on the subsurface ocean floor. The atmosphere of Europa can be categorized as thin and tenuous (often called an exosphere), primarily composed of oxygen and trace amounts"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_21",
    "chunk": "of water vapor. However, this quantity of oxygen is produced in a non-biological manner. Given that Europa's surface is icy, and subsequently very cold; as solar ultraviolet radiation and charged particles (ions and electrons) from the Jovian magnetospheric environment collide with Europa's surface, water vapor is created and instantaneously separated into oxygen and hydrogen constituents. As it continues to move, the hydrogen is light enough to pass through the surface gravity of the atmosphere leaving behind only oxygen. The surface-bounded atmosphere forms through radiolysis, the dissociation of molecules through radiation. This accumulated oxygen atmosphere can get to a height of 190 km (120 mi) above the surface of Europa. Molecular oxygen is the densest component of the atmosphere because it has a long lifetime; after returning to the surface, it does not stick (freeze) like a water or hydrogen peroxide molecule but rather desorbs from the surface and starts another ballistic arc. Molecular hydrogen never reaches the surface, as it is light enough to escape Europa's surface gravity. Europa is one of the few moons in the Solar System with a quantifiable atmosphere, along with Titan, Io, Triton, Ganymede and Callisto. Europa is also one of several moons in the"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_22",
    "chunk": "Solar System with very large quantities of ice (volatiles), otherwise known as \"icy moons\". Europa is also considered to be geologically active due to the constant release of hydrogen-oxygen mixtures into space. As a result of the moon's particle venting, the atmosphere requires continuous replenishment. Europa also contains a small magnetosphere (approximately 25% of Ganymede's). However, this magnetosphere varies in size as Europa orbits through Jupiter's magnetic field. This confirms that a conductive element, such as a large ocean, likely lies below its icy surface. As multiple studies have been conducted over Europa's atmosphere, several findings conclude that not all oxygen molecules are released into the atmosphere. This unknown percentage of oxygen may be absorbed into the surface and sink into the subsurface. Because the surface may interact with the subsurface ocean (considering the geological discussion above), this molecular oxygen may make its way to the ocean, where it could aid in biological processes. One estimate suggests that, given the turnover rate inferred from the apparent ~0.5 Gyr maximum age of Europa's surface ice, subduction of radiolytically generated oxidizing species might well lead to oceanic free oxygen concentrations that are comparable to those in terrestrial deep oceans. Through the slow"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_23",
    "chunk": "release of oxygen and hydrogen, a neutral torus around Europa's orbital plane is formed. This \"neutral cloud\" has been detected by both the Cassini and Galileo spacecraft, and has a greater content (number of atoms and molecules) than the neutral cloud surrounding Jupiter's inner moon Io. This torus was officially confirmed using Energetic Neutral Atom (ENA) imaging. Europa's torus ionizes through the process of neutral particles exchanging electrons with its charged particles. Since Europa's magnetic field rotates faster than its orbital velocity, these ions are left in the path of its magnetic field trajectory, forming a plasma. It has been hypothesized that these ions are responsible for the plasma within Jupiter's magnetosphere. On 4 March 2024, astronomers reported that the surface of Europa may have much less oxygen than previously inferred. The atmosphere of Europa was first discovered in 1995 by astronomers D. T. Hall and collaborators using the Goddard High Resolution Spectrograph instrument of the Hubble Space Telescope. This observation was further supported in 1997 by the Galileo orbiter during its mission within the Jovian system. The Galileo orbiter performed three radio occultation events of Europa, where the probe's radio contact with Earth was temporarily blocked by passing behind"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_24",
    "chunk": "Europa. By analyzing the effects Europa's sparse atmosphere had on the radio signal just before and after the occultation, for a total of six events, a team of astronomers led by A. J. Kliore established the presence of an ionized layer in Europa's atmosphere. Despite the presence of a gas torus, Europa has no weather producing clouds. As a whole, Europa has no wind, precipitation, or presence of sky color as its gravity is too low to hold an atmosphere substantial enough for those features. Europa's gravity is approximately 13% of Earth's. The temperature on Europa varies from −160 °C at the equator, to −220 °C at either of its poles. Europa's subsurface ocean is thought to be significantly warmer however. It is hypothesized that because of radioactive and tidal heating (as mentioned in the sections above), there are points in the depths of Europa's ocean that may be only slightly cooler than Earth's oceans. Studies have also concluded that Europa's ocean would have been rather acidic at first, with large concentrations of sulfate, calcium, and carbon dioxide. But over the course of 4.5 billion years, it became full of chloride, thus resembling our 1.94% chloride oceans on Earth. Exploration"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_25",
    "chunk": "of Europa began with the Jupiter flybys of Pioneer 10 and 11 in 1973 and 1974, respectively. The first closeup photos were of low resolution compared to later missions. The two Voyager probes traveled through the Jovian system in 1979, providing more-detailed images of Europa's icy surface. The images caused many scientists to speculate about the possibility of a liquid ocean underneath. Starting in 1995, the Galileo space probe orbited Jupiter for eight years, until 2003, and provided the most detailed examination of the Galilean moons to date. It included the \"Galileo Europa Mission\" and \"Galileo Millennium Mission\", with numerous close flybys of Europa. In 2007, New Horizons imaged Europa, as it flew by the Jovian system while on its way to Pluto. In 2022, the Juno orbiter flew by Europa at a distance of 352 km (219 mi). In 2012, Jupiter Icy Moons Explorer (JUICE) was selected by the European Space Agency (ESA) as a planned mission. That mission includes two flybys of Europa, but is more focused on Ganymede. It was launched in 2023, and is expected to reach Jupiter in July 2031 after four gravity assists and eight years of travel. In 2011, a Europa mission was"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_26",
    "chunk": "recommended by the U.S. Planetary Science Decadal Survey. In response, NASA commissioned concept studies of a Europa lander in 2011, along with concepts for a Europa flyby (Europa Clipper), and a Europa orbiter. The orbiter element option concentrates on the \"ocean\" science, while the multiple-flyby element (Clipper) concentrates on the chemistry and energy science. On 13 January 2014, the House Appropriations Committee announced a new bipartisan bill that includes $80 million in funding to continue the Europa mission concept studies. In July 2013 an updated concept for a flyby Europa mission called Europa Clipper was presented by the Jet Propulsion Laboratory (JPL) and the Applied Physics Laboratory (APL). In May 2015, NASA announced that it had accepted development of the Europa Clipper mission, and revealed the instruments it would use. The aim of Europa Clipper is to explore Europa in order to investigate its habitability, and to aid in selecting sites for a future lander. The Europa Clipper would not orbit Europa, but instead orbit Jupiter and conduct 45 low-altitude flybys of Europa during its envisioned mission. The probe would carry an ice-penetrating radar, short-wave infrared spectrometer, topographical imager, and an ion- and neutral-mass spectrometer. The mission was launched on"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_27",
    "chunk": "14 October 2024 aboard a Falcon Heavy. Conjectures regarding extraterrestrial life have ensured a high profile for Europa and have led to steady lobbying for future missions. The aims of these missions have ranged from examining Europa's chemical composition to searching for extraterrestrial life in its hypothesized subsurface oceans. Robotic missions to Europa need to endure the high-radiation environment around Jupiter. Because it is deeply embedded within Jupiter's magnetosphere, Europa receives about 5.40 Sv of radiation per day. In the early 2000s, Jupiter Europa Orbiter led by NASA and the Jupiter Ganymede Orbiter led by the ESA were proposed together as an Outer Planet Flagship Mission to Jupiter's icy moons called Europa Jupiter System Mission, with a planned launch in 2020. In 2009 it was given priority over Titan Saturn System Mission. At that time, there was competition from other proposals. Japan proposed Jupiter Magnetospheric Orbiter. Jovian Europa Orbiter was an ESA Cosmic Vision concept study from 2007. Another concept was Ice Clipper, which would have used an impactor similar to the Deep Impact mission—it would make a controlled crash into the surface of Europa, generating a plume of debris that would then be collected by a small spacecraft flying"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_28",
    "chunk": "through the plume. Jupiter Icy Moons Orbiter (JIMO) was a partially developed fission-powered spacecraft with ion thrusters that was cancelled in 2006. It was part of Project Prometheus. The Europa Lander Mission proposed a small nuclear-powered Europa lander for JIMO. It would travel with the orbiter, which would also function as a communication relay to Earth. Europa Orbiter – Its objective would be to characterize the extent of the ocean and its relation to the deeper interior. Instrument payload could include a radio subsystem, laser altimeter, magnetometer, Langmuir probe, and a mapping camera. The Europa Orbiter received the go-ahead in 1999 but was canceled in 2002. This orbiter featured a special ice-penetrating radar that would allow it to scan below the surface. More ambitious ideas have been put forward including an impactor in combination with a thermal drill to search for biosignatures that might be frozen in the shallow subsurface. Another proposal put forward in 2001 calls for a large nuclear-powered \"melt probe\" (cryobot) that would melt through the ice until it reached an ocean below. Once it reached the water, it would deploy an autonomous underwater vehicle (hydrobot) that would gather information and send it back to Earth. Both"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_29",
    "chunk": "the cryobot and the hydrobot would have to undergo some form of extreme sterilization to prevent detection of Earth organisms instead of native life and to prevent contamination of the subsurface ocean. This suggested approach has not yet reached a formal conceptual planning stage. So far, there is no evidence that life exists on Europa, but the moon has emerged as one of the most likely locations in the Solar System for potential habitability. Life could exist in its under-ice ocean, perhaps in an environment similar to Earth's deep-ocean hydrothermal vents. Even if Europa lacks volcanic hydrothermal activity, a 2016 NASA study found that Earth-like levels of hydrogen and oxygen could be produced through processes related to serpentinization and ice-derived oxidants, which do not directly involve volcanism. In 2015, scientists announced that salt from a subsurface ocean may likely be coating some geological features on Europa, suggesting that the ocean is interacting with the seafloor. This may be important in determining if Europa could be habitable. The likely presence of liquid water in contact with Europa's rocky mantle has spurred calls to send a probe there. The energy provided by tidal forces drives active geological processes within Europa's interior, just"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_30",
    "chunk": "as they do to a far more obvious degree on its sister moon Io. Although Europa, like the Earth, may possess an internal energy source from radioactive decay, the energy generated by tidal flexing would be several orders of magnitude greater than any radiological source. Life on Europa could exist clustered around hydrothermal vents on the ocean floor, or below the ocean floor, where endoliths are known to inhabit on Earth. Alternatively, it could exist clinging to the lower surface of Europa's ice layer, much like algae and bacteria in Earth's polar regions, or float freely in Europa's ocean. Should Europa's oceans be too cold, biological processes similar to those known on Earth could not occur; too salty, only extreme halophiles could survive in that environment. In 2010, a model proposed by Richard Greenberg of the University of Arizona proposed that irradiation of ice on Europa's surface could saturate its crust with oxygen and peroxide, which could then be transported by tectonic processes into the interior ocean. Such a process could render Europa's ocean as oxygenated as our own within just 12 million years, allowing the existence of complex, multicellular lifeforms. Evidence suggests the existence of lakes of liquid water"
  },
  {
    "source": "Europa (moon).txt",
    "chunk_id": "Europa (moon).txt_31",
    "chunk": "entirely encased in Europa's icy outer shell and distinct from a liquid ocean thought to exist farther down beneath the ice shell, as well as pockets of water that form M-shaped ice ridges when the water freezes on the surface – as in Greenland. If confirmed, the lakes and pockets of water could be yet another potential habitat for life. Evidence suggests that hydrogen peroxide is abundant across much of the surface of Europa. Because hydrogen peroxide decays into oxygen and water when combined with liquid water, the authors argue that it could be an important energy supply for simple life forms. Nonetheless, on 4 March 2024, astronomers reported that the surface of Europa may have much less oxygen than previously inferred. Clay-like minerals (specifically, phyllosilicates), often associated with organic matter on Earth, have been detected on the icy crust of Europa. The presence of the minerals may have been the result of a collision with an asteroid or comet. Some scientists have speculated that life on Earth could have been blasted into space by asteroid collisions and arrived on the moons of Jupiter in a process called lithopanspermia."
  },
  {
    "source": "Extinction (astronomy).txt",
    "chunk_id": "Extinction (astronomy).txt_0",
    "chunk": "# Extinction (astronomy) In astronomy, extinction is the absorption and scattering of electromagnetic radiation by dust and gas between an emitting astronomical object and the observer. Interstellar extinction was first documented as such in 1930 by Robert Julius Trumpler. However, its effects had been noted in 1847 by Friedrich Georg Wilhelm von Struve, and its effect on the colors of stars had been observed by a number of individuals who did not connect it with the general presence of galactic dust. For stars lying near the plane of the Milky Way which are within a few thousand parsecs of the Earth, extinction in the visual band of frequencies (photometric system) is roughly 1.8 magnitudes per kiloparsec. For Earth-bound observers, extinction arises both from the interstellar medium and the Earth's atmosphere; it may also arise from circumstellar dust around an observed object. Strong extinction in Earth's atmosphere of some wavelength regions (such as X-ray, ultraviolet, and infrared) is overcome by the use of space-based observatories. Since blue light is much more strongly attenuated than red light, extinction causes objects to appear redder than expected; this phenomenon is called interstellar reddening. Interstellar reddening is a phenomenon associated with interstellar extinction where the"
  },
  {
    "source": "Extinction (astronomy).txt",
    "chunk_id": "Extinction (astronomy).txt_1",
    "chunk": "spectrum of electromagnetic radiation from a radiation source changes characteristics from that which the object originally emitted. Reddening occurs due to the light scattering off dust and other matter in the interstellar medium. Interstellar reddening is a different phenomenon from redshift, which is the proportional frequency shifts of spectra without distortion. Reddening preferentially removes shorter wavelength photons from a radiated spectrum while leaving behind the longer wavelength photons, leaving the spectroscopic lines unchanged. In most photometric systems, filters (passbands) are used from which readings of magnitude of light may take account of latitude and humidity among terrestrial factors. Interstellar reddening equates to the \"color excess\", defined as the difference between an object's observed color index and its intrinsic color index (sometimes referred to as its normal color index). The latter is the theoretical value which it would have if unaffected by extinction. In the first system, the UBV photometric system devised in the 1950s and its most closely related successors, the object's color excess E B − V {\\displaystyle E_{B-V}} is related to the object's B−V color (calibrated blue minus calibrated visible) by: E B − V = ( B − V ) observed − ( B − V )"
  },
  {
    "source": "Extinction (astronomy).txt",
    "chunk_id": "Extinction (astronomy).txt_2",
    "chunk": "intrinsic {\\displaystyle E_{B-V}=(B-V)_{\\textrm {observed}}-(B-V)_{\\textrm {intrinsic}}\\,} For an A0-type main sequence star (these have median wavelength and heat among the main sequence) the color indices are calibrated at 0 based on an intrinsic reading of such a star (± exactly 0.02 depending on which spectral point, i.e. precise passband within the abbreviated color name is in question, see color index). At least two and up to five measured passbands in magnitude are then compared by subtraction: U, B, V, I, or R during which the color excess from extinction is calculated and deducted. The name of the four sub-indices (R minus I etc.) and order of the subtraction of recalibrated magnitudes is from right to immediate left within this sequence. Interstellar reddening occurs because interstellar dust absorbs and scatters blue light waves more than red light waves, making stars appear redder than they are. This is similar to the effect seen when dust particles in the atmosphere of Earth contribute to red sunsets. Broadly speaking, interstellar extinction is strongest at short wavelengths, generally observed by using techniques from spectroscopy. Extinction results in a change in the shape of an observed spectrum. Superimposed on this general shape are absorption features (wavelength bands"
  },
  {
    "source": "Extinction (astronomy).txt",
    "chunk_id": "Extinction (astronomy).txt_3",
    "chunk": "where the intensity is lowered) that have a variety of origins and can give clues as to the chemical composition of the interstellar material, e.g. dust grains. Known absorption features include the 2175 Å bump, the diffuse interstellar bands, the 3.1 μm water ice feature, and the 10 and 18 μm silicate features. In the solar neighborhood, the rate of interstellar extinction in the Johnson–Cousins V-band (visual filter) averaged at a wavelength of 540 nm is usually taken to be 0.7–1.0 mag/kpc−simply an average due to the clumpiness of interstellar dust. In general, however, this means that a star will have its brightness reduced by about a factor of 2 in the V-band viewed from a good night sky vantage point on earth for every kiloparsec (3,260 light years) it is farther away from us. The amount of extinction can be significantly higher than this in specific directions. For example, some regions of the Galactic Center are awash with obvious intervening dark dust from our spiral arm (and perhaps others) and themselves in a bulge of dense matter, causing as much as more than 30 magnitudes of extinction in the optical, meaning that less than 1 optical photon in 10"
  },
  {
    "source": "Extinction (astronomy).txt",
    "chunk_id": "Extinction (astronomy).txt_4",
    "chunk": "passes through. This results in the zone of avoidance, where our view of the extra-galactic sky is severely hampered, and background galaxies, such as Dwingeloo 1, were only discovered recently through observations in radio and infrared. The general shape of the ultraviolet through near-infrared (0.125 to 3.5 μm) extinction curve (plotting extinction in magnitude against wavelength, often inverted) looking from our vantage point at other objects in the Milky Way, is fairly well characterized by the stand-alone parameter of relative visibility (of such visible light) R(V) (which is different along different lines of sight), but there are known deviations from this characterization. Extending the extinction law into the mid-infrared wavelength range is difficult due to the lack of suitable targets and various contributions by absorption features. R(V) compares aggregate and particular extinctions. It is A V / E ( B − V ) {\\displaystyle A_{V}/E(B-V)\\,} Restated, it is the total extinction, A(V) divided by the selective total extinction (A(B)−A(V)) of those two wavelengths (bands). A(B) and A(V) are the total extinction at the B and V filter bands. Another measure used in the literature is the absolute extinction A(λ)/A(V) at wavelength λ, comparing the total extinction at that wavelength to"
  },
  {
    "source": "Extinction (astronomy).txt",
    "chunk_id": "Extinction (astronomy).txt_5",
    "chunk": "that at the V band. R(V) is known to be correlated with the average size of the dust grains causing the extinction. For the Milky Way Galaxy, the typical value for R(V) is 3.1, but is found to vary considerably across different lines of sight. As a result, when computing cosmic distances it can be advantageous to move to star data from the near-infrared (of which the filter or passband Ks is quite standard) where the variations and amount of extinction are significantly less, and similar ratios as to R(Ks): 0.49±0.02 and 0.528±0.015 were found respectively by independent groups. Those two more modern findings differ substantially relative to the commonly referenced historical value ≈0.7. The relationship between the total extinction, A(V) (measured in magnitudes), and the column density of neutral hydrogen atoms column, NH (usually measured in cm), shows how the gas and dust in the interstellar medium are related. From studies using ultraviolet spectroscopy of reddened stars and X-ray scattering halos in the Milky Way, Predehl and Schmitt found the relationship between NH and A(V) to be approximately: Astronomers have determined the three-dimensional distribution of extinction in the \"solar circle\" (our region of our galaxy), using visible and near-infrared"
  },
  {
    "source": "Extinction (astronomy).txt",
    "chunk_id": "Extinction (astronomy).txt_6",
    "chunk": "stellar observations and a model of distribution of stars. The dust causing extinction mainly lies along the spiral arms, as observed in other spiral galaxies. To measure the extinction curve for a star, the star's spectrum is compared to the observed spectrum of a similar star known not to be affected by extinction (unreddened). It is also possible to use a theoretical spectrum instead of the observed spectrum for the comparison, but this is less common. In the case of emission nebulae, it is common to look at the ratio of two emission lines which should not be affected by the temperature and density in the nebula. For example, the ratio of hydrogen-alpha to hydrogen-beta emission is always around 2.85 under a wide range of conditions prevailing in nebulae. A ratio other than 2.85 must therefore be due to extinction, and the amount of extinction can thus be calculated. One prominent feature in measured extinction curves of many objects within the Milky Way is a broad 'bump' at about 2175 Å, well into the ultraviolet region of the electromagnetic spectrum. This feature was first observed in the 1960s, but its origin is still not well understood. Several models have been"
  },
  {
    "source": "Extinction (astronomy).txt",
    "chunk_id": "Extinction (astronomy).txt_7",
    "chunk": "presented to account for this bump which include graphitic grains with a mixture of PAH molecules. Investigations of interstellar grains embedded in interplanetary dust particles (IDP) observed this feature and identified the carrier with organic carbon and amorphous silicates present in the grains. The form of the standard extinction curve depends on the composition of the ISM, which varies from galaxy to galaxy. In the Local Group, the best-determined extinction curves are those of the Milky Way, the Small Magellanic Cloud (SMC) and the Large Magellanic Cloud (LMC). In the LMC, there is significant variation in the characteristics of the ultraviolet extinction with a weaker 2175 Å bump and stronger far-UV extinction in the region associated with the LMC2 supershell (near the 30 Doradus starbursting region) than seen elsewhere in the LMC and in the Milky Way. In the SMC, more extreme variation is seen with no 2175 Å bump and very strong far-UV extinction in the star forming Bar and fairly normal ultraviolet extinction seen in the more quiescent Wing. This gives clues as to the composition of the ISM in the various galaxies. Previously, the different average extinction curves in the Milky Way, LMC, and SMC were thought"
  },
  {
    "source": "Extinction (astronomy).txt",
    "chunk_id": "Extinction (astronomy).txt_8",
    "chunk": "to be the result of the different metallicities of the three galaxies: the LMC's metallicity is about 40% of that of the Milky Way, while the SMC's is about 10%. Finding extinction curves in both the LMC and SMC which are similar to those found in the Milky Way and finding extinction curves in the Milky Way that look more like those found in the LMC2 supershell of the LMC and in the SMC Bar has given rise to a new interpretation. The variations in the curves seen in the Magellanic Clouds and Milky Way may instead be caused by processing of the dust grains by nearby star formation. This interpretation is supported by work in starburst galaxies (which are undergoing intense star formation episodes) which shows that their dust lacks the 2175 Å bump. Atmospheric extinction gives the rising or setting Sun an orange hue and varies with location and altitude. Astronomical observatories generally are able to characterise the local extinction curve very accurately, to allow observations to be corrected for the effect. Nevertheless, the atmosphere is completely opaque to many wavelengths requiring the use of satellites to make observations. This extinction has three main components: Rayleigh scattering by"
  },
  {
    "source": "Extinction (astronomy).txt",
    "chunk_id": "Extinction (astronomy).txt_9",
    "chunk": "air molecules, scattering by particulates, and molecular absorption. Molecular absorption is often referred to as telluric absorption, as it is caused by the Earth (telluric is a synonym for terrestrial). The most important sources of telluric absorption are molecular oxygen and ozone, which strongly absorb radiation near ultraviolet, and water, which strongly absorbs infrared. The amount of such extinction is lowest at the observer's zenith and highest near the horizon. A given star, preferably at solar opposition, reaches its greatest celestial altitude and optimal time for observation when the star is near the local meridian around solar midnight and if the star has a favorable declination (i.e., similar to the observer's latitude); thus, the seasonal time due to axial tilt is key. Extinction is approximated by multiplying the standard atmospheric extinction curve (plotted against each wavelength) by the mean air mass calculated over the duration of the observation. A dry atmosphere reduces infrared extinction significantly."
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_0",
    "chunk": "# Formation and evolution of the Solar System There is evidence that the formation of the Solar System began about 4.6 billion years ago with the gravitational collapse of a small part of a giant molecular cloud. Most of the collapsing mass collected in the center, forming the Sun, while the rest flattened into a protoplanetary disk out of which the planets, moons, asteroids, and other small Solar System bodies formed. This model, known as the nebular hypothesis, was first developed in the 18th century by Emanuel Swedenborg, Immanuel Kant, and Pierre-Simon Laplace. Its subsequent development has interwoven a variety of scientific disciplines including astronomy, chemistry, geology, physics, and planetary science. Since the dawn of the Space Age in the 1950s and the discovery of exoplanets in the 1990s, the model has been both challenged and refined to account for new observations. The Solar System has evolved considerably since its initial formation. Many moons have formed from circling discs of gas and dust around their parent planets, while other moons are thought to have formed independently and later to have been captured by their planets. Still others, such as Earth's Moon, may be the result of giant collisions. Collisions between"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_1",
    "chunk": "bodies have occurred continually up to the present day and have been central to the evolution of the Solar System. Beyond Neptune, many sub-planet sized objects formed. Several thousand trans-Neptunian objects have been observed. Unlike the planets, these trans-Neptunian objects mostly move on eccentric orbits, inclined to the plane of the planets. The positions of the planets might have shifted due to gravitational interactions. The process of planetary migration explains parts of the Solar System's current structure. In roughly 5 billion years, the Sun will cool and expand outward to many times its current diameter (becoming a red giant), before casting off its outer layers as a planetary nebula and leaving behind a stellar remnant known as a white dwarf. In the distant future, the gravity of passing stars will gradually reduce the Sun's retinue of planets. Some planets will be destroyed, and others ejected into interstellar space. Ultimately, over the course of tens of billions of years, it is likely that the Sun will be left with none of the original bodies in orbit around it. Ideas concerning the origin and fate of the world date from the earliest known writings; however, for almost all of that time, there"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_2",
    "chunk": "was no attempt to link such theories to the existence of a \"Solar System\", simply because it was not generally thought that the Solar System, in the sense we now understand it, existed. The first step toward a theory of Solar System formation and evolution was the general acceptance of heliocentrism, which placed the Sun at the centre of the system and the Earth in orbit around it. This concept had been developed for millennia (Aristarchus of Samos had suggested it as early as 250 BC), but was not widely accepted until the end of the 17th century. The first recorded use of the term \"Solar System\" dates from 1704. The current standard theory for Solar System formation, the nebular hypothesis, has fallen into and out of favour since its formulation by Emanuel Swedenborg, Immanuel Kant, and Pierre-Simon Laplace in the 18th century. The most significant criticism of the hypothesis was its apparent inability to explain the Sun's relative lack of angular momentum when compared to the planets. However, since the early 1980s studies of young stars have shown them to be surrounded by cool discs of dust and gas, exactly as the nebular hypothesis predicts, which has led to"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_3",
    "chunk": "its re-acceptance. Understanding of how the Sun is expected to continue to evolve required an understanding of the source of its power. Arthur Stanley Eddington's confirmation of Albert Einstein's theory of relativity led to his realisation that the Sun's energy comes from nuclear fusion reactions in its core, fusing hydrogen into helium. In 1935, Eddington went further and suggested that other elements also might form within stars. Fred Hoyle elaborated on this premise by arguing that evolved stars called red giants created many elements heavier than hydrogen and helium in their cores. When a red giant finally casts off its outer layers, these elements would then be recycled to form other star systems. The nebular hypothesis says that the Solar System formed from the gravitational collapse of a fragment of a giant molecular cloud, most likely at the edge of a Wolf-Rayet bubble. The cloud was about 20 parsecs (65 light years) across, while the fragments were roughly 1 parsec (three and a quarter light-years) across. The further collapse of the fragments led to the formation of dense cores 0.01–0.1 parsec (2,000–20,000 AU) in size. One of these collapsing fragments (known as the presolar nebula) formed what became the Solar"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_4",
    "chunk": "System. The composition of this region with a mass just over that of the Sun (M☉) was about the same as that of the Sun today, with hydrogen, along with helium and trace amounts of lithium produced by Big Bang nucleosynthesis, forming about 98% of its mass. The remaining 2% of the mass consisted of heavier elements that were created by nucleosynthesis in earlier generations of stars. Late in the life of these stars, they ejected heavier elements into the interstellar medium. Some scientists have given the name Coatlicue to a hypothetical star that went supernova and created the presolar nebula. The oldest inclusions found in meteorites, thought to trace the first solid material to form in the presolar nebula, are 4,568.2 million years old, which is one definition of the age of the Solar System. Studies of ancient meteorites reveal traces of stable daughter nuclei of short-lived isotopes, such as iron-60, that only form in exploding, short-lived stars. This indicates that one or more supernovae occurred nearby. A shock wave from a supernova may have triggered the formation of the Sun by creating relatively dense regions within the cloud, causing these regions to collapse. The highly homogeneous distribution of"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_5",
    "chunk": "iron-60 in the Solar System points to the occurrence of this supernova and its injection of iron-60 being well before the accretion of nebular dust into planetary bodies. Because only massive, short-lived stars produce supernovae, the Sun must have formed in a large star-forming region that produced massive stars, possibly similar to the Orion Nebula. Studies of the structure of the Kuiper belt and of anomalous materials within it suggest that the Sun formed within a cluster of between 1,000 and 10,000 stars with a diameter of between 6.5 and 19.5 light years and a collective mass of 3,000 M☉. This cluster began to break apart between 135 million and 535 million years after formation. Several simulations of our young Sun interacting with close-passing stars over the first 100 million years of its life produced anomalous orbits observed in the outer Solar System, such as detached objects. A recent study suggests that such a passing star is not only responsible for the orbits of the detached objects but also the hot and cold Kuiper belt population, the Sedna-like objects, the extreme TNOs and the retrograde TNOs. Because of the conservation of angular momentum, the nebula spun faster as it collapsed."
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_6",
    "chunk": "As the material within the nebula condensed, the temperature rose. The center, where most of the mass collected, became increasingly hotter than the surrounding disc. Over about 100,000 years, the competing forces of gravity, gas pressure, magnetic fields, and rotation caused the contracting nebula to flatten into a spinning protoplanetary disc with a diameter of about 200 AU and form a hot, dense protostar (a star in which hydrogen fusion has not yet begun) at the centre. Since about half of all known stars form systems of multiple stars, and because Jupiter is made of the same elements as the Sun (hydrogen and helium), it has been suggested that the Solar System might have been early in its formation a protostar system with Jupiter being the second but failed protostar, but Jupiter has far too little mass to trigger fusion in its core and so became a gas giant; it is in fact younger than the Sun and the oldest planet of the Solar System. At this point in the Sun's evolution, the Sun is thought to have been a T Tauri star. Studies of T Tauri stars show that they are often accompanied by discs of pre-planetary matter with"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_7",
    "chunk": "masses of 0.001–0.1 M☉. These discs extend to several hundred AU—the Hubble Space Telescope has observed protoplanetary discs of up to 1000 AU in diameter in star-forming regions such as the Orion Nebula—and are rather cool, reaching a surface temperature of only about 1,000 K (730 °C; 1,340 °F) at their hottest. Within 50 million years, the temperature and pressure at the core of the Sun became so great that its hydrogen began to fuse, creating an internal source of energy that countered gravitational contraction until hydrostatic equilibrium was achieved. This marked the Sun's entry into the prime phase of its life, known as the main sequence. Main-sequence stars derive energy from the fusion of hydrogen into helium in their cores. The Sun remains a main-sequence star today. As the early Solar System continued to evolve, it eventually drifted away from its siblings in the stellar nursery, and continued orbiting the Milky Way's center on its own. The Sun likely drifted from its original orbital distance from the center of the galaxy. The chemical history of the Sun suggests it may have formed as much as 3 kpc closer to the galaxy core. Like most stars, the Sun likely formed"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_8",
    "chunk": "not in isolation but as part of a young star cluster. There are several indications that hint at the cluster environment having had some influence over the young, still-forming Solar System. For example, the decline in mass beyond Neptune and the extreme eccentric-orbit of Sedna have been interpreted as a signature of the Solar System having been influenced by its birth environment. Whether the presence of the isotopes iron-60 and aluminium-26 can be interpreted as a sign of a birth cluster containing massive stars is still under debate. If the Sun was part of a star cluster, it might have been influenced by close flybys of other stars, the strong radiation of nearby massive stars and ejecta from supernovae occurring close by. The various planets are thought to have formed from the solar nebula, the disc-shaped cloud of gas and dust left over from the Sun's formation. The currently accepted method by which the planets formed is accretion, in which the planets began as dust grains in orbit around the central protostar. Through direct contact and self-organization, these grains formed into clumps up to 200 m (660 ft) in diameter, which in turn collided to form larger bodies (planetesimals) of"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_9",
    "chunk": "~10 km (6.2 mi) in size. These gradually increased through further collisions, growing at the rate of centimetres per year over the course of the next few million years. The inner Solar System, the region of the Solar System inside 4 AU, was too warm for volatile molecules like water and methane to condense, so the planetesimals that formed there could only form from compounds with high melting points, such as metals (like iron, nickel, and aluminium) and rocky silicates. These rocky bodies would become the terrestrial planets (Mercury, Venus, Earth, and Mars). These compounds are quite rare in the Universe, comprising only 0.6% of the mass of the nebula, so the terrestrial planets could not grow very large. The terrestrial embryos grew to about 0.05 Earth masses (ME) and ceased accumulating matter about 100,000 years after the formation of the Sun; subsequent collisions and mergers between these planet-sized bodies allowed terrestrial planets to grow to their present sizes. When terrestrial planets were forming, they remained immersed in a disk of gas and dust. Pressure partially supported the gas and so did not orbit the Sun as rapidly as the planets. The resulting drag and, more importantly, gravitational interactions with"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_10",
    "chunk": "the surrounding material caused a transfer of angular momentum, and as a result the planets gradually migrated to new orbits. Models show that density and temperature variations in the disk governed this rate of migration, but the net trend was for the inner planets to migrate inward as the disk dissipated, leaving the planets in their current orbits. The giant planets (Jupiter, Saturn, Uranus, and Neptune) formed further out, beyond the frost line, which is the point between the orbits of Mars and Jupiter where the material is cool enough for volatile icy compounds to remain solid. The ices that formed the Jovian planets were more abundant than the metals and silicates that formed the terrestrial planets, allowing the giant planets to grow massive enough to capture hydrogen and helium, the lightest and most abundant elements. Planetesimals beyond the frost line accumulated up to 4 ME within about 3 million years. Today, the four giant planets comprise just under 99% of all the mass orbiting the Sun. Theorists believe it is no accident that Jupiter lies just beyond the frost line. Because the frost line accumulated large amounts of water via evaporation from infalling icy material, it created a region"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_11",
    "chunk": "of lower pressure that increased the speed of orbiting dust particles and halted their motion toward the Sun. In effect, the frost line acted as a barrier that caused the material to accumulate rapidly at ~5 AU from the Sun. This excess material coalesced into a large embryo (or core) on the order of 10 ME, which began to accumulate an envelope via accretion of gas from the surrounding disc at an ever-increasing rate. Once the envelope mass became about equal to the solid core mass, growth proceeded very rapidly, reaching about 150 Earth masses ~10 years thereafter and finally topping out at 318 ME. Saturn may owe its substantially lower mass simply to having formed a few million years after Jupiter, when there was less gas available to consume. T Tauri stars like the young Sun have far stronger stellar winds than more stable, older stars. Uranus and Neptune are thought to have formed after Jupiter and Saturn did, when the strong solar wind had blown away much of the disc material. As a result, those planets accumulated little hydrogen and helium—not more than 1 ME each. Uranus and Neptune are sometimes referred to as failed cores. The main"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_12",
    "chunk": "problem with formation theories for these planets is the timescale of their formation. At the current locations it would have taken millions of years for their cores to accrete. This means that Uranus and Neptune may have formed closer to the Sun—near or even between Jupiter and Saturn—later migrating or being ejected outward (see Planetary migration below). Motion in the planetesimal era was not all inward toward the Sun; the Stardust sample return from Comet Wild 2 has suggested that materials from the early formation of the Solar System migrated from the warmer inner Solar System to the region of the Kuiper belt. After between three and ten million years, the young Sun's solar wind would have cleared away all the gas and dust in the protoplanetary disc, blowing it into interstellar space, thus ending the growth of the planets. The planets were originally thought to have formed in or near their current orbits. This has been questioned during the last 20 years. Currently, many planetary scientists think that the Solar System might have looked very different after its initial formation: several objects at least as massive as Mercury may have been present in the inner Solar System, the outer"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_13",
    "chunk": "Solar System may have been much more compact than it is now, and the Kuiper belt may have been much closer to the Sun. At the end of the planetary formation epoch, the inner Solar System was populated by 50–100 Moon-to-Mars-sized protoplanets. Further growth was possible only because these bodies collided and merged, which took less than 100 million years. These objects would have gravitationally interacted with one another, tugging at each other's orbits until they collided, growing larger until the four terrestrial planets we know today took shape. One such giant collision is thought to have formed the Moon (see Moons below), while another removed the outer envelope of the young Mercury. One unresolved issue with this model is that it cannot explain how the initial orbits of the proto-terrestrial planets, which would have needed to be highly eccentric in order to collide, produced the remarkably stable and nearly circular orbits they have today. One hypothesis for this \"eccentricity dumping\" is that terrestrials formed in a disc of gas still not expelled by the Sun. The \"gravitational drag\" of this residual gas would have eventually lowered the planets' energy, smoothing out their orbits. However, such gas, if it existed,"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_14",
    "chunk": "would have prevented the terrestrial planets' orbits from becoming so eccentric in the first place. Another hypothesis is that gravitational drag occurred not between the planets and residual gas but between the planets and the remaining small bodies. As the large bodies moved through the crowd of smaller objects, the smaller objects, attracted by the larger planets' gravity, formed a region of higher density, a \"gravitational wake\", in the larger objects' path. As they did so, the increased gravity of the wake slowed the larger objects down into more regular orbits. The outer edge of the terrestrial region, between 2 and 4 AU from the Sun, is called the asteroid belt. The asteroid belt initially contained more than enough matter to form 2–3 Earth-like planets, and, indeed, a large number of planetesimals formed there. As with the terrestrials, planetesimals in this region later coalesced and formed 20–30 Moon- to Mars-sized planetary embryos; however, the proximity of Jupiter meant that after this planet formed, 3 million years after the Sun, the region's history changed dramatically. Orbital resonances with Jupiter and Saturn are particularly strong in the asteroid belt, and gravitational interactions with more massive embryos scattered many planetesimals into those resonances."
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_15",
    "chunk": "Jupiter's gravity increased the velocity of objects within these resonances, causing them to shatter upon collision with other bodies, rather than accrete. As Jupiter migrated inward following its formation (see Planetary migration below), resonances would have swept across the asteroid belt, dynamically exciting the region's population and increasing their velocities relative to each other. The cumulative action of the resonances and the embryos either scattered the planetesimals away from the asteroid belt or excited their orbital inclinations and eccentricities. Some of those massive embryos too were ejected by Jupiter, while others may have migrated to the inner Solar System and played a role in the final accretion of the terrestrial planets. During this primary depletion period, the effects of the giant planets and planetary embryos left the asteroid belt with a total mass equivalent to less than 1% that of the Earth, composed mainly of small planetesimals. This is still 10–20 times more than the current mass in the main belt, which is now about 0.0005 ME. A secondary depletion period that brought the asteroid belt down close to its present mass is thought to have followed when Jupiter and Saturn entered a temporary 2:1 orbital resonance (see below). The"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_16",
    "chunk": "inner Solar System's period of giant impacts probably played a role in Earth acquiring its current water content (~6×10 kg) from the early asteroid belt. Water is too volatile to have been present at Earth's formation and must have been subsequently delivered from outer, colder parts of the Solar System. The water was probably delivered by planetary embryos and small planetesimals thrown out of the asteroid belt by Jupiter. A population of main-belt comets discovered in 2006 has also been suggested as a possible source for Earth's water. In contrast, comets from the Kuiper belt or farther regions delivered not more than about 6% of Earth's water. The panspermia hypothesis holds that life itself may have been deposited on Earth in this way, although this idea is not widely accepted. According to the nebular hypothesis, the outer two planets may be in the \"wrong place\". Uranus and Neptune (known as the \"ice giants\") exist in a region where the reduced density of the solar nebula and longer orbital times render their formation there highly implausible. The two are instead thought to have formed in orbits near Jupiter and Saturn (known as the \"gas giants\"), where more material was available, and"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_17",
    "chunk": "to have migrated outward to their current positions over hundreds of millions of years. The migration of the outer planets is also necessary to account for the existence and properties of the Solar System's outermost regions. Beyond Neptune, the Solar System continues into the Kuiper belt, the scattered disc, and the Oort cloud, three sparse populations of small icy bodies thought to be the points of origin for most observed comets. At their distance from the Sun, accretion was too slow to allow planets to form before the solar nebula dispersed, and thus the initial disc lacked enough mass density to consolidate into a planet. The Kuiper belt lies between 30 and 55 AU from the Sun, while the farther scattered disc extends to over 100 AU, and the distant Oort cloud begins at about 50,000 AU. Originally, however, the Kuiper belt was much denser and closer to the Sun, with an outer edge at approximately 30 AU. Its inner edge would have been just beyond the orbits of Uranus and Neptune, which were in turn far closer to the Sun when they formed (most likely in the range of 15–20 AU), and in 50% of simulations ended up in"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_18",
    "chunk": "opposite locations, with Uranus farther from the Sun than Neptune. According to the Nice model, after the formation of the Solar System, the orbits of all the giant planets continued to change slowly, influenced by their interaction with the large number of remaining planetesimals. After 500–600 million years (about 4 billion years ago) Jupiter and Saturn fell into a 2:1 resonance: Saturn orbited the Sun once for every two Jupiter orbits. This resonance created a gravitational push against the outer planets, possibly causing Neptune to surge past Uranus and plough into the ancient Kuiper belt. The planets scattered the majority of the small icy bodies inwards, while themselves moving outwards. These planetesimals then scattered off the next planet they encountered in a similar manner, moving the planets' orbits outwards while they moved inwards. This process continued until the planetesimals interacted with Jupiter, whose immense gravity sent them into highly elliptical orbits or even ejected them outright from the Solar System. This caused Jupiter to move slightly inward. Those objects scattered by Jupiter into highly elliptical orbits formed the Oort cloud; those objects scattered to a lesser degree by the migrating Neptune formed the current Kuiper belt and scattered disc. This"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_19",
    "chunk": "scenario explains the Kuiper belt's and scattered disc's present low mass. Some of the scattered objects, including Pluto, became gravitationally tied to Neptune's orbit, forcing them into mean-motion resonances. Eventually, friction within the planetesimal disc made the orbits of Uranus and Neptune near-circular again. In contrast to the outer planets, the inner planets are not thought to have migrated significantly over the age of the Solar System, because their orbits have remained stable following the period of giant impacts. Another question is why Mars came out so small compared with Earth. A study by Southwest Research Institute, San Antonio, Texas, published June 6, 2011 (called the Grand tack hypothesis), proposes that Jupiter had migrated inward to 1.5 AU. After Saturn formed, migrated inward, and established the 2:3 mean motion resonance with Jupiter, the study assumes that both planets migrated back to their present positions. Jupiter thus would have consumed much of the material that would have created a bigger Mars. The same simulations also reproduce the characteristics of the modern asteroid belt, with dry asteroids and water-rich objects similar to comets. However, it is unclear whether conditions in the solar nebula would have allowed Jupiter and Saturn to move back"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_20",
    "chunk": "to their current positions, and according to current estimates this possibility appears unlikely. Moreover, alternative explanations for the small mass of Mars exist. Gravitational disruption from the outer planets' migration would have sent large numbers of asteroids into the inner Solar System, severely depleting the original belt until it reached today's extremely low mass. This event may have triggered the Late Heavy Bombardment that is hypothesised to have occurred approximately 4 billion years ago, 500–600 million years after the formation of the Solar System. However, a recent re-appraisal of the cosmo-chemical constraints indicates that there was likely no late spike (“terminal cataclysm”) in the bombardment rate. If it occurred, this period of heavy bombardment lasted several hundred million years and is evident in the cratering still visible on geologically dead bodies of the inner Solar System such as the Moon and Mercury. The oldest known evidence for life on Earth dates to 3.8 billion years ago—almost immediately after the end of the Late Heavy Bombardment. Impacts are thought to be a regular (if currently infrequent) part of the evolution of the Solar System. That they continue to happen is evidenced by the collision of Comet Shoemaker–Levy 9 with Jupiter in"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_21",
    "chunk": "1994, the 2009 Jupiter impact event, the Tunguska event, the Chelyabinsk meteor and the impact that created Meteor Crater in Arizona. The process of accretion, therefore, is not complete, and may still pose a threat to life on Earth. Over the course of the Solar System's evolution, comets were ejected out of the inner Solar System by the gravity of the giant planets and sent thousands of AU outward to form the Oort cloud, a spherical outer swarm of cometary nuclei at the farthest extent of the Sun's gravitational pull. Eventually, after about 800 million years, the gravitational disruption caused by galactic tides, passing stars and giant molecular clouds began to deplete the cloud, sending comets into the inner Solar System. The evolution of the outer Solar System also appears to have been influenced by space weathering from the solar wind, micrometeorites, and the neutral components of the interstellar medium. The evolution of the asteroid belt after Late Heavy Bombardment was mainly governed by collisions. Objects with large mass have enough gravity to retain any material ejected by a violent collision. In the asteroid belt this usually is not the case. As a result, many larger objects have been broken"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_22",
    "chunk": "apart, and sometimes newer objects have been forged from the remnants in less violent collisions. Moons around some asteroids currently can only be explained as consolidations of material flung away from the parent object without enough energy to entirely escape its gravity. Moons have come to exist around most planets and many other Solar System bodies. These natural satellites originated by one of three possible mechanisms: Jupiter and Saturn have several large moons, such as Io, Europa, Ganymede and Titan, which may have originated from discs around each giant planet in much the same way that the planets formed from the disc around the Sun. This origin is indicated by the large sizes of the moons and their proximity to the planet. These attributes are impossible to achieve via capture, while the gaseous nature of the primaries also makes formation from collision debris unlikely. The outer moons of the giant planets tend to be small and have eccentric orbits with arbitrary inclinations. These are the characteristics expected of captured bodies. Most such moons orbit in the direction opposite to the rotation of their primary. The largest irregular moon is Neptune's moon Triton, which is thought to be a captured Kuiper"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_23",
    "chunk": "belt object. Moons of solid Solar System bodies have been created by both collisions and capture. Mars's two small moons, Deimos and Phobos, are thought to be captured asteroids. The Earth's moon is thought to have formed as a result of a single, large head-on collision. The impacting object probably had a mass comparable to that of Mars, and the impact probably occurred near the end of the period of giant impacts. The collision kicked into orbit some of the impactor's mantle, which then coalesced into the Moon. The impact was probably the last in a series of mergers that formed the Earth. It has been further hypothesized that the Mars-sized object may have formed at one of the stable Earth–Sun Lagrangian points (either L4 or L5) and drifted from its position. The moons of trans-Neptunian objects Pluto (Charon) and Orcus (Vanth) may also have formed by means of a large collision: the Pluto–Charon, Orcus–Vanth and Earth–Moon systems are unusual in the Solar System in that the satellite's mass is at least 1% that of the larger body. Astronomers estimate that the current state of the Solar System will not change drastically until the Sun has fused almost all the"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_24",
    "chunk": "hydrogen fuel in its core into helium, beginning its evolution from the main sequence of the Hertzsprung–Russell diagram and into its red-giant phase. The Solar System will continue to evolve until then. Eventually, the Sun will likely expand sufficiently to overwhelm the inner planets (Mercury, Venus, and possibly Earth) but not the outer planets, including Jupiter and Saturn. Afterward, the Sun would be reduced to the size of a white dwarf, and the outer planets and their moons would continue orbiting this diminutive solar remnant. This future development may be similar to the observed detection of MOA-2010-BLG-477L b, a Jupiter-sized exoplanet orbiting its host white dwarf star MOA-2010-BLG-477L. The Solar System is chaotic over million- and billion-year timescales, with the orbits of the planets open to long-term variations. One notable example of this chaos is the Neptune–Pluto system, which lies in a 3:2 orbital resonance. Although the resonance itself will remain stable, it becomes impossible to predict the position of Pluto with any degree of accuracy more than 10–20 million years (the Lyapunov time) into the future. Another example is Earth's axial tilt, which, due to friction raised within Earth's mantle by tidal interactions with the Moon (see below), is"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_25",
    "chunk": "incomputable from some point between 1.5 and 4.5 billion years from now. The outer planets' orbits are chaotic over longer timescales, with a Lyapunov time in the range of 2–230 million years. In all cases, this means that the position of a planet along its orbit ultimately becomes impossible to predict with any certainty (so, for example, the timing of winter and summer becomes uncertain). Still, in some cases, the orbits themselves may change dramatically. Such chaos manifests most strongly as changes in eccentricity, with some planets' orbits becoming significantly more—or less—elliptical. Ultimately, the Solar System is stable in that none of the planets are likely to collide with each other or be ejected from the system in the next few billion years. Beyond this, within five billion years or so, Mars's eccentricity may grow to around 0.2, such that it lies on an Earth-crossing orbit, leading to a potential collision. In the same timescale, Mercury's eccentricity may grow even further, and a close encounter with Venus could theoretically eject it from the Solar System altogether or send it on a collision course with Venus or Earth. This could happen within a billion years, according to numerical simulations in which"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_26",
    "chunk": "Mercury's orbit is perturbed. The evolution of moon systems is driven by tidal forces. A moon will raise a tidal bulge in the object it orbits (the primary) due to the differential gravitational force across diameter of the primary. If a moon is revolving in the same direction as the planet's rotation and the planet is rotating faster than the orbital period of the moon, the bulge will constantly be pulled ahead of the moon. In this situation, angular momentum is transferred from the rotation of the primary to the revolution of the satellite. The moon gains energy and gradually spirals outward, while the primary rotates more slowly over time. The Earth and its Moon are one example of this configuration. Today, the Moon is tidally locked to the Earth; one of its revolutions around the Earth (currently about 29 days) is equal to one of its rotations about its axis, so it always shows one face to the Earth. The Moon will continue to recede from Earth, and Earth's spin will continue to slow gradually. Other examples are the Galilean moons of Jupiter (as well as many of Jupiter's smaller moons) and most of the larger moons of Saturn."
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_27",
    "chunk": "A different scenario occurs when the moon is either revolving around the primary faster than the primary rotates or is revolving in the direction opposite the planet's rotation. In these cases, the tidal bulge lags behind the moon in its orbit. In the former case, the direction of angular momentum transfer is reversed, so the rotation of the primary speeds up while the satellite's orbit shrinks. In the latter case, the angular momentum of the rotation and revolution have opposite signs, so transfer leads to decreases in the magnitude of each (that cancel each other out). In both cases, tidal deceleration causes the moon to spiral in towards the primary until it either is torn apart by tidal stresses, potentially creating a planetary ring system, or crashes into the planet's surface or atmosphere. Such a fate awaits the moons Phobos of Mars (within 30 to 50 million years), Triton of Neptune (in 3.6 billion years), and at least 16 small satellites of Uranus and Neptune. Uranus's Desdemona may even collide with one of its neighboring moons. A third possibility is where the primary and moon are tidally locked to each other. In that case, the tidal bulge stays directly under"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_28",
    "chunk": "the moon, there is no angular momentum transfer, and the orbital period will not change. Pluto and Charon are an example of this type of configuration. There is no consensus on the mechanism of the formation of the rings of Saturn. Although theoretical models indicated that the rings were likely to have formed early in the Solar System's history, data from the Cassini–Huygens spacecraft suggests they formed relatively late. In the long term, the greatest changes in the Solar System will come from changes in the Sun itself as it ages. As the Sun burns through its hydrogen fuel supply, it gets hotter and burns the remaining fuel even faster. As a result, the Sun is growing brighter at a rate of ten percent every 1.1 billion years. In about 600 million years, the Sun's brightness will have disrupted the Earth's carbon cycle to the point where trees and forests (C3 photosynthetic plant life) will no longer be able to survive; and in around 800 million years, the Sun will have killed all complex life on the Earth's surface and in the oceans. In 1.1 billion years, the Sun's increased radiation output will cause its circumstellar habitable zone to move"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_29",
    "chunk": "outwards, making the Earth's surface too hot for liquid water to exist there naturally. At this point, all life will be reduced to single-celled organisms. Evaporation of water, a potent greenhouse gas, from the oceans' surface could accelerate temperature increase, potentially ending all life on Earth even sooner. During this time, it is possible that as Mars's surface temperature gradually rises, carbon dioxide and water currently frozen under the surface regolith will release into the atmosphere, creating a greenhouse effect that will heat the planet until it achieves conditions parallel to Earth today, providing a potential future abode for life. By 3.5 billion years from now, Earth's surface conditions will be similar to those of Venus today. Around 5.4 billion years from now, the core of the Sun will become hot enough to trigger hydrogen fusion in its surrounding shell. This will cause the outer layers of the star to expand greatly, and the star will enter a phase of its life in which it is called a red giant. Within 7.5 billion years, the Sun will have expanded to a radius of 1.2 AU (180×10^ km; 110×10^ mi)—256 times its current size. At the tip of the red-giant branch,"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_30",
    "chunk": "as a result of the vastly increased surface area, the Sun's surface will be much cooler (about 2,600 K (2,330 °C; 4,220 °F)) than now, and its luminosity much higher—up to 2,700 current solar luminosities. For part of its red-giant life, the Sun will have a strong stellar wind that will carry away around 33% of its mass. During these times, it is possible that Saturn's moon Titan could achieve surface temperatures necessary to support life. As the Sun expands, it will swallow the planets Mercury and Venus. Earth's fate is less clear; although the Sun will envelop Earth's current orbit, the star's loss of mass (and thus weaker gravity) will cause the planets' orbits to move farther out. If it were only for this, Venus and Earth would probably escape incineration, but a 2008 study suggests that Earth will likely be swallowed up as a result of tidal interactions with the Sun's weakly-bound outer envelope. Additionally, the Sun's habitable zone will move into the outer Solar System and eventually beyond the Kuiper belt at the end of the red-giant phase, causing icy bodies such as Enceladus and Pluto to thaw. During this time, these worlds could support a water-based"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_31",
    "chunk": "hydrologic cycle, but as they were too small to hold a dense atmosphere like Earth, they would experience extreme day–night temperature differences. When the Sun leaves the red-giant branch and enters the asymptotic giant branch, the habitable zone will abruptly shrink to roughly the space between Jupiter and Saturn's present-day orbits, but toward the end of the 200 million-year duration of the asymptotic giant phase, it will expand outward to about the same distance as before. Gradually, the hydrogen burning in the shell around the solar core will increase the mass of the core until it reaches about 45% of the present solar mass. At this point, the density and temperature will become so high that the fusion of helium into carbon will begin, leading to a helium flash; the Sun will shrink from around 250 to 11 times its present (main-sequence) radius. Consequently, its luminosity will decrease from around 3,000 to 54 times its current level, and its surface temperature will increase to about 4,770 K (4,500 °C; 8,130 °F). The Sun will become a horizontal giant, burning helium in its core in a stable fashion, much like it burns hydrogen today. The helium-fusing stage will last only 100"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_32",
    "chunk": "million years. Eventually, it will have to again resort to the reserves of hydrogen and helium in its outer layers. It will expand a second time, becoming what is known as an asymptotic giant. Here the luminosity of the Sun will increase again, reaching about 2,090 present luminosities, and it will cool to about 3,500 K (3,230 °C; 5,840 °F). This phase lasts about 30 million years, after which, over the course of a further 100,000 years, the Sun's remaining outer layers will fall away, ejecting a vast stream of matter into space and forming a halo known (misleadingly) as a planetary nebula. The ejected material will contain the helium and carbon produced by the Sun's nuclear reactions, continuing the enrichment of the interstellar medium with heavy elements for future generations of stars and planets. This is a relatively peaceful event, nothing akin to a supernova, which the Sun is too small to undergo as part of its evolution. Any observer present to witness this occurrence would see a massive increase in the speed of the solar wind, but not enough to destroy a planet completely. However, the star's loss of mass could send the orbits of the surviving planets"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_33",
    "chunk": "into chaos, causing some to collide, others to be ejected from the Solar System, and others to be torn apart by tidal interactions. Afterwards, all that will remain of the Sun is a white dwarf, an extraordinarily dense object, 54% of its original mass but only the size of Earth. Initially, this white dwarf may be 100 times as luminous as the Sun is now. It will consist entirely of degenerate carbon and oxygen but will never reach temperatures hot enough to fuse these elements. Thus, the white dwarf Sun will gradually cool, growing dimmer and dimmer. As the Sun dies, its gravitational pull on orbiting bodies, such as planets, comets, and asteroids, will weaken due to its mass loss. All remaining planets' orbits will expand; if Venus, Earth, and Mars still exist, their orbits will lie roughly at 1.4 AU (210 million km; 130 million mi), 1.9 AU (280 million km; 180 million mi), and 2.8 AU (420 million km; 260 million mi), respectively. They and the other remaining planets will become dark, frigid husks, completely devoid of life. They will continue to orbit their star, their speed slowed due to their increased distance from the Sun and the"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_34",
    "chunk": "Sun's reduced gravity. Two billion years later, when the Sun has cooled to the 6,000–8,000 K (5,730–7,730 °C; 10,340–13,940 °F) range, the carbon and oxygen in the Sun's core will freeze, with over 90% of its remaining mass assuming a crystalline structure. Eventually, after roughly one quadrillion years, the Sun will finally cease to shine altogether, becoming a black dwarf. The Solar System travels alone through the Milky Way in a circular orbit approximately 30,000 light years from the Galactic Center. Its speed is about 220 km/s. The period required for the Solar System to complete one revolution around the Galactic Center, the galactic year, is in the range of 220–250 million years. Since its formation, the Solar System has completed at least 20 such revolutions. Various scientists have speculated that the Solar System's path through the galaxy is a factor in the periodicity of mass extinctions observed in the Earth's fossil record. One hypothesis supposes that vertical oscillations made by the Sun as it orbits the Galactic Centre cause it to regularly pass through the galactic plane. When the Sun's orbit takes it outside the galactic disc, the influence of the galactic tide is weaker; as it re-enters the"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_35",
    "chunk": "galactic disc, as it does every 20–25 million years, it comes under the influence of the far stronger \"disc tides\", which, according to mathematical models, increase the flux of Oort cloud comets into the Solar System by a factor of 4, leading to a massive increase in the likelihood of a devastating impact. However, others argue that the Sun is currently close to the galactic plane, and yet the last great extinction event was 15 million years ago. Therefore, the Sun's vertical position cannot alone explain such periodic extinctions, and that extinctions instead occur when the Sun passes through the galaxy's spiral arms. Spiral arms are home not only to larger numbers of molecular clouds, whose gravity may distort the Oort cloud, but also to higher concentrations of bright blue giants, which live for relatively short periods and then explode violently as supernovae. Although the vast majority of galaxies in the Universe are moving away from the Milky Way, the Andromeda Galaxy, the largest member of the Local Group of galaxies, is heading toward it at about 120 km/s. In 4 billion years, Andromeda and the Milky Way will collide, causing both to deform as tidal forces distort their outer"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_36",
    "chunk": "arms into vast tidal tails. If this initial disruption occurs, astronomers calculate a 12% chance that the Solar System will be pulled outward into the Milky Way's tidal tail and a 3% chance that it will become gravitationally bound to Andromeda and thus a part of that galaxy. After a further series of glancing blows, during which the likelihood of the Solar System's ejection rises to 30%, the galaxies' supermassive black holes will merge. Eventually, in roughly 6 billion years, the Milky Way and Andromeda will complete their merger into a giant elliptical galaxy. During the merger, if there is enough gas, the increased gravity will force the gas to the centre of the forming elliptical galaxy. This may lead to a short period of intensive star formation called a starburst. In addition, the infalling gas will feed the newly formed black hole, transforming it into an active galactic nucleus. The force of these interactions will likely push the Solar System into the new galaxy's outer halo, leaving it relatively unscathed by the radiation from these collisions. It is a common misconception that this collision will disrupt the orbits of the planets in the Solar System. Although it is true"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_37",
    "chunk": "that the gravity of passing stars can detach planets into interstellar space, distances between stars are so great that the likelihood of the Milky Way–Andromeda collision causing such disruption to any individual star system is negligible. Although the Solar System as a whole could be affected by these events, the Sun and planets are not expected to be disturbed. However, over time, the cumulative probability of a chance encounter with a star increases, and disruption of the planets becomes all but inevitable. Assuming that the Big Crunch or Big Rip scenarios for the end of the Universe do not occur, calculations suggest that the gravity of passing stars will have completely stripped the dead Sun of its remaining planets within 1 quadrillion (10) years. This point marks the end of the Solar System. Although the Sun and planets may survive, the Solar System, in any meaningful sense, will cease to exist. The time frame of the Solar System's formation has been determined using radiometric dating. Scientists estimate that the Solar System is 4.6 billion years old. The oldest known mineral grains on Earth are approximately 4.4 billion years old. Rocks this old are rare, as Earth's surface is constantly being"
  },
  {
    "source": "Formation and evolution of the Solar System.txt",
    "chunk_id": "Formation and evolution of the Solar System.txt_38",
    "chunk": "reshaped by erosion, volcanism, and plate tectonics. To estimate the age of the Solar System, scientists use meteorites, which were formed during the early condensation of the solar nebula. Almost all meteorites (see the Canyon Diablo meteorite) are found to have an age of 4.6 billion years, suggesting that the Solar System must be at least this old. Studies of discs around other stars have also done much to establish a time frame for Solar System formation. Stars between one and three million years old have discs rich in gas, whereas discs around stars more than 10 million years old have little to no gas, suggesting that giant planets within them have ceased forming. Note: All dates and times in this chronology are approximate and should be taken as an order of magnitude indicator only."
  },
  {
    "source": "Francesco Maria Grimaldi.txt",
    "chunk_id": "Francesco Maria Grimaldi.txt_0",
    "chunk": "# Francesco Maria Grimaldi Francesco Maria Grimaldi SJ (2 April 1618 – 28 December 1663) was an Italian Jesuit priest, mathematician and physicist who taught at the Jesuit college in Bologna. He was born in Bologna to Paride Grimaldi and Anna Cattani. Between 1640 and 1650, working with Riccioli, he investigated the free fall of objects, confirming that the distance of fall was proportional to the square of the time taken. Grimaldi and Riccioli also made a calculation of gravity at the Earth's surface by recording the oscillations of an accurate pendulum. In astronomy, he built and used instruments to measure lunar mountains as well as the height of clouds, and drew an accurate map or, selenograph, which was published by Riccioli and now adorns the entrance to the National Air and Space Museum in Washington D.C. He discovered and was the first to make accurate observations on the diffraction of light and coined the word 'diffraction'. In his book Physico-Mathesis de Lumine, Coloribus et Iride (1665), he stated the theory of the reconstitution of sunlight from refracted coloured light. (There is a far-fetched account that Leonardo da Vinci had earlier noted the effect.) Through experimentation he was able to"
  },
  {
    "source": "Francesco Maria Grimaldi.txt",
    "chunk_id": "Francesco Maria Grimaldi.txt_1",
    "chunk": "demonstrate that the observed passage of light could not be reconciled with the idea that it moved in a rectilinear path. Rather, the light that passed through the hole took on the shape of a cone. Later physicists used his work as evidence that light was a wave, significantly, Dutch mathematician Christiaan Huygens. He also discovered what are known as diffraction bands. The work is mainly remembered for being the first report of diffraction. In the work, he was mainly concerned with two questions: He argued that light is probably a subtle fluid (thus a substance), though it might still be an accident (as Aristotelians believed). He also argued that color is associated with undulations of the subtle fluid."
  },
  {
    "source": "Frank Shu.txt",
    "chunk_id": "Frank Shu.txt_0",
    "chunk": "# Frank Shu Frank Hsia-San Shu (Chinese: 徐遐生; Jyutping: Ceoi4 Haa4 Sang1; June 2, 1943 – April 22, 2023) was a Chinese-American astrophysicist, astronomer, and author. He served as a Professor Emeritus at the University of California, Berkeley and University of California, San Diego. He is best known for proposing the density wave theory to explain the structure of spiral galaxies, and for describing a model of star formation, where a giant dense molecular cloud collapses to form a star. Shu's hometown is Wenzhou, Zhejiang, but he was born in Kunming, Yunnan, in 1943. His father, Shu Shien-Siu, was a mathematician and an instructor at the National Tsing Hua University, which, at that time due to World War II, was temporarily relocated to Kunming from Beijing. The senior Shu would serve as the President of the National Tsing Hua University from 1970 to 1975. When Shu was two months old, his father went to the United States for study and, later, work. Shu and his family went to Taiwan through Hong Kong when he was five years old, stayed there for a year, and then traveled by steamship to the United States to re-unite with the senior Shu, who was"
  },
  {
    "source": "Frank Shu.txt",
    "chunk_id": "Frank Shu.txt_1",
    "chunk": "working at the Illinois Institute of Technology, Chicago. Shu completed his BSc in physics in 1963 at the Massachusetts Institute of Technology (MIT). While at MIT, he worked one summer for Chia-Chiao Lin on the structure of spiral galaxies, and the experience made him interested in astrophysics. He later continued working with Lin for his PhD project, as Max Krook, his formal doctoral supervisor at Harvard University, gave him freedom in his PhD research. He obtained his PhD from Harvard in 1968. Over his PhD study, he built on his undergraduate work and, together with Lin, proposed the density wave theory and published several articles explaining the structure of spiral galaxies. After his PhD, Shu joined the Stony Brook University as an assistant professor, and was promoted to associate professor in 1971. He moved to the University of California, Berkeley in 1973, and became a full professor in 1976. He had a brief visit at the Institute for Advanced Study in 1982. Between 1984 and 1988, he was the chair, or Head, of the Department of Astronomy. From 1994 to 1996, Shu was also the President of the American Astronomical Society (AAS). Shu was named a University Professor of the"
  },
  {
    "source": "Frank Shu.txt",
    "chunk_id": "Frank Shu.txt_2",
    "chunk": "University of California (UC) system in 1998, an honour that at the time was only endowed to 19 faculty members across the UC system. In 2002, Shu followed in his father's footsteps and went to Taiwan to take up the position of the President of the National Tsing Hua University, returning to the United States and joining the University of California, San Diego as a distinguished professor in 2006. Shu officially retired in 2009, becoming a University Professor Emeritus of the UC system, and a Distinguished Research Fellow at the Academia Sinica Institute of Astronomy and Astrophysics (until 2015). Latterly, Shu was an Emeritus Senior Fellow at the Hong Kong Institute for Advanced Study of the City University of Hong Kong. Shu wrote three textbooks: Physical Universe: An Introduction to Astronomy, The Physics of Astrophysics Vol. I: Radiation and The Physics of Astrophysics Vol. II: Gas Dynamics. Shu is best known for his work in spiral galaxies and star formation. He, together with his PhD supervisor Chia-Chiao Lin, proposed the density wave theory to explain the structure of spiral galaxies. In 1977, he published a model, known as the \"inside-out\" collapse model or the \"singular isothermal sphere\" model, of star"
  },
  {
    "source": "Frank Shu.txt",
    "chunk_id": "Frank Shu.txt_3",
    "chunk": "formation, whereby a star forms when a giant dense molecular cloud collapses."
  },
  {
    "source": "Future plc.txt",
    "chunk_id": "Future plc.txt_0",
    "chunk": "# Future plc Future plc is a British publishing company. It was started in 1985 by Chris Anderson. Zillah Byng-Thorne was chief executive officer from 2014 to 2023, when she was replaced by Jon Steinberg. It is listed on the London Stock Exchange and is a constituent of the FTSE 250 Index. The company was founded by Chris Anderson as Future Publishing in Somerton, Somerset, England, with the sole magazine Amstrad Action in 1985. An early innovation was the inclusion of free software on magazine covers. It acquired GP Publications and established what would become Future US in 1994. Anderson sold the company to Pearson plc for £52.7m in 1994, but bought it back in 1998, for £142 million. The company was floated on the London Stock Exchange in 1999. Anderson left the company in 2001. In 2004, the company was accused of corruption when it published positive reviews for the video game Driver 3 in two of its owned magazines, Xbox World and PSM2. Future published the official magazines for the consoles of all three major games console manufacturers (Microsoft, Nintendo, and Sony); however PlayStation: The Official Magazine ceased publishing in November 2012, and Official Nintendo Magazine ceased publishing"
  },
  {
    "source": "Future plc.txt",
    "chunk_id": "Future plc.txt_1",
    "chunk": "in October 2014. The chief executive and finance director both resigned at short notice after a profit warning in October 2011. It was noted that a re-structuring would be necessary as the company moved to a digital model. Future announced it would cut 55 jobs from its UK operation as part of a restructuring to adapt \"more effectively to the company's rapid transition to a primarily digital business model.\" The company announced in March 2014 that it would close all of its U.S.-based print publications and shift U.S. print support functions such as consumer marketing, production and editorial leadership for Future's international print brands to the UK. Later in 2014, Future sold its sport and craft titles to Immediate Media, and its auto titles to Kelsey Media. In April 2014, Zillah Byng-Thorne (then finance director) was appointed chief executive to replace Mark Wood, who had been in the position since 2011. In 2018, Future made further major acquisitions. It bought the What Hi-Fi?, FourFourTwo, Practical Caravan, and Practical Motorhome brands from Haymarket; and it acquired NewBay Media, publisher of numerous broadcast, professional-video, and systems-integration trade titles, as well as several consumer music magazines. This acquisition returned most of the U.S."
  },
  {
    "source": "Future plc.txt",
    "chunk_id": "Future plc.txt_2",
    "chunk": "consumer music magazines to Future, with the exception of Revolver which had been sold to Project M Group in 2017. It bought the Purch Group for $132m by September 2018, and in February 2019 bought Mobile Nations including the titles Android Central, iMore, Windows Central and Thrifter for $115 million. Future also acquired Procycling and Cyclingnews.com from Immediate Media. In July 2019 the company bought SmartBrief, a digital media publisher, for an initial sum of $45 million. In November 2019, the company bought Barcroft Studios for £23.5 million in a combination of cash and shares. It renamed it Future Studios and announced the launch of \"Future Originals\", an anthology gaming series, a \"factual\" series focusing on the paranormal, and a new true-crime show, in partnership with Marie Claire. In April 2020, it acquired TI Media with 41 brands for £140 million. In November, it agreed to a £594m takeover of GoCo plc, known for its Gocompare.com price-comparison website. In August 2021, it acquired Dennis Publishing and its 12 magazines, for £300 million. The company was criticised in February 2022 for the size of the remuneration package being offered to Zillah Byng-Thorne, the chief executive. It was noted that she could"
  },
  {
    "source": "Future plc.txt",
    "chunk_id": "Future plc.txt_3",
    "chunk": "receive £40 million if the company performed well. Byng-Thorne resigned with effect from 3 April 2023 and was replaced as chief executive by Jon Steinberg. In April 2023, the company sold its shooting magazines including Shooting Times and Sporting Gun to Fieldsports Press. In August 2024, the company announced that its American trade papers Broadcasting & Cable and Multichannel News would be closing after more than 90 years, with the main title Broadcasting having been first published in 1931 and the merged title Multichannel News dating from 1980. In October 2024, the company closed a number of consumer titles in the United Kingdom, including Play, All About Space, Total 911, and 3D World, with the monthly movie magazine Total Film ceasing publication after 27 years. Future's portfolio of brands includes TechRadar, PC Gamer, Tom's Guide, Tom's Hardware, Marie Claire, GamesRadar+, How it Works, Creative Bloq, CinemaBlend, Android Central, IT Pro, BikePerfect, Truly, Windows Central, Chat, and the website GoodToKnow.co.uk."
  },
  {
    "source": "Galactic bulge.txt",
    "chunk_id": "Galactic bulge.txt_0",
    "chunk": "# Galactic bulge In astronomy, a galactic bulge (or simply bulge) is a tightly packed group of stars within a larger star formation. The term almost exclusively refers to the group of stars found near the center of most spiral galaxies (see galactic spheroid). Bulges were historically thought to be elliptical galaxies that happened to have a disk of stars around them, but high-resolution images using the Hubble Space Telescope have revealed that many bulges lie at the heart of a spiral galaxy. It is now thought that there are at least two types of bulges: bulges that are like ellipticals and bulges that are like spiral galaxies. Bulges that have properties similar to those of elliptical galaxies are often called \"classical bulges\" due to their similarity to the historic view of bulges. These bulges are composed primarily of stars that are older, Population II stars, and hence have a reddish hue (see stellar evolution). These stars are also in orbits that are essentially random compared to the plane of the galaxy, giving the bulge a distinct spherical form. Due to the lack of dust and gases, bulges tend to have almost no star formation. The distribution of light is"
  },
  {
    "source": "Galactic bulge.txt",
    "chunk_id": "Galactic bulge.txt_1",
    "chunk": "described by a Sersic profile. Classical bulges are thought to be the result of collisions of smaller structures. Convulsing gravitational forces and torques disrupt the orbital paths of stars, resulting in the randomised bulge orbits. If either progenitor galaxy was gas-rich, the tidal forces can also cause inflows to the newly merged galaxy nucleus. Following a major merger, gas clouds are more likely to convert into stars, due to shocks (see star formation). One study has suggested that about 80% of galaxies in the field lack a classical bulge, indicating that they have never experienced a major merger. The bulgeless galaxy fraction of the Universe has remained roughly constant for at least the last 8 billion years. In contrast, about two thirds of galaxies in dense galaxy clusters (such as the Virgo Cluster) do possess a classical bulge, demonstrating the disruptive effect of their crowding. Many bulges have properties more similar to those of the central regions of spiral galaxies than elliptical galaxies. They are often referred to as pseudobulges or disky-bulges. These bulges have stars that are not orbiting randomly, but rather orbit in an ordered fashion in the same plane as the stars in the outer disk. This"
  },
  {
    "source": "Galactic bulge.txt",
    "chunk_id": "Galactic bulge.txt_2",
    "chunk": "contrasts greatly with elliptical galaxies. Subsequent studies (using the Hubble Space Telescope) show that the bulges of many galaxies are not devoid of dust, but rather show a varied and complex structure. This structure often looks similar to a spiral galaxy, but is much smaller. Giant spiral galaxies are typically 2–100 times the size of those spirals that exist in bulges. Where they exist, these central spirals dominate the light of the bulge in which they reside. Typically the rate at which new stars are formed in pseudobulges is similar to the rate at which stars form in disk galaxies. Sometimes bulges contain nuclear rings that are forming stars at much higher rate (per area) than is typically found in outer disks, as shown in NGC 4314 (see photo). Properties such as spiral structure and young stars suggest that some bulges did not form through the same process that made elliptical galaxies and classical bulges. Yet the theories for the formation of pseudobulges are less certain than those for classical bulges. Pseudobulges may be the result of extremely gas-rich mergers that happened more recently than those mergers that formed classical bulges (within the last 5 billion years). However, it is"
  },
  {
    "source": "Galactic bulge.txt",
    "chunk_id": "Galactic bulge.txt_3",
    "chunk": "difficult for disks to survive the merging process, casting doubt on this scenario. Many astronomers suggest that bulges that appear similar to disks form outside of the disk, and are not the product of a merging process. When left alone, disk galaxies can rearrange their stars and gas (as a response to instabilities). The products of this process (called secular evolution) are often observed in such galaxies; both spiral disks and galactic bars can result from secular evolution of galaxy disks. Secular evolution is also expected to send gas and stars to the center of a galaxy. If this happens that would increase the density at the center of the galaxy, and thus make a bulge that has properties similar to those of disk galaxies. If secular evolution, or the slow, steady evolution of a galaxy, is responsible for the formation of a significant number of bulges, then that many galaxies have not experienced a merger since the formation of their disk. This would then mean that current theories of galaxy formation and evolution greatly over-predict the number of mergers in the past few billion years. Edge-on galaxies can sometimes have a boxy/peanut bulge with an X-shape. The boxy nature"
  },
  {
    "source": "Galactic bulge.txt",
    "chunk_id": "Galactic bulge.txt_4",
    "chunk": "of the Milky Way bulge was revealed by the COBE satellite and later confirmed with the VVV survey with the help of red clump stars. The VVV survey also found two overlapping populations of red clump stars and an X-shape of the bulge. The WISE satellite later confirmed the X-shape of the bulge. The X-shape makes up 45% of the mass of the bulge in the Milky Way. The boxy/peanut bulges are in fact the bar of a galaxy seen edge-on. Other edge-on galaxies can also show a boxy/peanut bar sometimes with an X-shape. Most bulges and pseudo-bulges are thought to host a central relativistic compact mass, which is traditionally assumed to be a supermassive black hole. Such black holes by definition cannot be observed directly (light cannot escape them), but various pieces of evidence suggest their existence, both in the bulges of spiral galaxies and in the centers of ellipticals. The masses of the black holes correlate tightly with bulge properties. The M–sigma relation relates black hole mass to the velocity dispersion of bulge stars, while other correlations involve the total stellar mass or luminosity of the bulge, the central concentration of stars in the bulge, the richness of"
  },
  {
    "source": "Galactic bulge.txt",
    "chunk_id": "Galactic bulge.txt_5",
    "chunk": "the globular cluster system orbiting in the galaxy's far outskirts, and the winding angle of the spiral arms. Until recently it was thought that one could not have a supermassive black hole without a surrounding bulge. Galaxies hosting supermassive black holes without accompanying bulges have now been observed. The implication is that the bulge environment is not strictly essential to the initial seeding and growth of massive black holes."
  },
  {
    "source": "Galactic coordinate system.txt",
    "chunk_id": "Galactic coordinate system.txt_0",
    "chunk": "# Galactic coordinate system The galactic coordinate system is a celestial coordinate system in spherical coordinates, with the Sun as its center, the primary direction aligned with the approximate center of the Milky Way Galaxy, and the fundamental plane parallel to an approximation of the galactic plane but offset to its north. It uses the right-handed convention, meaning that coordinates are positive toward the north and toward the east in the fundamental plane. Longitude (symbol l) measures the angular distance of an object eastward along the galactic equator from the Galactic Center. Analogous to terrestrial longitude, galactic longitude is usually measured in degrees (°). Latitude (symbol b) measures the angle of an object northward of the galactic equator (or midplane) as viewed from Earth. Analogous to terrestrial latitude, galactic latitude is usually measured in degrees (°). The first galactic coordinate system was used by William Herschel in 1785. A number of different coordinate systems, each differing by a few degrees, were used until 1932, when Lund Observatory assembled a set of conversion tables that defined a standard galactic coordinate system based on a galactic north pole at RA 12 40, dec +28° (in the B1900.0 epoch convention) and a 0°"
  },
  {
    "source": "Galactic coordinate system.txt",
    "chunk_id": "Galactic coordinate system.txt_1",
    "chunk": "longitude at the point where the galactic plane and equatorial plane intersected. In 1958, the International Astronomical Union (IAU) defined the galactic coordinate system in reference to radio observations of galactic neutral hydrogen through the hydrogen line, changing the definition of the Galactic longitude by 32° and the latitude by 1.5°. In the equatorial coordinate system, for equinox and equator of 1950.0, the north galactic pole is defined at right ascension 12 49, declination +27.4°, in the constellation Coma Berenices, with a probable error of ±0.1°. Longitude 0° is the great semicircle that originates from this point along the line in position angle 123° with respect to the equatorial pole. The galactic longitude increases in the same direction as right ascension. Galactic latitude is positive towards the north galactic pole, with a plane passing through the Sun and parallel to the galactic equator being 0°, whilst the poles are ±90°. Based on this definition, the galactic poles and equator can be found from spherical trigonometry and can be precessed to other epochs; see the table. The IAU recommended that during the transition period from the old, pre-1958 system to the new, the old longitude and latitude should be designated l"
  },
  {
    "source": "Galactic coordinate system.txt",
    "chunk_id": "Galactic coordinate system.txt_2",
    "chunk": "and b while the new should be designated l and b. This convention is occasionally seen. Radio source Sagittarius A*, which is the best physical marker of the true Galactic Center, is located at 17 45 40.0409, −29° 00′ 28.118″ (J2000). Rounded to the same number of digits as the table, 17 45.7, −29.01° (J2000), there is an offset of about 0.07° from the defined coordinate center, well within the 1958 error estimate of ±0.1°. Due to the Sun's position, which currently lies 56.75±6.20 ly north of the midplane, and the heliocentric definition adopted by the IAU, the galactic coordinates of Sgr A* are latitude +0° 07′ 12″ south, longitude 0° 04′ 06″. Since as defined the galactic coordinate system does not rotate with time, Sgr A* is actually decreasing in longitude at the rate of galactic rotation at the sun, Ω, approximately 5.7 milliarcseconds per year (see Oort constants). An object's location expressed in the equatorial coordinate system can be transformed into the galactic coordinate system. In these equations, α is right ascension, δ is declination. NGP refers to the coordinate values of the north galactic pole and NCP to those of the north celestial pole. The reverse (galactic"
  },
  {
    "source": "Galactic coordinate system.txt",
    "chunk_id": "Galactic coordinate system.txt_3",
    "chunk": "to equatorial) can also be accomplished with the following conversion formulas. In some applications use is made of rectangular coordinates based on galactic longitude and latitude and distance. In some work regarding the distant past or future the galactic coordinate system is taken as rotating so that the x-axis always goes to the centre of the galaxy. There are two major rectangular variations of galactic coordinates, commonly used for computing space velocities of galactic objects. In these systems the xyz-axes are designated UVW, but the definitions vary by author. In one system, the U axis is directed toward the Galactic Center (l = 0°), and it is a right-handed system (positive towards the east and towards the north galactic pole); in the other, the U axis is directed toward the galactic anticenter (l = 180°), and it is a left-handed system (positive towards the east and towards the north galactic pole)."
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_0",
    "chunk": "# Galaxy A galaxy is a system of stars, stellar remnants, interstellar gas, dust, and dark matter bound together by gravity. The word is derived from the Greek galaxias (γαλαξίας), literally 'milky', a reference to the Milky Way galaxy that contains the Solar System. Galaxies, averaging an estimated 100 million stars, range in size from dwarfs with less than a thousand stars, to the largest galaxies known – supergiants with one hundred trillion stars, each orbiting its galaxy's centre of mass. Most of the mass in a typical galaxy is in the form of dark matter, with only a few per cent of that mass visible in the form of stars and nebulae. Supermassive black holes are a common feature at the centres of galaxies. Galaxies are categorised according to their visual morphology as elliptical, spiral, or irregular. The Milky Way is an example of a spiral galaxy. It is estimated that there are between 200 billion (2×10) to 2 trillion galaxies in the observable universe. Most galaxies are 1,000 to 100,000 parsecs in diameter (approximately 3,000 to 300,000 light years) and are separated by distances in the order of millions of parsecs (or megaparsecs). For comparison, the Milky Way"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_1",
    "chunk": "has a diameter of at least 26,800 parsecs (87,400 ly) and is separated from the Andromeda Galaxy, its nearest large neighbour, by just over 750,000 parsecs (2.5 million ly). The space between galaxies is filled with a tenuous gas (the intergalactic medium) with an average density of less than one atom per cubic metre. Most galaxies are gravitationally organised into groups, clusters and superclusters. The Milky Way is part of the Local Group, which it dominates along with the Andromeda Galaxy. The group is part of the Virgo Supercluster. At the largest scale, these associations are generally arranged into sheets and filaments surrounded by immense voids. Both the Local Group and the Virgo Supercluster are contained in a much larger cosmic structure named Laniakea. The word galaxy was borrowed via French and Medieval Latin from the Greek term for the Milky Way, galaxías (kúklos) γαλαξίας (κύκλος) 'milky (circle)', named after its appearance as a milky band of light in the sky. In the astronomical literature, the capitalised word \"Galaxy\" is often used to refer to the Milky Way galaxy, to distinguish it from the other galaxies in the universe. Galaxies were initially discovered telescopically and were known as spiral nebulae."
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_2",
    "chunk": "Most 18th- to 19th-century astronomers considered them as either unresolved star clusters or extragalactic nebulae, but their true composition and natures remained a mystery. Observations using larger telescopes of a few nearby bright galaxies, like the Andromeda Galaxy, began resolving them into huge conglomerations of stars, but based simply on the apparent faintness and sheer population of stars, the true distances of these objects placed them well beyond the Milky Way. For this reason they were popularly called island universes. Harlow Shapley began to advocate for the term \"galaxy\" and against using \"universes\" and \"nebula\" for the objects but the very influential Edwin Hubble stuck to nebulae. The nomenclature did not fully change in until Hubble's death in 1953. Millions of galaxies have been catalogued, but only a few have well-established names, such as the Andromeda Galaxy, the Magellanic Clouds, the Whirlpool Galaxy, and the Sombrero Galaxy. Astronomers work with numbers from certain catalogues, such as the Messier catalogue, the NGC (New General Catalogue), the IC (Index Catalogue), the CGCG (Catalogue of Galaxies and of Clusters of Galaxies), the MCG (Morphological Catalogue of Galaxies), the UGC (Uppsala General Catalogue of Galaxies), and the PGC (Catalogue of Principal Galaxies, also known"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_3",
    "chunk": "as LEDA). All the well-known galaxies appear in one or more of these catalogues but each time under a different number. For example, Messier 109 (or \"M109\") is a spiral galaxy having the number 109 in the catalogue of Messier. It also has the designations NGC 3992, UGC 6937, CGCG 269–023, MCG +09-20-044, and PGC 37617 (or LEDA 37617), among others. Millions of fainter galaxies are known by their identifiers in sky surveys such as the Sloan Digital Sky Survey. Greek philosopher Democritus (450–370 BCE) proposed that the bright band on the night sky known as the Milky Way might consist of distant stars. Aristotle (384–322 BCE), however, believed the Milky Way was caused by \"the ignition of the fiery exhalation of some stars that were large, numerous and close together\" and that the \"ignition takes place in the upper part of the atmosphere, in the region of the World that is continuous with the heavenly motions.\" Neoplatonist philosopher Olympiodorus the Younger (c. 495–570 CE) was critical of this view, arguing that if the Milky Way was sublunary (situated between Earth and the Moon) it should appear different at different times and places on Earth, and that it should have"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_4",
    "chunk": "parallax, which it did not. In his view, the Milky Way was celestial. According to Mohani Mohamed, Arabian astronomer Ibn al-Haytham (965–1037) made the first attempt at observing and measuring the Milky Way's parallax, and he thus \"determined that because the Milky Way had no parallax, it must be remote from the Earth, not belonging to the atmosphere.\" Persian astronomer al-Biruni (973–1048) proposed the Milky Way galaxy was \"a collection of countless fragments of the nature of nebulous stars.\" Andalusian astronomer Avempace (d. 1138) proposed that it was composed of many stars that almost touched one another, and appeared to be a continuous image due to the effect of refraction from sublunary material, citing his observation of the conjunction of Jupiter and Mars as evidence of this occurring when two objects were near. In the 14th century, Syrian-born Ibn Qayyim al-Jawziyya proposed the Milky Way galaxy was \"a myriad of tiny stars packed together in the sphere of the fixed stars.\" Actual proof of the Milky Way consisting of many stars came in 1610 when the Italian astronomer Galileo Galilei used a telescope to study it and discovered it was composed of a huge number of faint stars. In 1750,"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_5",
    "chunk": "English astronomer Thomas Wright, in his An Original Theory or New Hypothesis of the Universe, correctly speculated that it might be a rotating body of a huge number of stars held together by gravitational forces, akin to the Solar System but on a much larger scale, and that the resulting disk of stars could be seen as a band on the sky from a perspective inside it. In his 1755 treatise, Immanuel Kant elaborated on Wright's idea about the Milky Way's structure. The first project to describe the shape of the Milky Way and the position of the Sun was undertaken by William Herschel in 1785 by counting the number of stars in different regions of the sky. He produced a diagram of the shape of the galaxy with the Solar System close to the center. Using a refined approach, Kapteyn in 1920 arrived at the picture of a small (diameter about 15 kiloparsecs) ellipsoid galaxy with the Sun close to the center. A different method by Harlow Shapley based on the cataloguing of globular clusters led to a radically different picture: a flat disk with diameter approximately 70 kiloparsecs and the Sun far from the centre. Both analyses failed"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_6",
    "chunk": "to take into account the absorption of light by interstellar dust present in the galactic plane; but after Robert Julius Trumpler quantified this effect in 1930 by studying open clusters, the present picture of the Milky Way galaxy emerged. A few galaxies outside the Milky Way are visible on a dark night to the unaided eye, including the Andromeda Galaxy, Large Magellanic Cloud, Small Magellanic Cloud, and the Triangulum Galaxy. In the 10th century, Persian astronomer Abd al-Rahman al-Sufi made the earliest recorded identification of the Andromeda Galaxy, describing it as a \"small cloud\". In 964, he probably mentioned the Large Magellanic Cloud in his Book of Fixed Stars, referring to \"Al Bakr of the southern Arabs\", since at a declination of about 70° south it was not visible where he lived. It was not well known to Europeans until Magellan's voyage in the 16th century. The Andromeda Galaxy was later independently noted by Simon Marius in 1612. In 1734, philosopher Emanuel Swedenborg in his Principia speculated that there might be other galaxies outside that were formed into galactic clusters that were minuscule parts of the universe that extended far beyond what could be seen. Swedenborg's views \"are remarkably close"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_7",
    "chunk": "to the present-day views of the cosmos.\" In 1745, Pierre Louis Maupertuis conjectured that some nebula-like objects were collections of stars with unique properties, including a glow exceeding the light its stars produced on their own, and repeated Johannes Hevelius's view that the bright spots were massive and flattened due to their rotation. In 1750, Thomas Wright correctly speculated that the Milky Way was a flattened disk of stars, and that some of the nebulae visible in the night sky might be separate Milky Ways. Toward the end of the 18th century, Charles Messier compiled a catalog containing the 109 brightest celestial objects having nebulous appearance. Subsequently, William Herschel assembled a catalog of 5,000 nebulae. In 1845, Lord Rosse examined the nebulae catalogued by Herschel and observed the spiral structure of Messier object M51, now known as the Whirlpool Galaxy. In 1912, Vesto M. Slipher made spectrographic studies of the brightest spiral nebulae to determine their composition. Slipher discovered that the spiral nebulae have high Doppler shifts, indicating that they are moving at a rate exceeding the velocity of the stars he had measured. He found that the majority of these nebulae are moving away from us. In 1917, Heber"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_8",
    "chunk": "Doust Curtis observed nova S Andromedae within the \"Great Andromeda Nebula\", as the Andromeda Galaxy, Messier object M31, was then known. Searching the photographic record, he found 11 more novae. Curtis noticed that these novae were, on average, 10 magnitudes fainter than those that occurred within this galaxy. As a result, he was able to come up with a distance estimate of 150,000 parsecs. He became a proponent of the so-called \"island universes\" hypothesis, which holds that spiral nebulae are actually independent galaxies. In 1920 a debate took place between Harlow Shapley and Heber Curtis, the Great Debate, concerning the nature of the Milky Way, spiral nebulae, and the dimensions of the universe. To support his claim that the Great Andromeda Nebula is an external galaxy, Curtis noted the appearance of dark lanes resembling the dust clouds in the Milky Way, as well as the significant Doppler shift. In 1922, the Estonian astronomer Ernst Öpik gave a distance determination that supported the theory that the Andromeda Nebula is indeed a distant extra-galactic object. Using the new 100-inch Mt. Wilson telescope, Edwin Hubble was able to resolve the outer parts of some spiral nebulae as collections of individual stars and identified"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_9",
    "chunk": "some Cepheid variables, thus allowing him to estimate the distance to the nebulae: they were far too distant to be part of the Milky Way. In 1926 Hubble produced a classification of galactic morphology that is used to this day. Advances in astronomy have always been driven by technology. After centuries of success in optical astronomy, recent decades have seen major progress in other regions of the electromagnetic spectrum. The dust present in the interstellar medium is opaque to visual light. It is more transparent to far-infrared, which can be used to observe the interior regions of giant molecular clouds and galactic cores in great detail. Infrared is also used to observe distant, red-shifted galaxies that were formed much earlier. Water vapor and carbon dioxide absorb a number of useful portions of the infrared spectrum, so high-altitude or space-based telescopes are used for infrared astronomy. The first non-visual study of galaxies, particularly active galaxies, was made using radio frequencies. The Earth's atmosphere is nearly transparent to radio between 5 MHz and 30 GHz. The ionosphere blocks signals below this range. Large radio interferometers have been used to map the active jets emitted from active nuclei. Ultraviolet and X-ray telescopes can"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_10",
    "chunk": "observe highly energetic galactic phenomena. Ultraviolet flares are sometimes observed when a star in a distant galaxy is torn apart from the tidal forces of a nearby black hole. The distribution of hot gas in galactic clusters can be mapped by X-rays. The existence of supermassive black holes at the cores of galaxies was confirmed through X-ray astronomy. In 1944, Hendrik van de Hulst predicted that microwave radiation with wavelength of 21 cm would be detectable from interstellar atomic hydrogen gas; and in 1951 it was observed. This radiation is not affected by dust absorption, and so its Doppler shift can be used to map the motion of the gas in this galaxy. These observations led to the hypothesis of a rotating bar structure in the center of this galaxy. With improved radio telescopes, hydrogen gas could also be traced in other galaxies. In the 1970s, Vera Rubin uncovered a discrepancy between observed galactic rotation speed and that predicted by the visible mass of stars and gas. Today, the galaxy rotation problem is thought to be explained by the presence of large quantities of unseen dark matter. Beginning in the 1990s, the Hubble Space Telescope yielded improved observations. Among other"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_11",
    "chunk": "things, its data helped establish that the missing dark matter in this galaxy could not consist solely of inherently faint and small stars. The Hubble Deep Field, an extremely long exposure of a relatively empty part of the sky, provided evidence that there are about 125 billion (1.25×10) galaxies in the observable universe. Improved technology in detecting the spectra invisible to humans (radio telescopes, infrared cameras, and x-ray telescopes) allows detection of other galaxies that are not detected by Hubble. Particularly, surveys in the Zone of Avoidance (the region of sky blocked at visible-light wavelengths by the Milky Way) have revealed a number of new galaxies. A 2016 study published in The Astrophysical Journal, led by Christopher Conselice of the University of Nottingham, analyzed many sources of data to estimate that the observable universe (up to z=8) contained at least two trillion (2×10) galaxies, a factor of 10 more than are directly observed in Hubble images. However, later observations with the New Horizons space probe from outside the zodiacal light observed less cosmic optical light than Conselice while still suggesting that direct observations are missing galaxies. Galaxies come in three main types: ellipticals, spirals, and irregulars. A slightly more extensive"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_12",
    "chunk": "description of galaxy types based on their appearance is given by the Hubble sequence. Since the Hubble sequence is entirely based upon visual morphological type (shape), it may miss certain important characteristics of galaxies such as star formation rate in starburst galaxies and activity in the cores of active galaxies. Many galaxies are thought to contain a supermassive black hole at their center. This includes the Milky Way, whose core region is called the Galactic Center. The Hubble classification system rates elliptical galaxies on the basis of their ellipticity, ranging from E0, being nearly spherical, up to E7, which is highly elongated. These galaxies have an ellipsoidal profile, giving them an elliptical appearance regardless of the viewing angle. Their appearance shows little structure and they typically have relatively little interstellar matter. Consequently, these galaxies also have a low portion of open clusters and a reduced rate of new star formation. Instead, they are dominated by generally older, more evolved stars that are orbiting the common center of gravity in random directions. The stars contain low abundances of heavy elements because star formation ceases after the initial burst. In this sense they have some similarity to the much smaller globular clusters."
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_13",
    "chunk": "The largest galaxies are the type-cD galaxies. First described in 1964 by a paper by Thomas A. Matthews and others, they are a subtype of the more general class of D galaxies, which are giant elliptical galaxies, except that they are much larger. They are popularly known as the supergiant elliptical galaxies and constitute the largest and most luminous galaxies known. These galaxies feature a central elliptical nucleus with an extensive, faint halo of stars extending to megaparsec scales. The profile of their surface brightnesses as a function of their radius (or distance from their cores) falls off more slowly than their smaller counterparts. The formation of these cD galaxies remains an active area of research, but the leading model is that they are the result of the mergers of smaller galaxies in the environments of dense clusters, or even those outside of clusters with random overdensities. These processes are the mechanisms that drive the formation of fossil groups or fossil clusters, where a large, relatively isolated, supergiant elliptical resides in the middle of the cluster and are surrounded by an extensive cloud of X-rays as the residue of these galactic collisions. Another older model posits the phenomenon of cooling"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_14",
    "chunk": "flow, where the heated gases in clusters collapses towards their centers as they cool, forming stars in the process, a phenomenon observed in clusters such as Perseus, and more recently in the Phoenix Cluster. A shell galaxy is a type of elliptical galaxy where the stars in its halo are arranged in concentric shells. About one-tenth of elliptical galaxies have a shell-like structure, which has never been observed in spiral galaxies. These structures are thought to develop when a larger galaxy absorbs a smaller companion galaxy—that as the two galaxy centers approach, they start to oscillate around a center point, and the oscillation creates gravitational ripples forming the shells of stars, similar to ripples spreading on water. For example, galaxy NGC 3923 has over 20 shells. Spiral galaxies resemble spiraling pinwheels. Though the stars and other visible material contained in such a galaxy lie mostly on a plane, the majority of mass in spiral galaxies exists in a roughly spherical halo of dark matter which extends beyond the visible component, as demonstrated by the universal rotation curve concept. Spiral galaxies consist of a rotating disk of stars and interstellar medium, along with a central bulge of generally older stars. Extending"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_15",
    "chunk": "outward from the bulge are relatively bright arms. In the Hubble classification scheme, spiral galaxies are listed as type S, followed by a letter (a, b, or c) which indicates the degree of tightness of the spiral arms and the size of the central bulge. An Sa galaxy has tightly wound, poorly defined arms and possesses a relatively large core region. At the other extreme, an Sc galaxy has open, well-defined arms and a small core region. A galaxy with poorly defined arms is sometimes referred to as a flocculent spiral galaxy; in contrast to the grand design spiral galaxy that has prominent and well-defined spiral arms. The speed in which a galaxy rotates is thought to correlate with the flatness of the disc as some spiral galaxies have thick bulges, while others are thin and dense. In spiral galaxies, the spiral arms do have the shape of approximate logarithmic spirals, a pattern that can be theoretically shown to result from a disturbance in a uniformly rotating mass of stars. Like the stars, the spiral arms rotate around the center, but they do so with constant angular velocity. The spiral arms are thought to be areas of high-density matter, or"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_16",
    "chunk": "\"density waves\". As stars move through an arm, the space velocity of each stellar system is modified by the gravitational force of the higher density. (The velocity returns to normal after the stars depart on the other side of the arm.) This effect is akin to a \"wave\" of slowdowns moving along a highway full of moving cars. The arms are visible because the high density facilitates star formation, and therefore they harbor many bright and young stars. A majority of spiral galaxies, including the Milky Way galaxy, have a linear, bar-shaped band of stars that extends outward to either side of the core, then merges into the spiral arm structure. In the Hubble classification scheme, these are designated by an SB, followed by a lower-case letter (a, b or c) which indicates the form of the spiral arms (in the same manner as the categorization of normal spiral galaxies). Bars are thought to be temporary structures that can occur as a result of a density wave radiating outward from the core, or else due to a tidal interaction with another galaxy. Many barred spiral galaxies are active, possibly as a result of gas being channeled into the core along"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_17",
    "chunk": "the arms. Our own galaxy, the Milky Way, is a large disk-shaped barred-spiral galaxy about 30 kiloparsecs in diameter and a kiloparsec thick. It contains about two hundred billion (2×10) stars and has a total mass of about six hundred billion (6×10) times the mass of the Sun. Recently, researchers described galaxies called super-luminous spirals. They are very large with an upward diameter of 437,000 light-years (compared to the Milky Way's 87,400 light-year diameter). With a mass of 340 billion solar masses, they generate a significant amount of ultraviolet and mid-infrared light. They are thought to have an increased star formation rate around 30 times faster than the Milky Way. Despite the prominence of large elliptical and spiral galaxies, most galaxies are dwarf galaxies. They are relatively small when compared with other galactic formations, being about one hundredth the size of the Milky Way, with only a few billion stars. Blue compact dwarf galaxies contains large clusters of young, hot, massive stars. Ultra-compact dwarf galaxies have been discovered that are only 100 parsecs across. Many dwarf galaxies may orbit a single larger galaxy; the Milky Way has at least a dozen such satellites, with an estimated 300–500 yet to be"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_18",
    "chunk": "discovered. Most of the information we have about dwarf galaxies come from observations of the local group, containing two spiral galaxies, the Milky Way and Andromeda, and many dwarf galaxies. These dwarf galaxies are classified as either irregular or dwarf elliptical/dwarf spheroidal galaxies. A study of 27 Milky Way neighbors found that in all dwarf galaxies, the central mass is approximately 10 million solar masses, regardless of whether it has thousands or millions of stars. This suggests that galaxies are largely formed by dark matter, and that the minimum size may indicate a form of warm dark matter incapable of gravitational coalescence on a smaller scale. Interactions between galaxies are relatively frequent, and they can play an important role in galactic evolution. Near misses between galaxies result in warping distortions due to tidal interactions, and may cause some exchange of gas and dust. Collisions occur when two galaxies pass directly through each other and have sufficient relative momentum not to merge. The stars of interacting galaxies usually do not collide, but the gas and dust within the two forms interacts, sometimes triggering star formation. A collision can severely distort the galaxies' shapes, forming bars, rings or tail-like structures. At the"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_19",
    "chunk": "extreme of interactions are galactic mergers, where the galaxies' relative momentums are insufficient to allow them to pass through each other. Instead, they gradually merge to form a single, larger galaxy. Mergers can result in significant changes to the galaxies' original morphology. If one of the galaxies is much more massive than the other, the result is known as cannibalism, where the more massive larger galaxy remains relatively undisturbed, and the smaller one is torn apart. The Milky Way galaxy is currently in the process of cannibalizing the Sagittarius Dwarf Elliptical Galaxy and the Canis Major Dwarf Galaxy. Stars are created within galaxies from a reserve of cold gas that forms giant molecular clouds. Some galaxies have been observed to form stars at an exceptional rate, which is known as a starburst. If they continue to do so, they would consume their reserve of gas in a time span less than the galaxy's lifespan. Hence starburst activity usually lasts only about ten million years, a relatively brief period in a galaxy's history. Starburst galaxies were more common during the universe's early history, but still contribute an estimated 15% to total star production. Starburst galaxies are characterized by dusty concentrations of"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_20",
    "chunk": "gas and the appearance of newly formed stars, including massive stars that ionize the surrounding clouds to create H II regions. These stars produce supernova explosions, creating expanding remnants that interact powerfully with the surrounding gas. These outbursts trigger a chain reaction of star-building that spreads throughout the gaseous region. Only when the available gas is nearly consumed or dispersed does the activity end. Starbursts are often associated with merging or interacting galaxies. The prototype example of such a starburst-forming interaction is M82, which experienced a close encounter with the larger M81. Irregular galaxies often exhibit spaced knots of starburst activity. A radio galaxy is a galaxy with giant regions of radio emission extending well beyond its visible structure. These energetic radio lobes are powered by jets from its active galactic nucleus. Radio galaxies are classified according to their Fanaroff–Riley classification. The FR I class have lower radio luminosity and exhibit structures which are more elongated; the FR II class are higher radio luminosity. The correlation of radio luminosity and structure suggests that the sources in these two types of galaxies may differ. Radio galaxies can also be classified as giant radio galaxies (GRGs), whose radio emissions can extend to"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_21",
    "chunk": "scales of megaparsecs (3.26 million light-years). Alcyoneus is an FR II class low-excitation radio galaxy which has the largest observed radio emission, with lobed structures spanning 5 megaparsecs (16×10 ly). For comparison, another similarly sized giant radio galaxy is 3C 236, with lobes 15 million light-years across. It should however be noted that radio emissions are not always considered part of the main galaxy itself. A giant radio galaxy is a special class of objects characterized by the presence of radio lobes generated by relativistic jets powered by the central galaxy's supermassive black hole. Giant radio galaxies are different from ordinary radio galaxies in that they can extend to much larger scales, reaching upwards to several megaparsecs across, far larger than the diameters of their host galaxies. A \"normal\" radio galaxy do not have a source that is a supermassive black hole or monster neutron star; instead the source is synchrotron radiation from relativistic electrons accelerated by supernova. These sources are comparatively short lived, making the radio spectrum from normal radio galaxies an especially good way to study star formation. Some observable galaxies are classified as \"active\" if they contain an active galactic nucleus (AGN). A significant portion of the"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_22",
    "chunk": "galaxy's total energy output is emitted by the active nucleus instead of its stars, dust and interstellar medium. There are multiple classification and naming schemes for AGNs, but those in the lower ranges of luminosity are called Seyfert galaxies, while those with luminosities much greater than that of the host galaxy are known as quasi-stellar objects or quasars. Models of AGNs suggest that a significant fraction of their light is shifted to far-infrared frequencies because optical and UV emission in the nucleus is absorbed and remitted by dust and gas surrounding it. The standard model for an active galactic nucleus is based on an accretion disc that forms around a supermassive black hole (SMBH) at the galaxy's core region. The radiation from an active galactic nucleus results from the gravitational energy of matter as it falls toward the black hole from the disc. The AGN's luminosity depends on the SMBH's mass and the rate at which matter falls onto it. In about 10% of these galaxies, a diametrically opposed pair of energetic jets ejects particles from the galaxy core at velocities close to the speed of light. The mechanism for producing these jets is not well understood. Seyfert galaxies are"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_23",
    "chunk": "one of the two largest groups of active galaxies, along with quasars. They have quasar-like nuclei (very luminous, distant and bright sources of electromagnetic radiation) with very high surface brightnesses; but unlike quasars, their host galaxies are clearly detectable. Seen through a telescope, a Seyfert galaxy appears like an ordinary galaxy with a bright star superimposed atop the core. Seyfert galaxies are divided into two principal subtypes based on the frequencies observed in their spectra. Quasars are the most energetic and distant members of active galactic nuclei. Extremely luminous, they were first identified as high redshift sources of electromagnetic energy, including radio waves and visible light, that appeared more similar to stars than to extended sources similar to galaxies. Their luminosity can be 100 times that of the Milky Way. The nearest known quasar, Markarian 231, is about 581 million light-years from Earth, while others have been discovered as far away as UHZ1, roughly 13.2 billion light-years distant. Quasars are noteworthy for providing the first demonstration of the phenomenon that gravity can act as a lens for light. Blazars are believed to be active galaxies with a relativistic jet pointed in the direction of Earth. A radio galaxy emits radio"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_24",
    "chunk": "frequencies from relativistic jets. A unified model of these types of active galaxies explains their differences based on the observer's position. Possibly related to active galactic nuclei (as well as starburst regions) are low-ionization nuclear emission-line regions (LINERs). The emission from LINER-type galaxies is dominated by weakly ionized elements. The excitation sources for the weakly ionized lines include post-AGB stars, AGN, and shocks. Approximately one-third of nearby galaxies are classified as containing LINER nuclei. Luminous infrared galaxies (LIRGs) are galaxies with luminosities—the measurement of electromagnetic power output—above 10 L☉ (solar luminosities). In most cases, most of their energy comes from large numbers of young stars which heat surrounding dust, which reradiates the energy in the infrared. Luminosity high enough to be a LIRG requires a star formation rate of at least 18 M☉ yr. Ultra-luminous infrared galaxies (ULIRGs) are at least ten times more luminous still and form stars at rates >180 M☉ yr. Many LIRGs also emit radiation from an AGN. Infrared galaxies emit more energy in the infrared than all other wavelengths combined, with peak emission typically at wavelengths of 60 to 100 microns. LIRGs are believed to be created from the strong interaction and merger of spiral"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_25",
    "chunk": "galaxies. While uncommon in the local universe, LIRGs and ULIRGS were more prevalent when the universe was younger. Galaxies do not have a definite boundary by their nature, and are characterized by a gradually decreasing stellar density as a function of increasing distance from their center, making measurements of their true extents difficult. Nevertheless, astronomers over the past few decades have made several criteria in defining the sizes of galaxies. As early as the time of Edwin Hubble in 1936, there have been attempts to characterize the diameters of galaxies. The earliest efforts were based on the observed angle subtended by the galaxy and its estimated distance, leading to an angular diameter (also called \"metric diameter\"). The isophotal diameter is introduced as a conventional way of measuring a galaxy's size based on its apparent surface brightness. Isophotes are curves in a diagram - such as a picture of a galaxy - that adjoins points of equal brightnesses, and are useful in defining the extent of the galaxy. The apparent brightness flux of a galaxy is measured in units of magnitudes per square arcsecond (mag/arcsec; sometimes expressed as mag arcsec), which defines the brightness depth of the isophote. To illustrate how"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_26",
    "chunk": "this unit works, a typical galaxy has a brightness flux of 18 mag/arcsec at its central region. This brightness is equivalent to the light of an 18th magnitude hypothetical point object (like a star) being spread out evenly in a one square arcsecond area of the sky. The isophotal diameter is typically defined as the region enclosing all the light down to 25 mag/arcsec in the blue B-band, which is then referred to as the D25 standard. The half-light radius (also known as effective radius; Re) is a measure that is based on the galaxy's overall brightness flux. This is the radius upon which half, or 50%, of the total brightness flux of the galaxy was emitted. This was first proposed by Gérard de Vaucouleurs in 1948. The choice of using 50% was arbitrary, but proved to be useful in further works by R. A. Fish in 1963, where he established a luminosity concentration law that relates the brightnesses of elliptical galaxies and their respective Re, and by José Luis Sérsic in 1968 that defined a mass-radius relation in galaxies. In defining Re, it is necessary that the overall brightness flux galaxy should be captured, with a method employed by"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_27",
    "chunk": "Bershady in 2000 suggesting to measure twice the size where the brightness flux of an arbitrarily chosen radius, defined as the local flux, divided by the overall average flux equals to 0.2. Using half-light radius allows a rough estimate of a galaxy's size, but is not particularly helpful in determining its morphology. Variations of this method exist. In particular, in the ESO-Uppsala Catalogue of Galaxies values of 50%, 70%, and 90% of the total blue light (the light detected through a B-band specific filter) had been used to calculate a galaxy's diameter. First described by Vahe Petrosian in 1976, a modified version of this method has been used by the Sloan Digital Sky Survey (SDSS). This method employs a mathematical model on a galaxy whose radius is determined by the azimuthally (horizontal) averaged profile of its brightness flux. In particular, the SDSS employed the Petrosian magnitude in the R-band (658 nm, in the red part of the visible spectrum) to ensure that the brightness flux of a galaxy would be captured as much as possible while counteracting the effects of background noise. For a galaxy whose brightness profile is exponential, it is expected to capture all of its brightness flux,"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_28",
    "chunk": "and 80% for galaxies that follow a profile that follows de Vaucouleurs's law. Petrosian magnitudes have the advantage of being redshift and distance independent, allowing the measurement of the galaxy's apparent size since the Petrosian radius is defined in terms of the galaxy's overall luminous flux. A critique of an earlier version of this method has been issued by the Infrared Processing and Analysis Center, with the method causing a magnitude of error (upwards to 10%) of the values than using isophotal diameter. The use of Petrosian magnitudes also have the disadvantage of missing most of the light outside the Petrosian aperture, which is defined relative to the galaxy's overall brightness profile, especially for elliptical galaxies, with higher signal-to-noise ratios on higher distances and redshifts. A correction for this method has been issued by Graham et al. in 2005, based on the assumption that galaxies follow Sérsic's law. This method has been used by 2MASS as an adaptation from the previously used methods of isophotal measurement. Since 2MASS operates in the near infrared, which has the advantage of being able to recognize dimmer, cooler, and older stars, it has a different form of approach compared to other methods that normally"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_29",
    "chunk": "use B-filter. The detail of the method used by 2MASS has been described thoroughly in a document by Jarrett et al., with the survey measuring several parameters. The standard aperture ellipse (area of detection) is defined by the infrared isophote at the Ks band (roughly 2.2 μm wavelength) of 20 mag/arcsec. Gathering the overall luminous flux of the galaxy has been employed by at least four methods: the first being a circular aperture extending 7 arcseconds from the center, an isophote at 20 mag/arcsec, a \"total\" aperture defined by the radial light distribution that covers the supposed extent of the galaxy, and the Kron aperture (defined as 2.5 times the first-moment radius, an integration of the flux of the \"total\" aperture). Deep-sky surveys show that galaxies are often found in groups and clusters. Solitary galaxies that have not significantly interacted with other galaxies of comparable mass in the past few billion years are relatively scarce. Only about 5% of the galaxies surveyed are isolated in this sense. However, they may have interacted and even merged with other galaxies in the past, and may still be orbited by smaller satellite galaxies. On the largest scale, the universe is continually expanding, resulting"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_30",
    "chunk": "in an average increase in the separation between individual galaxies (see Hubble's law). Associations of galaxies can overcome this expansion on a local scale through their mutual gravitational attraction. These associations formed early, as clumps of dark matter pulled their respective galaxies together. Nearby groups later merged to form larger-scale clusters. This ongoing merging process, as well as an influx of infalling gas, heats the intergalactic gas in a cluster to very high temperatures of 30–100 megakelvins. About 70–80% of a cluster's mass is in the form of dark matter, with 10–30% consisting of this heated gas and the remaining few percent in the form of galaxies. Most galaxies are gravitationally bound to a number of other galaxies. These form a fractal-like hierarchical distribution of clustered structures, with the smallest such associations being termed groups. A group of galaxies is the most common type of galactic cluster; these formations contain the majority of galaxies (as well as most of the baryonic mass) in the universe. To remain gravitationally bound to such a group, each member galaxy must have a sufficiently low velocity to prevent it from escaping (see Virial theorem). If there is insufficient kinetic energy, however, the group may"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_31",
    "chunk": "evolve into a smaller number of galaxies through mergers. Clusters of galaxies consist of hundreds to thousands of galaxies bound together by gravity. Clusters of galaxies are often dominated by a single giant elliptical galaxy, known as the brightest cluster galaxy, which, over time, tidally destroys its satellite galaxies and adds their mass to its own. Superclusters contain tens of thousands of galaxies, which are found in clusters, groups and sometimes individually. At the supercluster scale, galaxies are arranged into sheets and filaments surrounding vast empty voids. Above this scale, the universe appears to be the same in all directions (isotropic and homogeneous), though this notion has been challenged in recent years by numerous findings of large-scale structures that appear to be exceeding this scale. The Hercules–Corona Borealis Great Wall, currently the largest structure in the universe found so far, is 10 billion light-years (three gigaparsecs) in length. The Milky Way galaxy is a member of an association named the Local Group, a relatively small group of galaxies that has a diameter of approximately one megaparsec. The Milky Way and the Andromeda Galaxy are the two brightest galaxies within the group; many of the other member galaxies are dwarf companions"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_32",
    "chunk": "of these two. The Local Group itself is a part of a cloud-like structure within the Virgo Supercluster, a large, extended structure of groups and clusters of galaxies centered on the Virgo Cluster. In turn, the Virgo Supercluster is a portion of the Laniakea Supercluster. Galaxies have magnetic fields of their own. A galaxy's magnetic field influences its dynamics in multiple ways, including affecting the formation of spiral arms and transporting angular momentum in gas clouds. The latter effect is particularly important, as it is a necessary factor for the gravitational collapse of those clouds, and thus for star formation. The typical average equipartition strength for spiral galaxies is about 10 μG (microgauss) or 1 nT (nanotesla). By comparison, the Earth's magnetic field has an average strength of about 0.3 G (Gauss) or 30 μT (microtesla). Radio-faint galaxies like M 31 and M33, the Milky Way's neighbors, have weaker fields (about 5 μG), while gas-rich galaxies with high star-formation rates, like M 51, M 83 and NGC 6946, have 15 μG on average. In prominent spiral arms, the field strength can be up to 25 μG, in regions where cold gas and dust are also concentrated. The strongest total equipartition"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_33",
    "chunk": "fields (50–100 μG) were found in starburst galaxies—for example, in M 82 and the Antennae; and in nuclear starburst regions, such as the centers of NGC 1097 and other barred galaxies. Current models of the formation of galaxies in the early universe are based on the ΛCDM model. About 300,000 years after the Big Bang, atoms of hydrogen and helium began to form, in an event called recombination. Nearly all the hydrogen was neutral (non-ionized) and readily absorbed light, and no stars had yet formed. As a result, this period has been called the \"dark ages\". It was from density fluctuations (or anisotropic irregularities) in this primordial matter that larger structures began to appear. As a result, masses of baryonic matter started to condense within cold dark matter halos. These primordial structures allowed gasses to condense in to protogalaxies, large scale gas clouds that were precursors to the first galaxies. As gas falls in to the gravity of the dark matter halos, its pressure and temperature rise. To condense further, the gas must radiate energy. This process was slow in the early universe dominated by hydrogen atoms and molecules which are inefficient radiators compared to heavier elements. As clumps of"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_34",
    "chunk": "gas aggregate forming rotating disks, temperatures and pressures continue to increase. Some places within the disk reach high enough density to form stars. Once protogalaxies began to form and contract, the first halo stars, called Population III stars, appeared within them. These were composed of primordial gas, almost entirely of hydrogen and helium. Emission from the first stars heats the remaining gas helping to trigger additional star formation; the ultraviolet light emission from the first generation of stars re-ionized the surrounding neutral hydrogen in expanding spheres eventually reaching the entire universe, an event called reionization. The most massive stars collapse in violent supernova explosions releasing heavy elements (\"metals\") into the interstellar medium. This metal content is incorporated into population II stars. Theoretical models for early galaxy formation have been verified and informed by a large number and variety of sophisticated astronomical observations. The photometric observations generally need spectroscopic confirmation due the large number mechanisms that can introduce systematic errors. For example, a high redshift (z ~ 16) photometric observation by James Webb Space Telescope (JWST) was later corrected to be closer to z ~ 5. Nevertheless, confirmed observations from the JWST and other observatories are accumulating, allowing systematic comparison of"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_35",
    "chunk": "early galaxies to predictions of theory. Evidence for individual Population III stars in early galaxies is even more challenging. Even seemingly confirmed spectroscopic evidence may turn out to have other origins. For example, astronomers reported HeII emission evidence for Population III stars in the Cosmos Redshift 7 galaxy, with a redshift value of 6.60. Subsequent observations found metallic emission lines, OIII, inconsistent with an early-galaxy star. Once stars begin to form, emit radiation, and in some cases explode, the process of galaxy formation becomes very complex, involving interactions between the forces of gravity, radiation, and thermal energy. Many details are still poorly understood. Within a billion years of a galaxy's formation, key structures begin to appear. Globular clusters, the central supermassive black hole, and a galactic bulge of metal-poor Population II stars form. The creation of a supermassive black hole appears to play a key role in actively regulating the growth of galaxies by limiting the total amount of additional matter added. During this early epoch, galaxies undergo a major burst of star formation. During the following two billion years, the accumulated matter settles into a galactic disc. A galaxy will continue to absorb infalling material from high-velocity clouds and"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_36",
    "chunk": "dwarf galaxies throughout its life. This matter is mostly hydrogen and helium. The cycle of stellar birth and death slowly increases the abundance of heavy elements, eventually allowing the formation of planets. Star formation rates in galaxies depend upon their local environment. Isolated 'void' galaxies have highest rate per stellar mass, with 'field' galaxies associated with spiral galaxies having lower rates and galaxies in dense cluster having the lowest rates. The evolution of galaxies can be significantly affected by interactions and collisions. Mergers of galaxies were common during the early epoch, and the majority of galaxies were peculiar in morphology. Given the distances between the stars, the great majority of stellar systems in colliding galaxies will be unaffected. However, gravitational stripping of the interstellar gas and dust that makes up the spiral arms produces a long train of stars known as tidal tails. Examples of these formations can be seen in NGC 4676 or the Antennae Galaxies. The Milky Way galaxy and the nearby Andromeda Galaxy are moving toward each other at about 130 km/s, and—depending upon the lateral movements—the two might collide in about five to six billion years. Although the Milky Way has never collided with a galaxy"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_37",
    "chunk": "as large as Andromeda before, it has collided and merged with other galaxies in the past. Cosmological simulations indicate that, 11 billion years ago, it merged with a particularly large galaxy that has been labeled the Kraken. Such large-scale interactions are rare. As time passes, mergers of two systems of equal size become less common. Most bright galaxies have remained fundamentally unchanged for the last few billion years, and the net rate of star formation probably also peaked about ten billion years ago. Spiral galaxies, like the Milky Way, produce new generations of stars as long as they have dense molecular clouds of interstellar hydrogen in their spiral arms. Elliptical galaxies are largely devoid of this gas, and so form few new stars. The supply of star-forming material is finite; once stars have converted the available supply of hydrogen into heavier elements, new star formation will come to an end. The current era of star formation is expected to continue for up to one hundred billion years, and then the \"stellar age\" will wind down after about ten trillion to one hundred trillion years (10–10 years), as the smallest, longest-lived stars in the visible universe, tiny red dwarfs, begin to"
  },
  {
    "source": "Galaxy.txt",
    "chunk_id": "Galaxy.txt_38",
    "chunk": "fade. At the end of the stellar age, galaxies will be composed of compact objects: brown dwarfs, white dwarfs that are cooling or cold (\"black dwarfs\"), neutron stars, and black holes. Eventually, as a result of gravitational relaxation, all stars will either fall into central supermassive black holes or be flung into intergalactic space as a result of collisions."
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_0",
    "chunk": "# Giant-impact hypothesis The giant-impact hypothesis, sometimes called the Theia Impact, is an astrogeology hypothesis for the formation of the Moon first proposed in 1946 by Canadian geologist Reginald Daly. The hypothesis suggests that the Early Earth collided with a Mars-sized protoplanet of the same orbit approximately 4.5 billion years ago in the early Hadean eon (about 20 to 100 million years after the Solar System coalesced), and the ejecta of the impact event later accreted to form the Moon. The impactor planet is sometimes called Theia, named after the mythical Greek Titan who was the mother of Selene, the goddess of the Moon. Analysis of lunar rocks published in a 2016 report suggests that the impact might have been a direct hit, causing a fragmentation and thorough mixing of both parent bodies. The giant-impact hypothesis is currently the favored hypothesis for lunar formation among astronomers. Evidence that supports this hypothesis includes: However, several questions remain concerning the best current models of the giant-impact hypothesis. The energy of such a giant impact is predicted to have heated Earth to produce a global magma ocean, and evidence of the resultant planetary differentiation of the heavier material sinking into Earth's mantle has"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_1",
    "chunk": "been documented. However, there is no self-consistent model that starts with the giant-impact event and follows the evolution of the debris into a single moon. In 1898, George Darwin made the suggestion that Earth and the Moon were once a single body. Darwin's hypothesis was that a molten Moon had been spun from Earth because of centrifugal forces, and this became the dominant academic explanation. Using Newtonian mechanics, he calculated that the Moon had orbited much more closely in the past and was drifting away from Earth. This drifting was later confirmed by American and Soviet experiments, using laser ranging targets placed on the Moon. Nonetheless, Darwin's calculations could not resolve the mechanics required to trace the Moon back to the surface of Earth. In 1946, Reginald Aldworth Daly of Harvard University challenged Darwin's explanation, adjusting it to postulate that the creation of the Moon was caused by an impact rather than centrifugal forces. Little attention was paid to Professor Daly's challenge until a conference on satellites in 1974, during which the idea was reintroduced and later published and discussed in Icarus in 1975 by William K. Hartmann and Donald R. Davis. Their models suggested that, at the end of"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_2",
    "chunk": "the planet formation period, several satellite-sized bodies had formed that could collide with the planets or be captured. They proposed that one of these objects might have collided with Earth, ejecting refractory, volatile-poor dust that could coalesce to form the Moon. This collision could potentially explain the unique geological and geochemical properties of the Moon. A similar approach was taken by Canadian astronomer Alastair G. W. Cameron and American astronomer William R. Ward, who suggested that the Moon was formed by the tangential impact upon Earth of a body the size of Mars. It is hypothesized that most of the outer silicates of the colliding body would be vaporized, whereas a metallic core would not. Hence, most of the collisional material sent into orbit would consist of silicates, leaving the coalescing Moon deficient in iron. The more volatile materials that were emitted during the collision probably would escape the Solar System, whereas silicates would tend to coalesce. Eighteen months prior to an October 1984 conference on lunar origins, Bill Hartmann, Roger Phillips, and Jeff Taylor challenged fellow lunar scientists: \"You have eighteen months. Go back to your Apollo data, go back to your computer, and do whatever you have to,"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_3",
    "chunk": "but make up your mind. Don't come to our conference unless you have something to say about the Moon's birth.\" At the 1984 conference at Kona, Hawaii, the giant-impact hypothesis emerged as the most favored hypothesis. Before the conference, there were partisans of the three \"traditional\" theories, plus a few people who were starting to take the giant impact seriously, and there was a huge apathetic middle who didn't think the debate would ever be resolved. Afterward, there were essentially only two groups: the giant impact camp and the agnostics. The name of the hypothesised protoplanet is derived from the mythical Greek titan Theia /ˈθiːə/, who gave birth to the Moon goddess Selene. This designation was proposed initially by the English geochemist Alex N. Halliday in 2000 and has become accepted in the scientific community. According to modern theories of planet formation, Theia was part of a population of Mars-sized bodies that existed in the Solar System 4.5 billion years ago. One of the attractive features of the giant-impact hypothesis is that the formation of the Moon and Earth align; during the course of its formation, Earth is thought to have experienced dozens of collisions with planet-sized bodies. The Moon-forming"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_4",
    "chunk": "collision would have been only one such \"giant impact\" but certainly the last significant impactor event. The Late Heavy Bombardment by much smaller asteroids may have occurred later – approximately 3.9 billion years ago. Astronomers think the collision between Earth and Theia happened at about 4.4 to 4.45 billion years ago (bya); about 0.1 billion years after the Solar System began to form. In astronomical terms, the impact would have been of moderate velocity. Theia is thought to have struck Earth at an oblique angle when Earth was nearly fully formed. Computer simulations of this \"late-impact\" scenario suggest an initial impactor velocity below 4 kilometres per second (2.5 mi/s) at \"infinity\" (far enough that gravitational attraction is not a factor), increasing as it approached to over 9.3 km/s (5.8 mi/s) at impact, and an impact angle of about 45°. However, oxygen isotope abundance in lunar rock suggests \"vigorous mixing\" of Theia and Earth, indicating a steep impact angle. Theia's iron core would have sunk into the young Earth's core, and most of Theia's mantle accreted onto Earth's mantle. However, a significant portion of the mantle material from both Theia and Earth would have been ejected into orbit around Earth (if"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_5",
    "chunk": "ejected with velocities between orbital velocity and escape velocity) or into individual orbits around the Sun (if ejected at higher velocities). Modelling has hypothesised that material in orbit around Earth may have accreted to form the Moon in three consecutive phases; accreting first from the bodies initially present outside Earth's Roche limit, which acted to confine the inner disk material within the Roche limit. The inner disk slowly and viscously spread back out to Earth's Roche limit, pushing along outer bodies via resonant interactions. After several tens of years, the disk spread beyond the Roche limit, and started producing new objects that continued the growth of the Moon, until the inner disk was depleted in mass after several hundreds of years. Material in stable Kepler orbits was thus likely to hit the Earth–Moon system sometime later (because the Earth–Moon system's Kepler orbit around the Sun also remains stable). Estimates based on computer simulations of such an event suggest that some twenty percent of the original mass of Theia would have ended up as an orbiting ring of debris around Earth, and about half of this matter coalesced into the Moon. Earth would have gained significant amounts of angular momentum and"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_6",
    "chunk": "mass from such a collision. Regardless of the speed and tilt of Earth's rotation before the impact, it would have experienced a day some five hours long after the impact, and Earth's equator and the Moon's orbit would have become coplanar. Not all of the ring material need have been swept up right away: the thickened crust of the Moon's far side suggests the possibility that a second moon about 1,000 km (620 mi) in diameter formed in a Lagrange point of the Moon. The smaller moon may have remained in orbit for tens of millions of years. As the two moons migrated outward from Earth, solar tidal effects would have made the Lagrange orbit unstable, resulting in a slow-velocity collision that \"pancaked\" the smaller moon onto what is now the far side of the Moon, adding material to its crust. Lunar magma cannot pierce through the thick crust of the far side, causing fewer lunar maria, while the near side has a thin crust displaying the large maria visible from Earth. Above a high resolution threshold for simulations, a study published in 2022 finds that giant impacts can immediately place a satellite with similar mass and iron content to"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_7",
    "chunk": "the Moon into orbit far outside Earth's Roche limit. Even satellites that initially pass within the Roche limit can reliably and predictably survive, by being partially stripped and then torqued onto wider, stable orbits. Furthermore, the outer layers of these directly formed satellites are molten over cooler interiors and are composed of around 60% proto-Earth material. This could alleviate the tension between the Moon's Earth-like isotopic composition and the different signature expected for the impactor. Immediate formation opens up new options for the Moon's early orbit and evolution, including the possibility of a highly tilted orbit to explain the lunar inclination, and offers a simpler, single-stage scenario for the origin of the Moon. In 2001, a team at the Carnegie Institution of Washington reported that the rocks from the Apollo program carried an isotopic signature that was identical with rocks from Earth, and were different from almost all other bodies in the Solar System. In 2014, a team in Germany reported that the Apollo samples had a slightly different isotopic signature from Earth rocks. The difference was slight, but statistically significant. One possible explanation is that Theia formed near Earth. This empirical data showing close similarity of composition can be"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_8",
    "chunk": "explained only by the standard giant-impact hypothesis, as it is extremely unlikely that two bodies prior to collision had such similar composition. In 2007, researchers from the California Institute of Technology showed that the likelihood of Theia having an identical isotopic signature as Earth was very small (less than 1 percent). They proposed that in the aftermath of the giant impact, while Earth and the proto-lunar disc were molten and vaporised, the two reservoirs were connected by a common silicate vapor atmosphere and that the Earth–Moon system became homogenised by convective stirring while the system existed in the form of a continuous fluid. Such an \"equilibration\" between the post-impact Earth and the proto-lunar disc is the only proposed scenario that explains the isotopic similarities of the Apollo rocks with rocks from Earth's interior. For this scenario to be viable, however, the proto-lunar disc would have to endure for about 100 years. Work is ongoing to determine whether or not this is possible. According to research (2012) to explain similar compositions of the Earth and the Moon based on simulations at the University of Bern by physicist Andreas Reufer and his colleagues, Theia collided directly with Earth instead of barely swiping"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_9",
    "chunk": "it. The collision speed may have been higher than originally assumed, and this higher velocity may have totally destroyed Theia. According to this modification, the composition of Theia is not so restricted, making a composition of up to 50% water ice possible. One effort, in 2018, to homogenise the products of the collision was to energise the primary body by way of a greater pre-collision rotational speed. This way, more material from the primary body would be spun off to form the Moon. Further computer modelling determined that the observed result could be obtained by having the pre-Earth body spinning very rapidly, so much so that it formed a new celestial object which was given the name 'synestia'. This is an unstable state that could have been generated by yet another collision to get the rotation spinning fast enough. Further modelling of this transient structure has shown that the primary body spinning as a doughnut-shaped object (the synestia) existed for about a century (a very short time) before it cooled down and gave birth to Earth and the Moon. Another model, in 2019, to explain the similarity of Earth and the Moon's compositions posits that shortly after Earth formed, it"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_10",
    "chunk": "was covered by a sea of hot magma, while the impacting object was likely made of solid material. Modelling suggests that this would lead to the impact heating the magma much more than solids from the impacting object, leading to more material being ejected from the proto-Earth, so that about 80% of the Moon-forming debris originated from the proto-Earth. Many prior models had suggested 80% of the Moon coming from the impactor. Indirect evidence for the giant impact scenario comes from rocks collected during the Apollo Moon landings, which show oxygen isotope ratios nearly identical to those of Earth. The highly anorthositic composition of the lunar crust, as well as the existence of KREEP-rich samples, suggest that a large portion of the Moon once was molten; and a giant impact scenario could easily have supplied the energy needed to form such a magma ocean. Several lines of evidence show that if the Moon has an iron-rich core, it must be a small one. In particular, the mean density, moment of inertia, rotational signature, and magnetic induction response of the Moon all suggest that the radius of its core is less than about 25% the radius of the Moon, in contrast"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_11",
    "chunk": "to about 50% for most of the other terrestrial bodies. Appropriate impact conditions satisfying the angular momentum constraints of the Earth–Moon system yield a Moon formed mostly from the mantles of Earth and the impactor, while the core of the impactor accretes to Earth. Earth has the highest density of all the planets in the Solar System; the absorption of the core of the impactor body explains this observation, given the proposed properties of the early Earth and Theia. Comparison of the zinc isotopic composition of lunar samples with that of Earth and Mars rocks provides further evidence for the impact hypothesis. Zinc is strongly fractionated when volatilised in planetary rocks, but not during normal igneous processes, so zinc abundance and isotopic composition can distinguish the two geological processes. Moon rocks contain more heavy isotopes of zinc, and overall less zinc, than corresponding igneous Earth or Mars rocks, which is consistent with zinc being depleted from the Moon through evaporation, as expected for the giant impact origin. Collisions between ejecta escaping Earth's gravity and asteroids would have left impact heating signatures in stony meteorites; analysis based on assuming the existence of this effect has been used to date the impact"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_12",
    "chunk": "event to 4.47 billion years ago, in agreement with the date obtained by other means. Warm silica-rich dust and abundant SiO gas, products of high velocity impacts – over 10 km/s (6.2 mi/s) – between rocky bodies, have been detected by the Spitzer Space Telescope around the nearby (29 pc distant) young (~12 My old) star HD 172555 in the Beta Pictoris moving group. A belt of warm dust in a zone between 0.25AU and 2AU from the young star HD 23514 in the Pleiades cluster appears similar to the predicted results of Theia's collision with the embryonic Earth, and has been interpreted as the result of planet-sized objects colliding with each other. A similar belt of warm dust was detected around the star BD+20°307 (HIP 8920, SAO 75016). On 1 November 2023, scientists reported that, according to computer simulations, remnants of Theia could be still visible inside the Earth as two giant anomalies of the Earth's mantle. This lunar origin hypothesis has some difficulties that have yet to be resolved. For example, the giant-impact hypothesis implies that a surface magma ocean would have formed following the impact. Yet there is no evidence that Earth ever had such a magma"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_13",
    "chunk": "ocean and it is likely there exists material that has never been processed in a magma ocean. If the Moon was formed by such an impact, it is possible that other inner planets also may have been subjected to comparable impacts. A moon that formed around Venus by this process would have been unlikely to escape. If such a moon-forming event had occurred there, a possible explanation of why the planet does not have such a moon might be that a second collision occurred that countered the angular momentum from the first impact. Another possibility is that the strong tidal forces from the Sun would tend to destabilise the orbits of moons around close-in planets. For this reason, if Venus's slow rotation rate began early in its history, any satellites larger than a few kilometers in diameter would likely have spiraled inwards and collided with Venus. Simulations of the chaotic period of terrestrial planet formation suggest that impacts like those hypothesised to have formed the Moon were common. For typical terrestrial planets with a mass of 0.5 to 1 Earth masses, such an impact typically results in a single moon containing 4% of the host planet's mass. The inclination of"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_14",
    "chunk": "the resulting moon's orbit is random, but this tilt affects the subsequent dynamic evolution of the system. For example, some orbits may cause the moon to spiral back into the planet. Likewise, the proximity of the planet to the star will also affect the orbital evolution. The net effect is that it is more likely for impact-generated moons to survive when they orbit more distant terrestrial planets and are aligned with the planetary orbit. In 2004, Princeton University mathematician Edward Belbruno and astrophysicist J. Richard Gott III proposed that Theia coalesced at the L4 or L5 Lagrangian point relative to Earth (in about the same orbit and about 60° ahead or behind), similar to a trojan asteroid. Two-dimensional computer models suggest that the stability of Theia's proposed trojan orbit would have been affected when its growing mass exceeded a threshold of approximately 10% of Earth's mass (the mass of Mars). In this scenario, gravitational perturbations by planetesimals caused Theia to depart from its stable Lagrangian location, and subsequent interactions with proto-Earth led to a collision between the two bodies. In 2008, evidence was presented that suggests that the collision might have occurred later than the accepted value of 4.53 Gya,"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_15",
    "chunk": "at approximately 4.48 Gya. A 2014 comparison of computer simulations with elemental abundance measurements in Earth's mantle indicated that the collision occurred approximately 95 My after the formation of the Solar System. It has been suggested that other significant objects might have been created by the impact, which could have remained in orbit between Earth and the Moon, stuck in Lagrangian points. Such objects might have stayed within the Earth–Moon system for as long as 100 million years, until the gravitational tugs of other planets destabilised the system enough to free the objects. A study published in 2011 suggested that a subsequent collision between the Moon and one of these smaller bodies caused the notable differences in physical characteristics between the two hemispheres of the Moon. This collision, simulations have supported, would have been at a low enough velocity so as not to form a crater; instead, the material from the smaller body would have spread out across the Moon (in what would become its far side), adding a thick layer of highlands crust. The resulting mass irregularities would subsequently produce a gravity gradient that resulted in tidal locking of the Moon so that today, only the near side remains"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_16",
    "chunk": "visible from Earth. However, mapping by the GRAIL mission has ruled out this scenario. In 2019, a team at the University of Münster reported that the molybdenum isotopic composition in Earth's primitive mantle originates from the outer Solar System, hinting at the source of water on Earth. One possible explanation is that Theia originated in the outer Solar System. Other mechanisms that have been suggested at various times for the Moon's origin are that the Moon was spun off from Earth's molten surface by centrifugal force; that it was formed elsewhere and was subsequently captured by Earth's gravitational field; or that Earth and the Moon formed at the same time and place from the same accretion disk. None of these hypotheses can account for the high angular momentum of the Earth–Moon system. Another hypothesis attributes the formation of the Moon to the impact of a large asteroid with Earth much later than previously thought, creating the satellite primarily from debris from Earth. In this hypothesis, the formation of the Moon occurs 60–140 million years after the formation of the Solar System (as compared to hypothesized Theia impact at 4.527 ± 0.010 billion years). The asteroid impact in this scenario would"
  },
  {
    "source": "Giant-impact hypothesis.txt",
    "chunk_id": "Giant-impact hypothesis.txt_17",
    "chunk": "have created a magma ocean on Earth and the proto-Moon with both bodies sharing a common plasma metal vapor atmosphere. The shared metal vapor bridge would have allowed material from Earth and the proto-Moon to exchange and equilibrate into a more common composition. Yet another hypothesis proposes that the Moon and Earth formed together, not from the collision of once-distant bodies. This model, published in 2012 by Robin M. Canup, suggests that the Moon and Earth formed from a massive collision of two planetary bodies, each larger than Mars, which then re-collided to form what is now called Earth. After the re-collision, Earth was surrounded by a disk of material which accreted to form the Moon."
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_0",
    "chunk": "# Gravitational wave Gravitational waves are oscillations of the gravitational field that travel through space at the speed of light; they are generated by the relative motion of gravitating masses. They were proposed by Oliver Heaviside in 1893 and then later by Henri Poincaré in 1905 as the gravitational equivalent of electromagnetic waves. In 1916, Albert Einstein demonstrated that gravitational waves result from his general theory of relativity as ripples in spacetime. Gravitational waves transport energy as gravitational radiation, a form of radiant energy similar to electromagnetic radiation. Newton's law of universal gravitation, part of classical mechanics, does not provide for their existence, instead asserting that gravity has instantaneous effect everywhere. Gravitational waves therefore stand as an important relativistic phenomenon that is absent from Newtonian physics. Gravitational-wave astronomy has the advantage that, unlike electromagnetic radiation, gravitational waves are not affected by intervening matter. Sources that can be studied this way include binary star systems composed of white dwarfs, neutron stars, and black holes; events such as supernovae; and the formation of the early universe shortly after the Big Bang. The first indirect evidence for the existence of gravitational waves came in 1974 from the observed orbital decay of the Hulse–Taylor"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_1",
    "chunk": "binary pulsar, which matched the decay predicted by general relativity for energy lost to gravitational radiation. In 1993, Russell Alan Hulse and Joseph Hooton Taylor Jr. received the Nobel Prize in Physics for this discovery. The first direct observation of gravitational waves was made in September 2015, when a signal generated by the merger of two black holes was received by the LIGO gravitational wave detectors in Livingston, Louisiana, and in Hanford, Washington. The 2017 Nobel Prize in Physics was subsequently awarded to Rainer Weiss, Kip Thorne and Barry Barish for their role in the direct detection of gravitational waves. In Albert Einstein's general theory of relativity, gravity is treated as a phenomenon resulting from the curvature of spacetime. This curvature is caused by the presence of mass. (See: Stress–energy tensor) If the masses move, the curvature of spacetime changes. If the motion is not spherically symmetric, the motion can cause gravitational waves which propagate away at the speed of light. As a gravitational wave passes an observer, that observer will find spacetime distorted by the effects of strain. Distances between objects increase and decrease rhythmically as the wave passes, at a frequency equal to that of the wave. The"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_2",
    "chunk": "magnitude of this effect is inversely proportional to the distance (not distance squared) from the source. Inspiraling binary neutron stars are predicted to be a powerful source of gravitational waves as they coalesce, due to the very large acceleration of their masses as they orbit close to one another. However, due to the astronomical distances to these sources, the effects when measured on Earth are predicted to be very small, having strains of less than 1 part in 10. Scientists demonstrate the existence of these waves with highly-sensitive detectors at multiple observation sites. As of 2012, the LIGO and Virgo observatories were the most sensitive detectors, operating at resolutions of about one part in 5×10. The Japanese detector KAGRA was completed in 2019; its first joint detection with LIGO and VIRGO was reported in 2021. Another European ground-based detector, the Einstein Telescope, is under development. A space-based observatory, the Laser Interferometer Space Antenna (LISA), is also being developed by the European Space Agency. Gravitational waves do not strongly interact with matter in the way that electromagnetic radiation does. This allows for the observation of events involving exotic objects in the distant universe that cannot be observed with more traditional means"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_3",
    "chunk": "such as optical telescopes or radio telescopes; accordingly, gravitational wave astronomy gives new insights into the workings of the universe. In particular, gravitational waves could be of interest to cosmologists as they offer a possible way of observing the very early universe. This is not possible with conventional astronomy, since before recombination the universe was opaque to electromagnetic radiation. Precise measurements of gravitational waves will also allow scientists to test more thoroughly the general theory of relativity. In principle, gravitational waves can exist at any frequency. Very low frequency waves can be detected using pulsar timing arrays. In this technique, the timing of approximately 100 pulsars spread widely across our galaxy is monitored over the course of years. Detectable changes in the arrival time of their signals can result from passing gravitational waves generated by merging supermassive black holes (SMBH) with wavelengths measured in lightyears. These timing changes can be used to locate the source of the waves. Using this technique, astronomers have discovered the 'hum' of various SMBH mergers occurring in the universe. Stephen Hawking and Werner Israel list different frequency bands for gravitational waves that could plausibly be detected, ranging from 10 Hz up to 10 Hz. The"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_4",
    "chunk": "speed of gravitational waves in the general theory of relativity is equal to the speed of light in vacuum, c. Within the theory of special relativity, the constant c is not only about light; instead it is the highest possible speed for any interaction in nature. Formally, c is a conversion factor for changing the unit of time to the unit of space. This makes it the only speed which does not depend either on the motion of an observer or a source of light and/or gravity. Thus, the speed of \"light\" is also the speed of gravitational waves, and, further, the speed of any massless particle. Such particles include the gluon (carrier of the strong force), the photons that make up light (hence carrier of electromagnetic force), and the hypothetical gravitons (which are the presumptive field particles associated with gravity; however, an understanding of the graviton, if any exist, requires an as-yet unavailable theory of quantum gravity). In August 2017, the LIGO and Virgo detectors received a gravitational wave signal, GW170817, at nearly the same time as gamma ray satellites and optical telescopes received signals from its source in galaxy NGC 4993, about 130 million light years away. This"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_5",
    "chunk": "measurement constrained the experimental difference between the speed of gravitational waves and light to be smaller than one part in 10. The possibility of gravitational waves and that those might travel at the speed of light was discussed in 1893 by Oliver Heaviside, using the analogy between the inverse-square law of gravitation and the electrostatic force. In 1905, Henri Poincaré proposed gravitational waves, emanating from a body and propagating at the speed of light, as being required by the Lorentz transformations and suggested that, in analogy to an accelerating electrical charge producing electromagnetic waves, accelerated masses in a relativistic field theory of gravity should produce gravitational waves. In 1915 Einstein published his general theory of relativity, a complete relativistic theory of gravitation. He conjectured, like Poincaré, that the equation would produce gravitational waves, but, as he mentions in a letter to Schwarzschild in February 1916, these could not be similar to electromagnetic waves. Electromagnetic waves can be produced by dipole motion, requiring both a positive and a negative charge. Gravitation has no equivalent to negative charge. Einstein continued to work through the complexity of the equations of general relativity to find an alternative wave model. The result was published in"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_6",
    "chunk": "June 1916, and there he came to the conclusion that the gravitational wave must propagate with the speed of light, and there must, in fact, be three types of gravitational waves dubbed longitudinal–longitudinal, transverse–longitudinal, and transverse–transverse by Hermann Weyl. However, the nature of Einstein's approximations led many (including Einstein himself) to doubt the result. In 1922, Arthur Eddington showed that two of Einstein's types of waves were artifacts of the coordinate system he used, and could be made to propagate at any speed by choosing appropriate coordinates, leading Eddington to jest that they \"propagate at the speed of thought\". This also cast doubt on the physicality of the third (transverse–transverse) type that Eddington showed always propagate at the speed of light regardless of coordinate system. In 1936, Einstein and Nathan Rosen submitted a paper to Physical Review in which they claimed gravitational waves could not exist in the full general theory of relativity because any such solution of the field equations would have a singularity. The journal sent their manuscript to be reviewed by Howard P. Robertson, who anonymously reported that the singularities in question were simply the harmless coordinate singularities of the employed cylindrical coordinates. Einstein, who was unfamiliar"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_7",
    "chunk": "with the concept of peer review, angrily withdrew the manuscript, never to publish in Physical Review again. Nonetheless, his assistant Leopold Infeld, who had been in contact with Robertson, convinced Einstein that the criticism was correct, and the paper was rewritten with the opposite conclusion and published elsewhere. In 1956, Felix Pirani remedied the confusion caused by the use of various coordinate systems by rephrasing the gravitational waves in terms of the manifestly observable Riemann curvature tensor. At the time, Pirani's work was overshadowed by the community's focus on a different question: whether gravitational waves could transmit energy. This matter was settled by a thought experiment proposed by Richard Feynman during the first \"GR\" conference at Chapel Hill in 1957. In short, his argument known as the \"sticky bead argument\" notes that if one takes a rod with beads then the effect of a passing gravitational wave would be to move the beads along the rod; friction would then produce heat, implying that the passing wave had done work. Shortly after, Hermann Bondi published a detailed version of the \"sticky bead argument\". This later led to a series of articles (1959 to 1989) by Bondi and Pirani that established the"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_8",
    "chunk": "existence of plane wave solutions for gravitational waves. Paul Dirac further postulated the existence of gravitational waves, declaring them to have \"physical significance\" in his 1959 lecture at the Lindau Meetings. Further, it was Dirac who predicted gravitational waves with a well-defined energy density in 1964. After the Chapel Hill conference, Joseph Weber started designing and building the first gravitational wave detectors now known as Weber bars. In 1969, Weber claimed to have detected the first gravitational waves, and by 1970 he was \"detecting\" signals regularly from the Galactic Center; however, the frequency of detection soon raised doubts on the validity of his observations as the implied rate of energy loss of the Milky Way would drain our galaxy of energy on a timescale much shorter than its inferred age. These doubts were strengthened when, by the mid-1970s, repeated experiments from other groups building their own Weber bars across the globe failed to find any signals, and by the late 1970s consensus was that Weber's results were spurious. In the same period, the first indirect evidence of gravitational waves was discovered. In 1974, Russell Alan Hulse and Joseph Hooton Taylor, Jr. discovered the first binary pulsar, which earned them the"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_9",
    "chunk": "1993 Nobel Prize in Physics. Pulsar timing observations over the next decade showed a gradual decay of the orbital period of the Hulse–Taylor pulsar that matched the loss of energy and angular momentum in gravitational radiation predicted by general relativity. This indirect detection of gravitational waves motivated further searches, despite Weber's discredited result. Some groups continued to improve Weber's original concept, while others pursued the detection of gravitational waves using laser interferometers. The idea of using a laser interferometer for this seems to have been floated independently by various people, including M.E. Gertsenshtein and V. I. Pustovoit in 1962, and Vladimir B. Braginskiĭ in 1966. The first prototypes were developed in the 1970s by Robert L. Forward and Rainer Weiss. In the decades that followed, ever more sensitive instruments were constructed, culminating in the construction of GEO600, LIGO, and Virgo. After years of producing null results, improved detectors became operational in 2015. On 11 February 2016, the LIGO-Virgo collaborations announced the first observation of gravitational waves, from a signal (dubbed GW150914) detected at 09:50:45 GMT on 14 September 2015 of two black holes with masses of 29 and 36 solar masses merging about 1.3 billion light-years away. During the final"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_10",
    "chunk": "fraction of a second of the merger, it released more than 50 times the power of all the stars in the observable universe combined. The signal increased in frequency from 35 to 250 Hz over 10 cycles (5 orbits) as it rose in strength for a period of 0.2 second. The mass of the new merged black hole was 62 solar masses. Energy equivalent to three solar masses was emitted as gravitational waves. The signal was seen by both LIGO detectors in Livingston and Hanford, with a time difference of 7 milliseconds due to the angle between the two detectors and the source. The signal came from the Southern Celestial Hemisphere, in the rough direction of (but much farther away than) the Magellanic Clouds. The confidence level of this being an observation of gravitational waves was 99.99994%. A year earlier, the BICEP2 collaboration claimed that they had detected the imprint of gravitational waves in the cosmic microwave background. However, they were later forced to retract this result. In 2017, the Nobel Prize in Physics was awarded to Rainer Weiss, Kip Thorne and Barry Barish for their role in the detection of gravitational waves. In 2023, NANOGrav, EPTA, PPTA, and IPTA"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_11",
    "chunk": "announced that they found evidence of a universal gravitational wave background. North American Nanohertz Observatory for Gravitational Waves states, that they were created over cosmological time scales by supermassive black holes, identifying the distinctive Hellings-Downs curve in 15 years of radio observations of 25 pulsars. Similar results are published by European Pulsar Timing Array, who claimed a 3 σ {\\displaystyle 3\\sigma } -significance. They expect that a 5 σ {\\displaystyle 5\\sigma } -significance will be achieved by 2025 by combining the measurements of several collaborations. Gravitational waves are constantly passing Earth; however, even the strongest have a minuscule effect since their sources are generally at a great distance. For example, the waves given off by the cataclysmic final merger of GW150914 reached Earth after travelling over a billion light-years, as a ripple in spacetime that changed the length of a 4 km LIGO arm by a thousandth of the width of a proton, proportionally equivalent to changing the distance to the nearest star outside the Solar System by one hair's width. This tiny effect from even extreme gravitational waves makes them observable on Earth only with the most sophisticated detectors. The effects of a passing gravitational wave, in an extremely"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_12",
    "chunk": "exaggerated form, can be visualized by imagining a perfectly flat region of spacetime with a group of motionless test particles lying in a plane, e.g., the surface of a computer screen. As a gravitational wave passes through the particles along a line perpendicular to the plane of the particles, i.e., following the observer's line of vision into the screen, the particles will follow the distortion in spacetime, oscillating in a \"cruciform\" manner, as shown in the animations. The area enclosed by the test particles does not change and there is no motion along the direction of propagation. The oscillations depicted in the animation are exaggerated for the purpose of discussion – in reality a gravitational wave has a very small amplitude (as formulated in linearized gravity). However, they help illustrate the kind of oscillations associated with gravitational waves as produced by a pair of masses in a circular orbit. In this case the amplitude of the gravitational wave is constant, but its plane of polarization changes or rotates at twice the orbital rate, so the time-varying gravitational wave size, or 'periodic spacetime strain', exhibits a variation as shown in the animation. If the orbit of the masses is elliptical then"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_13",
    "chunk": "the gravitational wave's amplitude also varies with time according to Einstein's quadrupole formula. As with other waves, there are a number of characteristics used to describe a gravitational wave: The speed, wavelength, and frequency of a gravitational wave are related by the equation c = λf, just like the equation for a light wave. For example, the animations shown here oscillate roughly once every two seconds. This would correspond to a frequency of 0.5 Hz, and a wavelength of about 600 000 km, or 47 times the diameter of the Earth. In the above example, it is assumed that the wave is linearly polarized with a \"plus\" polarization, written h+. Polarization of a gravitational wave is just like polarization of a light wave except that the polarizations of a gravitational wave are 45 degrees apart, as opposed to 90 degrees. In particular, in a \"cross\"-polarized gravitational wave, h×, the effect on the test particles would be basically the same, but rotated by 45 degrees, as shown in the second animation. Just as with light polarization, the polarizations of gravitational waves may also be expressed in terms of circularly polarized waves. Gravitational waves are polarized because of the nature of their"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_14",
    "chunk": "source. In general terms, gravitational waves are radiated by large, coherent motions of immense mass, especially in regions where gravity is so strong that Newtonian gravity begins to fail. The effect does not occur in a purely spherically symmetric system. A simple example of this principle is a spinning dumbbell. If the dumbbell spins around its axis of symmetry, it will not radiate gravitational waves; if it tumbles end over end, as in the case of two planets orbiting each other, it will radiate gravitational waves. The heavier the dumbbell, and the faster it tumbles, the greater is the gravitational radiation it will give off. In an extreme case, such as when the two weights of the dumbbell are massive stars like neutron stars or black holes, orbiting each other quickly, then significant amounts of gravitational radiation would be given off. More technically, the second time derivative of the quadrupole moment (or the l-th time derivative of the l-th multipole moment) of an isolated system's stress–energy tensor must be non-zero in order for it to emit gravitational radiation. This is analogous to the changing dipole moment of charge or current that is necessary for the emission of electromagnetic radiation. Gravitational"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_15",
    "chunk": "waves carry energy away from their sources and, in the case of orbiting bodies, this is associated with an in-spiral or decrease in orbit. Imagine for example a simple system of two masses – such as the Earth–Sun system – moving slowly compared to the speed of light in circular orbits. Assume that these two masses orbit each other in a circular orbit in the x–y plane. To a good approximation, the masses follow simple Keplerian orbits. However, such an orbit represents a changing quadrupole moment. That is, the system will give off gravitational waves. In theory, the loss of energy through gravitational radiation could eventually drop the Earth into the Sun. However, the total energy of the Earth orbiting the Sun (kinetic energy + gravitational potential energy) is about 1.14×10 joules of which only 200 watts (joules per second) is lost through gravitational radiation, leading to a decay in the orbit by about 1×10 meters per day or roughly the diameter of a proton. At this rate, it would take the Earth approximately 3×10 times more than the current age of the universe to spiral onto the Sun. This estimate overlooks the decrease in r over time, but the"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_16",
    "chunk": "radius varies only slowly for most of the time and plunges at later stages, as r ( t ) = r 0 ( 1 − t t coalesce ) 1 / 4 , {\\displaystyle r(t)=r_{0}\\left(1-{\\frac {t}{t_{\\text{coalesce}}}}\\right)^{1/4},} with r 0 {\\displaystyle r_{0}} the initial radius and t coalesce {\\displaystyle t_{\\text{coalesce}}} the total time needed to fully coalesce. where r is the separation between the bodies, t time, G the gravitational constant, c the speed of light, and m1 and m2 the masses of the bodies. This leads to an expected time to merger of Compact stars like white dwarfs and neutron stars can be constituents of binaries. For example, a pair of solar mass neutron stars in a circular orbit at a separation of 1.89×10 m (189,000 km) has an orbital period of 1,000 seconds, and an expected lifetime of 1.30×10 seconds or about 414,000 years. Such a system could be observed by LISA if it were not too far away. A far greater number of white dwarf binaries exist with orbital periods in this range. White dwarf binaries have masses in the order of the Sun, and diameters in the order of the Earth. They cannot get much closer together"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_17",
    "chunk": "than 10,000 km before they will merge and explode in a supernova which would also end the emission of gravitational waves. Until then, their gravitational radiation would be comparable to that of a neutron star binary. When the orbit of a neutron star binary has decayed to 1.89×10 m (1890 km), its remaining lifetime is about 130,000 seconds or 36 hours. The orbital frequency will vary from 1 orbit per second at the start, to 918 orbits per second when the orbit has shrunk to 20 km at merger. The majority of gravitational radiation emitted will be at twice the orbital frequency. Just before merger, the inspiral could be observed by LIGO if such a binary were close enough. LIGO has only a few minutes to observe this merger out of a total orbital lifetime that may have been billions of years. In August 2017, LIGO and Virgo observed the first binary neutron star inspiral in GW170817, and 70 observatories collaborated to detect the electromagnetic counterpart, a kilonova in the galaxy NGC 4993, 40 megaparsecs away, emitting a short gamma ray burst (GRB 170817A) seconds after the merger, followed by a longer optical transient (AT 2017gfo) powered by r-process nuclei."
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_18",
    "chunk": "Advanced LIGO detectors should be able to detect such events up to 200 megaparsecs away; at this range, around 40 detections per year would be expected. Black hole binaries emit gravitational waves during their in-spiral, merger, and ring-down phases. Hence, in the early 1990s the physics community rallied around a concerted effort to predict the waveforms of gravitational waves from these systems with the Binary Black Hole Grand Challenge Alliance. The largest amplitude of emission occurs during the merger phase, which can be modeled with the techniques of numerical relativity. The first direct detection of gravitational waves, GW150914, came from the merger of two black holes. A supernova is a transient astronomical event that occurs during the last stellar evolutionary stages of a massive star's life, whose dramatic and catastrophic destruction is marked by one final titanic explosion. This explosion can happen in one of many ways, but in all of them a significant proportion of the matter in the star is blown away into the surrounding space at extremely high velocities (up to 10% of the speed of light). Unless there is perfect spherical symmetry in these explosions (i.e., unless matter is spewed out evenly in all directions), there"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_19",
    "chunk": "will be gravitational radiation from the explosion. This is because gravitational waves are generated by a changing quadrupole moment, which can happen only when there is asymmetrical movement of masses. Since the exact mechanism by which supernovae take place is not fully understood, it is not easy to model the gravitational radiation emitted by them. As noted above, a mass distribution will emit gravitational radiation only when there is spherically asymmetric motion among the masses. A spinning neutron star will generally emit no gravitational radiation because neutron stars are highly dense objects with a strong gravitational field that keeps them almost perfectly spherical. In some cases, however, there might be slight deformities on the surface called \"mountains\", which are bumps extending no more than 10 centimeters (4 inches) above the surface, that make the spinning spherically asymmetric. This gives the star a quadrupole moment that changes with time, and it will emit gravitational waves until the deformities are smoothed out. Gravitational waves from the early universe could provide a unique probe for cosmology. Because these wave interact very weakly with matter they would propagate freely from very early time when other signals are trapped by the large density of energy."
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_20",
    "chunk": "If this gravitational radiation could be detected today it would be gravitational wave background complementary to the cosmic microwave background data. Water waves, sound waves, and electromagnetic waves are able to carry energy, momentum, and angular momentum and by doing so they carry those away from the source. Gravitational waves perform the same function. Thus, for example, a binary system loses angular momentum as the two orbiting objects spiral towards each other – the angular momentum is radiated away by gravitational waves. The waves can also carry off linear momentum, a possibility that has some interesting implications for astrophysics. After two supermassive black holes coalesce, emission of linear momentum can produce a \"kick\" with amplitude as large as 4000 km/s. This is fast enough to eject the coalesced black hole completely from its host galaxy. Even if the kick is too small to eject the black hole completely, it can remove it temporarily from the nucleus of the galaxy, after which it will oscillate about the center, eventually coming to rest. A kicked black hole can also carry a star cluster with it, forming a hyper-compact stellar system. Or it may carry gas, allowing the recoiling black hole to appear"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_21",
    "chunk": "temporarily as a \"naked quasar\". The quasar SDSS J092712.65+294344.0 is thought to contain a recoiling supermassive black hole. Like electromagnetic waves, gravitational waves should exhibit shifting of wavelength and frequency due to the relative velocities of the source and observer (the Doppler effect), but also due to distortions of spacetime, such as cosmic expansion. Redshifting of gravitational waves is different from redshifting due to gravity (gravitational redshift). In the framework of quantum field theory, the graviton is the name given to a hypothetical elementary particle speculated to be the force carrier that mediates gravity. However the graviton is not yet proven to exist, and no scientific model yet exists that successfully reconciles general relativity, which describes gravity, and the Standard Model, which describes all other fundamental forces. Attempts, such as quantum gravity, have been made, but are not yet accepted. If such a particle exists, it is expected to be massless (because the gravitational force appears to have unlimited range) and must be a spin-2 boson. It can be shown that any massless spin-2 field would give rise to a force indistinguishable from gravitation, because a massless spin-2 field must couple to (interact with) the stress-energy tensor in the same"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_22",
    "chunk": "way that the gravitational field does; therefore if a massless spin-2 particle were ever discovered, it would be likely to be the graviton without further distinction from other massless spin-2 particles. Such a discovery would unite quantum theory with gravity. Due to the weakness of the coupling of gravity to matter, gravitational waves experience very little absorption or scattering, even as they travel over astronomical distances. In particular, gravitational waves are expected to be unaffected by the opacity of the very early universe. In these early phases, space had not yet become \"transparent\", so observations based upon light, radio waves, and other electromagnetic radiation that far back into time are limited or unavailable. Therefore, gravitational waves are expected in principle to have the potential to provide a wealth of observational data about the very early universe. The difficulty in directly detecting gravitational waves means it is also difficult for a single detector to identify by itself the direction of a source. Therefore, multiple detectors are used, both to distinguish signals from other \"noise\" by confirming the signal is not of earthly origin, and also to determine direction by means of triangulation. This technique uses the fact that the waves travel"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_23",
    "chunk": "at the speed of light and will reach different detectors at different times depending on their source direction. Although the differences in arrival time may be just a few milliseconds, this is sufficient to identify the direction of the origin of the wave with considerable precision. Only in the case of GW170814 were three detectors operating at the time of the event, therefore, the direction is precisely defined. The detection by all three instruments led to a very accurate estimate of the position of the source, with a 90% credible region of just 60 deg, a factor 20 more accurate than before. During the past century, astronomy has been revolutionized by the use of new methods for observing the universe. Astronomical observations were initially made using visible light. Galileo Galilei pioneered the use of telescopes to enhance these observations. However, visible light is only a small portion of the electromagnetic spectrum, and not all objects in the distant universe shine strongly in this particular band. More information may be found, for example, in radio wavelengths. Using radio telescopes, astronomers have discovered pulsars and quasars, for example. Observations in the microwave band led to the detection of faint imprints of the"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_24",
    "chunk": "Big Bang, a discovery Stephen Hawking called the \"greatest discovery of the century, if not all time\". Similar advances in observations using gamma rays, x-rays, ultraviolet light, and infrared light have also brought new insights to astronomy. As each of these regions of the spectrum has opened, new discoveries have been made that could not have been made otherwise. The astronomy community hopes that the same holds true of gravitational waves. Gravitational waves have two important and unique properties. First, there is no need for any type of matter to be present nearby in order for the waves to be generated by a binary system of uncharged black holes, which would emit no electromagnetic radiation. Second, gravitational waves can pass through any intervening matter without being scattered significantly. Whereas light from distant stars may be blocked out by interstellar dust, for example, gravitational waves will pass through essentially unimpeded. These two features allow gravitational waves to carry information about astronomical phenomena heretofore never observed by humans. The sources of gravitational waves described above are in the low-frequency end of the gravitational-wave spectrum (10 to 10 Hz). An astrophysical source at the high-frequency end of the gravitational-wave spectrum (above 10 Hz"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_25",
    "chunk": "and probably 10 Hz) generates relic gravitational waves that are theorized to be faint imprints of the Big Bang like the cosmic microwave background. At these high frequencies it is potentially possible that the sources may be \"man made\" that is, gravitational waves generated and detected in the laboratory. A supermassive black hole, created from the merger of the black holes at the center of two merging galaxies detected by the Hubble Space Telescope, is theorized to have been ejected from the merger center by gravitational waves. Although the waves from the Earth–Sun system are minuscule, astronomers can point to other sources for which the radiation should be substantial. One important example is the Hulse–Taylor binary – a pair of stars, one of which is a pulsar. The characteristics of their orbit can be deduced from the Doppler shifting of radio signals given off by the pulsar. Each of the stars is about 1.4 M☉ and the size of their orbits is about 1/75 of the Earth–Sun orbit, just a few times larger than the diameter of our own Sun. The combination of greater masses and smaller separation means that the energy given off by the Hulse–Taylor binary will be"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_26",
    "chunk": "far greater than the energy given off by the Earth–Sun system – roughly 10 times as much. The information about the orbit can be used to predict how much energy (and angular momentum) would be radiated in the form of gravitational waves. As the binary system loses energy, the stars gradually draw closer to each other, and the orbital period decreases. The resulting trajectory of each star is an inspiral, a spiral with decreasing radius. General relativity precisely describes these trajectories; in particular, the energy radiated in gravitational waves determines the rate of decrease in the period, defined as the time interval between successive periastrons (points of closest approach of the two stars). For the Hulse–Taylor pulsar, the predicted current change in radius is about 3 mm per orbit, and the change in the 7.75 hr period is about 2 seconds per year. Following a preliminary observation showing an orbital energy loss consistent with gravitational waves, careful timing observations by Taylor and Joel Weisberg dramatically confirmed the predicted period decrease to within 10%. With the improved statistics of more than 30 years of timing data since the pulsar's discovery, the observed change in the orbital period currently matches the prediction"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_27",
    "chunk": "from gravitational radiation assumed by general relativity to within 0.2 percent. In 1993, spurred in part by this indirect detection of gravitational waves, the Nobel Committee awarded the Nobel Prize in Physics to Hulse and Taylor for \"the discovery of a new type of pulsar, a discovery that has opened up new possibilities for the study of gravitation.\" The lifetime of this binary system, from the present to merger is estimated to be a few hundred million years. Inspirals are very important sources of gravitational waves. Any time two compact objects (white dwarfs, neutron stars, or black holes) are in close orbits, they send out intense gravitational waves. As they spiral closer to each other, these waves become more intense. At some point they should become so intense that direct detection by their effect on objects on Earth or in space is possible. This direct detection is the goal of several large-scale experiments. The only difficulty is that most systems like the Hulse–Taylor binary are so far away. The amplitude of waves given off by the Hulse–Taylor binary at Earth would be roughly h ≈ 10. There are some sources, however, that astrophysicists expect to find that produce much greater"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_28",
    "chunk": "amplitudes of h ≈ 10. At least eight other binary pulsars have been discovered. Gravitational waves are not easily detectable. When they reach the Earth, they have a small amplitude with strain approximately 10, meaning that an extremely sensitive detector is needed, and that other sources of noise can overwhelm the signal. Gravitational waves are expected to have frequencies 10 Hz < f < 10 Hz. Though the Hulse–Taylor observations were very important, they give only indirect evidence for gravitational waves. A more conclusive observation would be a direct measurement of the effect of a passing gravitational wave, which could also provide more information about the system that generated it. Any such direct detection is complicated by the extraordinarily small effect the waves would produce on a detector. The amplitude of a spherical wave will fall off as the inverse of the distance from the source (the 1/R term in the formulas for h above). Thus, even waves from extreme systems like merging binary black holes die out to very small amplitudes by the time they reach the Earth. Astrophysicists expect that some gravitational waves passing the Earth may be as large as h ≈ 10, but generally no bigger."
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_29",
    "chunk": "A simple device theorised to detect the expected wave motion is called a Weber bar – a large, solid bar of metal isolated from outside vibrations. This type of instrument was the first type of gravitational wave detector. Strains in space due to an incident gravitational wave excite the bar's resonant frequency and could thus be amplified to detectable levels. Conceivably, a nearby supernova might be strong enough to be seen without resonant amplification. With this instrument, Joseph Weber claimed to have detected daily signals of gravitational waves. His results, however, were contested in 1974 by physicists Richard Garwin and David Douglass. Modern forms of the Weber bar are still operated, cryogenically cooled, with superconducting quantum interference devices to detect vibration. Weber bars are not sensitive enough to detect anything but extremely powerful gravitational waves. MiniGRAIL is a spherical gravitational wave antenna using this principle. It is based at Leiden University, consisting of an exactingly machined 1,150 kg sphere cryogenically cooled to 20 millikelvins. The spherical configuration allows for equal sensitivity in all directions, and is somewhat experimentally simpler than larger linear devices requiring high vacuum. Events are detected by measuring deformation of the detector sphere. MiniGRAIL is highly sensitive"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_30",
    "chunk": "in the 2–4 kHz range, suitable for detecting gravitational waves from rotating neutron star instabilities or small black hole mergers. There are currently two detectors focused on the higher end of the gravitational wave spectrum (10 to 10 Hz): one at University of Birmingham, England, and the other at INFN Genoa, Italy. A third is under development at Chongqing University, China. The Birmingham detector measures changes in the polarization state of a microwave beam circulating in a closed loop about one meter across. Both detectors are expected to be sensitive to periodic spacetime strains of h ~ 2×10 /√Hz, given as an amplitude spectral density. The INFN Genoa detector is a resonant antenna consisting of two coupled spherical superconducting harmonic oscillators a few centimeters in diameter. The oscillators are designed to have (when uncoupled) almost equal resonant frequencies. The system is currently expected to have a sensitivity to periodic spacetime strains of h ~ 2×10 /√Hz, with an expectation to reach a sensitivity of h ~ 2×10 /√Hz. The Chongqing University detector is planned to detect relic high-frequency gravitational waves with the predicted typical parameters ≈10 Hz (100 GHz) and h ≈10 to 10. A more sensitive class of detector"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_31",
    "chunk": "uses a laser Michelson interferometer to measure gravitational-wave induced motion between separated 'free' masses. This allows the masses to be separated by large distances (increasing the signal size); a further advantage is that it is sensitive to a wide range of frequencies (not just those near a resonance as is the case for Weber bars). After years of development ground-based interferometers made the first detection of gravitational waves in 2015. Currently, the most sensitive is LIGO – the Laser Interferometer Gravitational Wave Observatory. LIGO has three detectors: one in Livingston, Louisiana, one at the Hanford site in Richland, Washington and a third (formerly installed as a second detector at Hanford) that is planned to be moved to India. Each observatory has two light storage arms that are 4 kilometers in length. These are at 90 degree angles to each other, with the light passing through 1 m diameter vacuum tubes running the entire 4 kilometers. A passing gravitational wave will slightly stretch one arm as it shortens the other. This is the motion to which an interferometer is most sensitive. Even with such long arms, the strongest gravitational waves will only change the distance between the ends of the arms"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_32",
    "chunk": "by at most roughly 10 m. LIGO should be able to detect gravitational waves as small as h ~ 5×10. Upgrades to LIGO and Virgo should increase the sensitivity still further. Another highly sensitive interferometer, KAGRA, which is located in the Kamioka Observatory in Japan, is in operation since February 2020. A key point is that a tenfold increase in sensitivity (radius of 'reach') increases the volume of space accessible to the instrument by one thousand times. This increases the rate at which detectable signals might be seen from one per tens of years of observation, to tens per year. Interferometric detectors are limited at high frequencies by shot noise, which occurs because the lasers produce photons randomly; one analogy is to rainfall – the rate of rainfall, like the laser intensity, is measurable, but the raindrops, like photons, fall at random times, causing fluctuations around the average value. This leads to noise at the output of the detector, much like radio static. In addition, for sufficiently high laser power, the random momentum transferred to the test masses by the laser photons shakes the mirrors, masking signals of low frequencies. Thermal noise (e.g., Brownian motion) is another limit to sensitivity."
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_33",
    "chunk": "In addition to these 'stationary' (constant) noise sources, all ground-based detectors are also limited at low frequencies by seismic noise and other forms of environmental vibration, and other 'non-stationary' noise sources; creaks in mechanical structures, lightning or other large electrical disturbances, etc. may also create noise masking an event or may even imitate an event. All of these must be taken into account and excluded by analysis before detection may be considered a true gravitational wave event. The simplest gravitational waves are those with constant frequency. The waves given off by a spinning, non-axisymmetric neutron star would be approximately monochromatic: a pure tone in acoustics. Unlike signals from supernovae or binary black holes, these signals evolve little in amplitude or frequency over the period it would be observed by ground-based detectors. However, there would be some change in the measured signal, because of Doppler shifting caused by the motion of the Earth. Despite the signals being simple, detection is extremely computationally expensive, because of the long stretches of data that must be analysed. The Einstein@Home project is a distributed computing project similar to SETI@home intended to detect this type of gravitational wave. By taking data from LIGO and GEO, and"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_34",
    "chunk": "sending it out in little pieces to thousands of volunteers for parallel analysis on their home computers, Einstein@Home can sift through the data far more quickly than would be possible otherwise. Space-based interferometers, such as LISA and DECIGO, are also being developed. LISA's design calls for three test masses forming an equilateral triangle, with lasers from each spacecraft to each other spacecraft forming two independent interferometers. LISA is planned to occupy a solar orbit trailing the Earth, with each arm of the triangle being 2.5 million kilometers. This puts the detector in an excellent vacuum far from Earth-based sources of noise, though it will still be susceptible to heat, shot noise, and artifacts caused by cosmic rays and solar wind. Pulsars are rapidly rotating stars. A pulsar emits beams of radio waves that, like lighthouse beams, sweep through the sky as the pulsar rotates. The signal from a pulsar can be detected by radio telescopes as a series of regularly spaced pulses, essentially like the ticks of a clock. GWs affect the time it takes the pulses to travel from the pulsar to a telescope on Earth. A pulsar timing array uses millisecond pulsars to seek out perturbations due to"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_35",
    "chunk": "GWs in measurements of the time of arrival of pulses to a telescope, in other words, to look for deviations in the clock ticks. To detect GWs, pulsar timing arrays search for a distinct quadrupolar pattern of correlation and anti-correlation between the time of arrival of pulses from different pulsar pairs as a function of their angular separation in the sky. Although pulsar pulses travel through space for hundreds or thousands of years to reach us, pulsar timing arrays are sensitive to perturbations in their travel time of much less than a millionth of a second. The most likely source of GWs to which pulsar timing arrays are sensitive are supermassive black hole binaries, which form from the collision of galaxies. In addition to individual binary systems, pulsar timing arrays are sensitive to a stochastic background of GWs made from the sum of GWs from many galaxy mergers. Other potential signal sources include cosmic strings and the primordial background of GWs from cosmic inflation. Globally there are three active pulsar timing array projects. The North American Nanohertz Observatory for Gravitational Waves uses data collected by the Arecibo Radio Telescope and Green Bank Telescope. The Australian Parkes Pulsar Timing Array uses"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_36",
    "chunk": "data from the Parkes radio-telescope. The European Pulsar Timing Array uses data from the four largest telescopes in Europe: the Lovell Telescope, the Westerbork Synthesis Radio Telescope, the Effelsberg Telescope and the Nancay Radio Telescope. These three groups also collaborate under the title of the International Pulsar Timing Array project. In June 2023, NANOGrav published the 15-year data release, which contained the first evidence for a stochastic gravitational wave background. In particular, it included the first measurement of the Hellings-Downs curve, the tell-tale sign of the gravitational wave origin of the observed background. Primordial gravitational waves are gravitational waves observed in the cosmic microwave background. They were allegedly detected by the BICEP2 instrument, an announcement made on 17 March 2014, which was withdrawn on 30 January 2015 (\"the signal can be entirely attributed to dust in the Milky Way\"). On 11 February 2016, the LIGO collaboration announced the first observation of gravitational waves, from a signal detected at 09:50:45 GMT on 14 September 2015 of two black holes with masses of 29 and 36 solar masses merging about 1.3 billion light-years away. During the final fraction of a second of the merger, it released more than 50 times the power"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_37",
    "chunk": "of all the stars in the observable universe combined. The signal increased in frequency from 35 to 250 Hz over 10 cycles (5 orbits) as it rose in strength for a period of 0.2 second. The mass of the new merged black hole was 62 solar masses. Energy equivalent to three solar masses was emitted as gravitational waves. The signal was seen by both LIGO detectors in Livingston and Hanford, with a time difference of 7 milliseconds due to the angle between the two detectors and the source. The signal came from the Southern Celestial Hemisphere, in the rough direction of (but much farther away than) the Magellanic Clouds. The gravitational waves were observed in the region more than 5 sigma (in other words, 99.99997% chances of showing/getting the same result), the probability of finding enough to have been assessed/considered as the evidence/proof in an experiment of statistical physics. Since then LIGO and Virgo have reported more gravitational wave observations from merging black hole binaries. On 16 October 2017, the LIGO and Virgo collaborations announced the first-ever detection of gravitational waves originating from the coalescence of a binary neutron star system. The observation of the GW170817 transient, which occurred on"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_38",
    "chunk": "17 August 2017, allowed for constraining the masses of the neutron stars involved between 0.86 and 2.26 solar masses. Further analysis allowed a greater restriction of the mass values to the interval 1.17–1.60 solar masses, with the total system mass measured to be 2.73–2.78 solar masses. The inclusion of the Virgo detector in the observation effort allowed for an improvement of the localization of the source by a factor of 10. This in turn facilitated the electromagnetic follow-up of the event. The signal lasted about 100 seconds, much longer than the few seconds measured from binary black holes. Also in contrast to the case of binary black hole mergers, binary neutron star mergers were expected to yield an electromagnetic counterpart, that is, a light signal associated with the event. A gamma-ray burst (GRB 170817A) was detected by the Fermi Gamma-ray Space Telescope, occurring 1.7 seconds after the gravitational wave transient. The signal, originating near the galaxy NGC 4993, was associated with the neutron star merger. This was corroborated by the electromagnetic follow-up of the event (AT 2017gfo), involving 70 telescopes and observatories and yielding observations over a large region of the electromagnetic spectrum which further confirmed the neutron star nature"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_39",
    "chunk": "of the merged objects and the associated kilonova. In 2021, the detection of the first two neutron star-black hole binaries by the LIGO and VIRGO detectors was published in the Astrophysical Journal Letters, allowing to first set bounds on the quantity of such systems. No neutron star-black hole binary had ever been observed using conventional means before the gravitational observation. In 1964, L. Halpern and B. Laurent theoretically proved that gravitational spin-2 electron transitions are possible in atoms. Compared to electric and magnetic transitions the emission probability is extremely low. Stimulated emission was discussed for increasing the efficiency of the process. Due to the lack of mirrors or resonators for gravitational waves, they determined that a single pass GASER (a kind of laser emitting gravitational waves) is practically unfeasible. In 1998, the possibility of a different implementation of the above theoretical analysis was proposed by Giorgio Fontana. The required coherence for a practical GASER could be obtained by Cooper pairs in superconductors that are characterized by a macroscopic collective wave-function. Cuprate high temperature superconductors are characterized by the presence of s-wave and d-wave Cooper pairs. Transitions between s-wave and d-wave are gravitational spin-2. Out of equilibrium conditions can be induced"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_40",
    "chunk": "by injecting s-wave Cooper pairs from a low temperature superconductor, for instance lead or niobium, which is pure s-wave, by means of a Josephson junction with high critical current. The amplification mechanism can be described as the effect of superradiance, and 10 cubic centimeters of cuprate high temperature superconductor seem sufficient for the mechanism to properly work. A detailed description of the approach can be found in \"High Temperature Superconductors as Quantum Sources of Gravitational Waves: The HTSC GASER\". Chapter 3 of this book. An episode of the 1962 Russian science-fiction novel Space Apprentice by Arkady and Boris Strugatsky shows an experiment monitoring the propagation of gravitational waves at the expense of annihilating a chunk of asteroid 15 Eunomia the size of Mount Everest. In Stanislaw Lem's 1986 novel Fiasco, a \"gravity gun\" or \"gracer\" (gravity amplification by collimated emission of resonance) is used to reshape a collapsar, so that the protagonists can exploit the extreme relativistic effects and make an interstellar journey. In Greg Egan's 1997 novel Diaspora, the analysis of a gravitational wave signal from the inspiral of a nearby binary neutron star reveals that its collision and merger is imminent, implying a large gamma-ray burst is going"
  },
  {
    "source": "Gravitational wave.txt",
    "chunk_id": "Gravitational wave.txt_41",
    "chunk": "to impact the Earth. In Liu Cixin's 2006 Remembrance of Earth's Past series, gravitational waves are used as an interstellar broadcast signal, which serves as a central plot point in the conflict between civilizations within the galaxy."
  },
  {
    "source": "Gustav Kirchhoff.txt",
    "chunk_id": "Gustav Kirchhoff.txt_0",
    "chunk": "# Gustav Kirchhoff Gustav Robert Kirchhoff (German: [ˈgʊstaːf ˈʁoːbɛʁt ˈkɪʁçhɔf]; 12 March 1824 – 17 October 1887) was a German chemist, mathematician and physicist who contributed to the fundamental understanding of electrical circuits, spectroscopy and the emission of black-body radiation by heated objects. He also coined the term black body in 1860. Several different sets of concepts are named \"Kirchhoff's laws\" after him, which include Kirchhoff's circuit laws, Kirchhoff's law of thermal radiation, and Kirchhoff's law of thermochemistry. The Bunsen–Kirchhoff Award for spectroscopy is named after Kirchhoff and his colleague, Robert Bunsen. Gustav Kirchhoff was born on 12 March 1824 in Königsberg, Prussia, the son of Friedrich Kirchhoff, a lawyer, and Johanna Henriette Wittke. His family were Lutherans in the Evangelical Church of Prussia. He graduated from the Albertus University of Königsberg in 1847 where he attended the mathematico-physical seminar directed by Carl Gustav Jacob Jacobi, Franz Ernst Neumann and Friedrich Julius Richelot. In the same year, he moved to Berlin, where he stayed until he received a professorship at Breslau. Later, in 1857, he married Clara Richelot, the daughter of his mathematics professor Richelot. The couple had five children. Clara died in 1869. He married Luise Brömmel in 1872."
  },
  {
    "source": "Gustav Kirchhoff.txt",
    "chunk_id": "Gustav Kirchhoff.txt_1",
    "chunk": "Kirchhoff formulated his circuit laws, which are now ubiquitous in electrical engineering, in 1845, while he was still a student. He completed this study as a seminar exercise; it later became his doctoral dissertation. He was called to the University of Heidelberg in 1854, where he collaborated in spectroscopic work with Robert Bunsen. In 1857, he calculated that an electric signal in a resistanceless wire travels along the wire at the speed of light. He proposed his law of thermal radiation in 1859, and gave a proof in 1861. Together Kirchhoff and Bunsen invented the spectroscope, which Kirchhoff used to pioneer the identification of the elements in the Sun, showing in 1859 that the Sun contains sodium. He and Bunsen discovered caesium and rubidium in 1861. At Heidelberg he ran a mathematico-physical seminar, modelled on Franz Ernst Neumann's, with the mathematician Leo Koenigsberger. Among those who attended this seminar were Arthur Schuster and Sofia Kovalevskaya. He contributed greatly to the field of spectroscopy by formalizing three laws that describe the spectral composition of light emitted by incandescent objects, building substantially on the discoveries of David Alter and Anders Jonas Ångström. In 1862, he was awarded the Rumford Medal for his"
  },
  {
    "source": "Gustav Kirchhoff.txt",
    "chunk_id": "Gustav Kirchhoff.txt_2",
    "chunk": "researches on the fixed lines of the solar spectrum, and on the inversion of the bright lines in the spectra of artificial light. In 1875 Kirchhoff accepted the first chair dedicated specifically to theoretical physics at Berlin. He also contributed to optics, carefully solving the wave equation to provide a solid foundation for Huygens' principle (and correct it in the process). In 1884, he became foreign member of the Royal Netherlands Academy of Arts and Sciences. Kirchhoff died in 1887, and was buried in the St Matthäus Kirchhof Cemetery in Schöneberg, Berlin (just a few meters from the graves of the Brothers Grimm). Leopold Kronecker is buried in the same cemetery. Kirchhoff's first law is that the algebraic sum of currents in a network of conductors meeting at a point (or node) is zero. The second law is that in a closed circuit, the directed sums of the voltages in the system is zero. Kirchhoff did not know about the existence of energy levels in atoms. The existence of discrete spectral lines had been known since Fraunhofer discovered them in 1814. That the lines formed a discrete mathematical pattern was described by Johann Balmer in 1885. Joseph Larmor explained the"
  },
  {
    "source": "Gustav Kirchhoff.txt",
    "chunk_id": "Gustav Kirchhoff.txt_3",
    "chunk": "splitting of the spectral lines in a magnetic field known as the Zeeman Effect by the oscillation of electrons. These discrete spectral lines were not explained as electron transitions until the Bohr model of the atom in 1913, which helped lead to quantum mechanics. It was Kirchhoff's law of thermal radiation in which he proposed an unknown universal law for radiation that led Max Planck to the discovery of the quantum of action leading to quantum mechanics. Kirchhoff showed in 1858 that, in thermochemistry, the variation of the heat of a chemical reaction is given by the difference in heat capacity between products and reactants: Integration of this equation permits the evaluation of the heat of reaction at one temperature from measurements at another temperature. Kirchhoff also worked in the mathematical field of graph theory, in which he proved Kirchhoff's matrix tree theorem."
  },
  {
    "source": "Harrison Schmitt.txt",
    "chunk_id": "Harrison Schmitt.txt_0",
    "chunk": "# Harrison Schmitt Harrison Hagan \"Jack\" Schmitt (born July 3, 1935) is an American geologist, former NASA astronaut, university professor, former U.S. senator from New Mexico, and the most recent living person—and only person without a background in military aviation—to have walked on the Moon. In December 1972, as one of the crew on board Apollo 17, Schmitt became the first member of NASA's first scientist-astronaut group to fly in space. As Apollo 17 was the last of the Apollo missions, he also became the twelfth and second-youngest person to set foot on the Moon and the second-to-last person to step off of the Moon (he boarded the Lunar Module shortly before commander Eugene Cernan). Schmitt also remains the only professional scientist to have flown beyond low Earth orbit and to have visited the Moon. He was influential within the community of geologists supporting the Apollo program and, before starting his own preparations for an Apollo mission, had been one of the scientists training those Apollo astronauts chosen to visit the lunar surface. Schmitt resigned from NASA in August 1975 to run for election to the United States Senate as a member from New Mexico. As the Republican candidate in"
  },
  {
    "source": "Harrison Schmitt.txt",
    "chunk_id": "Harrison Schmitt.txt_1",
    "chunk": "the 1976 election, he defeated Democratic incumbent Joseph Montoya. In the 1982 election, Schmitt was defeated by Democrat Jeff Bingaman. Born July 3, 1935, in Santa Rita, New Mexico, Schmitt grew up in nearby Silver City, and is a graduate of the Western High School (Class of 1953). He received a Bachelor of Science degree in geology from the California Institute of Technology in 1957 and then spent a year studying geology at the University of Oslo in Norway, as a Fulbright Scholar. He received a Doctor of Philosophy in geology from Harvard University in 1964, based on his geological field studies in Norway. Before joining NASA as a member of the first group of scientist-astronauts in June 1965, he worked at the U.S. Geological Survey's Astrogeology Center at Flagstaff, Arizona, developing geological field techniques that would be used by the Apollo crews. Following his selection, Schmitt spent his first year at Air Force UPT learning to become a jet pilot. Upon his return to the astronaut corps in Houston, he played a key role in training Apollo crews to be geologic observers when they were in lunar orbit and competent geologic field workers when they were on the lunar"
  },
  {
    "source": "Harrison Schmitt.txt",
    "chunk_id": "Harrison Schmitt.txt_2",
    "chunk": "surface. After each of the landing missions, he participated in the examination and evaluation of the returned lunar samples and helped the crews with the scientific aspects of their mission reports. Schmitt spent considerable time becoming proficient in the CSM and LM systems. In March 1970 he became the first of the scientist-astronauts to be assigned to space flight, joining Richard F. Gordon Jr. (Commander) and Vance Brand (Command Module Pilot) on the Apollo 15 backup crew. The flight rotation put these three in line to fly as prime crew on the third following mission, Apollo 18. When Apollo 18 and Apollo 19 were canceled in September 1970, the community of lunar geologists supporting Apollo felt so strongly about the need to land a professional geologist on the Moon, that they pressured NASA to reassign Schmitt to a remaining flight. As a result, Schmitt was assigned in August 1971 to fly on Apollo 17, replacing Joe Engle as Lunar Module Pilot. Schmitt landed on the Moon with commander Gene Cernan in December 1972. Schmitt claims to have taken the photograph of the Earth known as The Blue Marble, one of the most widely distributed photographic images in existence. His Apollo"
  },
  {
    "source": "Harrison Schmitt.txt",
    "chunk_id": "Harrison Schmitt.txt_3",
    "chunk": "17 crewmates, Gene Cernan (Mission Commander) and Ronald Evans (Command Module Pilot), have made the same claim, and NASA's official position is to credit all three together. \"Perhaps the hardest thing to get used to on the Moon is that the sky is completely black. There's no blue at all.\" While on the Moon's surface, Schmitt—the only geologist in the astronaut corps—collected the rock sample designated Troctolite 76535, which has been called \"without doubt the most interesting sample returned from the Moon\". Among other distinctions, it is the central piece of evidence suggesting that the Moon once possessed an active magnetic field. As he returned to the Lunar Module before Cernan, Schmitt is the next-to-last person to have walked on the Moon's surface. Since the death of Cernan in 2017, Schmitt is the most recent person to have walked on the Moon who is still alive. After the completion of the Apollo 17 mission, Schmitt played an active role in documenting the Apollo geologic results and also took on the task of organizing NASA's Energy Program Office. On April 29, 2018, the Schmitt Space Communicator SC-1x named in his honor was carried aboard the Blue Origin New Shepard crew capsule"
  },
  {
    "source": "Harrison Schmitt.txt",
    "chunk_id": "Harrison Schmitt.txt_4",
    "chunk": "in a project partly funded by NASA. It launched the first commercial two-way data and Wi-Fi hotspot service in space and sent the first commercial Twitter message from space. The three-pound (1.4 kg) device was developed by Solstar, which Schmitt had joined as an advisor, and launched 66 miles (106 km) above the Earth's surface, just past the edge of space, as a technology demonstration. The device was admitted to the Smithsonian National Air and Space Museum. On August 30, 1975, Schmitt retired from NASA to seek election as a Republican to the United States Senate representing New Mexico in the 1976 election. The astronaut-politician campaigned for fourteen months, and his campaign focused on the future. In the Republican primary, held on June 1, 1976, Schmitt defeated the unknown Eugene Peirce. In the election, Schmitt opposed two-term Democratic incumbent Joseph Montoya. He defeated Montoya 57% to 43%. He served one term and, notably, was the chairman of the Science, Technology, and Space Subcommittee of the United States Senate Committee on Commerce. He sought a second term in 1982, facing state Attorney General Jeff Bingaman. Bingaman criticized Schmitt for not paying enough attention to local matters; his campaign slogan asked, \"What"
  },
  {
    "source": "Harrison Schmitt.txt",
    "chunk_id": "Harrison Schmitt.txt_5",
    "chunk": "on Earth has he done for you lately?\" This, combined with the deep recession, proved too much for Schmitt to overcome; he was defeated, 54% to 46%. Following his Senate term, Schmitt has been a consultant in business, geology, space, and public policy. Schmitt is an adjunct professor of engineering physics at the University of Wisconsin–Madison, and has long been a proponent of lunar resource utilization. In 1997 he proposed the Interlune InterMars Initiative, listing among its goals the advancement of private-sector acquisition and use of lunar resources, particularly lunar helium-3 as a fuel for notional nuclear fusion reactors. Schmitt was chair of the NASA Advisory Council, whose mandate is to provide technical advice to the NASA Administrator, from November 2005 until his abrupt resignation on October 16, 2008. In November 2008, he quit the Planetary Society over policy advocacy differences, citing the organization's statements on \"focusing on Mars as the driving goal of human spaceflight\" (Schmitt said that going back to the Moon would speed progress toward a crewed Mars mission), on \"accelerating research into global climate change through more comprehensive Earth observations\" (Schmitt voiced objections to the notion of a present \"scientific consensus\" on climate change as any"
  },
  {
    "source": "Harrison Schmitt.txt",
    "chunk_id": "Harrison Schmitt.txt_6",
    "chunk": "policy guide), and on international cooperation (which he felt would retard rather than accelerate progress), among other points of divergence. Schmitt also serves as a visiting senior research scientist at the Florida Institute for Human & Machine Cognition. In January 2011, he was appointed as secretary of the New Mexico Energy, Minerals and Natural Resources Department in the cabinet of Governor Susana Martinez, but was forced to give up the appointment the following month after refusing to submit to a required background investigation. El Paso Times called him the \"most celebrated\" candidate for New Mexico energy secretary. Schmitt wrote a book entitled Return to the Moon: Exploration, Enterprise, and Energy in the Human Settlement of Space in 2006. Schmitt is also involved in several civic projects, including the improvement of the Senator Harrison H. Schmitt Big Sky Hang Glider Park in Albuquerque, New Mexico. Schmitt has rejected the scientific consensus on climate change, which states that climate change is real, progressing, dangerous, and primarily human-caused. He has claimed that climate change is predominantly caused by natural factors, as opposed to human activity. Schmitt has argued that the risks posed by climate change are overstated and has instead supported the notion"
  },
  {
    "source": "Harrison Schmitt.txt",
    "chunk_id": "Harrison Schmitt.txt_7",
    "chunk": "that climate change is a \"tool\" used to advocate for the expansion of the government. Schmitt resigned from the Planetary Society due to disagreements over their \"Roadmap to Space Exploration\", which recommended prioritizing earlier human missions to Mars over U.S. lunar expeditions. He believed lunar exploration was crucial for Mars missions, stating, \"The fastest way to get to Mars is by way of the Moon.\" Additionally, Schmitt criticized the society's stance on global warming, writing in his resignation letter that the \"'global warming scare' is being used as a political tool to increase government control over American lives, incomes and decision making,\" asserting it should not be part of the Society's activities. Schmitt spoke at the March 2009 International Conference on Climate Change, an anthropogenic climate change denier event hosted by the conservative Heartland Institute, where he said that climate change was a \"stalking horse for National Socialism.\" He appeared in December that year on the Fox Business Network, saying that \"[t]he CO2 scare is a red herring\". In a 2009 interview with far-right conspiracy theorist and radio host Alex Jones, Schmitt asserted a link between the collapse of the Soviet Union and the American environmental movement: \"I think the"
  },
  {
    "source": "Harrison Schmitt.txt",
    "chunk_id": "Harrison Schmitt.txt_8",
    "chunk": "whole trend really began with the fall of the Soviet Union. Because the great champion of the opponents of liberty, namely communism, had to find some other place to go and they basically went into the environmental movement.\" In 2013, Schmitt co-authored an opinion column in The Wall Street Journal with William Happer, contending that increasing levels of carbon dioxide in the atmosphere are not significantly correlated with global warming, attributing the \"single-minded demonization of this natural and essential atmospheric gas\" to advocates of government control of energy production. Noting a positive relationship between crop resistance to drought and increasing carbon dioxide levels, the authors argued, \"Contrary to what some would have us believe, increased carbon dioxide in the atmosphere will benefit the increasing population on the planet by increasing agricultural productivity.\" Schmitt was one of five inductees into the International Space Hall of Fame in 1977. He was one of 24 Apollo astronauts who were inducted into the U.S. Astronaut Hall of Fame in 1997. Schmitt is one of the astronauts featured in the 2007 documentary In the Shadow of the Moon. He also contributed to the 2006 book NASA's Scientist-Astronauts by David Shayler and Colin Burgess."
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_0",
    "chunk": "# Heavy metals Heavy metals is a controversial and ambiguous term for metallic elements with relatively high densities, atomic weights, or atomic numbers. The criteria used, and whether metalloids are included, vary depending on the author and context and it has been argued that the term \"heavy metal\" should be avoided. A heavy metal may be defined on the basis of density, atomic number or chemical behaviour. More specific definitions have been published, none of which have been widely accepted. The definitions surveyed in this article encompass up to 96 out of the 118 known chemical elements; only mercury, lead and bismuth meet all of them. Despite this lack of agreement, the term (plural or singular) is widely used in science. A density of more than 5 g/cm is sometimes quoted as a commonly used criterion and is used in the body of this article. The earliest-known metals—common metals such as iron, copper, and tin, and precious metals such as silver, gold, and platinum—are heavy metals. From 1809 onward, light metals, such as magnesium, aluminium, and titanium, were discovered, as well as less well-known heavy metals including gallium, thallium, and hafnium. Some heavy metals are either essential nutrients (typically iron,"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_1",
    "chunk": "cobalt, copper and zinc), or relatively harmless (such as ruthenium, silver and indium), but can be toxic in larger amounts or certain forms. Other heavy metals, such as arsenic, cadmium, mercury, and lead, are highly poisonous. Potential sources of heavy metal poisoning include mining, tailings, smelting, industrial waste, agricultural runoff, occupational exposure, paints and treated timber. Physical and chemical characterisations of heavy metals need to be treated with caution, as the metals involved are not always consistently defined. As well as being relatively dense, heavy metals tend to be less reactive than lighter metals and have far fewer soluble sulfides and hydroxides. While it is relatively easy to distinguish a heavy metal such as tungsten from a lighter metal such as sodium, a few heavy metals, such as zinc, mercury, and lead, have some of the characteristics of lighter metals; and lighter metals such as beryllium, scandium, and titanium, have some of the characteristics of heavier metals. Heavy metals are relatively rare in the Earth's crust but are present in many aspects of modern life. They are used in, for example, golf clubs, cars, antiseptics, self-cleaning ovens, plastics, solar panels, mobile phones, and particle accelerators. The International Union of Pure"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_2",
    "chunk": "and Applied Chemistry (IUPAC), which standardizes nomenclature, says \"the term heavy metals is both meaningless and misleading\". The IUPAC report focuses on the legal and toxicological implications of describing \"heavy metals\" as toxins when there is no scientific evidence to support a connection. The density implied by the adjective \"heavy\" has almost no biological consequences and pure metals are rarely the biologically active substance. This characterization has been echoed by numerous reviews. The most widely used toxicology textbook, Casarett and Doull’s Toxicology uses \"toxic metal\", not \"heavy metal\". Nevertheless, there are scientific and science related articles which continue to use \"heavy metal\" as a term for toxic substances. To be an acceptable term in scientific papers, a strict definition has been encouraged. Even in applications other than toxicity, there no widely agreed criterion-based definition of a heavy metal. Reviews have recommended that it not be used. Different meanings may be attached to the term, depending on the context. For example, a heavy metal may be defined on the basis of density, the distinguishing criterion might be atomic number or chemical behaviour. Density criteria range from above 3.5 g/cm to above 7 g/cm. Atomic weight definitions can range from greater than"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_3",
    "chunk": "sodium (atomic weight 22.98); greater than 40 (excluding s- and f-block metals, hence starting with scandium); or more than 200, i.e. from mercury onwards. Atomic numbers are sometimes capped at 92 (uranium). Definitions based on atomic number have been criticised for including metals with low densities. For example, rubidium in group (column) 1 of the periodic table has an atomic number of 37 but a density of only 1.532 g/cm, which is below the threshold figure used by other authors. The same problem may occur with definitions which are based on atomic weight. The United States Pharmacopeia includes a test for heavy metals that involves precipitating metallic impurities as their coloured sulfides. On the basis of this type of chemical test, the group would include the transition metals and post-transition metals. A different chemistry-based approach advocates replacing the term \"heavy metal\" with two groups of metals and a gray area. Class A metal ions prefer oxygen donors; class B ions prefer nitrogen or sulfur donors; and borderline or ambivalent ions show either class A or B characteristics, depending on the circumstances. The distinction between the class A metals and the other two categories is sharp. The class A and class"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_4",
    "chunk": "B terminology is analogous to the \"hard acid\" and \"soft base\" terminology sometimes used to refer to the behaviour of metal ions in inorganic systems. The system groups the elements by X m 2 r {\\displaystyle X_{m}^{2}r} where X m {\\displaystyle X_{m}} is the metal ion electronegativity and r {\\displaystyle r} is its ionic radius. This index gauges the importance of covalent interactions vs ionic interactions for a given metal ion. This scheme has been applied to analyze biologically active metals in sea water for example, but it has not been widely adopted. The heaviness of naturally occurring metals such as gold, copper, and iron may have been noticed in prehistory and, in light of their malleability, led to the first attempts to craft metal ornaments, tools, and weapons. In 1817, the German chemist Leopold Gmelin divided the elements into nonmetals, light metals, and heavy metals. Light metals had densities of 0.860–5.0 g/cm; heavy metals 5.308–22.000. The term heavy metal is sometimes used interchangeably with the term heavy element. For example, in discussing the history of nuclear chemistry, Magee notes that the actinides were once thought to represent a new heavy element transition group whereas Seaborg and co-workers \"favoured ..."
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_5",
    "chunk": "a heavy metal rare-earth like series ...\". The counterparts to the heavy metals, the light metals, are defined by The Minerals, Metals and Materials Society as including \"the traditional (aluminium, magnesium, beryllium, titanium, lithium, and other reactive metals) and emerging light metals (composites, laminates, etc.)\" Trace amounts of some heavy metals, mostly in period 4, are required for certain biological processes. These are iron and copper (oxygen and electron transport); cobalt (complex syntheses and cell metabolism); vanadium and manganese (enzyme regulation or functioning); chromium (glucose utilisation); nickel (cell growth); arsenic (metabolic growth in some animals and possibly in humans) and selenium (antioxidant functioning and hormone production). Periods 5 and 6 contain fewer essential heavy metals, consistent with the general pattern that heavier elements tend to be less abundant and that scarcer elements are less likely to be nutritionally essential. In period 5, molybdenum is required for the catalysis of redox reactions; cadmium is used by some marine diatoms for the same purpose; and tin may be required for growth in a few species. In period 6, tungsten is required by some archaea and bacteria for metabolic processes. A deficiency of any of these period 4–6 essential heavy metals may increase"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_6",
    "chunk": "susceptibility to heavy metal poisoning (conversely, an excess may also have adverse biological effects). An average 70 kg human body is about 0.01% heavy metals (~7 g, equivalent to the weight of two dried peas, with iron at 4 g, zinc at 2.5 g, and lead at 0.12 g comprising the three main constituents), 2% light metals (~1.4 kg, the weight of a bottle of wine) and nearly 98% nonmetals (mostly water). A few non-essential heavy metals have been observed to have biological effects. Gallium, germanium (a metalloid), indium, and most lanthanides can stimulate metabolism, and titanium promotes growth in plants (though it is not always considered a heavy metal). Heavy metals are often assumed to be highly toxic or damaging to the environment. Some are, while certain others are toxic only if taken in excess or encountered in certain forms. Inhalation of certain metals, either as fine dust or most commonly as fumes, can also result in a condition called metal fume fever. Chromium, arsenic, cadmium, mercury, and lead have the greatest potential to cause harm on account of their extensive use, the toxicity of some of their combined or elemental forms, and their widespread distribution in the environment."
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_7",
    "chunk": "Hexavalent chromium, for example, is highly toxic as are mercury vapour and many mercury compounds. These five elements have a strong affinity for sulfur; in the human body they usually bind, via thiol groups (–SH), to enzymes responsible for controlling the speed of metabolic reactions. The resulting sulfur-metal bonds inhibit the proper functioning of the enzymes involved; human health deteriorates, sometimes fatally. Chromium (in its hexavalent form) and arsenic are carcinogens; cadmium causes a degenerative bone disease; and mercury and lead damage the central nervous system. Lead is the most prevalent heavy metal contaminant. Levels in the aquatic environments of industrialised societies have been estimated to be two to three times those of pre-industrial levels. As a component of tetraethyl lead, (CH3CH2)4Pb, it was used extensively in gasoline from the 1930s until the 1970s. Although the use of leaded gasoline was largely phased out in North America by 1996, soils next to roads built before this time retain high lead concentrations. Later research demonstrated a statistically significant correlation between the usage rate of leaded gasoline and violent crime in the United States; taking into account a 22-year time lag (for the average age of violent criminals), the violent crime curve"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_8",
    "chunk": "virtually tracked the lead exposure curve. Other heavy metals noted for their potentially hazardous nature, usually as toxic environmental pollutants, include manganese (central nervous system damage); cobalt and nickel (carcinogens); copper, zinc, selenium and silver (endocrine disruption, congenital disorders, or general toxic effects in fish, plants, birds, or other aquatic organisms); tin, as organotin (central nervous system damage); antimony (a suspected carcinogen); and thallium (central nervous system damage). A few other non-essential heavy metals have one or more toxic forms. Kidney failure and fatalities have been recorded arising from the ingestion of germanium dietary supplements (~15 to 300 g in total consumed over a period of two months to three years). Exposure to osmium tetroxide (OsO4) may cause permanent eye damage and can lead to respiratory failure and death. Indium salts are toxic if more than few milligrams are ingested and will affect the kidneys, liver, and heart. Cisplatin (PtCl2(NH3)2), an important drug used to kill cancer cells, is also a kidney and nerve poison. Bismuth compounds can cause liver damage if taken in excess; insoluble uranium compounds, as well as the dangerous radiation they emit, can cause permanent kidney damage. Heavy metals can degrade air, water, and soil quality,"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_9",
    "chunk": "and subsequently cause health issues in plants, animals, and people, when they become concentrated as a result of industrial activities. Common sources of heavy metals in this context include vehicle emissions; motor oil; fertilisers; glassworking; incinerators; treated timber; aging water supply infrastructure; and microplastics floating in the world's oceans. Recent examples of heavy metal contamination and health risks include the occurrence of Minamata disease, in Japan (1932–1968; lawsuits ongoing as of 2016); the Bento Rodrigues dam disaster in Brazil, high levels of lead in drinking water supplied to the residents of Flint, Michigan, in the north-east of the United States and 2015 Hong Kong heavy metal in drinking water incidents. Heavy metals up to the vicinity of iron (in the periodic table) are largely made via stellar nucleosynthesis. In this process, lighter elements from hydrogen to silicon undergo successive fusion reactions inside stars, releasing light and heat and forming heavier elements with higher atomic numbers. Heavier heavy metals are not usually formed this way since fusion reactions involving such nuclei would consume rather than release energy. Rather, they are largely synthesised (from elements with a lower atomic number) by neutron capture, with the two main modes of this repetitive capture"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_10",
    "chunk": "being the s-process and the r-process. In the s-process (\"s\" stands for \"slow\"), singular captures are separated by years or decades, allowing the less stable nuclei to beta decay, while in the r-process (\"rapid\"), captures happen faster than nuclei can decay. Therefore, the s-process takes a more or less clear path: for example, stable cadmium-110 nuclei are successively bombarded by free neutrons inside a star until they form cadmium-115 nuclei which are unstable and decay to form indium-115 (which is nearly stable, with a half-life 30,000 times the age of the universe). These nuclei capture neutrons and form indium-116, which is unstable, and decays to form tin-116, and so on. In contrast, there is no such path in the r-process. The s-process stops at bismuth due to the short half-lives of the next two elements, polonium and astatine, which decay to bismuth or lead. The r-process is so fast it can skip this zone of instability and go on to create heavier elements such as thorium and uranium. Heavy metals condense in planets as a result of stellar evolution and destruction processes. Stars lose much of their mass when it is ejected late in their lifetimes, and sometimes thereafter as"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_11",
    "chunk": "a result of a neutron star merger, thereby increasing the abundance of elements heavier than helium in the interstellar medium. When gravitational attraction causes this matter to coalesce and collapse, new stars and planets are formed. The Earth's crust is made of approximately 5% of heavy metals by weight, with iron comprising 95% of this quantity. Light metals (~20%) and nonmetals (~75%) make up the other 95% of the crust. Despite their overall scarcity, heavy metals can become concentrated in economically extractable quantities as a result of mountain building, erosion, or other geological processes. Heavy metals are found primarily as lithophiles (rock-loving) or chalcophiles (ore-loving). Lithophile heavy metals are mainly f-block elements and the more reactive of the d-block elements. They have a strong affinity for oxygen and mostly exist as relatively low density silicate minerals. Chalcophile heavy metals are mainly the less reactive d-block elements, and period 4–6 p-block metals and metalloids. They are usually found in (insoluble) sulfide minerals. Being denser than the lithophiles, hence sinking lower into the crust at the time of its solidification, the chalcophiles tend to be less abundant than the lithophiles. In contrast, gold is a siderophile, or iron-loving element. It does not"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_12",
    "chunk": "readily form compounds with either oxygen or sulfur. At the time of the Earth's formation, and as the most noble (inert) of metals, gold sank into the core due to its tendency to form high-density metallic alloys. Consequently, it is a relatively rare metal. Some other (less) noble heavy metals—molybdenum, rhenium, the platinum group metals (ruthenium, rhodium, palladium, osmium, iridium, and platinum), germanium, and tin—can be counted as siderophiles but only in terms of their primary occurrence in the Earth (core, mantle and crust), rather the crust. These metals otherwise occur in the crust, in small quantities, chiefly as chalcophiles (less so in their native form). Concentrations of heavy metals below the crust are generally higher, with most being found in the largely iron-silicon-nickel core. Platinum, for example, comprises approximately 1 part per billion of the crust whereas its concentration in the core is thought to be nearly 6,000 times higher. Recent speculation suggests that uranium (and thorium) in the core may generate a substantial amount of the heat that drives plate tectonics and (ultimately) sustains the Earth's magnetic field. Broadly speaking, and with some exceptions, lithophile heavy metals can be extracted from their ores by electrical or chemical treatments,"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_13",
    "chunk": "while chalcophile heavy metals are obtained by roasting their sulphide ores to yield the corresponding oxides, and then heating these to obtain the raw metals. Radium occurs in quantities too small to be economically mined and is instead obtained from spent nuclear fuels. The chalcophile platinum group metals (PGM) mainly occur in small (mixed) quantities with other chalcophile ores. The ores involved need to be smelted, roasted, and then leached with sulfuric acid to produce a residue of PGM. This is chemically refined to obtain the individual metals in their pure forms. Compared to other metals, PGM are expensive due to their scarcity and high production costs. Gold, a siderophile, is most commonly recovered by dissolving the ores in which it is found in a cyanide solution. The gold forms a dicyanoaurate(I), for example: 2 Au + H2O +½ O2 + 4 KCN → 2 K[Au(CN)2] + 2 KOH. Zinc is added to the mix and, being more reactive than gold, displaces the gold: 2 K[Au(CN)2] + Zn → K2[Zn(CN)4] + 2 Au. The gold precipitates out of solution as a sludge, and is filtered off and melted. Some common uses of heavy metals depend on the general characteristics of"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_14",
    "chunk": "metals such as electrical conductivity and reflectivity or the general characteristics of heavy metals such as density, strength, and durability. Other uses depend on the characteristics of the specific element, such as their biological role as nutrients or poisons or some other specific atomic properties. Examples of such atomic properties include: partly filled d- or f- orbitals (in many of the transition, lanthanide, and actinide heavy metals) that enable the formation of coloured compounds; the capacity of heavy metal ions (such as platinum, cerium or bismuth) to exist in different oxidation states and are used in catalysts; strong exchange interactions in 3d or 4f orbitals (in iron, cobalt, and nickel, or the lanthanide heavy metals) that give rise to magnetic effects; and high atomic numbers and electron densities that underpin their nuclear science applications. Typical uses of heavy metals can be broadly grouped into the following categories. Some uses of heavy metals, including in sport, mechanical engineering, military ordnance, and nuclear science, take advantage of their relatively high densities. In underwater diving, lead is used as a ballast; in handicap horse racing each horse must carry a specified lead weight, based on factors including past performance, so as to equalize"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_15",
    "chunk": "the chances of the various competitors. In golf, tungsten, brass, or copper inserts in fairway clubs and irons lower the centre of gravity of the club making it easier to get the ball into the air; and golf balls with tungsten cores are claimed to have better flight characteristics. In fly fishing, sinking fly lines have a PVC coating embedded with tungsten powder, so that they sink at the required rate. In track and field sport, steel balls used in the hammer throw and shot put events are filled with lead in order to attain the minimum weight required under international rules. Tungsten was used in hammer throw balls at least up to 1980; the minimum size of the ball was increased in 1981 to eliminate the need for what was, at that time, an expensive metal (triple the cost of other hammers) not generally available in all countries. Tungsten hammers were so dense that they penetrated too deeply into the turf. The higher the projectile density, the more effectively it can penetrate heavy armor plate ... Os, Ir, Pt, and Re ... are expensive ... U offers an appealing combination of high density, reasonable cost and high fracture toughness."
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_16",
    "chunk": "Heavy metals are used for ballast in boats, aeroplanes, and motor vehicles; or in balance weights on wheels and crankshafts, gyroscopes, and propellers, and centrifugal clutches, in situations requiring maximum weight in minimum space (for example in watch movements). In military ordnance, tungsten or uranium is used in armour plating and armour piercing projectiles, as well as in nuclear weapons to increase efficiency (by reflecting neutrons and momentarily delaying the expansion of reacting materials). In the 1970s, tantalum was found to be more effective than copper in shaped charge and explosively formed anti-armour weapons on account of its higher density, allowing greater force concentration, and better deformability. Less-toxic heavy metals, such as copper, tin, tungsten, and bismuth, and probably manganese (as well as boron, a metalloid), have replaced lead and antimony in the green bullets used by some armies and in some recreational shooting munitions. Doubts have been raised about the safety (or green credentials) of tungsten. The biocidal effects of some heavy metals have been known since antiquity. Platinum, osmium, copper, ruthenium, and other heavy metals, including arsenic, are used in anti-cancer treatments, or have shown potential. Antimony (anti-protozoal), bismuth (anti-ulcer), gold (anti-arthritic), and iron (anti-malarial) are also important"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_17",
    "chunk": "in medicine. Copper, zinc, silver, gold, or mercury are used in antiseptic formulations; small amounts of some heavy metals are used to control algal growth in, for example, cooling towers. Depending on their intended use as fertilisers or biocides, agrochemicals may contain heavy metals such as chromium, cobalt, nickel, copper, zinc, arsenic, cadmium, mercury, or lead. Selected heavy metals are used as catalysts in fuel processing (rhenium, for example), synthetic rubber and fibre production (bismuth), emission control devices (palladium and platinum), and in self-cleaning ovens (where cerium(IV) oxide in the walls of such ovens helps oxidise carbon-based cooking residues). In soap chemistry, heavy metals form insoluble soaps that are used in lubricating greases, paint dryers, and fungicides (apart from lithium, the alkali metals and the ammonium ion form soluble soaps). The colours of glass, ceramic glazes, paints, pigments, and plastics are commonly produced by the inclusion of heavy metals (or their compounds) such as chromium, manganese, cobalt, copper, zinc, zirconium, molybdenum, silver, tin, praseodymium, neodymium, erbium, tungsten, iridium, gold, lead, or uranium. Tattoo inks may contain heavy metals, such as chromium, cobalt, nickel, and copper. The high reflectivity of some heavy metals is important in the construction of mirrors, including"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_18",
    "chunk": "precision astronomical instruments. Headlight reflectors rely on the excellent reflectivity of a thin film of rhodium. Heavy metals or their compounds can be found in electronic components, electrodes, and wiring and solar panels. Molybdenum powder is used in circuit board inks. Home electrical systems, for the most part, are wired with copper wire for its good conducting properties. Silver and gold are used in electrical and electronic devices, particularly in contact switches, as a result of their high electrical conductivity and capacity to resist or minimise the formation of impurities on their surfaces. Heavy metals have been used in batteries for over 200 years, at least since Volta invented his copper and silver voltaic pile in 1800. Magnets are often made of heavy metals such as manganese, iron, cobalt, nickel, niobium, bismuth, praseodymium, neodymium, gadolinium, and dysprosium. Neodymium magnets are the strongest type of permanent magnet commercially available. They are key components of, for example, car door locks, starter motors, fuel pumps, and power windows. Heavy metals are used in lighting, lasers, and light-emitting diodes (LEDs). Fluorescent lighting relies on mercury vapour for its operation. Ruby lasers generate deep red beams by exciting chromium atoms in aluminum oxide; the lanthanides"
  },
  {
    "source": "Heavy metals.txt",
    "chunk_id": "Heavy metals.txt_19",
    "chunk": "are also extensively employed in lasers. Copper, iridium, and platinum are used in organic LEDs. Because denser materials absorb more of certain types of radioactive emissions such as gamma rays than lighter ones, heavy metals are useful for radiation shielding and to focus radiation beams in linear accelerators and radiotherapy applications. Niche uses of heavy metals with high atomic numbers occur in diagnostic imaging, electron microscopy, and nuclear science. In diagnostic imaging, heavy metals such as cobalt or tungsten make up the anode materials found in x-ray tubes. In electron microscopy, heavy metals such as lead, gold, palladium, platinum, or uranium have been used in the past to make conductive coatings and to introduce electron density into biological specimens by staining, negative staining, or vacuum deposition. In nuclear science, nuclei of heavy metals such as chromium, iron, or zinc are sometimes fired at other heavy metal targets to produce superheavy elements; heavy metals are also employed as spallation targets for the production of neutrons or isotopes of non-primordial elements such as astatine (using lead, bismuth, thorium, or uranium in the latter case)."
  },
  {
    "source": "Helium flash.txt",
    "chunk_id": "Helium flash.txt_0",
    "chunk": "# Helium flash A helium flash is a very brief thermal runaway nuclear fusion of large quantities of helium into carbon through the triple-alpha process in the core of low-mass stars (between 0.8 solar masses (M☉) and 2.0 M☉) during their red giant phase. The Sun is predicted to experience a flash 1.2 billion years after it leaves the main sequence. A much rarer runaway helium fusion process can also occur on the surface of accreting white dwarf stars. Low-mass stars do not produce enough gravitational pressure to initiate normal helium fusion. As the hydrogen in the core is exhausted, some of the helium left behind is instead compacted into degenerate matter, supported against gravitational collapse by quantum mechanical pressure rather than thermal pressure. Subsequent hydrogen shell fusion further increases the mass of the core until it reaches temperature of approximately 100 million kelvin, which is hot enough to initiate helium fusion (or \"helium burning\") in the core. However, a property of degenerate matter is that increases in temperature do not produce an increase in the pressure of the matter until the thermal pressure becomes so very high that it exceeds degeneracy pressure. In main sequence stars, thermal expansion regulates"
  },
  {
    "source": "Helium flash.txt",
    "chunk_id": "Helium flash.txt_1",
    "chunk": "the core temperature, but in degenerate cores, this does not occur. Helium fusion increases the temperature, which increases the fusion rate, which further increases the temperature in a runaway reaction which quickly spans the entire core. This produces a flash of very intense helium fusion that lasts only a few minutes, but during that time, produces energy at a rate comparable to the entire Milky Way galaxy. In the case of normal low-mass stars, the vast energy release causes much of the core to come out of degeneracy, allowing it to thermally expand. This consumes most of the total energy released by the helium flash, and any left-over energy is absorbed into the star's upper layers. Thus the helium flash is mostly undetectable by observation, and is described solely by astrophysical models. After the core's expansion and cooling, the star's surface rapidly cools and contracts in as little as 10,000 years until it is roughly 2% of its former radius and luminosity. It is estimated that the electron-degenerate helium core weighs about 40% of the star mass and that 6% of the core is converted into carbon. Subflashes are pulsational instabilities that occur after the main helium flash. They are"
  },
  {
    "source": "Helium flash.txt",
    "chunk_id": "Helium flash.txt_2",
    "chunk": "driven by stars that do not have good convective or radiative boundaries. Subflashes can last several hours to days and can occur for many years with each subsequent flash generally being weaker. Subflashes can be detected by applying fourier transforms to the light curve data. During the red giant phase of stellar evolution in stars with less than 2.0 M☉, the nuclear fusion of hydrogen ceases in the core as it is depleted, leaving a helium-rich core. While fusion of hydrogen continues in the star's shell causing a continuation of the accumulation of helium in the core, making the core denser, the temperature is still unable to reach the level required for helium fusion, as happens in more massive stars. Thus the thermal pressure from fusion is no longer sufficient to counter the gravitational collapse and create the hydrostatic equilibrium found in most stars. This causes the star to start contracting and increasing in temperature until it eventually becomes compressed enough for the helium core to become degenerate matter. This degeneracy pressure is finally sufficient to stop further collapse of the most central material but the rest of the core continues to contract and the temperature continues to rise until"
  },
  {
    "source": "Helium flash.txt",
    "chunk_id": "Helium flash.txt_3",
    "chunk": "it reaches a point (≈1×10 K) at which the helium can ignite and start to fuse. The explosive nature of the helium flash arises from its taking place in degenerate matter. Once the temperature reaches 100 million–200 million kelvin and helium fusion begins using the triple-alpha process, the temperature rapidly increases, further raising the helium fusion rate and, because degenerate matter is a good conductor of heat, widening the reaction region. However, since degeneracy pressure (which is purely a function of density) is dominating thermal pressure (proportional to the product of density and temperature), the total pressure is only weakly dependent on temperature. Thus, the dramatic increase in temperature only causes a slight increase in pressure, so there is no stabilizing cooling expansion of the core. This runaway reaction quickly climbs to about 100 billion times the star's normal energy production (for a few seconds) until the temperature increases to the point that thermal pressure again becomes dominant, eliminating the degeneracy. The core can then expand and cool down and a stable burning of helium will continue. A star with mass greater than about 2.25 M☉ starts to burn helium without its core becoming degenerate, and so does not exhibit"
  },
  {
    "source": "Helium flash.txt",
    "chunk_id": "Helium flash.txt_4",
    "chunk": "this type of helium flash. In a very low-mass star (less than about 0.5 M☉), the core is never hot enough to ignite helium. The degenerate helium core will keep on contracting, and finally becomes a helium white dwarf. The helium flash is not directly observable on the surface by electromagnetic radiation. The flash occurs in the core deep inside the star, and the net effect will be that all released energy is absorbed by the entire core, causing the degenerate state to become nondegenerate. Earlier computations indicated that a nondisruptive mass loss would be possible in some cases, but later star modeling taking neutrino energy loss into account indicates no such mass loss. In a one solar mass star, the helium flash is estimated to release about 5×10 J, or about 0.3% of the energy release of a 1.5×10 J type Ia supernova, which is triggered by an analogous ignition of carbon fusion in a carbon–oxygen white dwarf. When hydrogen gas is accreted onto a white dwarf from a binary companion star, the hydrogen can fuse to form helium for a narrow range of accretion rates, but most systems develop a layer of hydrogen over the degenerate white dwarf"
  },
  {
    "source": "Helium flash.txt",
    "chunk_id": "Helium flash.txt_5",
    "chunk": "interior. This hydrogen can build up to form a shell near the surface of the star. When the mass of hydrogen becomes sufficiently large, runaway fusion causes a nova. In a few binary systems where the hydrogen fuses on the surface, the mass of helium built up can burn in an unstable helium flash. In certain binary systems the companion star may have lost most of its hydrogen and donate helium-rich material to the compact star. Note that similar flashes occur on neutron stars. Helium shell flashes are a somewhat analogous but much less violent, nonrunaway helium ignition event, taking place in the absence of degenerate matter. They occur periodically in asymptotic giant branch stars in a shell outside the core. This is late in the life of a star in its giant phase. The star has burnt most of the helium available in the core, which is now composed of carbon and oxygen. Helium fusion continues in a thin shell around this core, but then turns off as helium becomes depleted. This allows hydrogen fusion to start in a layer above the helium layer. After enough additional helium accumulates, helium fusion is reignited, leading to a thermal pulse which"
  },
  {
    "source": "Helium flash.txt",
    "chunk_id": "Helium flash.txt_6",
    "chunk": "eventually causes the star to expand and brighten temporarily (the pulse in luminosity is delayed because it takes a number of years for the energy from restarted helium fusion to reach the surface). Such pulses may last a few hundred years, and are thought to occur periodically every 10,000 to 100,000 years. After the flash, helium fusion continues at an exponentially decaying rate for about 40% of the cycle as the helium shell is consumed. Thermal pulses may cause a star to shed circumstellar shells of gas and dust."
  },
  {
    "source": "Henry Draper Catalogue.txt",
    "chunk_id": "Henry Draper Catalogue.txt_0",
    "chunk": "# Henry Draper Catalogue The Henry Draper Catalogue (HD) is an astronomical star catalogue published between 1918 and 1924, giving spectroscopic classifications for 225,300 stars; it was later expanded by the Henry Draper Extension (HDE), published between 1925 and 1936, which gave classifications for 46,850 more stars, and by the Henry Draper Extension Charts (HDEC), published from 1937 to 1949 in the form of charts, which gave classifications for 86,933 more stars. In all, 359,083 stars were classified as of August 2017. The HD catalogue is named after Henry Draper, an amateur astronomer, and covers the entire sky almost completely down to an apparent photographic magnitude of about 9; the extensions added fainter stars in certain areas of the sky. The construction of the Henry Draper Catalogue was part of a pioneering effort to classify stellar spectra, and its catalogue numbers are commonly used as a way of identifying stars. The origin of the Henry Draper Catalogue dates back to the earliest photographic studies of stellar spectra. Henry Draper made the first photograph of a star's spectrum showing distinct spectral lines when he photographed Vega in 1872. He took over a hundred more photographs of stellar spectra before his death"
  },
  {
    "source": "Henry Draper Catalogue.txt",
    "chunk_id": "Henry Draper Catalogue.txt_1",
    "chunk": "in 1882. In 1885, Edward Pickering began to supervise photographic spectroscopy at Harvard College Observatory, using the objective prism method. In 1886, Draper's widow, Mary Anna Palmer Draper, became interested in Pickering's research and agreed to fund it under the name Henry Draper Memorial. Pickering and his coworkers then began to take an objective-prism survey of the sky and to classify the resulting spectra. A first result of this work was the Draper Catalogue of Stellar Spectra, published in 1890. This catalogue contained spectroscopic classifications for 10,351 stars, mostly north of declination −25°. Most of the classification was done by Williamina Fleming. The classification scheme used was to subdivide the previously used Secchi classes (I to IV) into more specific classes, given letters from A to N. Also, the letter O was used for stars whose spectra consisted mainly of bright lines, the letter P for planetary nebulae, and the letter Q for spectra not fitting into any of the classes A through P. No star of type N appeared in the catalogue, and the only star of type O was the Wolf–Rayet star HR 2583. Antonia Maury and Pickering published a more detailed study of the spectra of bright"
  },
  {
    "source": "Henry Draper Catalogue.txt",
    "chunk_id": "Henry Draper Catalogue.txt_2",
    "chunk": "stars in the northern hemisphere in 1897. Maury used classifications numbered from I to XXII; groups I to XX corresponded to subdivisions of the Draper Catalogue types B, A, F, G, K, and M, while XXI and XXII corresponded to the Draper Catalogue types N and O. She was the first to place B stars in their current position, prior to A stars, in the spectral classification. In 1890, the Harvard College Observatory constructed an observation station in Arequipa, Peru in order to study the sky in the Southern Hemisphere, and a study of bright stars in the southern hemisphere was published by Annie Jump Cannon and Pickering in 1901. Cannon used the lettered types of the Draper Catalogue of Stellar Spectra, but dropped all letters except O, B, A, F, G, K, and M, used in that order, as well as P for planetary nebulae and Q for some peculiar spectra. She also used types such as B5A for stars halfway between types B and A, F2G for stars one-fifth of the way from F to G, and so forth. Between 1910 and 1915, new discoveries increased interest in stellar classification, and work on the Henry Draper Catalogue itself"
  },
  {
    "source": "Henry Draper Catalogue.txt",
    "chunk_id": "Henry Draper Catalogue.txt_3",
    "chunk": "started in 1911. From 1912 to 1915, Cannon and her coworkers classified spectra at the rate of approximately 5,000 per month. The catalogue was published in 9 volumes of the Annals of Harvard College Observatory between 1918 and 1924. It contains rough positions, magnitudes, spectral classifications, and, where possible, cross-references to the Durchmusterung catalogs for 225,300 stars. The classification scheme used was similar to that used in Cannon's 1901 work, except that types such as B, A, B5A, F2G, and so on, had been changed to B0, A0, B5, F2, and so on. As well as the classes O through M, P was used for nebulae and R and N for carbon stars. Pickering died on February 3, 1919, leaving 6 volumes to be overseen by Cannon. Cannon found spectral classifications for 46,850 fainter stars in selected regions of the sky in the Henry Draper Extension, published in six parts between 1925 and 1936. She continued classifying stars until her death in 1941. Most of these classifications were published in 1949 in the Henry Draper Extension Charts (the first portion of these charts was published in 1937.) These charts also contained some classifications by Margaret Walton Mayall, who supervised the"
  },
  {
    "source": "Henry Draper Catalogue.txt",
    "chunk_id": "Henry Draper Catalogue.txt_4",
    "chunk": "work after Cannon's death. The catalogue and its extensions were the first large-scale attempt to catalogue spectral types of stars, and its construction led to the Harvard classification scheme of stellar spectra which is still used today. Stars contained in the main portion of the catalogue are of medium magnitude, down to about 9 (about ⁠1/15⁠ as bright as the faintest stars visible with the naked eye). The extensions contain stars as faint as the 11th magnitude selected from certain regions of the sky. Stars in the original catalogue are numbered from 1 to 225300 (prefix HD) and are numbered in order of increasing right ascension for the epoch 1900.0. Stars in the first extension are numbered from 225301 to 272150 (prefix HDE), and stars from the extension charts are numbered from 272151 to 359083 (prefix HDEC). However, as the numbering is continuous throughout the catalog and its extensions, the prefix HD may be used regardless as its use produces no ambiguity. Many stars are customarily identified by their HD numbers. The Henry Draper Catalogue and the Extension were available from the NASA Astronomical Data Center as part of their third CD-ROM of astronomical catalogues. Currently, the Catalogue and Extension"
  },
  {
    "source": "Henry Draper Catalogue.txt",
    "chunk_id": "Henry Draper Catalogue.txt_5",
    "chunk": "are available from the VizieR service of the Centre de Données astronomiques (French for \"Astronomical Data Center\") at Strasbourg as catalogue number III/135A. Because of their format, putting the Henry Draper Extension Charts into a machine-readable format was more difficult, but this task was eventually completed by 1995 by Nesterov, Röser and their coworkers, and the charts are now available at VizieR as catalogue number III/182. The Henry Draper Catalogue and its extensions are available on line free of charge at the VizieR service of the Centre de Données astronomiques de Strasbourg:"
  },
  {
    "source": "Henry Norris Russell.txt",
    "chunk_id": "Henry Norris Russell.txt_0",
    "chunk": "# Henry Norris Russell Henry Norris Russell ForMemRS HFRSE FRAS (October 25, 1877 – February 18, 1957) was an American astronomer who, along with Ejnar Hertzsprung, developed the Hertzsprung–Russell diagram (1910). In 1923, working with Frederick Saunders, he developed Russell–Saunders coupling, which is also known as LS coupling. Russell was born on 25 October 1877, at Oyster Bay, New York, the son of Rev Alexander Gatherer Russell (1845-1911) and his wife, Eliza Hoxie Norris. After graduating from Princeton Preparatory School in 1893, he studied astronomy at Princeton University, obtaining his B.A. in 1897 and his doctorate in 1900, studying under Charles Augustus Young. From 1903 to 1905, he worked at the Cambridge Observatory with Arthur Robert Hinks as a research assistant of the Carnegie Institution and came under the strong influence of George Darwin. He returned to Princeton to become an instructor in astronomy (1905–1908), assistant professor (1908–1911), professor (1911–1927) and research professor (1927–1947). He was also the director of the Princeton University Observatory from 1912 to 1947 where Charlotte Moore Sitterly helped him measure and calculate the properties of stars. He died in Princeton, New Jersey on 18 February 1957 at the age of 79. He is buried in"
  },
  {
    "source": "Henry Norris Russell.txt",
    "chunk_id": "Henry Norris Russell.txt_1",
    "chunk": "Princeton Cemetery. In November 1908 Russell married Lucy May Cole (1881-1968). They had four children. Their youngest daughter, Margaret Russell (1914-1999), married the astronomer Frank K. Edmondson in the 1930s. Russell co-wrote an influential two-volume textbook in 1927 with Raymond Smith Dugan and John Quincy Stewart: Astronomy: A Revision of Young’s Manual of Astronomy (Ginn & Co., Boston, 1926–27, 1938, 1945). This became the standard astronomy textbook for about two decades. There were two volumes: the first was The Solar System and the second was Astrophysics and Stellar Astronomy. The textbook popularized the idea that a star's properties (radius, surface temperature, luminosity, etc.) were largely determined by the star's mass and chemical composition, which became known as the Vogt–Russell theorem (including Heinrich Vogt who independently discovered the result). Since a star's chemical composition gradually changes with age (usually in a non-homogeneous fashion), stellar evolution results. Russell dissuaded Cecilia Payne-Gaposchkin from concluding that the composition of the Sun is different from that of the Earth in her thesis, as it contradicted the accepted wisdom at the time. He realized she was correct four years later after deriving the same result by different means. In his paper Russell credited Payne with discovering"
  },
  {
    "source": "Henry Norris Russell.txt",
    "chunk_id": "Henry Norris Russell.txt_2",
    "chunk": "that the Sun had a different chemical composition from Earth but never shared the rewards of the fame he readily accepted for her work which he’d failed to recognize until years later."
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_0",
    "chunk": "# History of astronomy The history of astronomy focuses on the contributions civilizations have made to further their understanding of the universe beyond earth's atmosphere. Astronomy is one of the oldest natural sciences, achieving a high level of success in the second half of the first millennium. Astronomy has origins in the religious, mythological, cosmological, calendrical, and astrological beliefs and practices of prehistory. Early astronomical records date back to the Babylonians around 1000 BCE. There is also astronomical evidence of interest from early Chinese, Central American and North European cultures. Astronomy was used by early cultures for a variety of reasons. These include timekeeping, navigation, spiritual and religious practices, and agricultural planning. Ancient astronomers used their observations to chart the skies in an effort to learn about the workings of the universe. During the Renaissance Period, revolutionary ideas emerged about astronomy. One such idea was contributed in 1593 by Polish astronomer Nicolaus Copernicus, who developed a heliocentric model that depicted the planets orbiting the sun. This was the start of the Copernican Revolution. The success of astronomy, compared to other sciences, was achieved because of several reasons. Astronomy was the first science to have a mathematical foundation and have sophisticated"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_1",
    "chunk": "procedures such as using armillary spheres and quadrants. This provided a solid base for collecting and verifying data. Throughout the years, astronomy has broadened into multiple subfields such as astrophysics, observational astronomy, theoretical astronomy, and astrobiology. Early cultures identified celestial objects with gods and spirits. They related these objects (and their movements) to phenomena such as rain, drought, seasons, and tides. It is generally believed that the first astronomers were priests who understood celestial objects and events to be manifestations of the divine, hence the connection to what is now called astrology. A 32,500-year-old carved ivory mammoth tusk could contain the oldest known star chart (resembling the constellation Orion). It has also been suggested that drawings on the wall of the Lascaux caves in France dating from 33,000 to 10,000 years ago could be a graphical representation of the Pleiades, the Summer Triangle, and the Northern Crown. Ancient structures with possibly astronomical alignments (such as Stonehenge) probably fulfilled astronomical, religious, and social functions. Calendars of the world have often been set by observations of the Sun and Moon (marking the day, month, and year) and were important to agricultural societies, in which the harvest depended on planting at the correct"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_2",
    "chunk": "time of year. The nearly full moon was also the only lighting for night-time travel into city markets. The common modern calendar is based on the Roman calendar. Although originally a lunar calendar, it broke the traditional link of the month to the phases of the Moon and divided the year into twelve almost-equal months, that mostly alternated between thirty and thirty-one days. Julius Caesar instigated calendar reform in 46 BC and introduced what is now called the Julian calendar, based upon the 365+1⁄4 day year length originally proposed by the 4th century BC Greek astronomer Callippus. Ancient astronomical artifacts have been found throughout Europe. The artifacts demonstrate that Neolithic and Bronze Age Europeans had a sophisticated knowledge of mathematics and astronomy. The origins of astronomy can be found in Mesopotamia, the \"land between the rivers\" Tigris and Euphrates, where the ancient kingdoms of Sumer, Assyria, and Babylonia were located. A form of writing known as cuneiform emerged among the Sumerians around 3500–3000 BC. Our knowledge of Sumerian astronomy is indirect, via the earliest Babylonian star catalogues dating from about 1200 BC. The fact that many star names appear in Sumerian suggests a continuity reaching into the Early Bronze Age."
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_3",
    "chunk": "Astral theology, which gave planetary gods an important role in Mesopotamian mythology and religion, began with the Sumerians. They also used a sexagesimal (base 60) place-value number system, which simplified the task of recording very large and very small numbers. The modern practice of dividing a circle into 360 degrees, or an hour into 60 minutes, began with the Sumerians. For more information, see the articles on Babylonian numerals and mathematics. Mesopotamia is worldwide the place of the earliest known astronomer and poet by name: Enheduanna, Akkadian high priestess to the lunar deity Nanna/Sin and princess, daughter of Sargon the Great (c. 2334 – c. 2279 BCE). She had the Moon tracked in her chambers and wrote poems about her divine Moon. Classical sources frequently use the term Chaldeans for the astronomers of Mesopotamia, who were originally a people, before being identified with priest-scribes specializing in astrology and other forms of divination. The first evidence of recognition that astronomical phenomena are periodic and of the application of mathematics to their prediction is Babylonian. Tablets dating back to the Old Babylonian period document the application of mathematics to the variation in the length of daylight over a solar year. Centuries of"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_4",
    "chunk": "Babylonian observations of celestial phenomena are recorded in the series of cuneiform tablets known as the Enūma Anu Enlil. The oldest significant astronomical text that we possess is Tablet 63 of the Enūma Anu Enlil, the Venus tablet of Ammi-saduqa, which lists the first and last visible risings of Venus over a period of about 21 years and is the earliest evidence that the phenomena of a planet were recognized as periodic. The MUL.APIN contains catalogues of stars and constellations as well as schemes for predicting heliacal risings and the settings of the planets, lengths of daylight measured by a water clock, gnomon, shadows, and intercalations. The Babylonian GU text arranges stars in 'strings' that lie along declination circles and thus measure right-ascensions or time-intervals, and also employs the stars of the zenith, which are also separated by given right-ascensional differences. A significant increase in the quality and frequency of Babylonian observations appeared during the reign of Nabonassar (747–733 BC). The systematic records of ominous phenomena in Babylonian astronomical diaries that began at this time allowed for the discovery of a repeating 18-year cycle of lunar eclipses, for example. The Greek astronomer Ptolemy later used Nabonassar's reign to fix the"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_5",
    "chunk": "beginning of an era, since he felt that the earliest usable observations began at this time. The last stages in the development of Babylonian astronomy took place during the time of the Seleucid Empire (323–60 BC). In the 3rd century BC, astronomers began to use \"goal-year texts\" to predict the motions of the planets. These texts compiled records of past observations to find repeating occurrences of ominous phenomena for each planet. About the same time, or shortly afterwards, astronomers created mathematical models that allowed them to predict these phenomena directly, without consulting records. A notable Babylonian astronomer from this time was Seleucus of Seleucia, who was a supporter of the heliocentric model. Babylonian astronomy was the basis for much of what was done in Greek and Hellenistic astronomy, in classical Indian astronomy, in Sassanian Iran, in Byzantium, in Syria, in Islamic astronomy, in Central Asia, and in Western Europe. Astronomy in the Indian subcontinent dates back to the period of Indus Valley Civilisation during 3rd millennium BC, when it was used to create calendars. As the Indus Valley civilization did not leave behind written documents, the oldest extant Indian astronomical text is the Vedanga Jyotisha, dating from the Vedic period."
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_6",
    "chunk": "The Vedanga Jyotisha is attributed to Lagadha and has an internal date of approximately 1350 BC, and describes rules for tracking the motions of the Sun and the Moon for the purposes of ritual. It is available in two recensions, one belonging to the Rig Veda, and the other to the Yajur Veda. According to the Vedanga Jyotisha, in a yuga or \"era\", there are 5 solar years, 67 lunar sidereal cycles, 1,830 days, 1,835 sidereal days, and 62 synodic months. During the sixth century, astronomy was influenced by the Greek and Byzantine astronomical traditions. Aryabhata (476–550), in his magnum opus Aryabhatiya (499), propounded a computational system based on a planetary model in which the Earth was taken to be spinning on its axis and the periods of the planets were given with respect to the Sun. He accurately calculated many astronomical constants, such as the periods of the planets, times of the solar and lunar eclipses, and the instantaneous motion of the Moon. Early followers of Aryabhata's model included Varāhamihira, Brahmagupta, and Bhāskara II. Astronomy was advanced during the Shunga Empire, and many star catalogues were produced during this time. The Shunga period is known as the \"Golden age"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_7",
    "chunk": "of astronomy in India\". It saw the development of calculations for the motions and places of various planets, their rising and setting, conjunctions, and the calculation of eclipses. By the sixth century, Indian astronomers believed that comets were celestial bodies that re-appeared periodically. This was the view expressed in the sixth century by the astronomers Varahamihira and Bhadrabahu. The tenth-century astronomer Bhattotpala listed the names and estimated periods of certain comets, but it is not known how these figures were calculated or how accurate they were. The Ancient Greeks developed astronomy, which they treated as a branch of mathematics, to a highly sophisticated level. The first geometrical, three-dimensional models to explain the apparent motion of the planets were developed in the 4th century BC by Eudoxus of Cnidus and Callippus of Cyzicus. Their models were based on nested homocentric spheres centered upon the Earth. Their younger contemporary Heraclides Ponticus proposed that the Earth rotates around its axis. A different approach to celestial phenomena was taken by natural philosophers such as Plato and Aristotle. They were less concerned with developing mathematical predictive models than with developing an explanation of the reasons for the motions of the Cosmos. In his Timaeus, Plato"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_8",
    "chunk": "described the universe as a spherical body divided into circles carrying the planets and governed according to harmonic intervals by a world soul. Aristotle, drawing on the mathematical model of Eudoxus, proposed that the universe was made of a complex system of concentric spheres, whose circular motions combined to carry the planets around the Earth. This basic cosmological model prevailed, in various forms, until the 16th century. In the 3rd century BC Aristarchus of Samos was the first to suggest a heliocentric system, although only fragmentary descriptions of his idea survive. Eratosthenes estimated the circumference of the Earth with great accuracy (see also: history of geodesy). Greek geometrical astronomy developed away from the model of concentric spheres to employ more complex models in which an eccentric circle would carry around a smaller circle, called an epicycle which in turn carried around a planet. The first such model is attributed to Apollonius of Perga and further developments in it were carried out in the 2nd century BC by Hipparchus of Nicea. Hipparchus made a number of other contributions, including the first measurement of precession and the compilation of the first star catalog in which he proposed our modern system of apparent"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_9",
    "chunk": "magnitudes. The Antikythera mechanism, an ancient Greek astronomical observational device for calculating the movements of the Sun and the Moon, possibly the planets, dates from about 150–100 BC, and was the first ancestor of an astronomical computer. It was discovered in an ancient shipwreck off the Greek island of Antikythera, between Kythera and Crete. The device became famous for its use of a differential gear, previously believed to have been invented in the 16th century, and the miniaturization and complexity of its parts, comparable to a clock made in the 18th century. The original mechanism is displayed in the Bronze collection of the National Archaeological Museum of Athens, accompanied by a replica. Depending on the historian's viewpoint, the acme or corruption of Classical physical astronomy is seen with Ptolemy, a Greco-Roman astronomer from Alexandria of Egypt, who wrote the classic comprehensive presentation of geocentric astronomy, the Megale Syntaxis (Great Synthesis), better known by its Arabic title Almagest, which had a lasting effect on astronomy up to the Renaissance. In his Planetary Hypotheses, Ptolemy ventured into the realm of cosmology, developing a physical model of his geometric system, in a universe many times smaller than the more realistic conception of Aristarchus"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_10",
    "chunk": "of Samos four centuries earlier. The precise orientation of the Egyptian pyramids affords a lasting demonstration of the high degree of technical skill in watching the heavens attained in the 3rd millennium BC. It has been shown the Pyramids were aligned towards the pole star, which, because of the precession of the equinoxes, was at that time Thuban, a faint star in the constellation of Draco. Evaluation of the site of the temple of Amun-Re at Karnak, taking into account the change over time of the obliquity of the ecliptic, has shown that the Great Temple was aligned on the rising of the midwinter Sun. The length of the corridor down which sunlight would travel would have limited illumination at other times of the year. The Egyptians also found the position of Sirius (the dog star), who they believed was Anubis, their jackal-headed god, moving through the heavens. Its position was critical to their civilisation as when it rose heliacal in the east before sunrise it foretold the flooding of the Nile. It is also the origin of the phrase \"dog days of summer\". Astronomy played a considerable part in religious matters for fixing the dates of festivals and determining"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_11",
    "chunk": "the hours of the night. The titles of several temple books are preserved recording the movements and phases of the Sun, Moon, and stars. The rising of Sirius (Egyptian: Sopdet, Greek: Sothis) at the beginning of the inundation was a particularly important point to fix in the yearly calendar. Writing in the Roman era, Clement of Alexandria gives some idea of the importance of astronomical observations to the sacred rites: And after the Singer advances the Astrologer (ὡροσκόπος), with a horologium (ὡρολόγιον) in his hand, and a palm (φοίνιξ), the symbols of astrology. He must know by heart the Hermetic astrological books, which are four in number. Of these, one is about the arrangement of the fixed stars that are visible; one on the positions of the Sun and Moon and five planets; one on the conjunctions and phases of the Sun and Moon; and one concerns their risings. The Astrologer's instruments (horologium and palm) are a plumb line and sighting instrument. They have been identified with two inscribed objects in the Berlin Museum; a short handle from which a plumb line was hung, and a palm branch with a sight-slit in the broader end. The latter was held close"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_12",
    "chunk": "to the eye, the former in the other hand, perhaps at arm's length. The \"Hermetic\" books which Clement refers to are the Egyptian theological texts, which probably have nothing to do with Hellenistic Hermetism. From the tables of stars on the ceiling of the tombs of Rameses VI and Rameses IX it seems that for fixing the hours of the night a man seated on the ground faced the Astrologer in such a position that the line of observation of the pole star passed over the middle of his head. On the different days of the year each hour was determined by a fixed star culminating or nearly culminating in it, and the position of these stars at the time is given in the tables as in the centre, on the left eye, on the right shoulder, etc. According to the texts, in founding or rebuilding temples the north axis was determined by the same apparatus, and we may conclude that it was the usual one for astronomical observations. In careful hands it might give results of a high degree of accuracy. The astronomy of East Asia began in China. Solar term was completed in Warring States period. The knowledge"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_13",
    "chunk": "of Chinese astronomy was introduced into East Asia. Astronomy in China has a long history. Detailed records of astronomical observations were kept from about the 6th century BC, until the introduction of Western astronomy and the telescope in the 17th century. Chinese astronomers were able to precisely predict eclipses. Much of early Chinese astronomy was for the purpose of timekeeping. The Chinese used a lunisolar calendar, but because the cycles of the Sun and the Moon are different, astronomers often prepared new calendars and made observations for that purpose. Astrological divination was also an important part of astronomy. Astronomers took careful note of \"guest stars\" (Chinese: 客星; pinyin: kèxīng; lit. 'guest star') which suddenly appeared among the fixed stars. They were the first to record a supernova, in the Astrological Annals of the Houhanshu in 185 AD. Also, the supernova that created the Crab Nebula in 1054 is an example of a \"guest star\" observed by Chinese astronomers, although it was not recorded by their European contemporaries. Ancient astronomical records of phenomena like supernovae and comets are sometimes used in modern astronomical studies. The world's first star catalogue was made by Gan De, a Chinese astronomer, in the 4th century"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_14",
    "chunk": "BC. Maya astronomical codices include detailed tables for calculating phases of the Moon, the recurrence of eclipses, and the appearance and disappearance of Venus as morning and evening star. The Maya based their calendrics in the carefully calculated cycles of the Pleiades, the Sun, the Moon, Venus, Jupiter, Saturn, Mars, and also they had a precise description of the eclipses as depicted in the Dresden Codex, as well as the ecliptic or zodiac, and the Milky Way was crucial in their Cosmology. A number of important Maya structures are believed to have been oriented toward the extreme risings and settings of Venus. To the ancient Maya, Venus was the patron of war and many recorded battles are believed to have been timed to the motions of this planet. Mars is also mentioned in preserved astronomical codices and early mythology. Although the Maya calendar was not tied to the Sun, John Teeple has proposed that the Maya calculated the solar year to somewhat greater accuracy than the Gregorian calendar. Both astronomy and an intricate numerological scheme for the measurement of time were vitally important components of Maya religion. The Maya believed that the Earth was the center of all things, and"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_15",
    "chunk": "that the stars, moons, and planets were gods. They believed that their movements were the gods traveling between the Earth and other celestial destinations. Many key events in Maya culture were timed around celestial events, in the belief that certain gods would be present. The Arabic and the Persian world under Islam had become highly cultured, and many important works of knowledge from Greek astronomy, Indian astronomy, and Persian astronomy were translated into Arabic, which were then used and stored in libraries throughout the area. An important contribution by Islamic astronomers was their emphasis on observational astronomy. This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century. Zij star catalogues were produced at these observatories. In the ninth century, Persian astrologer Albumasar was thought to be one of the greatest astrologer at that time. His practical manuals for training astrologers profoundly influenced Muslim intellectual history and, through translations, that of western Europe and Byzantium In the 10th century, Albumasar's \"Introduction\" was one of the most important sources for the recovery of Aristotle for medieval European scholars. Abd al-Rahman al-Sufi (Azophi) carried out observations on the stars and described their positions, magnitudes,"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_16",
    "chunk": "brightness, and colour and drawings for each constellation in his Book of Fixed Stars. He also gave the first descriptions and pictures of \"A Little Cloud\" now known as the Andromeda Galaxy. He mentions it as lying before the mouth of a Big Fish, an Arabic constellation. This \"cloud\" was apparently commonly known to the Isfahan astronomers, very probably before 905 AD. The first recorded mention of the Large Magellanic Cloud was also given by al-Sufi. In 1006, Ali ibn Ridwan observed SN 1006, the brightest supernova in recorded history, and left a detailed description of the temporary star. In the late tenth century, a huge observatory was built near Tehran, Iran, by the astronomer Abu-Mahmud al-Khujandi who observed a series of meridian transits of the Sun, which allowed him to calculate the tilt of the Earth's axis relative to the Sun. He noted that measurements by earlier (Indian, then Greek) astronomers had found higher values for this angle, possible evidence that the axial tilt is not constant but was in fact decreasing. In 11th-century Persia, Omar Khayyám compiled many tables and performed a reformation of the calendar that was more accurate than the Julian and came close to the"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_17",
    "chunk": "Gregorian. Other Muslim advances in astronomy included the collection and correction of previous astronomical data, resolving significant problems in the Ptolemaic model, the development of the universal latitude-independent astrolabe by Arzachel, the invention of numerous other astronomical instruments, Ja'far Muhammad ibn Mūsā ibn Shākir's belief that the heavenly bodies and celestial spheres were subject to the same physical laws as Earth, and the introduction of empirical testing by Ibn al-Shatir, who produced the first model of lunar motion which matched physical observations. Natural philosophy (particularly Aristotelian physics) was separated from astronomy by Ibn al-Haytham (Alhazen) in the 11th century, by Ibn al-Shatir in the 14th century, and Qushji in the 15th century. Bhāskara II (1114–1185) was the head of the astronomical observatory at Ujjain, continuing the mathematical tradition of Brahmagupta. He wrote the Siddhantasiromani which consists of two parts: Goladhyaya (sphere) and Grahaganita (mathematics of the planets). He also calculated the time taken for the Sun to orbit the Earth to nine decimal places. The Buddhist University of Nalanda at the time offered formal courses in astronomical studies. Other important astronomers from India include Madhava of Sangamagrama, Nilakantha Somayaji and Jyeshtadeva, who were members of the Kerala school of astronomy"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_18",
    "chunk": "and mathematics from the 14th century to the 16th century. Nilakantha Somayaji, in his Aryabhatiyabhasya, a commentary on Aryabhata's Aryabhatiya, developed his own computational system for a partially heliocentric planetary model, in which Mercury, Venus, Mars, Jupiter and Saturn orbit the Sun, which in turn orbits the Earth, similar to the Tychonic system later proposed by Tycho Brahe in the late 16th century. Nilakantha's system, however, was mathematically more efficient than the Tychonic system, due to correctly taking into account the equation of the centre and latitudinal motion of Mercury and Venus. Most astronomers of the Kerala school of astronomy and mathematics who followed him accepted his planetary model. After the significant contributions of Greek scholars to the development of astronomy, it entered a relatively static era in Western Europe from the Roman era through the 12th century. This lack of progress has led some astronomers to assert that nothing happened in Western European astronomy during the Middle Ages. Recent investigations, however, have revealed a more complex picture of the study and teaching of astronomy in the period from the 4th to the 16th centuries. Western Europe entered the Middle Ages with great difficulties that affected the continent's intellectual production."
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_19",
    "chunk": "The advanced astronomical treatises of classical antiquity were written in Greek, and with the decline of knowledge of that language, only simplified summaries and practical texts were available for study. The most influential writers to pass on this ancient tradition in Latin were Macrobius, Pliny, Martianus Capella, and Calcidius. In the 6th century Bishop Gregory of Tours noted that he had learned his astronomy from reading Martianus Capella, and went on to employ this rudimentary astronomy to describe a method by which monks could determine the time of prayer at night by watching the stars. In the 7th century the English monk Bede of Jarrow published an influential text, On the Reckoning of Time, providing churchmen with the practical astronomical knowledge needed to compute the proper date of Easter using a procedure called the computus. This text remained an important element of the education of clergy from the 7th century until well after the rise of the Universities in the 12th century. The range of surviving ancient Roman writings on astronomy and the teachings of Bede and his followers began to be studied in earnest during the revival of learning sponsored by the emperor Charlemagne. By the 9th century rudimentary"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_20",
    "chunk": "techniques for calculating the position of the planets were circulating in Western Europe; medieval scholars recognized their flaws, but texts describing these techniques continued to be copied, reflecting an interest in the motions of the planets and in their astrological significance. Building on this astronomical background, in the 10th century European scholars such as Gerbert of Aurillac began to travel to Spain and Sicily to seek out learning which they had heard existed in the Arabic-speaking world. There they first encountered various practical astronomical techniques concerning the calendar and timekeeping, most notably those dealing with the astrolabe. Soon scholars such as Hermann of Reichenau were writing texts in Latin on the uses and construction of the astrolabe and others, such as Walcher of Malvern, were using the astrolabe to observe the time of eclipses in order to test the validity of computistical tables. By the 12th century, scholars were traveling to Spain and Sicily to seek out more advanced astronomical and astrological texts, which they translated into Latin from Arabic and Greek to further enrich the astronomical knowledge of Western Europe. The arrival of these new texts coincided with the rise of the universities in medieval Europe, in which they"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_21",
    "chunk": "soon found a home. Reflecting the introduction of astronomy into the universities, John of Sacrobosco wrote a series of influential introductory astronomy textbooks: the Sphere, a Computus, a text on the Quadrant, and another on Calculation. In the 14th century, Nicole Oresme, later bishop of Liseux, showed that neither the scriptural texts nor the physical arguments advanced against the movement of the Earth were demonstrative and adduced the argument of simplicity for the theory that the Earth moves, and not the heavens. However, he concluded \"everyone maintains, and I think myself, that the heavens do move and not the earth: For God hath established the world which shall not be moved.\" In the 15th century, Cardinal Nicholas of Cusa suggested in some of his scientific writings that the Earth revolved around the Sun, and that each star is itself a distant sun. During the renaissance period, astronomy began to undergo a revolution in thought known as the Copernican Revolution, which gets the name from the astronomer Nicolaus Copernicus, who proposed a heliocentric system, in which the planets revolved around the Sun and not the Earth. His De revolutionibus orbium coelestium was published in 1543. While in the long term this"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_22",
    "chunk": "was a very controversial claim, in the very beginning it only brought minor controversy. The theory became the dominant view because many figures, most notably Galileo Galilei, Johannes Kepler and Isaac Newton championed and improved upon the work. Other figures also aided this new model despite not believing the overall theory, like Tycho Brahe, with his well-known observations. Brahe, a Danish noble, was an essential astronomer in this period. He came on the astronomical scene with the publication of De nova stella, in which he disproved conventional wisdom on the supernova SN 1572 (As bright as Venus at its peak, SN 1572 later became invisible to the naked eye, disproving the Aristotelian doctrine of the immutability of the heavens.) He also created the Tychonic system, where the Sun and Moon and the stars revolve around the Earth, but the other five planets revolve around the Sun. This system blended the mathematical benefits of the Copernican system with the \"physical benefits\" of the Ptolemaic system. This was one of the systems people believed in when they did not accept heliocentrism, but could no longer accept the Ptolemaic system. He is most known for his highly accurate observations of the stars and"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_23",
    "chunk": "the planets. Later he moved to Prague and continued his work. In Prague he was at work on the Rudolphine Tables, that were not finished until after his death. The Rudolphine Tables was a star map designed to be more accurate than either the Alfonsine tables, made in the 1300s, and the Prutenic Tables, which were inaccurate. He was assisted at this time by his assistant Johannes Kepler, who would later use his observations to finish Brahe's works and for his theories as well. After the death of Brahe, Kepler was deemed his successor and was given the job of completing Brahe's uncompleted works, like the Rudolphine Tables. He completed the Rudolphine Tables in 1624, although it was not published for several years. Like many other figures of this era, he was subject to religious and political troubles, like the Thirty Years' War, which led to chaos that almost destroyed some of his works. Kepler was, however, the first to attempt to derive mathematical predictions of celestial motions from assumed physical causes. He discovered the three Kepler's laws of planetary motion that now carry his name, those laws being as follows: With these laws, he managed to improve upon the"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_24",
    "chunk": "existing heliocentric model. The first two were published in 1609. Kepler's contributions improved upon the overall system, giving it more credibility because it adequately explained events and could cause more reliable predictions. Before this, the Copernican model was just as unreliable as the Ptolemaic model. This improvement came because Kepler realized the orbits were not perfect circles, but ellipses. The invention of the telescope in 1608 revolutionized the study of astronomy. Galileo Galilei was among the first to use a telescope to observe the sky, after constructing a 20x refractor telescope. He discovered the four largest moons of Jupiter in 1610, which are now collectively known as the Galilean moons, in his honor. This discovery was the first known observation of satellites orbiting another planet. He also found that the Moon had craters and observed, and correctly explained sunspots, and that Venus exhibited a full set of phases resembling lunar phases. Galileo argued that these facts demonstrated incompatibility with the Ptolemaic model, which could not explain the phenomenon and would even contradict it. With Jupiter's moons, he demonstrated that the Earth does not have to have everything orbiting it and that other bodies could orbit another planet, such as the"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_25",
    "chunk": "Earth orbiting the Sun. In the Ptolemaic system the celestial bodies were supposed to be perfect so such objects should not have craters or sunspots. The phases of Venus could only happen in the event that Venus orbits around the Sun, which did not happen in the Ptolemaic system. He, as the most famous example, had to face challenges from church officials, more specifically the Roman Inquisition. They accused him of heresy because these beliefs went against the teachings of the Roman Catholic Church and were challenging the Catholic church's authority when it was at its weakest. While he was able to avoid punishment for a little while he was eventually tried and pled guilty to heresy in 1633. Although this came at some expense, his book was banned, and he was put under house arrest until he died in 1642. Sir Isaac Newton developed further ties between physics and astronomy through his law of universal gravitation. Realizing that the same force that attracts objects to the surface of the Earth held the Moon in orbit around the Earth, Newton was able to explain – in one theoretical framework – all known gravitational phenomena. In his Philosophiæ Naturalis Principia Mathematica,"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_26",
    "chunk": "he derived Kepler's laws from first principles. Those first principles are as follows: Thus while Kepler explained how the planets moved, Newton accurately managed to explain why the planets moved the way they do. Newton's theoretical developments laid many of the foundations of modern physics. Outside of England, Newton's theory took some time to become established. René Descartes' theory of vortices held sway in France, and Christiaan Huygens, Gottfried Wilhelm Leibniz and Jacques Cassini accepted only parts of Newton's system, preferring their own philosophies. Voltaire published a popular account in 1738. In 1748, the French Academy of Sciences offered a reward for solving the question of the perturbations of Jupiter and Saturn, which was eventually done by Euler and Lagrange. Laplace completed the theory of the planets, publishing from 1798 to 1825. The early origins of the solar nebular model of planetary formation had begun. Edmond Halley succeeded John Flamsteed as Astronomer Royal in England and succeeded in predicting the return of the comet that bears his name in 1758. William Herschel found the first new planet, Uranus, to be observed in modern times in 1781. The gap between the planets Mars and Jupiter disclosed by the Titius–Bode law was"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_27",
    "chunk": "filled by the discovery of the asteroids Ceres and Pallas in 1801 and 1802 with many more following. At first, astronomical thought in America was based on Aristotelian philosophy, but interest in the new astronomy began to appear in Almanacs as early as 1659. Cosmic pluralism is the name given to the idea that the stars are distant suns, perhaps with their own planetary systems. Ideas in this direction were expressed in antiquity, by Anaxagoras and by Aristarchus of Samos, but did not find mainstream acceptance. The first astronomer of the European Renaissance to suggest that the stars were distant suns was Giordano Bruno in his De l'infinito universo et mondi (1584). This idea, together with a belief in intelligent extraterrestrial life, was among the charges brought against him by the Inquisition. The idea became mainstream in the later 17th century, especially following the publication of Conversations on the Plurality of Worlds by Bernard Le Bovier de Fontenelle (1686), and by the early 18th century it was the default working assumptions in stellar astronomy. The Italian astronomer Geminiano Montanari recorded observing variations in luminosity of the star Algol in 1667. Edmond Halley published the first measurements of the proper motion"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_28",
    "chunk": "of a pair of nearby \"fixed\" stars, demonstrating that they had changed positions since the time of the ancient Greek astronomers Ptolemy and Hipparchus. William Herschel was the first astronomer to attempt to determine the distribution of stars in the sky. During the 1780s, he established a series of gauges in 600 directions and counted the stars observed along each line of sight. From this he deduced that the number of stars steadily increased toward one side of the sky, in the direction of the Milky Way core. His son John Herschel repeated this study in the southern hemisphere and found a corresponding increase in the same direction. In addition to his other accomplishments, William Herschel is noted for his discovery that some stars do not merely lie along the same line of sight, but are physical companions that form binary star systems. Pre-photography, data recording of astronomical data was limited by the human eye. In 1840, John W. Draper, a chemist, created the earliest known astronomical photograph of the Moon. And by the late 19th century thousands of photographic plates of images of planets, stars, and galaxies were created. Most photography had lower quantum efficiency (i.e. captured less of"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_29",
    "chunk": "the incident photons) than human eyes but had the advantage of long integration times (100 ms for the human eye compared to hours for photos). This vastly increased the data available to astronomers, which led to the rise of human computers, famously the Harvard Computers, to track and analyze the data. Scientists began discovering forms of light which were invisible to the naked eye: X-rays, gamma rays, radio waves, microwaves, ultraviolet radiation, and infrared radiation. This had a major impact on astronomy, spawning the fields of infrared astronomy, radio astronomy, x-ray astronomy and finally gamma-ray astronomy. With the advent of spectroscopy it was proven that other stars were similar to the Sun, but with a range of temperatures, masses and sizes. The science of stellar spectroscopy was pioneered by Joseph von Fraunhofer and Angelo Secchi. By comparing the spectra of stars such as Sirius to the Sun, they found differences in the strength and number of their absorption lines—the dark lines in stellar spectra caused by the atmosphere's absorption of specific frequencies. In 1865, Secchi began classifying stars into spectral types. The first evidence of helium was observed on August 18, 1868, as a bright yellow spectral line with a"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_30",
    "chunk": "wavelength of 587.49 nanometers in the spectrum of the chromosphere of the Sun. The line was detected by French astronomer Jules Janssen during a total solar eclipse in Guntur, India. The first direct measurement of the distance to a star (61 Cygni at 11.4 light-years) was made in 1838 by Friedrich Bessel using the parallax technique. Parallax measurements demonstrated the vast separation of the stars in the heavens. Observation of double stars gained increasing importance during the 19th century. In 1834, Friedrich Bessel observed changes in the proper motion of the star Sirius and inferred a hidden companion. Edward Pickering discovered the first spectroscopic binary in 1899 when he observed the periodic splitting of the spectral lines of the star Mizar in a 104-day period. Detailed observations of many binary star systems were collected by astronomers such as Friedrich Georg Wilhelm von Struve and S. W. Burnham, allowing the masses of stars to be determined from the computation of orbital elements. The first solution to the problem of deriving an orbit of binary stars from telescope observations was made by Felix Savary in 1827. In 1847, Maria Mitchell discovered a comet using a telescope. With the accumulation of large sets"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_31",
    "chunk": "of astronomical data, teams like the Harvard Computers rose in prominence which led to many female astronomers, previously relegated as assistants to male astronomers, gaining recognition in the field. The United States Naval Observatory (USNO) and other astronomy research institutions hired human \"computers\", who performed the tedious calculations while scientists performed research requiring more background knowledge. A number of discoveries in this period were originally noted by the women \"computers\" and reported to their supervisors. Henrietta Swan Leavitt discovered the cepheid variable star period-luminosity relation which she further developed into a method of measuring distance outside of the Solar System. A veteran of the Harvard Computers, Annie J. Cannon developed the modern version of the stellar classification scheme in during the early 1900s (O B A F G K M, based on color and temperature), manually classifying more stars in a lifetime than anyone else (around 350,000). The twentieth century saw increasingly rapid advances in the scientific study of stars. Karl Schwarzschild discovered that the color of a star and, hence, its temperature, could be determined by comparing the visual magnitude against the photographic magnitude. The development of the photoelectric photometer allowed precise measurements of magnitude at multiple wavelength intervals."
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_32",
    "chunk": "In 1921 Albert A. Michelson made the first measurements of a stellar diameter using an interferometer on the Hooker telescope at Mount Wilson Observatory. Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung–Russell diagram was developed, propelling the astrophysical study of stars. In Potsdam in 1906, the Danish astronomer Ejnar Hertzsprung published the first plots of color versus luminosity for these stars. These plots showed a prominent and continuous sequence of stars, which he named the Main Sequence. At Princeton University, Henry Norris Russell plotted the spectral types of these stars against their absolute magnitude, and found that dwarf stars followed a distinct relationship. This allowed the real brightness of a dwarf star to be predicted with reasonable accuracy. Successful models were developed to explain the interiors of stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 doctoral thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined. As evolutionary models of stars were developed during the 1930s, Bengt Strömgren introduced"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_33",
    "chunk": "the term Hertzsprung–Russell diagram to denote a luminosity-spectral class diagram. A refined scheme for stellar classification was published in 1943 by William Wilson Morgan and Philip Childs Keenan. The existence of our galaxy, the Milky Way, as a separate group of stars was only proven in the 20th century, along with the existence of \"external\" galaxies, and soon after, the expansion of the universe seen in the recession of most galaxies from us. The \"Great Debate\" between Harlow Shapley and Heber Curtis, in the 1920s, concerned the nature of the Milky Way, spiral nebulae, and the dimensions of the universe. The Sun was found to be part of a galaxy made up of more than 10 stars (10 billion stars). The existence of other galaxies, one of the matters of the great debate, was settled by Edwin Hubble, who identified the Andromeda nebula as a different galaxy, and many others at large distances and receding, moving away from our galaxy. Physical cosmology, a discipline that has a large intersection with astronomy, made huge advances during the 20th century, with the model of the hot Big Bang heavily supported by the evidence provided by astronomy and physics, such as the redshifts"
  },
  {
    "source": "History of astronomy.txt",
    "chunk_id": "History of astronomy.txt_34",
    "chunk": "of very distant galaxies and radio sources, the cosmic microwave background radiation, Hubble's law and cosmological abundances of elements."
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_0",
    "chunk": "# History of the telescope The history of the telescope can be traced to before the invention of the earliest known telescope, which appeared in 1608 in the Netherlands, when a patent was submitted by Hans Lippershey, an eyeglass maker. Although Lippershey did not receive his patent, news of the invention soon spread across Europe. The design of these early refracting telescopes consisted of a convex objective lens and a concave eyepiece. Galileo improved on this design the following year and applied it to astronomy. In 1611, Johannes Kepler described how a far more useful telescope could be made with a convex objective lens and a convex eyepiece lens. By 1655, astronomers such as Christiaan Huygens were building powerful but unwieldy Keplerian telescopes with compound eyepieces. Isaac Newton is credited with building the first reflector in 1668 with a design that incorporated a small flat diagonal mirror to reflect the light to an eyepiece mounted on the side of the telescope. Laurent Cassegrain in 1672 described the design of a reflector with a small convex secondary mirror to reflect light through a central hole in the main mirror. The achromatic lens, which greatly reduced color aberrations in objective lenses and"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_1",
    "chunk": "allowed for shorter and more functional telescopes, first appeared in a 1733 telescope made by Chester Moore Hall, who did not publicize it. John Dollond learned of Hall's invention and began producing telescopes using it in commercial quantities, starting in 1758. Important developments in reflecting telescopes were John Hadley's production of larger paraboloidal mirrors in 1721; the process of silvering glass mirrors introduced by Léon Foucault in 1857; and the adoption of long-lasting aluminized coatings on reflector mirrors in 1932. The Ritchey-Chretien variant of Cassegrain reflector was invented around 1910, but not widely adopted until after 1950; many modern telescopes including the Hubble Space Telescope use this design, which gives a wider field of view than a classic Cassegrain. During the period 1850–1900, reflectors suffered from problems with speculum metal mirrors, and a considerable number of \"Great Refractors\" were built from 60 cm to 1 metre aperture, culminating in the Yerkes Observatory refractor in 1897; however, starting from the early 1900s a series of ever-larger reflectors with glass mirrors were built, including the Mount Wilson 60-inch (1.5 metre), the 100-inch (2.5 metre) Hooker Telescope (1917) and the 200-inch (5 metre) Hale Telescope (1948); essentially all major research telescopes since 1900"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_2",
    "chunk": "have been reflectors. A number of 4-metre class (160 inch) telescopes were built on superior higher altitude sites including Hawaii and the Chilean desert in the 1975–1985 era. The development of the computer-controlled alt-azimuth mount in the 1970s and active optics in the 1980s enabled a new generation of even larger telescopes, starting with the 10-metre (400 inch) Keck telescopes in 1993/1996, and a number of 8-metre telescopes including the ESO Very Large Telescope, Gemini Observatory and Subaru Telescope. The era of radio telescopes (along with radio astronomy) was born with Karl Guthe Jansky's serendipitous discovery of an astronomical radio source in 1931. Many types of telescopes were developed in the 20th century for a wide range of wavelengths from radio to gamma-rays. The development of space observatories after 1960 allowed access to several bands impossible to observe from the ground, including X-rays and longer wavelength infrared bands. Objects resembling lenses date back 4000 years although it is unknown if they were used for their optical properties or just as decoration. Greek accounts of the optical properties of water-filled spheres (5th century BC) were followed by many centuries of writings on optics, including Ptolemy (2nd century) in his Optics, who"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_3",
    "chunk": "wrote about the properties of light including reflection, refraction, and color, followed by Ibn Sahl (10th century) and Ibn Al-Haytham (11th century). Actual use of lenses dates back to the widespread manufacture and use of eyeglasses in Northern Italy beginning in the late 13th century. The invention of the use of concave lenses to correct near-sightedness is ascribed to Nicholas of Cusa in 1451. The first record of a telescope comes from the Netherlands in 1608. It is in a patent filed by Middelburg spectacle-maker Hans Lippershey with the States General of the Netherlands on 2 October 1608 for his instrument \"for seeing things far away as if they were nearby.\" A few weeks later another Dutch instrument-maker, Jacob Metius also applied for a patent. The States General did not award a patent since the knowledge of the device already seemed to be ubiquitous but the Dutch government awarded Lippershey with a contract for copies of his design. The original Dutch telescopes were composed of a convex and a concave lens—telescopes that are constructed this way do not invert the image. Lippershey's original design had only 3x magnification. Telescopes seem to have been made in the Netherlands in considerable numbers"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_4",
    "chunk": "soon after this date of \"invention\", and rapidly found their way all over Europe. In 1655, the Dutch diplomat William de Boreel tried to solve the mystery of who invented the telescope. He had a local magistrate in Middelburg follow-up on Boreel's childhood and early adult recollections of a spectacle-maker named \"Hans\", who he remembered as the inventor of the telescope. The magistrate was contacted by a then-unknown claimant– the Middelburg spectacle-maker Johannes Zachariassen, who testified that his father– Zacharias Janssen, invented the telescope and the microscope as early as 1590. This testimony seemed convincing to Boreel, who now recollected that Zacharias and his father– Hans Martens, must have been who he remembered. Boreel's conclusion that Zacharias Janssen invented the telescope a little ahead of another spectacle maker, Hans Lippershey, was adopted by Pierre Borel in his 1656 book De vero telescopii inventore. Discrepancies in Boreel's investigation and Zachariassen's testimony (including Zachariassen misrepresenting his date of birth and role in the invention) has led some historians to consider this claim dubious. The \"Janssen\" claim would continue over the years and be added on to with Zacharias Snijder in 1841 presenting 4 iron tubes with lenses in them claimed to be"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_5",
    "chunk": "1590 examples of Janssen's telescope and historian Cornelis de Waard's 1906 claim that the man who tried to sell a broken telescope to astronomer Simon Marius at the 1608 Frankfurt Book Fair must have been Janssen. In 1682, the minutes of the Royal Society in London Robert Hooke noted Thomas Digges' 1571 Pantometria, (a book on measurement, partially based on his father Leonard Digges' notes and observations) seemed to support an English claim to the invention of the telescope, describing Leonard as having a fare seeing glass in the mid-1500s based on an idea by Roger Bacon. Thomas described it as \"by proportional Glasses duly situate in convenient angles, not only discovered things far off, read letters, numbered pieces of money with the very coin and superscription thereof, cast by some of his friends of purpose upon downs in open fields, but also seven miles off declared what hath been done at that instant in private places.\" Comments on the use of proportional or \"perspective glass\" are also made in the writings of John Dee (1575) and William Bourne (1585). Bourne was asked in 1580 to investigate the Diggs device by Queen Elizabeth I's chief advisor Lord Burghley. Bourne's is"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_6",
    "chunk": "the best description of it, and from his writing it seemed to consist of peering into a large curved mirror that reflected the image produced by a large lens. The idea of an \"Elizabethan Telescope\" has been expanded over the years, including astronomer and historian Colin Ronan concluding in the 1990s that this reflecting/refracting telescope was built by Leonard Digges between 1540 and 1559. This \"backwards\" reflecting telescope would have been unwieldy; it needed very large mirrors and lens to work; the observer had to stand backwards to look at an upside down view, and Bourne noted that it had a very narrow field of view, making it unsuitable for military purposes. The optical performance required to see the details of coins lying about in fields, or private activities seven miles away, seems to be far beyond the technology of the time, and it may be that the \"perspective glass\" being described was a far simpler idea, originating with Bacon, of using a single lens held in front of the eye to magnify a distant view. A 1959 research paper by Simon de Guilleuma claimed that evidence he had uncovered pointed to the French born spectacle maker Juan Roget (died"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_7",
    "chunk": "before 1624) as another possible builder of an early telescope that predated Hans Lippershey's patent application. In 2022, the Italian professor of physics Alessandro Bettini published a paper on whether Leonardo da Vinci could have invented a telescope. Building upon 1939 observations by Domenico Argentieri of what look like lenses arranged like a telescope in da Vinci drawings, Bettini superimposed Argentieri's lens arrangement on an adjacent drawing of diverging rays, coming up with an arrangement that also looked like a telescope. Bettini also noted the writings of Italian scholar and professor Girolamo Fracastoro in 1538, about combining lenses in eyeglasses to make the \"moon or at another star\" \"so near that they would appear not higher than the towers\". Lippershey's application for a patent was mentioned at the end of a diplomatic report on an embassy to Holland from the Kingdom of Siam sent by the Siamese king Ekathotsarot: Ambassades du Roy de Siam envoyé à l'Excellence du Prince Maurice, arrivé à La Haye le 10 Septemb. 1608 (Embassy of the King of Siam sent to his Excellency Prince Maurice, arrived at The Hague on 10 September 1608). This report was issued in October 1608 and distributed across Europe, leading"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_8",
    "chunk": "to experiments by other scientists, such as the Italian Paolo Sarpi, who received the report in November, and the English mathematician and astronomer Thomas Harriot, who used a six-powered telescope by the summer of 1609 to observe features on the moon. The Italian polymath Galileo Galilei was in Venice in June 1609 and there heard of the \"Dutch perspective glass\", a military spyglass, by means of which distant objects appeared nearer and larger. Galileo states that he solved the problem of the construction of a telescope the first night after his return to Padua from Venice and made his first telescope the next day by using a convex objective lens in one extremity of a leaden tube and a concave eyepiece lens in the other end, an arrangement that came to be called a Galilean telescope. A few days afterwards, having succeeded in making a better telescope than the first, he took it to Venice where he communicated the details of his invention to the public and presented the instrument itself to the doge Leonardo Donato, who was sitting in full council. The senate in return settled him for life in his lectureship at Padua and doubled his salary. Galileo"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_9",
    "chunk": "set himself to improving the telescope, producing telescopes of increased magnification. His first telescope had a 3x magnification, but he soon made instruments which magnified 8x, and finally, one nearly a meter long with a 37mm objective (which he would stop down to 16mm or 12mm) and a 23x magnification. With this last instrument, he began a series of astronomical observations in October or November of 1609, observing the satellites of Jupiter, hills and valleys on the moon, the phases of Venus and spots on the sun (using the projection method rather than direct observation). Galileo noted that the revolution of the satellites of Jupiter, the phases of Venus, rotation of the sun and the tilted path its spots followed for part of the year pointed to the validity of the sun-centered Copernican system over other Earth-centered systems such as the one proposed by Ptolemy. Galileo's instrument was the first to be given the name \"telescope\". The name was invented by the Greek poet/theologian Giovanni Demisiani at a banquet held on April 14, 1611, by Prince Federico Cesi to make Galileo Galilei a member of the Accademia dei Lincei. The word was created from the Greek tele = 'far' and"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_10",
    "chunk": "skopein = 'to look or see'; teleskopos = 'far-seeing'. By 1626 knowledge of the telescope had spread to China when German Jesuit and astronomer Johann Adam Schall von Bell published Yuan jing shuo, (遠鏡說, Explanation of the Telescope) in Chinese and Latin. Johannes Kepler first explained the theory and some of the practical advantages of a telescope constructed of two convex lenses in his Catoptrics (1611). The first person who actually constructed a telescope of this form was the Jesuit Christoph Scheiner who gives a description of it in his Rosa Ursina (1630). William Gascoigne was the first who commanded a chief advantage of the form of telescope suggested by Kepler: that a small material object could be placed at the common focal plane of the objective and the eyepiece. This led to his invention of the micrometer, and his application of telescopic sights to precision astronomical instruments. It was not until about the middle of the 17th century that Kepler's telescope came into general use: not so much because of the advantages pointed out by Gascoigne, but because its field of view was much larger than in the Galilean telescope. The first powerful telescopes of Keplerian construction were made"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_11",
    "chunk": "by Christiaan Huygens after much labor—in which his brother assisted him. With one of these: an objective diameter of 2.24 inches (57 mm) and a 12 ft (3.7 m) focal length, he discovered the brightest of Saturn's satellites (Titan) in 1655; in 1659, he published his \"Systema Saturnium\" which, for the first time, gave a true explanation of Saturn's ring—founded on observations made with the same instrument. The sharpness of the image in Kepler's telescope was limited by the chromatic aberration introduced by the non-uniform refractive properties of the objective lens. The only way to overcome this limitation at high magnifying powers was to create objectives with very long focal lengths. Giovanni Cassini discovered Saturn's fifth satellite (Rhea) in 1672 with a telescope 35 feet (11 m) long. Astronomers such as Johannes Hevelius were constructing telescopes with focal lengths as long as 150 feet (46 m). Besides having really long tubes these telescopes needed scaffolding or long masts and cranes to hold them up. Their value as research tools was minimal since the telescope's frame \"tube\" flexed and vibrated in the slightest breeze and sometimes collapsed altogether. In some of the very long refracting telescopes constructed after 1675, no tube"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_12",
    "chunk": "was employed at all. The objective was mounted on a swiveling ball-joint on top of a pole, tree, or any available tall structure and aimed by means of string or connecting rod. The eyepiece was handheld or mounted on a stand at the focus, and the image was found by trial and error. These were consequently termed aerial telescopes. and have been attributed to Christiaan Huygens and his brother Constantijn Huygens, Jr. although it is not clear that they invented it. Christiaan Huygens and his brother made objectives up to 8.5 inches (220 mm) diameter and 210 ft (64 m) focal length and others such as Adrien Auzout made telescopes with focal lengths up to 600 ft (180 m). Telescopes of such great length were naturally difficult to use and must have taxed to the utmost the skill and patience of the observers. Aerial telescopes were employed by several other astronomers. Cassini discovered Saturn's third and fourth satellites in 1684 with aerial telescope objectives made by Giuseppe Campani that were 100 and 136 ft (30 and 41 m) in focal length. The ability of a curved mirror to form an image may have been known since the time of Euclid"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_13",
    "chunk": "and had been extensively studied by Alhazen in the 11th century. Galileo, Giovanni Francesco Sagredo, and others, spurred on by their knowledge that curved mirrors had similar properties to lenses, discussed the idea of building a telescope using a mirror as the image forming objective. Niccolò Zucchi, an Italian Jesuit astronomer and physicist, wrote in his book Optica philosophia of 1652 that he tried replacing the lens of a refracting telescope with a bronze concave mirror in 1616. Zucchi tried looking into the mirror with a hand held concave lens but did not get a satisfactory image, possibly due to the poor quality of the mirror, the angle it was tilted at, or the fact that his head partially obstructed the image. In 1636 Marin Mersenne proposed a telescope consisting of a paraboloidal primary mirror and a paraboloidal secondary mirror bouncing the image through a hole in the primary, solving the problem of viewing the image. James Gregory went into further detail in his book Optica Promota (1663), pointing out that a reflecting telescope with a mirror that was shaped like the part of a conic section, would correct spherical aberration as well as the chromatic aberration seen in refractors."
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_14",
    "chunk": "The design he came up with bears his name: the \"Gregorian telescope\"; but according to his own confession, Gregory had no practical skill and he could find no optician capable of realizing his ideas and after some fruitless attempts, was obliged to abandon all hope of bringing his telescope into practical use. In 1666 Isaac Newton, based on his theories of refraction and color, perceived that the faults of the refracting telescope were due more to a lens's varying refraction of light of different colors than to a lens's imperfect shape. He concluded that light could not be refracted through a lens without causing chromatic aberrations, although he incorrectly concluded from some rough experiments that all refracting substances would diverge the prismatic colors in a constant proportion to their mean refraction. From these experiments Newton concluded that no improvement could be made in the refracting telescope. Newton's experiments with mirrors showed that they did not suffer from the chromatic errors of lenses, for all colors of light the angle of incidence reflected in a mirror was equal to the angle of reflection, so as a proof to his theories Newton set out to build a reflecting telescope. Newton completed his"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_15",
    "chunk": "first telescope in 1668 and it is the earliest known functional reflecting telescope. After much experiment, he chose an alloy (speculum metal) of tin and copper as the most suitable material for his objective mirror. He later devised means for grinding and polishing them, but chose a spherical shape for his mirror instead of a parabola to simplify construction. He added to his reflector what is the hallmark of the design of a \"Newtonian telescope\", a secondary \"diagonal\" mirror near the primary mirror's focus to reflect the image at 90° angle to an eyepiece mounted on the side of the telescope. This unique addition allowed the image to be viewed with minimal obstruction of the objective mirror. He also made all the tube, mount, and fittings. Newton's first compact reflecting telescope had a mirror diameter of 1.3 inches and a focal ratio of f/5. With it he found that he could see the four Galilean moons of Jupiter and the crescent phase of the planet Venus. Encouraged by this success, he made a second telescope with a magnifying power of 38x which he presented to the Royal Society of London in December 1671. This type of telescope is still called"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_16",
    "chunk": "a Newtonian telescope. A third form of reflecting telescope, the \"Cassegrain reflector\" was devised in 1672 by Laurent Cassegrain. The telescope had a small convex hyperboloidal secondary mirror placed near the prime focus to reflect light through a central hole in the main mirror. No further practical advance appears to have been made in the design or construction of the reflecting telescopes for another 50 years until John Hadley (best known as the inventor of the octant) developed ways to make precision aspheric and parabolic speculum metal mirrors. In 1721 he showed the first parabolic Newtonian reflector to the Royal Society. It had a 6-inch (15 cm) diameter, 62+3⁄4-inch (159 cm) focal length speculum metal objective mirror. The instrument was examined by James Pound and James Bradley. After remarking that Newton's telescope had lain neglected for fifty years, they stated that Hadley had sufficiently shown that the invention did not consist in bare theory. They compared its performance with that of a 7.5 inches (190 mm) diameter aerial telescope originally presented to the Royal Society by Constantijn Huygens, Jr. and found that Hadley's reflector, \"will bear such a charge as to make it magnify the object as many times as"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_17",
    "chunk": "the latter with its due charge\", and that it represents objects as distinct, though not altogether so clear and bright. Bradley and Samuel Molyneux, having been instructed by Hadley in his methods of polishing speculum metal, succeeded in producing large reflecting telescopes of their own, one of which had a focal length of 8 ft (2.4 m). These methods of fabricating mirrors were passed on by Molyneux to two London opticians —Scarlet and Hearn— who started a business manufacturing telescopes. The British mathematician, optician James Short began experimenting with building telescopes based on Gregory's designs in the 1730s. He first tried making his mirrors out of glass as suggested by Gregory, but he later switched to speculum metal mirrors creating Gregorian telescopes with original designers parabolic and elliptic figures. Short then adopted telescope-making as his profession which he practised first in Edinburgh, and afterward in London. All Short's telescopes were of the Gregorian form. Short died in London in 1768, having made a considerable fortune selling telescopes. Since speculum metal mirror secondaries or diagonal mirrors greatly reduced the light that reached the eyepiece, several reflecting telescope designers tried to do away with them. In 1762 Mikhail Lomonosov presented a reflecting"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_18",
    "chunk": "telescope before the Russian Academy of Sciences forum. It had its primary mirror tilted at four degrees to telescope's axis so the image could be viewed via an eyepiece mounted at the front of the telescope tube without the observer's head blocking the incoming light. This innovation was not published until 1827, so this type came to be called the Herschelian telescope after a similar design by William Herschel. About the year 1774 William Herschel (then a teacher of music in Bath, England) began to occupy his leisure hours with the construction of reflector telescope mirrors, finally devoted himself entirely to their construction and use in astronomical research. In 1778, he selected a 6+1⁄4-inch (16 cm) reflector mirror (the best of some 400 telescope mirrors which he had made) and with it, built a 7-foot (2.1 m) focal length telescope. Using this telescope, he made his early brilliant astronomical discoveries. In 1783, Herschel completed a reflector of approximately 18 inches (46 cm) in diameter and 20 ft (6.1 m) focal length. He observed the heavens with this telescope for some twenty years, replacing the mirror several times. In 1789 Herschel finished building his largest reflecting telescope with a mirror of"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_19",
    "chunk": "49 inches (120 cm) and a focal length of 40 ft (12 m), (commonly known as his 40-foot telescope) at his new home, at Observatory House in Slough, England. To cut down on the light loss from the poor reflectivity of the speculum mirrors of that day, Herschel eliminated the small diagonal mirror from his design and tilted his primary mirror so he could view the formed image directly. This design has come to be called the Herschelian telescope. He discovered Saturn's sixth known moon, Enceladus, the first night he used it (August 28, 1789), and on September 17, its seventh known moon, Mimas. This telescope was world's largest telescope for over 50 years. However, this large scope was difficult to handle and thus less used than his favorite 18.7-inch reflector. In 1845 William Parsons, 3rd Earl of Rosse built his 72-inch (180 cm) Newtonian reflector called the \"Leviathan of Parsonstown\" with which he discovered the spiral form of galaxies. All of these larger reflectors suffered from the poor reflectivity and fast tarnishing nature of their speculum metal mirrors. This meant they need more than one mirror per telescope since mirrors had to be frequently removed and re-polished. This was"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_20",
    "chunk": "time-consuming since the polishing process could change the curve of the mirror, so it usually had to be \"re-figured\" to the correct shape. From the time of the invention of the first refracting telescopes it was generally supposed that chromatic errors seen in lenses simply arose from errors in the spherical figure of their surfaces. Opticians tried to construct lenses of varying forms of curvature to correct these errors. Isaac Newton discovered in 1666 that chromatic colors actually arose from the un-even refraction of light as it passed through the glass medium. This led opticians to experiment with lenses constructed of more than one type of glass in an attempt to canceling the errors produced by each type of glass. It was hoped that this would create an \"achromatic lens\"; a lens that would focus all colors to a single point, and produce instruments of much shorter focal length. The first person who succeeded in making a practical achromatic refracting telescope was Chester Moore Hall from Essex, England. He argued that the different humours of the human eye refract rays of light to produce an image on the retina which is free from color, and he reasonably argued that it"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_21",
    "chunk": "might be possible to produce a like result by combining lenses composed of different refracting media. After devoting some time to the inquiry he found that by combining two lenses formed of different kinds of glass, he could make an achromatic lens where the effects of the unequal refractions of two colors of light (red and blue) was corrected. In 1733, he succeeded in constructing telescope lenses which exhibited much reduced chromatic aberration. One of his instruments had an objective measuring 2+1⁄2 inches (6.4 cm) with a relatively short focal length of 20 inches (51 cm). Hall was a man of independent means and seems to have been careless of fame; at least he took no trouble to communicate his invention to the world. At a trial in Westminster Hall about the patent rights granted to John Dollond (Watkin v. Dollond), Hall was admitted to be the first inventor of the achromatic telescope. However, it was ruled by Lord Mansfield that \"it was not the person who locked his invention in his scrutoire who ought to profit for such invention, but the one who brought it forth for the benefit of mankind.\" In 1747, Leonhard Euler sent to the Prussian"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_22",
    "chunk": "Academy of Sciences a paper in which he tried to prove the possibility of correcting both the chromatic and the spherical aberration of a lens. Like Gregory and Hall, he argued that since the various humours of the human eye were so combined as to produce a perfect image, it should be possible by suitable combinations of lenses of different refracting media to construct a perfect telescope objective. Adopting a hypothetical law of the dispersion of differently colored rays of light, he proved analytically the possibility of constructing an achromatic objective composed of lenses of glass and water. All of Euler's efforts to produce an actual objective of this construction were fruitless—a failure which he attributed solely to the difficulty of procuring lenses that worked precisely to the requisite curves. John Dollond agreed with the accuracy of Euler's analysis, but disputed his hypothesis on the grounds that it was purely a theoretical assumption: that the theory was opposed to the results of Newton's experiments on the refraction of light, and that it was impossible to determine a physical law from analytical reasoning alone. In 1754, Euler sent to the Berlin Academy a further paper in which starting from the hypothesis"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_23",
    "chunk": "that light consists of vibrations excited in an elastic fluid by luminous bodies—and that the difference of color of light is due to the greater or lesser frequency of these vibrations in a given time— he deduced his previous results. He did not doubt the accuracy of Newton's experiments quoted by Dollond. Dollond did not reply to this, but soon afterwards he received an abstract of a paper by the Swedish mathematician and astronomer, Samuel Klingenstierna, which led him to doubt the accuracy of the results deduced by Newton on the dispersion of refracted light. Klingenstierna showed from purely geometrical considerations (fully appreciated by Dollond) that the results of Newton's experiments could not be brought into harmony with other universally accepted facts of refraction. As a practical man, Dollond at once put his doubts to the test of experiment: he confirmed the conclusions of Klingenstierna, discovered a difference far beyond his hopes in the refractive qualities of different kinds of glass with respect to the divergence of colors, and was thus rapidly led to the construction of lenses in which first the chromatic aberration—and afterwards—the spherical aberration were corrected. Dollond was aware of the conditions necessary for the attainment of"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_24",
    "chunk": "achromatism in refracting telescopes, but relied on the accuracy of experiments made by Newton. His writings show that with the exception of his bravado, he would have arrived sooner at a discovery for which his mind was fully prepared. Dollond's paper recounts the successive steps by which he arrived at his discovery independently of Hall's earlier invention—and the logical processes by which these steps were suggested to his mind. In 1765 Peter Dollond (son of John Dollond) introduced the triple objective, which consisted of a combination of two convex lenses of crown glass with a concave flint lens between them. He made many telescopes of this kind. The difficulty of procuring disks of glass (especially of flint glass) of suitable purity and homogeneity limited the diameter and light gathering power of the lenses found in the achromatic telescope. It was in vain that the French Academy of Sciences offered prizes for large perfect disks of optical flint glass. The difficulties with the impractical metal mirrors of reflecting telescopes led to the construction of large refracting telescopes. By 1866 refracting telescopes had reached 18 inches (46 cm) in aperture with many larger \"Great refractors\" being built in the mid to late"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_25",
    "chunk": "19th century. In 1897, the refractor reached its maximum practical limit in a research telescope with the construction of the Yerkes Observatorys' 40-inch (100 cm) refractor (although a larger refractor Great Paris Exhibition Telescope of 1900 with an objective of 49.2 inches (1.25 m) diameter was temporarily exhibited at the Paris 1900 Exposition). No larger refractors could be built because of gravity's effect on the lens. Since a lens can only be held in place by its edge, the center of a large lens will sag due to gravity, distorting the image it produces. In 1856–57, Karl August von Steinheil and Léon Foucault introduced a process of depositing a layer of silver on glass telescope mirrors. The silver layer was not only much more reflective and longer lasting than the finish on speculum mirrors, it had the advantage of being able to be removed and re-deposited without changing the shape of the glass substrate. Towards the end of the 19th century very large silver on glass mirror reflecting telescopes were built. The beginning of the 20th century saw construction of the first of the \"modern\" large research reflectors, designed for precision photographic imaging and located at remote high altitude clear"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_26",
    "chunk": "sky locations such as the 60-inch Hale Telescope of 1908, and the 100-inch (2.5 m) Hooker telescope in 1917, both located at Mount Wilson Observatory. These and other telescopes of this size had to have provisions to allow for the removal of their main mirrors for re-silvering every few months. John Donavan Strong, a young physicist at the California Institute of Technology, developed a technique for coating a mirror with a much longer lasting aluminum coating using thermal vacuum evaporation. In 1932, he became the first person to \"aluminize\" a mirror; three years later the 60-inch (1,500 mm) and 100-inch (2,500 mm) telescopes became the first large astronomical telescopes to have their mirrors aluminized. 1948 saw the completion of the 200-inch (510 cm) Hale reflector at Mount Palomar which was the largest telescope in the world up until the completion of the massive 605 cm (238 in) BTA-6 in Russia twenty-seven years later. The Hale reflector introduced several technical innovations used in future telescopes, including hydrostatic bearings for very low friction, the Serrurier truss for equal deflections of the two mirrors as the tube sags under gravity, and the use of Pyrex low-expansion glass for the mirrors. The arrival of"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_27",
    "chunk": "substantially larger telescopes had to await the introduction of methods other than the rigidity of glass to maintain the proper shape of the mirror. The 1980s saw the introduction of two new technologies for building larger telescopes and improving image quality, known as active optics and adaptive optics. In active optics, an image analyser senses the aberrations of a star image a few times per minute, and a computer adjusts many support forces on the primary mirror and the location of the secondary mirror to maintain the optics in optimal shape and alignment. This is too slow to correct for atmospheric blurring effects, but enables the use of thin single mirrors up to 8 m diameter, or even larger segmented mirrors. This method was pioneered by the ESO New Technology Telescope in the late 1980s. The 1990s saw a new generation of giant telescopes appear using active optics, beginning with the construction of the first of the two 10 m (390 in) Keck telescopes in 1993. Other giant telescopes built since then include: the two Gemini telescopes, the four separate telescopes of the Very Large Telescope, and the Large Binocular Telescope. Adaptive optics uses a similar principle, but applying corrections"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_28",
    "chunk": "several hundred times per second to compensate the effects of rapidly changing optical distortion due to the motion of turbulence in Earth's atmosphere. Adaptive optics works by measuring the distortions in a wavefront and then compensating for them by rapid changes of actuators applied to a small deformable mirror or with a liquid crystal array filter. AO was first envisioned by Horace W. Babcock in 1953, but did not come into common usage in astronomical telescopes until advances in computer and detector technology during the 1990s made it possible to calculate the compensation needed in real time. In adaptive optics, the high-speed corrections needed mean that a fairly bright star is needed very close to the target of interest (or an artificial star is created by a laser). Also, with a single star or laser the corrections are only effective over a very narrow field (tens of arcsec), and current systems operating on several 8-10m telescopes work mainly in near-infrared wavelengths for single-object observations. Developments of adaptive optics include systems with multiple lasers over a wider corrected field, and/or working above kiloHertz rates for good correction at visible wavelengths; these are currently in progress but not yet in routine operation"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_29",
    "chunk": "as of 2015. The twentieth century saw the construction of telescopes which could produce images using wavelengths other than visible light starting in 1931 when Karl Jansky discovered astronomical objects gave off radio emissions; this prompted a new era of observational astronomy after World War II, with telescopes being developed for other parts of the electromagnetic spectrum from radio to gamma-rays. Radio astronomy began in 1931 when Karl Jansky discovered that the Milky Way was a source of radio emission while doing research on terrestrial static with a direction antenna. Building on Jansky's work, Grote Reber built a more sophisticated purpose-built radio telescope in 1937, with a 31.4-foot (9.6 m) dish; using this, he discovered various unexplained radio sources in the sky. Interest in radio astronomy grew after the Second World War when much larger dishes were built including: the 250-foot (76 m) Jodrell bank telescope (1957), the 300-foot (91 m) Green Bank Telescope (1962), and the 100-metre (330 ft) Effelsberg telescope (1971). The huge 1,000-foot (300 m) Arecibo telescope (1963) was so large that it was fixed into a natural depression in the ground; the central antenna could be steered to allow the telescope to study objects up to"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_30",
    "chunk": "twenty degrees from the zenith. However, not every radio telescope is of the dish type. For example, the Mills Cross Telescope (1954) was an early example of an array which used two perpendicular lines of antennae 1,500 feet (460 m) in length to survey the sky. High-energy radio waves are known as microwaves and this has been an important area of astronomy ever since the discovery of the cosmic microwave background radiation in 1964. Many ground-based radio telescopes can study microwaves. Short wavelength microwaves are best studied from space because water vapor (even at high altitudes) strongly weakens the signal. The Cosmic Background Explorer (1989) revolutionized the study of the microwave background radiation. Because radio telescopes have low resolution, they were the first instruments to use interferometry allowing two or more widely separated instruments to simultaneously observe the same source. Very long baseline interferometry extended the technique over thousands of kilometers and allowed resolutions down to a few milli-arcseconds. A telescope like the Large Millimeter Telescope (active since 2006) observes from 0.85 to 4 mm (850 to 4,000 μm), bridging between the far-infrared/submillimeter telescopes and longer wavelength radio telescopes including the microwave band from about 1 mm (1,000 μm) to"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_31",
    "chunk": "1,000 mm (1.0 m) in wavelength. Although most infrared radiation is absorbed by the atmosphere, infrared astronomy at certain wavelengths can be conducted on high mountains where there is little absorption by atmospheric water vapor. Ever since suitable detectors became available, most optical telescopes at high-altitudes have been able to image at infrared wavelengths. Some telescopes such as the 3.8-metre (150 in) UKIRT, and the 3-metre (120 in) IRTF — both on Mauna Kea — are dedicated infrared telescopes. The launch of the IRAS satellite in 1983 revolutionized infrared astronomy from space. This reflecting telescope which had a 60-centimetre (24 in) mirror, operated for nine months until its supply of coolant (liquid helium) ran out. It surveyed the entire sky detecting 245,000 infrared sources—more than 100 times the number previously known. Although optical telescopes can image the near ultraviolet, the ozone layer in the stratosphere absorbs ultraviolet radiation shorter than 300 nm so most ultra-violet astronomy is conducted with satellites. Ultraviolet telescopes resemble optical telescopes, but conventional aluminium-coated mirrors cannot be used and alternative coatings such as magnesium fluoride or lithium fluoride are used instead. The Orbiting Solar Observatory satellite carried out observations in the ultra-violet as early as 1962."
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_32",
    "chunk": "The International Ultraviolet Explorer (1978) systematically surveyed the sky for eighteen years, using a 45-centimetre (18 in) aperture telescope with two spectroscopes. Extreme-ultraviolet astronomy (10–100 nm) is a discipline in its own right and involves many of the techniques of X-ray astronomy; the Extreme Ultraviolet Explorer (1992) was a satellite operating at these wavelengths. X-rays from space do not reach Earth's surface, so X-ray astronomy must be conducted above Earth's atmosphere. The first X-ray experiments were conducted on sub-orbital rocket flights, which enabled the first detection of X-rays from the sun (1948), and then from the first galactic X-ray sources: Scorpius X-1 (June 1962) and the Crab Nebula (October 1962). Since then, X-ray telescopes (Wolter telescopes) have been built using nested grazing-incidence mirrors which deflect X-rays to a detector. Some of the OAO satellites conducted X-ray astronomy in the late 1960s, but the first dedicated X-ray satellite was the Uhuru (1970) which discovered 300 sources. More recent X-ray satellites include: the EXOSAT (1983), ROSAT (1990), Chandra (1999), and Newton (1999). Gamma rays are absorbed high in Earth's atmosphere, so most gamma-ray astronomy is conducted with satellites. Gamma-ray telescopes use scintillation counters, spark chambers and more recently, solid-state detectors. The angular"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_33",
    "chunk": "resolution of these devices is typically very poor. There were balloon-borne experiments in the early 1960s, but gamma-ray astronomy really began with the launch of the OSO 3 satellite in 1967; the first dedicated gamma-ray satellites were SAS B (1972) and Cos B (1975). The Compton Gamma Ray Observatory (1991) was a big improvement on previous surveys. Very high-energy gamma-rays (above 200 GeV) can be detected from the ground via the Cerenkov radiation that is produced by the passage of the gamma-rays through Earth's atmosphere. Several Cerenkov imaging telescopes have been built around the world, including: the HEGRA (1987), STACEE (2001), HESS (2003), and MAGIC (2004). In 1868, Fizeau noted that the purpose of the arrangement of mirrors or glass lenses in a conventional telescope was simply to provide an approximation to a Fourier transform of the optical wave field entering the telescope. As this mathematical transformation was well understood and could be performed mathematically on paper, he noted that by using an array of small instruments it would be possible to measure the diameter of a star with the same precision as a single telescope which was as large as the whole array— a technique which later became known"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_34",
    "chunk": "as astronomical interferometry. It was not until 1891 that Albert A. Michelson successfully used this technique for the measurement of astronomical angular diameters: the diameters of Jupiter's satellites (Michelson 1891). Thirty years later, a direct interferometric measurement of a stellar diameter was finally realized by Michelson & Francis G. Pease (1921) which was applied by their 20 ft (6.1 m) interferometer mounted on the 100 inch Hooker Telescope on Mount Wilson. The next major development came in 1946, when Martin Ryle and Derek Vonberg located a number of new cosmic radio sources by constructing a radio analogue of the Michelson interferometer. The signals from two radio antennas were added electronically to produce interference. Ryle and Vonberg's telescope used Earth's rotation to scan the sky in one dimension. With the development of larger arrays and of computers which could rapidly perform the necessary Fourier transforms, the first aperture synthesis imaging instruments were soon developed which could obtain high resolution images without the need of a giant parabolic reflector to perform the Fourier transform. This technique is now used in most radio astronomy observations. Radio astronomers soon developed the mathematical methods to perform aperture synthesis Fourier imaging using much larger arrays of"
  },
  {
    "source": "History of the telescope.txt",
    "chunk_id": "History of the telescope.txt_35",
    "chunk": "telescopes —often spread across more than one continent. In the 1980s, the aperture synthesis technique was extended to visible light as well as infrared astronomy, providing the first very high resolution optical and infrared images of nearby stars. In 1995, this imaging technique was demonstrated on an array of separate optical telescopes for the first time, allowing a further improvement in resolution, and also allowing even higher resolution imaging of stellar surfaces. The same techniques have now been applied at a number of other astronomical telescope arrays including: the Navy Prototype Optical Interferometer, the CHARA array, and the IOTA array. In 2008, Max Tegmark and Matias Zaldarriaga proposed a \"Fast Fourier Transform Telescope\" design in which the lenses and mirrors could be dispensed with altogether when computers become fast enough to perform all the necessary transforms."
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_0",
    "chunk": "# Hubble's law Hubble's law, also known as the Hubble–Lemaître law, is the observation in physical cosmology that galaxies are moving away from Earth at speeds proportional to their distance. In other words, the farther a galaxy is from the Earth, the faster it moves away. A galaxy's recessional velocity is typically determined by measuring its redshift, a shift in the frequency of light emitted by the galaxy. The discovery of Hubble's law is attributed to work published by Edwin Hubble in 1929, but the notion of the universe expanding at a calculable rate was first derived from general relativity equations in 1922 by Alexander Friedmann. The Friedmann equations showed the universe might be expanding, and presented the expansion speed if that were the case. Before Hubble, astronomer Carl Wilhelm Wirtz had, in 1922 and 1924, deduced with his own data that galaxies that appeared smaller and dimmer had larger redshifts and thus that more distant galaxies recede faster from the observer. In 1927, Georges Lemaître concluded that the universe might be expanding by noting the proportionality of the recessional velocity of distant bodies to their respective distances. He estimated a value for this ratio, which—after Hubble confirmed cosmic expansion"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_1",
    "chunk": "and determined a more precise value for it two years later—became known as the Hubble constant. Hubble inferred the recession velocity of the objects from their redshifts, many of which were earlier measured and related to velocity by Vesto Slipher in 1917. Combining Slipher's velocities with Henrietta Swan Leavitt's intergalactic distance calculations and methodology allowed Hubble to better calculate an expansion rate for the universe. Hubble's law is considered the first observational basis for the expansion of the universe, and is one of the pieces of evidence most often cited in support of the Big Bang model. The motion of astronomical objects due solely to this expansion is known as the Hubble flow. It is described by the equation v = H0D, with H0 the constant of proportionality—the Hubble constant—between the \"proper distance\" D to a galaxy (which can change over time, unlike the comoving distance) and its speed of separation v, i.e. the derivative of proper distance with respect to the cosmic time coordinate. Though the Hubble constant H0 is constant at any given moment in time, the Hubble parameter H, of which the Hubble constant is the current value, varies with time, so the term constant is sometimes"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_2",
    "chunk": "thought of as somewhat of a misnomer. The Hubble constant is most frequently quoted in km/s/Mpc, which gives the speed of a galaxy 1 megaparsec (3.09×10 km) away as 70 km/s. Simplifying the units of the generalized form reveals that H0 specifies a frequency (SI unit: s), leading the reciprocal of H0 to be known as the Hubble time (14.4 billion years). The Hubble constant can also be stated as a relative rate of expansion. In this form H0 = 7%/Gyr, meaning that, at the current rate of expansion, it takes one billion years for an unbound structure to grow by 7%. A decade before Hubble made his observations, a number of physicists and mathematicians had established a consistent theory of an expanding universe by using Einstein field equations of general relativity. Applying the most general principles to the nature of the universe yielded a dynamic solution that conflicted with the then-prevalent notion of a static universe. In 1912, Vesto M. Slipher measured the first Doppler shift of a \"spiral nebula\" (the obsolete term for spiral galaxies) and soon discovered that almost all such objects were receding from Earth. He did not grasp the cosmological implications of this fact, and"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_3",
    "chunk": "indeed at the time it was highly controversial whether or not these nebulae were \"island universes\" outside the Milky Way galaxy. In 1922, Alexander Friedmann derived his Friedmann equations from Einstein field equations, showing that the universe might expand at a rate calculable by the equations. The parameter used by Friedmann is known today as the scale factor and can be considered as a scale invariant form of the proportionality constant of Hubble's law. Georges Lemaître independently found a similar solution in his 1927 paper discussed in the following section. The Friedmann equations are derived by inserting the metric for a homogeneous and isotropic universe into Einstein's field equations for a fluid with a given density and pressure. This idea of an expanding spacetime would eventually lead to the Big Bang and Steady State theories of cosmology. In 1927, two years before Hubble published his own article, the Belgian priest and astronomer Georges Lemaître was the first to publish research deriving what is now known as Hubble's law. According to the Canadian astronomer Sidney van den Bergh, \"the 1927 discovery of the expansion of the universe by Lemaître was published in French in a low-impact journal. In the 1931 high-impact"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_4",
    "chunk": "English translation of this article, a critical equation was changed by omitting reference to what is now known as the Hubble constant.\" It is now known that the alterations in the translated paper were carried out by Lemaître himself. Before the advent of modern cosmology, there was considerable talk about the size and shape of the universe. In 1920, the Shapley–Curtis debate took place between Harlow Shapley and Heber D. Curtis over this issue. Shapley argued for a small universe the size of the Milky Way galaxy, and Curtis argued that the universe was much larger. The issue was resolved in the coming decade with Hubble's improved observations. Edwin Hubble did most of his professional astronomical observing work at Mount Wilson Observatory, home to the world's most powerful telescope at the time. His observations of Cepheid variable stars in \"spiral nebulae\" enabled him to calculate the distances to these objects. Surprisingly, these objects were discovered to be at distances which placed them well outside the Milky Way. They continued to be called nebulae, and it was only gradually that the term galaxies replaced it. The velocities and distances that appear in Hubble's law are not directly measured. The velocities are"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_5",
    "chunk": "inferred from the redshift z = ∆λ/λ of radiation and distance is inferred from brightness. Hubble sought to correlate brightness with parameter z. Combining his measurements of galaxy distances with Vesto Slipher and Milton Humason's measurements of the redshifts associated with the galaxies, Hubble discovered a rough proportionality between redshift of an object and its distance. Though there was considerable scatter (now known to be caused by peculiar velocities—the 'Hubble flow' is used to refer to the region of space far enough out that the recession velocity is larger than local peculiar velocities), Hubble was able to plot a trend line from the 46 galaxies he studied and obtain a value for the Hubble constant of 500 (km/s)/Mpc (much higher than the currently accepted value due to errors in his distance calibrations; see cosmic distance ladder for details). Hubble's law can be easily depicted in a \"Hubble diagram\" in which the velocity (assumed approximately proportional to the redshift) of an object is plotted with respect to its distance from the observer. A straight line of positive slope on this diagram is the visual depiction of Hubble's law. After Hubble's discovery was published, Albert Einstein abandoned his work on the cosmological"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_6",
    "chunk": "constant, a term he had inserted into his equations of general relativity to coerce them into producing the static solution he previously considered the correct state of the universe. The Einstein equations in their simplest form model either an expanding or contracting universe, so Einstein introduced the constant to counter expansion or contraction and lead to a static and flat universe. After Hubble's discovery that the universe was, in fact, expanding, Einstein called his faulty assumption that the universe is static his \"greatest mistake\". On its own, general relativity could predict the expansion of the universe, which (through observations such as the bending of light by large masses, or the precession of the orbit of Mercury) could be experimentally observed and compared to his theoretical calculations using particular solutions of the equations he had originally formulated. In 1931, Einstein went to Mount Wilson Observatory to thank Hubble for providing the observational basis for modern cosmology. The cosmological constant has regained attention in recent decades as a hypothetical explanation for dark energy. The discovery of the linear relationship between redshift and distance, coupled with a supposed linear relation between recessional velocity and redshift, yields a straightforward mathematical expression for Hubble's law"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_7",
    "chunk": "as follows: Hubble's law is considered a fundamental relation between recessional velocity and distance. However, the relation between recessional velocity and redshift depends on the cosmological model adopted and is not established except for small redshifts. For distances D larger than the radius of the Hubble sphere rHS, objects recede at a rate faster than the speed of light (See Uses of the proper distance for a discussion of the significance of this): r HS = c H 0 . {\\displaystyle r_{\\text{HS}}={\\frac {c}{H_{0}}}\\ .} Since the Hubble \"constant\" is a constant only in space, not in time, the radius of the Hubble sphere may increase or decrease over various time intervals. The subscript '0' indicates the value of the Hubble constant today. Current evidence suggests that the expansion of the universe is accelerating (see Accelerating universe), meaning that for any given galaxy, the recession velocity dD/dt is increasing over time as the galaxy moves to greater and greater distances; however, the Hubble parameter is actually thought to be decreasing with time, meaning that if we were to look at some fixed distance D and watch a series of different galaxies pass that distance, later galaxies would pass that distance at"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_8",
    "chunk": "a smaller velocity than earlier ones. Redshift can be measured by determining the wavelength of a known transition, such as hydrogen α-lines for distant quasars, and finding the fractional shift compared to a stationary reference. Thus, redshift is a quantity unambiguously acquired from observation. Care is required, however, in translating these to recessional velocities: for small redshift values, a linear relation of redshift to recessional velocity applies, but more generally the redshift-distance law is nonlinear, meaning the co-relation must be derived specifically for each given model and epoch. The redshift z is often described as a redshift velocity, which is the recessional velocity that would produce the same redshift if it were caused by a linear Doppler effect (which, however, is not the case, as the velocities involved are too large to use a non-relativistic formula for Doppler shift). This redshift velocity can easily exceed the speed of light. In other words, to determine the redshift velocity vrs, the relation: is used. That is, there is no fundamental difference between redshift velocity and redshift: they are rigidly proportional, and not related by any theoretical reasoning. The motivation behind the \"redshift velocity\" terminology is that the redshift velocity agrees with the"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_9",
    "chunk": "velocity from a low-velocity simplification of the so-called Fizeau–Doppler formula z = λ o λ e − 1 = 1 + v c 1 − v c − 1 ≈ v c . {\\displaystyle z={\\frac {\\lambda _{\\text{o}}}{\\lambda _{\\text{e}}}}-1={\\sqrt {\\frac {1+{\\frac {v}{c}}}{1-{\\frac {v}{c}}}}}-1\\approx {\\frac {v}{c}}.} Here, λo, λe are the observed and emitted wavelengths respectively. The \"redshift velocity\" vrs is not so simply related to real velocity at larger velocities, however, and this terminology leads to confusion if interpreted as a real velocity. Next, the connection between redshift or redshift velocity and recessional velocity is discussed. Suppose R(t) is called the scale factor of the universe, and increases as the universe expands in a manner that depends upon the cosmological model selected. Its meaning is that all measured proper distances D(t) between co-moving points increase proportionally to R. (The co-moving points are not moving relative to their local environments.) In other words: D ( t ) D ( t 0 ) = R ( t ) R ( t 0 ) , {\\displaystyle {\\frac {D(t)}{D(t_{0})}}={\\frac {R(t)}{R(t_{0})}},} where t0 is some reference time. If light is emitted from a galaxy at time te and received by us at t0, it is redshifted"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_10",
    "chunk": "due to the expansion of the universe, and this redshift z is simply: z = R ( t 0 ) R ( t e ) − 1. {\\displaystyle z={\\frac {R(t_{0})}{R(t_{\\text{e}})}}-1.} Suppose a galaxy is at distance D, and this distance changes with time at a rate dtD. We call this rate of recession the \"recession velocity\" vr: v r = d t D = d t R R D . {\\displaystyle v_{\\text{r}}=d_{t}D={\\frac {d_{t}R}{R}}D.} From this perspective, Hubble's law is a fundamental relation between (i) the recessional velocity associated with the expansion of the universe and (ii) the distance to an object; the connection between redshift and distance is a crutch used to connect Hubble's law with observations. This law can be related to redshift z approximately by making a Taylor series expansion: z = R ( t 0 ) R ( t e ) − 1 ≈ R ( t 0 ) R ( t 0 ) ( 1 + ( t e − t 0 ) H ( t 0 ) ) − 1 ≈ ( t 0 − t e ) H ( t 0 ) , {\\displaystyle z={\\frac {R(t_{0})}{R(t_{e})}}-1\\approx {\\frac {R(t_{0})}{R(t_{0})\\left(1+(t_{e}-t_{0})H(t_{0})\\right)}}-1\\approx (t_{0}-t_{e})H(t_{0}),} If the distance is not"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_11",
    "chunk": "too large, all other complications of the model become small corrections, and the time interval is simply the distance divided by the speed of light: z ≈ ( t 0 − t e ) H ( t 0 ) ≈ D c H ( t 0 ) , {\\displaystyle z\\approx (t_{0}-t_{\\text{e}})H(t_{0})\\approx {\\frac {D}{c}}H(t_{0}),} According to this approach, the relation cz = vr is an approximation valid at low redshifts, to be replaced by a relation at large redshifts that is model-dependent. See velocity-redshift figure. Strictly speaking, neither v nor D in the formula are directly observable, because they are properties now of a galaxy, whereas our observations refer to the galaxy in the past, at the time that the light we currently see left it. For relatively nearby galaxies (redshift z much less than one), v and D will not have changed much, and v can be estimated using the formula v = zc where c is the speed of light. This gives the empirical relation found by Hubble. For distant galaxies, v (or D) cannot be calculated from z without specifying a detailed model for how H changes with time. The redshift is not even directly related to the"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_12",
    "chunk": "recession velocity at the time the light set out, but it does have a simple interpretation: (1 + z) is the factor by which the universe has expanded while the photon was traveling towards the observer. In using Hubble's law to determine distances, only the velocity due to the expansion of the universe can be used. Since gravitationally interacting galaxies move relative to each other independent of the expansion of the universe, these relative velocities, called peculiar velocities, need to be accounted for in the application of Hubble's law. Such peculiar velocities give rise to redshift-space distortions. The parameter H is commonly called the \"Hubble constant\", but that is a misnomer since it is constant in space only at a fixed time; it varies with time in nearly all cosmological models, and all observations of far distant objects are also observations into the distant past, when the \"constant\" had a different value. \"Hubble parameter\" is a more correct term, with H0 denoting the present-day value. Another common source of confusion is that the accelerating universe does not imply that the Hubble parameter is actually increasing with time; since H ( t ) ≡ a ˙ ( t ) / a"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_13",
    "chunk": "( t ) {\\displaystyle H(t)\\equiv {\\dot {a}}(t)/a(t)} , in most accelerating models a {\\displaystyle a} increases relatively faster than a ˙ {\\displaystyle {\\dot {a}}} , so H decreases with time. (The recession velocity of one chosen galaxy does increase, but different galaxies passing a sphere of fixed radius cross the sphere more slowly at later times.) On defining the dimensionless deceleration parameter q ≡ − a ¨ a a ˙ 2 {\\textstyle q\\equiv -{\\frac {{\\ddot {a}}\\,a}{{\\dot {a}}^{2}}}} , it follows that d H d t = − H 2 ( 1 + q ) {\\displaystyle {\\frac {dH}{dt}}=-H^{2}(1+q)} From this it is seen that the Hubble parameter is decreasing with time, unless q < -1; the latter can only occur if the universe contains phantom energy, regarded as theoretically somewhat improbable. However, in the standard Lambda cold dark matter model (Lambda-CDM or ΛCDM model), q will tend to −1 from above in the distant future as the cosmological constant becomes increasingly dominant over matter; this implies that H will approach from above to a constant value of ≈ 57 (km/s)/Mpc, and the scale factor of the universe will then grow exponentially in time. The mathematical derivation of an idealized Hubble's law"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_14",
    "chunk": "for a uniformly expanding universe is a fairly elementary theorem of geometry in 3-dimensional Cartesian/Newtonian coordinate space, which, considered as a metric space, is entirely homogeneous and isotropic (properties do not vary with location or direction). Simply stated, the theorem is this: Any two points which are moving away from the origin, each along straight lines and with speed proportional to distance from the origin, will be moving away from each other with a speed proportional to their distance apart. In fact, this applies to non-Cartesian spaces as long as they are locally homogeneous and isotropic, specifically to the negatively and positively curved spaces frequently considered as cosmological models (see shape of the universe). An observation stemming from this theorem is that seeing objects recede from us on Earth is not an indication that Earth is near to a center from which the expansion is occurring, but rather that every observer in an expanding universe will see objects receding from them. The value of the Hubble parameter changes over time, either increasing or decreasing depending on the value of the so-called deceleration parameter q, which is defined by q = − ( 1 + H ˙ H 2 ) ."
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_15",
    "chunk": "{\\displaystyle q=-\\left(1+{\\frac {\\dot {H}}{H^{2}}}\\right).} In a universe with a deceleration parameter equal to zero, it follows that H = 1/t, where t is the time since the Big Bang. A non-zero, time-dependent value of q simply requires integration of the Friedmann equations backwards from the present time to the time when the comoving horizon size was zero. It was long thought that q was positive, indicating that the expansion is slowing down due to gravitational attraction. This would imply an age of the universe less than 1/H (which is about 14 billion years). For instance, a value for q of 1/2 (once favoured by most theorists) would give the age of the universe as 2/(3H). The discovery in 1998 that q is apparently negative means that the universe could actually be older than 1/H. However, estimates of the age of the universe are very close to 1/H. The expansion of space summarized by the Big Bang interpretation of Hubble's law is relevant to the old conundrum known as Olbers' paradox: If the universe were infinite in size, static, and filled with a uniform distribution of stars, then every line of sight in the sky would end on a star, and"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_16",
    "chunk": "the sky would be as bright as the surface of a star. However, the night sky is largely dark. Since the 17th century, astronomers and other thinkers have proposed many possible ways to resolve this paradox, but the currently accepted resolution depends in part on the Big Bang theory, and in part on the Hubble expansion: in a universe that existed for a finite amount of time, only the light of a finite number of stars has had enough time to reach us, and the paradox is resolved. Additionally, in an expanding universe, distant objects recede from us, which causes the light emanated from them to be redshifted and diminished in brightness by the time we see it. Instead of working with Hubble's constant, a common practice is to introduce the dimensionless Hubble constant, usually denoted by h and commonly referred to as \"little h\", then to write Hubble's constant H0 as h × 100 km⋅s⋅Mpc, all the relative uncertainty of the true value of H0 being then relegated to h. The dimensionless Hubble constant is often used when giving distances that are calculated from redshift z using the formula d ≈ ⁠c/H0⁠ × z. Since H0 is not precisely"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_17",
    "chunk": "known, the distance is expressed as: c z / H 0 ≈ ( 2998 × z ) Mpc h − 1 {\\displaystyle cz/H_{0}\\approx (2998\\times z){\\text{ Mpc }}h^{-1}} In other words, one calculates 2998 × z and one gives the units as Mpc h or h Mpc. Occasionally a reference value other than 100 may be chosen, in which case a subscript is presented after h to avoid confusion; e.g. h70 denotes H0 = 70 h70 (km/s)/Mpc, which implies h70 = h / 0.7. This should not be confused with the dimensionless value of Hubble's constant, usually expressed in terms of Planck units, obtained by multiplying H0 by 1.75×10 (from definitions of parsec and tP), for example for H0 = 70, a Planck unit version of 1.2×10 is obtained. A value for q measured from standard candle observations of Type Ia supernovae, which was determined in 1998 to be negative, surprised many astronomers with the implication that the expansion of the universe is currently \"accelerating\" (although the Hubble factor is still decreasing with time, as mentioned above in the Interpretation section; see the articles on dark energy and the ΛCDM model). H 2 ≡ ( a ˙ a ) 2 ="
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_18",
    "chunk": "8 π G 3 ρ − k c 2 a 2 + Λ c 2 3 , {\\displaystyle H^{2}\\equiv \\left({\\frac {\\dot {a}}{a}}\\right)^{2}={\\frac {8\\pi G}{3}}\\rho -{\\frac {kc^{2}}{a^{2}}}+{\\frac {\\Lambda c^{2}}{3}},} where H is the Hubble parameter, a is the scale factor, G is the gravitational constant, k is the normalised spatial curvature of the universe and equal to −1, 0, or 1, and Λ is the cosmological constant. If the universe is matter-dominated, then the mass density of the universe ρ can be taken to include just matter so ρ = ρ m ( a ) = ρ m 0 a 3 , {\\displaystyle \\rho =\\rho _{m}(a)={\\frac {\\rho _{m_{0}}}{a^{3}}},} where ρm0 is the density of matter today. From the Friedmann equation and thermodynamic principles we know for non-relativistic particles that their mass density decreases proportional to the inverse volume of the universe, so the equation above must be true. We can also define (see density parameter for Ωm) ρ c = 3 H 0 2 8 π G ; Ω m ≡ ρ m 0 ρ c = 8 π G 3 H 0 2 ρ m 0 ; {\\displaystyle {\\begin{aligned}\\rho _{c}&={\\frac {3H_{0}^{2}}{8\\pi G}};\\\\\\Omega _{m}&\\equiv {\\frac {\\rho _{m_{0}}}{\\rho _{c}}}={\\frac {8\\pi G}{3H_{0}^{2}}}\\rho _{m_{0}};\\end{aligned}}}"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_19",
    "chunk": "ρ = ρ c Ω m a 3 . {\\displaystyle \\rho ={\\frac {\\rho _{c}\\Omega _{m}}{a^{3}}}.} Also, by definition, Ω k ≡ − k c 2 ( a 0 H 0 ) 2 Ω Λ ≡ Λ c 2 3 H 0 2 , {\\displaystyle {\\begin{aligned}\\Omega _{k}&\\equiv {\\frac {-kc^{2}}{(a_{0}H_{0})^{2}}}\\\\\\Omega _{\\Lambda }&\\equiv {\\frac {\\Lambda c^{2}}{3H_{0}^{2}}},\\end{aligned}}} where the subscript 0 refers to the values today, and a0 = 1. Substituting all of this into the Friedmann equation at the start of this section and replacing a with a = 1/(1+z) gives H 2 ( z ) = H 0 2 ( Ω m ( 1 + z ) 3 + Ω k ( 1 + z ) 2 + Ω Λ ) . {\\displaystyle H^{2}(z)=H_{0}^{2}\\left(\\Omega _{m}(1+z)^{3}+\\Omega _{k}(1+z)^{2}+\\Omega _{\\Lambda }\\right).} If the universe is both matter-dominated and dark energy-dominated, then the above equation for the Hubble parameter will also be a function of the equation of state of dark energy. So now: ρ = ρ m ( a ) + ρ d e ( a ) , {\\displaystyle \\rho =\\rho _{m}(a)+\\rho _{de}(a),} where ρde is the mass density of the dark energy. By definition, an equation of state in cosmology is P = wρc,"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_20",
    "chunk": "and if this is substituted into the fluid equation, which describes how the mass density of the universe evolves with time, then ρ ˙ + 3 a ˙ a ( ρ + P c 2 ) = 0 ; d ρ ρ = − 3 d a a ( 1 + w ) . {\\displaystyle {\\begin{aligned}{\\dot {\\rho }}+3{\\frac {\\dot {a}}{a}}\\left(\\rho +{\\frac {P}{c^{2}}}\\right)=0;\\\\{\\frac {d\\rho }{\\rho }}=-3{\\frac {da}{a}}(1+w).\\end{aligned}}} ln ⁡ ρ = − 3 ( 1 + w ) ln ⁡ a ; {\\displaystyle \\ln {\\rho }=-3(1+w)\\ln {a};} Therefore, for dark energy with a constant equation of state w, ρ d e ( a ) = ρ d e 0 a − 3 ( 1 + w ) {\\displaystyle \\rho _{de}(a)=\\rho _{de0}a^{-3(1+w)}} . If this is substituted into the Friedman equation in a similar way as before, but this time set k = 0, which assumes a spatially flat universe, then (see shape of the universe) H 2 ( z ) = H 0 2 ( Ω m ( 1 + z ) 3 + Ω d e ( 1 + z ) 3 ( 1 + w ) ) . {\\displaystyle H^{2}(z)=H_{0}^{2}\\left(\\Omega _{m}(1+z)^{3}+\\Omega _{de}(1+z)^{3(1+w)}\\right).} If the dark energy derives from a"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_21",
    "chunk": "cosmological constant such as that introduced by Einstein, it can be shown that w = −1. The equation then reduces to the last equation in the matter-dominated universe section, with Ωk set to zero. In that case the initial dark energy density ρde0 is given by ρ d e 0 = Λ c 2 8 π G , Ω d e = Ω Λ . {\\displaystyle {\\begin{aligned}\\rho _{de0}&={\\frac {\\Lambda c^{2}}{8\\pi G}}\\,,\\\\\\Omega _{de}&=\\Omega _{\\Lambda }.\\end{aligned}}} ρ d e ( a ) = ρ d e 0 e − 3 ∫ d a a ( 1 + w ( a ) ) , {\\displaystyle \\rho _{de}(a)=\\rho _{de0}e^{-3\\int {\\frac {da}{a}}\\left(1+w(a)\\right)},} and to solve this, w(a) must be parametrized, for example if w(a) = w0 + wa(1−a), giving H 2 ( z ) = H 0 2 ( Ω m a − 3 + Ω d e a − 3 ( 1 + w 0 + w a ) e − 3 w a ( 1 − a ) ) . {\\displaystyle H^{2}(z)=H_{0}^{2}\\left(\\Omega _{m}a^{-3}+\\Omega _{de}a^{-3\\left(1+w_{0}+w_{a}\\right)}e^{-3w_{a}(1-a)}\\right).} The Hubble constant H0 has units of inverse time; the Hubble time tH is simply defined as the inverse of the Hubble constant, i.e. t H ≡ 1 H"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_22",
    "chunk": "0 = 1 67.8 ( k m / s ) / M p c = 4.55 × 10 17 s = 14.4 billion years . {\\displaystyle t_{H}\\equiv {\\frac {1}{H_{0}}}={\\frac {1}{67.8\\mathrm {~(km/s)/Mpc} }}=4.55\\times 10^{17}\\mathrm {~s} =14.4{\\text{ billion years}}.} This is slightly different from the age of the universe, which is approximately 13.8 billion years. The Hubble time is the age it would have had if the expansion had been linear, and it is different from the real age of the universe because the expansion is not linear; it depends on the energy content of the universe (see § Derivation of the Hubble parameter). We currently appear to be approaching a period where the expansion of the universe is exponential due to the increasing dominance of vacuum energy. In this regime, the Hubble parameter is constant, and the universe grows by a factor e each Hubble time: H ≡ a ˙ a = constant ⟹ a ∝ e H t = e t t H {\\displaystyle H\\equiv {\\frac {\\dot {a}}{a}}={\\textrm {constant}}\\quad \\Longrightarrow \\quad a\\propto e^{Ht}=e^{\\frac {t}{t_{H}}}} Likewise, the generally accepted value of 2.27 Es means that (at the current rate) the universe would grow by a factor of e in one exasecond."
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_23",
    "chunk": "Over long periods of time, the dynamics are complicated by general relativity, dark energy, inflation, etc., as explained above. The Hubble length or Hubble distance is a unit of distance in cosmology, defined as cH — the speed of light multiplied by the Hubble time. It is equivalent to 4,420 million parsecs or 14.4 billion light years. (The numerical value of the Hubble length in light years is, by definition, equal to that of the Hubble time in years.) Substituting D = cH into the equation for Hubble's law, v = H0D reveals that the Hubble distance specifies the distance from our location to those galaxies which are currently receding from us at the speed of light. The Hubble volume is sometimes defined as a volume of the universe with a comoving size of cH. The exact definition varies: it is sometimes defined as the volume of a sphere with radius cH, or alternatively, a cube of side cH. Some cosmologists even use the term Hubble volume to refer to the volume of the observable universe, although this has a radius approximately three times larger. The value of the Hubble constant, H0, cannot be measured directly, but is derived from"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_24",
    "chunk": "a combination of astronomical observations and model-dependent assumptions. Increasingly accurate observations and new models over many decades have led to two sets of highly precise values which do not agree. This difference is known as the \"Hubble tension\". For the original 1929 estimate of the constant now bearing his name, Hubble used observations of Cepheid variable stars as \"standard candles\" to measure distance. The result he obtained was 500 (km/s)/Mpc, much larger than the value astronomers currently calculate. Later observations by astronomer Walter Baade led him to realize that there were distinct \"populations\" for stars (Population I and Population II) in a galaxy. The same observations led him to discover that there are two types of Cepheid variable stars with different luminosities. Using this discovery, he recalculated Hubble constant and the size of the known universe, doubling the previous calculation made by Hubble in 1929. He announced this finding to considerable astonishment at the 1952 meeting of the International Astronomical Union in Rome. For most of the second half of the 20th century, the value of H0 was estimated to be between 50 and 90 (km/s)/Mpc. The value of the Hubble constant was the topic of a long and rather"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_25",
    "chunk": "bitter controversy between Gérard de Vaucouleurs, who claimed the value was around 100, and Allan Sandage, who claimed the value was near 50. In one demonstration of vitriol shared between the parties, when Sandage and Gustav Andreas Tammann (Sandage's research colleague) formally acknowledged the shortcomings of confirming the systematic error of their method in 1975, Vaucouleurs responded \"It is unfortunate that this sober warning was so soon forgotten and ignored by most astronomers and textbook writers\". In 1996, a debate moderated by John Bahcall between Sidney van den Bergh and Gustav Tammann was held in similar fashion to the earlier Shapley–Curtis debate over these two competing values. This previously wide variance in estimates was partially resolved with the introduction of the ΛCDM model of the universe in the late 1990s. Incorporating the ΛCDM model, observations of high-redshift clusters at X-ray and microwave wavelengths using the Sunyaev–Zel'dovich effect, measurements of anisotropies in the cosmic microwave background radiation, and optical surveys all gave a value of around 50–70 km/s/Mpc for the constant. By the late 1990s, advances in ideas and technology allowed higher precision measurements. However, two major categories of methods, each with high precision, fail to agree. \"Late universe\" measurements using"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_26",
    "chunk": "calibrated distance ladder techniques have converged on a value of approximately 73 (km/s)/Mpc. Since 2000, \"early universe\" techniques based on measurements of the cosmic microwave background have become available, and these agree on a value near 67.7 (km/s)/Mpc. (This accounts for the change in the expansion rate since the early universe, so is comparable to the first number.) Initially, this discrepancy was within the estimated measurement uncertainties and thus no cause for concern. However, as techniques have improved, the estimated measurement uncertainties have shrunk, but the discrepancies have not, to the point that the disagreement is now highly statistically significant. This discrepancy is called the Hubble tension. An example of an \"early\" measurement, the Planck mission published in 2018 gives a value for H0 = of 67.4±0.5 (km/s)/Mpc. In the \"late\" camp is the higher value of 74.03±1.42 (km/s)/Mpc determined by the Hubble Space Telescope and confirmed by the James Webb Space Telescope in 2023. The \"early\" and \"late\" measurements disagree at the >5 σ level, beyond a plausible level of chance. The resolution to this disagreement is an ongoing area of active research. Since 2013 much effort has gone in to new measurements to check for possible systematic errors"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_27",
    "chunk": "and improved reproducibility. The \"late universe\" or distance ladder measurements typically employ three stages or \"rungs\". In the first rung distances to Cepheids are determined while trying to reduce luminosity errors from dust and correlations of metallicity with luminosity. The second rung uses Type Ia supernova, explosions of almost constant amount of mass and thus very similar amounts of light; the primary source of systematic error is the limited number of objects that can be observed. The third rung of the distance ladder measures the red-shift of supernova to extract the Hubble flow and from that the constant. At this rung corrections due to motion other than expansion are applied. As an example of the kind of work needed to reduce systematic errors, photometry on observations from the James Webb Space Telescope of extra-galactic Cepheids confirm the findings from the HST. The higher resolution avoided confusion from crowding of stars in the field of view but came to the same value for H0. The \"early universe\" or inverse distance ladder measures the observable consequences of spherical sound waves on primordial plasma density. These pressure waves – called baryon acoustic oscillations (BAO) – cease once the universe cooled enough for electrons"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_28",
    "chunk": "to stay bound to nuclei, ending the plasma and allowing the photons trapped by interaction with the plasma to escape. The pressure waves then become very small perturbations in density imprinted on the cosmic microwave background and on the large scale density of galaxies across the sky. Detailed structure in high precision measurements of the CMB can be matched to physics models of the oscillations. These models depend upon the Hubble constant such that a match reveals a value for the constant. Similarly, the BAO affects the statistical distribution of matter, observed as distant galaxies across the sky. These two independent kinds of measurements produce similar values for the constant from the current models, giving strong evidence that systematic errors in the measurements themselves do not affect the result. In addition to measurements based on calibrated distance ladder techniques or measurements of the CMB, other methods have been used to determine the Hubble constant. One alternative method for constraining the Hubble constant involves transient events seen in multiple images of a strongly lensed object. A transient event, such as a supernova, is seen at different times in each of the lensed images, and if this time delay between each image"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_29",
    "chunk": "can be measured, it can be used to constrain the Hubble constant. This method is commonly known as \"time-delay cosmography\", and was first proposed by Refsdal in 1964, years before the first strongly lensed object was observed. The first strongly lensed supernova to be discovered was named SN Refsdal in his honor. While Refsdal suggested this could be done with supernovae, he also noted that extremely luminous and distant star-like objects could also be used. These objects were later named quasars, and to date (April 2025) the majority of time-delay cosmography measurements have been done with strongly lensed quasars. This is because current samples of lensed quasars vastly outnumber known lensed supernovae, of which <10 are known. This is expected to change dramatically in the next few years, with surveys such as LSST expected to discover ~10 lensed SNe in the first three years of observation. For example time-delay constraints on H0, see the results from STRIDES and H0LiCOW in the table below. In October 2018, scientists used information from gravitational wave events (especially those involving the merger of neutron stars, like GW170817), of determining the Hubble constant. In July 2019, astronomers reported that a new method to determine the"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_30",
    "chunk": "Hubble constant, and resolve the discrepancy of earlier methods, has been proposed based on the mergers of pairs of neutron stars, following the detection of the neutron star merger of GW170817, an event known as a dark siren. Their measurement of the Hubble constant is 73.3+5.3−5.0 (km/s)/Mpc. Also in July 2019, astronomers reported another new method, using data from the Hubble Space Telescope and based on distances to red giant stars calculated using the tip of the red-giant branch (TRGB) distance indicator. Their measurement of the Hubble constant is 69.8+1.9−1.9 (km/s)/Mpc. In February 2020, the Megamaser Cosmology Project published independent results based on astrophysical masers visible at cosmological distances and which do not require multi-step calibration. That work confirmed the distance ladder results and differed from the early-universe results at a statistical significance level of 95%. In July 2020, measurements of the cosmic background radiation by the Atacama Cosmology Telescope predict that the Universe should be expanding more slowly than is currently observed. In July 2023, an independent estimate of the Hubble constant was derived from a kilonova, the optical afterglow of a neutron star merger, using the expanding photosphere method. Due to the blackbody nature of early kilonova spectra,"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_31",
    "chunk": "such systems provide strongly constraining estimators of cosmic distance. Using the kilonova AT2017gfo (the aftermath of, once again, GW170817), these measurements indicate a local-estimate of the Hubble constant of 67.0±3.6 (km/s)/Mpc. The cause of the Hubble tension is unknown, and there are many possible proposed solutions. The most conservative is that there is an unknown systematic error affecting either early-universe or late-universe observations. Although intuitively appealing, this explanation requires multiple unrelated effects regardless of whether early-universe or late-universe observations are incorrect, and there are no obvious candidates. Furthermore, any such systematic error would need to affect multiple different instruments, since both the early-universe and late-universe observations come from several different telescopes. Alternatively, it could be that the observations are correct, but some unaccounted-for effect is causing the discrepancy. If the cosmological principle fails (see Lambda-CDM model § Violations of the cosmological principle), then the existing interpretations of the Hubble constant and the Hubble tension have to be revised, which might resolve the Hubble tension. In particular, we would need to be located within a very large void, up to about a redshift of 0.5, for such an explanation to conflate with supernovae and baryon acoustic oscillation observations. Yet another possibility"
  },
  {
    "source": "Hubble's law.txt",
    "chunk_id": "Hubble's law.txt_32",
    "chunk": "is that the uncertainties in the measurements could have been underestimated, but given the internal agreements this is neither likely, nor resolves the overall tension. Finally, another possibility is new physics beyond the currently accepted cosmological model of the universe, the ΛCDM model. There are very many theories in this category, for example, replacing general relativity with a modified theory of gravity could potentially resolve the tension, as can a dark energy component in the early universe, dark energy with a time-varying equation of state, or dark matter that decays into dark radiation. A problem faced by all these theories is that both early-universe and late-universe measurements rely on multiple independent lines of physics, and it is difficult to modify any of those lines while preserving their successes elsewhere. The scale of the challenge can be seen from how some authors have argued that new early-universe physics alone is not sufficient; while other authors argue that new late-universe physics alone is also not sufficient. Nonetheless, astronomers are trying, with interest in the Hubble tension growing strongly since the mid 2010s."
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_0",
    "chunk": "# Hydrogen Hydrogen is a chemical element; it has symbol H and atomic number 1. It is the lightest and most abundant chemical element in the universe, constituting about 75% of all normal matter. Stars, including the Sun, mainly consist of hydrogen in a plasma state, while on Earth, hydrogen is found in water and organic compounds as the gas H2 (dihydrogen), and in other molecular forms. The most common isotope of hydrogen (H) consists of one proton, one electron, and no neutrons. Under standard conditions, hydrogen is a gas of diatomic molecules with the formula H2, called dihydrogen, or sometimes hydrogen gas, molecular hydrogen, or simply hydrogen. Dihydrogen is colorless, odorless, non-toxic, and highly combustible. Hydrogen gas was first produced artificially in the 17th century by the reaction of acids with metals. Henry Cavendish, in 1766–1781, identified hydrogen gas as a distinct substance and discovered its property of producing water when burned; hence its name means 'water-former' in Greek. Understanding the colors of light absorbed and emitted by hydrogen was a crucial part of developing quantum mechanics. Hydrogen, typically nonmetallic except under extreme pressure, readily forms covalent bonds with most nonmetals, contributing to the formation of compounds like water"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_1",
    "chunk": "and various organic substances. Its role is crucial in acid-base reactions, which mainly involve proton exchange among soluble molecules. In ionic compounds, hydrogen can take the form of either a negatively charged anion, where it is known as hydride, or as a positively charged cation, H, called a proton. Although tightly bonded to water molecules, protons strongly affect the behavior of aqueous solutions, as reflected in the importance of pH. Hydride, on the other hand, is rarely observed because it tends to deprotonate solvents, yielding H2. In the early universe, neutral hydrogen atoms formed about 370,000 years after the Big Bang as the universe expanded and plasma had cooled enough for electrons to remain bound to protons. Once stars formed most of the atoms in the intergalactic medium re-ionized. Nearly all hydrogen production is done by transforming fossil fuels, particularly steam reforming of natural gas. It can also be produced from electricity by electrolysis, however this process is more expensive. Its main industrial uses include fossil fuel processing and ammonia production for fertilizer. Emerging uses for hydrogen include the use of fuel cells to generate electricity. The ground state energy level of the electron in a hydrogen atom is −13.6"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_2",
    "chunk": "eV, equivalent to an ultraviolet photon of roughly 91 nm wavelength. The energy levels of hydrogen are referred to by consecutive quantum numbers, with n = 1 {\\displaystyle n=1} being the ground state. The hydrogen spectral series corresponds to emission of light due to transitions from higher to lower energy levels. Each energy level is further split by spin interactions between the electron and proton into 4 hyperfine levels. High precision values for the hydrogen atom energy levels are required for definitions of physical constants. Quantum calculations have identified 9 contributions to the energy levels. The eigenvalue from the Dirac equation is the largest contribution. Other terms include relativistic recoil, the self-energy, and the vacuum polarization terms. Hydrogen has three naturally occurring isotopes, denoted H, H and H. Other, highly unstable nuclei (H to H) have been synthesized in the laboratory but not observed in nature. H is the most common hydrogen isotope, with an abundance of >99.98%. Because the nucleus of this isotope consists of only a single proton, it is given the descriptive but rarely used formal name protium. It is the only stable isotope with no neutrons; see diproton for a discussion of why others do not"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_3",
    "chunk": "exist. H, the other stable hydrogen isotope, is known as deuterium and contains one proton and one neutron in the nucleus. Nearly all deuterium nuclei in the universe is thought to have been produced at the time of the Big Bang, and has endured since then. Deuterium is not radioactive, and is not a significant toxicity hazard. Water enriched in molecules that include deuterium instead of normal hydrogen is called heavy water. Deuterium and its compounds are used as a non-radioactive label in chemical experiments and in solvents for H-NMR spectroscopy. Heavy water is used as a neutron moderator and coolant for nuclear reactors. Deuterium is also a potential fuel for commercial nuclear fusion. H is known as tritium and contains one proton and two neutrons in its nucleus. It is radioactive, decaying into helium-3 through beta decay with a half-life of 12.32 years. It is radioactive enough to be used in luminous paint to enhance the visibility of data displays, such as for painting the hands and dial-markers of watches. The watch glass prevents the small amount of radiation from escaping the case. Small amounts of tritium are produced naturally by cosmic rays striking atmospheric gases; tritium has also"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_4",
    "chunk": "been released in nuclear weapons tests. It is used in nuclear fusion, as a tracer in isotope geochemistry, and in specialized self-powered lighting devices. Tritium has also been used in chemical and biological labeling experiments as a radiolabel. Unique among the elements, distinct names are assigned to its isotopes in common use. During the early study of radioactivity, heavy radioisotopes were given their own names, but these are mostly no longer used. The symbols D and T (instead of H and H) are sometimes used for deuterium and tritium, but the symbol P was already used for phosphorus and thus was not available for protium. In its nomenclatural guidelines, the International Union of Pure and Applied Chemistry (IUPAC) allows any of D, T, H, and H to be used, though H and H are preferred. Antihydrogen (H) is the antimatter counterpart to hydrogen. It consists of an antiproton with a positron. Antihydrogen is the only type of antimatter atom to have been produced as of 2015. The exotic atom muonium (symbol Mu), composed of an antimuon and an electron, is analogous hydrogen and IUPAC nomenclature incorporates such hypothetical compounds as muonium chloride (MuCl) and sodium muonide (NaMu), analogous to hydrogen"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_5",
    "chunk": "chloride and sodium hydride respectively. Under standard conditions, hydrogen is a gas of diatomic molecules with the formula H2, officially called \"dihydrogen\", but also called \"molecular hydrogen\", or simply hydrogen. Dihydrogen is a colorless, odorless, flammable gas. Hydrogen gas is highly flammable, reacting with oxygen in air, to produce liquid water: The amount of heat released per mole of hydrogen is −286 kJ or 141.865 MJ for a kilogram mass. Hydrogen gas forms explosive mixtures with air in concentrations from 4–74% and with chlorine at 5–95%. The hydrogen autoignition temperature, the temperature of spontaneous ignition in air, is 500 °C (932 °F). In a high-pressure hydrogen leak, the shock wave from the leak itself can heat air to the autoignition temperature, leading to flaming and possibly explosion. Hydrogen flames emit faint blue and ultraviolet light. Flame detectors are used to detect hydrogen fires as they are nearly invisible to the naked eye in daylight. Molecular H2 exists as two nuclear isomers that differ in the spin states of their nuclei. In the orthohydrogen form, the spins of the two nuclei are parallel, forming a spin triplet state having a total molecular spin S = 1 {\\displaystyle S=1} ; in the"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_6",
    "chunk": "parahydrogen form the spins are antiparallel and form a spin singlet state having spin S = 0 {\\displaystyle S=0} . The equilibrium ratio of ortho- to para-hydrogen depends on temperature. At room temperature or warmer, equilibrium hydrogen gas contains about 25% of the para form and 75% of the ortho form. The ortho form is an excited state, having higher energy than the para form by 1.455 kJ/mol, and it converts to the para form over the course of several minutes when cooled to low temperature. The thermal properties of these isomers differ because each has distinct rotational quantum states. The ortho-to-para ratio in H2 is an important consideration in the liquefaction and storage of liquid hydrogen: the conversion from ortho to para is exothermic and produces sufficient heat to evaporate most of the liquid if not converted first to parahydrogen during the cooling process. Catalysts for the ortho-para interconversion, such as ferric oxide and activated carbon compounds, are used during hydrogen cooling to avoid this loss of liquid. Liquid hydrogen can exist at temperatures below hydrogen's critical point of 33 K. However, for it to be in a fully liquid state at atmospheric pressure, H2 needs to be cooled"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_7",
    "chunk": "to 20.28 K (−252.87 °C; −423.17 °F). Hydrogen was liquefied by James Dewar in 1898 by using regenerative cooling and his invention, the vacuum flask. Liquid hydrogen becomes solid hydrogen at standard pressure below hydrogen's melting point of 14.01 K (−259.14 °C; −434.45 °F). Distinct solid phases exist, known as Phase I through Phase V, each exhibiting a characteristic molecular arrangement. Liquid and solid phases can exist in combination at the triple point, a substance known as slush hydrogen. Metallic hydrogen, a phase obtained at extremely high pressures (in excess of 400 gigapascals (3,900,000 atm; 58,000,000 psi)), is an electrical conductor. It is believed to exist deep within giant planets like Jupiter. When ionized, hydrogen becomes a plasma. This is the form in which hydrogen exists within stars. In 1671, Irish scientist Robert Boyle discovered and described the reaction between iron filings and dilute acids, which results in the production of hydrogen gas. Boyle did not note that the gas was inflammable, but hydrogen would play a key role in overturning the phlogiston theory of combustion. In 1766, Henry Cavendish was the first to recognize hydrogen gas as a discrete substance, by naming the gas from a metal-acid reaction \"inflammable"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_8",
    "chunk": "air\". He speculated that \"inflammable air\" was in fact identical to the hypothetical substance \"phlogiston\" and further finding in 1781 that the gas produces water when burned. He is usually given credit for the discovery of hydrogen as an element. In 1783, Antoine Lavoisier identified the element that came to be known as hydrogen when he and Laplace reproduced Cavendish's finding that water is produced when hydrogen is burned. Lavoisier produced hydrogen for his experiments on mass conservation by treating metallic iron with a stream of H2O through an incandescent iron tube heated in a fire. Anaerobic oxidation of iron by the protons of water at high temperature can be schematically represented by the set of following reactions: Many metals react similarly with water leading to the production of hydrogen. In some situations, this H2-producing process is problematic as is the case of zirconium cladding on nuclear fuel rods. By 1806 hydrogen was used to fill balloons. François Isaac de Rivaz built the first de Rivaz engine, an internal combustion engine powered by a mixture of hydrogen and oxygen in 1806. Edward Daniel Clarke invented the hydrogen gas blowpipe in 1819. The Döbereiner's lamp and limelight were invented in 1823."
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_9",
    "chunk": "Hydrogen was liquefied for the first time by James Dewar in 1898 by using regenerative cooling and his invention, the vacuum flask. He produced solid hydrogen the next year. One of the first quantum effects to be explicitly noticed (but not understood at the time) was James Clerk Maxwell's observation that the specific heat capacity of H2 unaccountably departs from that of a diatomic gas below room temperature and begins to increasingly resemble that of a monatomic gas at cryogenic temperatures. According to quantum theory, this behavior arises from the spacing of the (quantized) rotational energy levels, which are particularly wide-spaced in H2 because of its low mass. These widely spaced levels inhibit equal partition of heat energy into rotational motion in hydrogen at low temperatures. Diatomic gases composed of heavier atoms do not have such widely spaced levels and do not exhibit the same effect. The existence of the hydride anion was suggested by Gilbert N. Lewis in 1916 for group 1 and 2 salt-like compounds. In 1920, Moers electrolyzed molten lithium hydride (LiH), producing a stoichiometric quantity of hydrogen at the anode. Because of its simple atomic structure, consisting only of a proton and an electron, the hydrogen"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_10",
    "chunk": "atom, together with the spectrum of light produced from it or absorbed by it, has been central to the development of the theory of atomic structure. The energy levels of hydrogen can be calculated fairly accurately using the Bohr model of the atom, in which the electron \"orbits\" the proton, like how Earth orbits the Sun. However, the electron and proton are held together by electrostatic attraction, while planets and celestial objects are held by gravity. Due to the discretization of angular momentum postulated in early quantum mechanics by Bohr, the electron in the Bohr model can only occupy certain allowed distances from the proton, and therefore only certain allowed energies. Hydrogen's unique position as the only neutral atom for which the Schrödinger equation can be directly solved, has significantly contributed to the understanding of quantum mechanics through the exploration of its energetics. Furthermore, study of the corresponding simplicity of the hydrogen molecule and the corresponding cation H+2 brought understanding of the nature of the chemical bond, which followed shortly after the quantum mechanical treatment of the hydrogen atom had been developed in the mid-1920s. Because H2 is only 7% the density of air, it was once widely used as"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_11",
    "chunk": "a lifting gas in balloons and airships. The first hydrogen-filled balloon was invented by Jacques Charles in 1783. Hydrogen provided the lift for the first reliable form of air-travel following the 1852 invention of the first hydrogen-lifted airship by Henri Giffard. German count Ferdinand von Zeppelin promoted the idea of rigid airships lifted by hydrogen that later were called Zeppelins; the first of which had its maiden flight in 1900. Regularly scheduled flights started in 1910 and by the outbreak of World War I in August 1914, they had carried 35,000 passengers without a serious incident. Hydrogen-lifted airships in the form of blimps were used as observation platforms and bombers during the War II, especially on the US Eastern seaboard. The first non-stop transatlantic crossing was made by the British airship R34 in 1919 and regular passenger service resumed in the 1920s. Hydrogen was used in the Hindenburg airship, which caught fire over New Jersey on 6 May 1937. The hydrogen that filled the airship was ignited, possibly by static electricity, and burst into flames. Following this Hindenburg disaster, commercial hydrogen airship travel ceased. Hydrogen is still used, in preference to non-flammable but more expensive helium, as a lifting gas"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_12",
    "chunk": "for weather balloons. Deuterium was discovered in December 1931 by Harold Urey, and tritium was prepared in 1934 by Ernest Rutherford, Mark Oliphant, and Paul Harteck. Heavy water, which consists of deuterium in the place of regular hydrogen, was discovered by Urey's group in 1932. H2 is relatively unreactive. The thermodynamic basis of this low reactivity is the very strong H–H bond, with a bond dissociation energy of 435.7 kJ/mol. It does form coordination complexes called dihydrogen complexes. These species provide insights into the early steps in the interactions of hydrogen with metal catalysts. According to neutron diffraction, the metal and two H atoms form a triangle in these complexes. The H-H bond remains intact but is elongated. They are acidic. Although exotic on Earth, the H+3 ion is common in the universe. It is a triangular species, like the aforementioned dihydrogen complexes. It is known as protonated molecular hydrogen or the trihydrogen cation. Hydrogen reacts with chlorine to produce HCl and with bromine to produce HBr by a chain reaction. The reaction requires initiation. For example in the case of Br2, the diatomic molecule is broken into atoms, Br2 + (UV light) → 2Br. Propagating reactions consume hydrogen molecules"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_13",
    "chunk": "and produce HBr, as well as Br and H atoms: The addition of H2 to unsaturated organic compounds, such as alkenes and alkynes, is called hydrogenation. Even if the reaction is energitically favorable, it does not take place even at higher temperatures. In the presence of a catalyst like finely divided platinum or nickel, the reaction proceeds at room temperature. Most known compounds contain hydrogen, not as H2, but as covalently bonded H atoms. This interaction is the basis of organic chemistry and biochemistry. Hydrogen forms many compounds with carbon, called the hydrocarbons. Hydrocarbons are organic compounds. In nature, organic compounds almost always contain \"heteroatoms\" such as nitrogen, oxygen, and sulfur. The study of their properties is known as organic chemistry and their study in the context of living organisms is called biochemistry. By some definitions, \"organic\" compounds are only required to contain carbon. However, most of them also contain hydrogen, and because it is the carbon-hydrogen bond that gives this class of compounds most of its particular chemical characteristics, carbon-hydrogen bonds are required in some definitions of the word \"organic\" in chemistry. Hydrogen forms hydrides with many metals. The hydrides can be ionic (aka saline), covalent, nor metallic. With"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_14",
    "chunk": "heating, H2 reacts efficiently with the alkali and alkaline earth metals to give the ionic hydrides of the formula MH and MH2, respectively. These salt-like crystalline compounds have high melting points and all react with water to liberate hydrogen. Covalent hydrides are include boranes and polymeric aluminium hydride. Transition metals form metal hydrides via continuous dissolution of hydrogen into the metal. A well known hydride is lithium aluminium hydride, the [AlH4] anion carries hydridic centers firmly attached to the Al(III). Perhaps the most extensive series of hydrides are the boranes, compounds consisting only of boron and hydrogen. Hydrides can bond to these electropositive elements not only as a terminal ligand but also as bridging ligands. In diborane (B2H6), four H's are terminal and two bridge between the two B atoms. When bonded to a more electronegative element, particularly fluorine, oxygen, or nitrogen, hydrogen can participate in a form of medium-strength noncovalent bonding with another electronegative element with a lone pair like oxygen or nitrogen, a phenomenon called hydrogen bonding that is critical to the stability of many biological molecules. Hydrogen bonding alters molecule structures, viscosity, solubility, as well as melting and boiling points even protein folding dynamics. In water, hydrogen"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_15",
    "chunk": "bonding plays an important role in reaction thermodynamics. A hydrogen bond can shift over to proton transfer. Under the Brønsted–Lowry acid–base theory, acids are proton donors, while bases are proton acceptors. A bare proton, H essentially cannot exist in anything other than a vacuum. Otherwise it attaches to other atoms, ions, or molecules. Even species as inert as methane can be protonated. The term 'proton' is used loosely and metaphorically to refer to refer to solvated H\" without any implication that any single protons exist freely as a species. To avoid the implication of the naked proton in solution, acidic aqueous solutions are sometimes considered to contain the \"hydronium ion\" ([H3O]) or still more accurately, [H9O4]. Other oxonium ions are found when water is in acidic solution with other solvents. Hydrogen, as atomic H, is the most abundant chemical element in the universe, making up 75% of normal matter by mass and >90% by number of atoms. In the early universe, the protons formed in the first second after the Big Bang; neutral hydrogen atoms formed about 370,000 years later during the recombination epoch as the universe expanded and plasma had cooled enough for electrons to remain bound to protons."
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_16",
    "chunk": "In astrophysics, neutral hydrogen in the interstellar medium is called H I and ionized hydrogen is called H II. Radiation from stars ionizes H I to H II, creating spheres of ionized H II around stars. In the chronology of the universe neutral hydrogen dominated until the birth of stars during the era of reionization led to bubbles of ionized hydrogen that grew and merged over 500 million of years. They are the source of the 21-cm hydrogen line at 1420 MHz that is detected in order to probe primordial hydrogen. The large amount of neutral hydrogen found in the damped Lyman-alpha systems is thought to dominate the cosmological baryonic density of the universe up to a redshift of z = 4. Hydrogen is found in great abundance in stars and gas giant planets. Molecular clouds of H2 are associated with star formation. Hydrogen plays a vital role in powering stars through the proton-proton reaction in lower-mass stars, and through the CNO cycle of nuclear fusion in case of stars more massive than the Sun. A molecular form called protonated molecular hydrogen (H+3) is found in the interstellar medium, where it is generated by ionization of molecular hydrogen from cosmic"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_17",
    "chunk": "rays. This ion has also been observed in the upper atmosphere of Jupiter. The ion is long-lived in outer space due to the low temperature and density. H+3 is one of the most abundant ions in the universe, and it plays a notable role in the chemistry of the interstellar medium. Neutral triatomic hydrogen H3 can exist only in an excited form and is unstable. Hydrogen is the third most abundant element on the Earth's surface, mostly in the form of chemical compounds such as hydrocarbons and water. Elemental hydrogen is normally in the form of a gas, H2. It is present in a very low concentration in Earth's atmosphere (around 0.53 ppm on a molar basis) because of its light weight, which enables it to escape the atmosphere more rapidly than heavier gases. Despite its low concentration in our atmosphere, terrestrial hydrogen is sufficiently abundant to support the metabolism of several bacteria. Large underground deposits of hydrogen gas have been discovered in several countries including Mali, France and Australia. As of 2024, it is uncertain how much underground hydrogen can be extracted economically. Nearly all of the world's current supply of hydrogen gas (H2) is created from fossil fuels."
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_18",
    "chunk": "Many methods exist for producing H2, but three dominate commercially: steam reforming often coupled to water-gas shift, partial oxidation of hydrocarbons, and water electrolysis. Hydrogen is mainly produced by steam methane reforming (SMR), the reaction of water and methane. Thus, at high temperature (1000–1400 K, 700–1100 °C or 1300–2000 °F), steam (water vapor) reacts with methane to yield carbon monoxide and H2. Producing one tonne of hydrogen through this process emits 6.6–9.3 tonnes of carbon dioxide. The production of natural gas feedstock also produces emissions such as vented and fugitive methane, which further contributes to the overall carbon footprint of hydrogen. This reaction is favored at low pressures, Nonetheless, conducted at high pressures (2.0 MPa, 20 atm or 600 inHg) because high-pressure H2 is the most marketable product, and pressure swing adsorption (PSA) purification systems work better at higher pressures. The product mixture is known as \"synthesis gas\" because it is often used directly for the production of methanol and many other compounds. Hydrocarbons other than methane can be used to produce synthesis gas with varying product ratios. One of the many complications to this highly optimized technology is the formation of coke or carbon: Therefore, steam reforming typically employs"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_19",
    "chunk": "an excess of H2O. Additional hydrogen can be recovered from the steam by using carbon monoxide through the water gas shift reaction (WGS). This process requires an iron oxide catalyst: Hydrogen is sometimes produced and consumed in the same industrial process, without being separated. In the Haber process for ammonia production, hydrogen is generated from natural gas. Other methods for CO and H2 production include partial oxidation of hydrocarbons: Although less important commercially, coal can serve as a prelude to the shift reaction above: Olefin production units may produce substantial quantities of byproduct hydrogen particularly from cracking light feedstocks like ethane or propane. Commercial electrolyzers use nickel-based catalysts in strongly alkaline solution. Platinum is a better catalyst but is expensive. The hydrogen created through electrolysis using renewable energy is commonly referred to as \"green hydrogen\". Electrolysis of brine to yield chlorine also produces high purity hydrogen as a co-product, which is used for a variety of transformations such as hydrogenations. The electrolysis process is more expensive than producing hydrogen from methane without carbon capture and storage. Innovation in hydrogen electrolyzers could make large-scale production of hydrogen from electricity more cost-competitive. Hydrogen can be produced by pyrolysis of natural gas (methane),"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_20",
    "chunk": "producing hydrogen gas and solid carbon with the aid a catalyst and 74 kJ/mol input heat: The carbon may be sold as a manufacturing feedstock or fuel, or landfilled. This route could have a lower carbon footprint than existing hydrogen production processes, but mechanisms for removing the carbon and preventing it from reacting with the catalyst remain obstacles for industrial scale use. Water splitting is the process by which water is decomposed into its components. Relevant to the biological scenario is this simple equation: The reaction occurs in the light reactions in all photosynthetic organisms. A few organisms, including the alga Chlamydomonas reinhardtii and cyanobacteria, have evolved a second step in the dark reactions in which protons and electrons are reduced to form H2 gas by specialized hydrogenases in the chloroplast. Efforts have been undertaken to genetically modify cyanobacterial hydrogenases to more efficiently generate H2 gas even in the presence of oxygen. Efforts have also been undertaken with genetically modified alga in a bioreactor. More than 200 thermochemical cycles can be used for water splitting. Many of these cycles such as the iron oxide cycle, cerium(IV) oxide–cerium(III) oxide cycle, zinc zinc-oxide cycle, sulfur-iodine cycle, copper-chlorine cycle and hybrid sulfur cycle"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_21",
    "chunk": "have been evaluated for their commercial potential to produce hydrogen and oxygen from water and heat without using electricity. A number of labs (including in France, Germany, Greece, Japan, and the United States) are developing thermochemical methods to produce hydrogen from solar energy and water. H2 is produced by enzymes called hydrogenases. This process allows the host organism to use fermentation as a source of energy. These same enzymes also can oxidize H2, such that the host organisms can subsist by reducing oxidized substrates using electrons extracted from H2. The hydrogenase enzyme feature iron or nickel-iron centers at their active sites. The natural cycle of hydrogen production and consumption by organisms is called the hydrogen cycle. Some bacteria such as Mycobacterium smegmatis can use the small amount of hydrogen in the atmosphere as a source of energy when other sources are lacking. Their hydrogenase are designed with small channels that exclude oxygen and so permits the reaction to occur even though the hydrogen concentration is very low and the oxygen concentration is as in normal air. Confirming the existence of hydrogenases in the human gut, H2 occurs in human breath. The concentration in the breath of fasting people at rest"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_22",
    "chunk": "is typically less than 5 parts per million (ppm) but can be 50 ppm when people with intestinal disorders consume molecules they cannot absorb during diagnostic hydrogen breath tests. Serpentinization is a geological mechanism that produce highly reducing conditions. Under these conditions, water is capable of oxidizing ferrous (Fe) ions in fayalite, generating hydrogen gas: This process also is relevant to the corrosion of iron and steel in oxygen-free groundwater and in reducing soils below the water table. If H2 is to be used as an energy source, its storage is important. It dissolves only poorly in solvents. For example, at room temperature and 0.1 Mpascal, ca. 0.05 moles dissolves in one kilogram of diethyl ether. The H2 can be stored in compressed form, although compressing costs energy. Liquifaction is impractical given its low critical temperature. In contrast, ammonia and many hydrocarbons can be liquified at room temperature under pressure. For these reasons, hydrogen carriers - materials that reversibly bind H2 - have attracted much attention. The key question is then the weight percent of H2-equivalents within the carrier material. For example, hydrogen can be reversibly absorbed into many rare earth and transition metals and is soluble in both nanocrystalline"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_23",
    "chunk": "and amorphous metals. Hydrogen solubility in metals is influenced by local distortions or impurities in the crystal lattice. These properties may be useful when hydrogen is purified by passage through hot palladium disks, but the gas's high solubility is also a metallurgical problem, contributing to the embrittlement of many metals, complicating the design of pipelines and storage tanks. The most problematic aspect of metal hydrides for storage is their modest H2 content, often on the order of 1%. For this reason, there is interest in storage of H2 in compounds of low molecular weight. For example, ammonia borane (H3N−BH3) contains 19.8 weight percent of H2. The problem with this material is that after release of H2, the resulting boron nitride does not re-add H2, i.e. ammonia borane is an irreversible hydrogen carrier. More attractive, somewhat ironically, are hydrocarbons such as tetrahydroquinoline, which reversibly release some H2 when heated in the presence of a catalyst: Large quantities of H2 are used in the \"upgrading\" of fossil fuels. Key consumers of H2 include hydrodesulfurization, and hydrocracking. Many of these reactions can be classified as hydrogenolysis, i.e., the cleavage of bonds by hydrogen. Illustrative is the separation of sulfur from liquid fossil fuels:"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_24",
    "chunk": "Hydrogenation, the addition of H2 to various substrates, is done on a large scale. Hydrogenation of N2 to produce ammonia by the Haber process, consumes a few percent of the energy budget in the entire industry. The resulting ammonia is used in fertilizers critical to the supply of protein consumed by humans. Hydrogenation is used to convert unsaturated fats and oils to saturated fats and oils. The major application is the production of margarine. Methanol is produced by hydrogenation of carbon dioxide. It is similarly the source of hydrogen in the manufacture of hydrochloric acid. H2 is also used as a reducing agent for the conversion of some ores to the metals. Hydrogen is used in as a coolant in large power stations generators because it is 1/14th as dense as air but has 6.7 times the thermal conductivity. The reduced density reduces the internal wind resistance, improving efficiency by 1%, noise is reduced, and the cooling surface required for hydrogen is much smaller than air coolers due the high heat transfer. The first hydrogen-cooled turbogenerator went into service using gaseous hydrogen as a coolant in the rotor and the stator in 1937 at Dayton, Ohio, owned by the Dayton"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_25",
    "chunk": "Power & Light Co. The potential for using hydrogen (H2) as a fuel has been widely discussed. Hydrogen can be used in fuel cells to produce electricity, or burned to generate heat. When hydrogen is consumed in fuel cells, the only emission at the point of use is water vapor. When burned, hydrogen produces relatively little pollution at the point of combustion, but can lead to thermal formation of harmful nitrogen oxides. If hydrogen is produced with low or zero greenhouse gas emissions (green hydrogen), it can play a significant role in decarbonizing energy systems where there are challenges and limitations to replacing fossil fuels with direct use of electricity. Hydrogen fuel can produce the intense heat required for industrial production of steel, cement, glass, and chemicals, thus contributing to the decarbonization of industry alongside other technologies, such as electric arc furnaces for steelmaking. However, it is likely to play a larger role in providing industrial feedstock for cleaner production of ammonia and organic chemicals. For example, in steelmaking, hydrogen could function as a clean fuel and also as a low-carbon catalyst, replacing coal-derived coke (carbon): Hydrogen used to decarbonize transportation is likely to find its largest applications in shipping,"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_26",
    "chunk": "aviation and, to a lesser extent, heavy goods vehicles, through the use of hydrogen-derived synthetic fuels such as ammonia and methanol and fuel cell technology. For light-duty vehicles including cars, hydrogen is far behind other alternative fuel vehicles, especially compared with the rate of adoption of battery electric vehicles, and may not play a significant role in future. Liquid hydrogen and liquid oxygen together serve as cryogenic propellants in liquid-propellant rockets, as in the Space Shuttle main engines. NASA has investigated the use of rocket propellant made from atomic hydrogen, boron or carbon that is frozen into solid molecular hydrogen particles suspended in liquid helium. Upon warming, the mixture vaporizes to allow the atomic species to recombine, heating the mixture to high temperature. Hydrogen produced when there is a surplus of variable renewable electricity could in principle be stored and later used to generate heat or to re-generate electricity. It can be further transformed into synthetic fuels such as ammonia and methanol. Disadvantages of hydrogen fuel include high costs of storage and distribution due to hydrogen's explosivity, its large volume compared to other fuels, and its tendency to make pipes brittle. The very long-lived, rechargeable nickel–hydrogen battery developed for satellite"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_27",
    "chunk": "power systems uses pressurized gaseous H2. The International Space Station, Mars Odyssey and the Mars Global Surveyor are equipped with nickel-hydrogen batteries. In the dark part of its orbit, the Hubble Space Telescope is also powered by nickel-hydrogen batteries, which were finally replaced in May 2009, more than 19 years after launch and 13 years beyond their design life. Hydrogen is employed to saturate broken (\"dangling\") bonds of amorphous silicon and amorphous carbon that helps stabilizing material properties. Hydrogen, introduced as a unintended side-effect of production, acts as a shallow electron donor leading to n-type conductivity in ZnO, with important uses in transducers and phosphors. Detailed analysis of ZnO and of MgO show evidence of four and six-fold hydrogen multicentre bonds. The doping behavior of hydrogen varies with the material. In hydrogen pipelines and steel storage vessels, hydrogen molecules are prone to react with metals, causing hydrogen embrittlement and leaks in the pipeline or storage vessel. Since it is lighter than air, hydrogen does not easily accumulate to form a combustible gas mixture. However, even without ignition sources, high-pressure hydrogen leakage may cause spontaneous combustion and detonation. Hydrogen is flammable when mixed even in small amounts with air. Ignition can"
  },
  {
    "source": "Hydrogen.txt",
    "chunk_id": "Hydrogen.txt_28",
    "chunk": "occur at a volumetric ratio of hydrogen to air as low as 4%. In approximately 70% of hydrogen ignition accidents, the ignition source cannot be found, and it is widely believed by scholars that spontaneous ignition of hydrogen occurs. Hydrogen fire, while being extremely hot, is almost invisible, and thus can lead to accidental burns. Hydrogen is non-toxic, but like most gases it can cause asphyxiation in the absence of adequate ventilation."
  },
  {
    "source": "IM-2.txt",
    "chunk_id": "IM-2.txt_0",
    "chunk": "# IM-2 IM-2 was a lunar mission run by Intuitive Machines as part of NASA's Commercial Lunar Payload Services (CLPS) program. It was launched on 27 February 2025, at 00:16:30 UTC. The Nova-C lunar lander, named Athena, reached the surface of the Moon on 6 March 2025, at 17:28:50 UTC. Contact was temporarily lost during the landing process; when it was re-established, it indicated that the spacecraft was not in the correct orientation and one of the two radio antennas was not operating. The sideways orientation prevented the spacecraft from generating sufficient power. By 7 March, Athena's power had been fully depleted and was not expected to replenish, bringing the mission to its end. Athena was designed to investigate the presence and quantity of lunar water ice using PRIME-1, a payload of a drill and mass spectrometer. Athena also carried a drone that was equipped with a neutron spectrometer to explore the permanently shadowed region (PSR) of Marston crater near the landing site. The mission aimed to measure hydrogen in the PSR, looking for indications of solid water ice. NASA uses its CLPS program to partner with commercial providers to fly experiments to the Moon. A drilling mission was originally"
  },
  {
    "source": "IM-2.txt",
    "chunk_id": "IM-2.txt_1",
    "chunk": "set to launch in December 2022, for which Intuitive Machines supplied its Nova-C as the lander. This was the company's second CLPS contract. This IM-2 mission on 27 February 2025 was the fourth CLPS launch and was en route to the Moon when the Blue Ghost landing occurred. A CLPS contract was awarded to IM in October 2020 to land a second Nova-C lander near the lunar south pole. NASA designated the landing site at a ridge near the Shackleton crater, where there could be ice below the surface. After the rough landing of IM-1, several adjustments were made, including improvements to the primary laser rangefinder system, which helps determine variables such as altitude and horizontal velocity. The MiniPIX TPX3 SPACE payload, provided by the Czech company ADVACAM, was onboard the Nova-C lunar lander. This payload is designed to monitor the radiation field on the Moon and help understand how to protect crew and equipment from the negative effects of cosmic rays. This marks the first Czech payload planned to be delivered to the Moon's surface. During the mission, IM would deploy a second vehicle, its μNova (Micro-Nova) Hopper. Micro-Nova would separate from the Nova-C lander after landing and function"
  },
  {
    "source": "IM-2.txt",
    "chunk_id": "IM-2.txt_2",
    "chunk": "as a standalone hopper lander, exploring multiple difficult-to-reach areas such as deep craters on the lunar surface, by firing hydrazine rockets in controlled bursts to propel itself short distances. It would hop across craters in search of lunar ice, which could contain water critical to future crewed missions to the Moon. Water ice could be processed into rocket propellant or used to support a permanent lunar habitat in the future. Micro-Nova is also planned to take the first pictures from inside craters at the lunar south pole and would be able to carry a 1-kilogram payload for more than 25 kilometers. The hopper would explore permanently shaded regions and could \"fly into a lava tube and report images back\", according to IM co-founder and CTO Tim Crain. Space technology company Lunar Outpost would send their first lunar rover, the Mobile Autonomous Prospecting Platform (MAPP), on this mission in partnership with Nokia Bell Labs, Quantum Aerospace, and IM. MAPP would collect lunar samples for NASA under a contract worth just $1, which is symbolic of a new incentive for the emerging commercial space industry to access resources in space. MAPP would have a mass of 5–10 kilograms, a payload mass of"
  },
  {
    "source": "IM-2.txt",
    "chunk_id": "IM-2.txt_3",
    "chunk": "up to 15 kilograms, and a top speed of 10 cm/s. On its multi-day journey, the rover would autonomously map the lunar surface, capture stereo images and thermal data, and inspect samples of lunar regolith in a special bin mounted on its wheels. Photos of the samples and other data would be transmitted through radio equipment and antennas to communicate with the Nova-C lander. MAPP would snap 3D images and record videos using the RESOURCE camera, developed by MIT. It would also deploy MIT's AstroAnt, a miniature rover the size of a matchbox, to conduct contactless temperature measurements as it drives around on MAPP's roof. A collaboration in order to demonstrate 4G cellular connectivity, in partnership with Nokia Bell Labs and NASA was aboard the lander. Nokia's equipment was a Network-In-a-Box and would connect the Nova-C lander with Lunar Outpost's MAPP rover and IM's Micro-Nova Hopper. This 4G/LTE network would provide more bandwidth than the more conventional ultra-high frequency (UHF) systems used for space communication. Nokia says they hope that future missions would use shared infrastructure to interlink bases on the lunar surface. IM-2 carried the Moon rover Yaoki. It was made by Japanese company Dymon [ja] and weighed 498"
  },
  {
    "source": "IM-2.txt",
    "chunk_id": "IM-2.txt_4",
    "chunk": "grams. The primary payload, Polar Resources Ice Mining Experiment-1 (PRIME-1) was designed to search for water ice on the Moon at a permanently shadowed location near Shackleton Crater. It included the TRIDENT ice drill to sample ice from below the lunar surface and the MSolo mass spectrometer to measure the amount of ice in the samples. ILO-1 prime contractor Canadensys was working to deliver \"a flight-ready low-cost optical payload for the ILO-1 mission, ruggedized for the Moon South Pole environment\". On February 27, IM-2 released photography taken with the help of Canadensys technology. In November 2023, a mission simulation was undertaken by engineers at the Kennedy Space Center. The mission was to measure the volatile content of subsurface samples on the Moon. The scientific equipment consisted primarily of two components mounted to the lander: A version of TRIDENT and MSolo was to be used on NASA's canceled VIPER rover in the search for water ice. PRMIE-1 weighed 40 kg (88 lb). It was to investigate in situ resource utilization (ISRU). In May 2024, the company announced that IM-2 was entering the final assembly stage. It was also reported that the company was upgrading both software and hardware, including the landing"
  },
  {
    "source": "IM-2.txt",
    "chunk_id": "IM-2.txt_5",
    "chunk": "legs in order for better precision and control during descent and landing on the IM-2 mission. In September 2024, the company said it was on track for launch in January 2025. The Lunar Trailblazer orbiter was a secondary payload on the same Falcon 9 launch. In November 2024, during an earnings call, Intuitive Machines said the launch of IM-2 was targeting February 2025. During mission planning, IM-2 intended to land at Shackleton connecting ridge. Nevertheless, prior to launch, the targeted landing site was changed to Mons Mouton, a high plateau near the Lunar south pole which was planned to be the landing site of the canceled VIPER rover. IM-2 launched on 27 February 2025, at 00:16 UTC, aboard a SpaceX Falcon 9 with a number of other payloads. Following an on-target orbital insertion, Athena deployed 45 minutes after launch and established contact with ground controllers at 01:17 UTC. It was confirmed the next morning that the IM-2 mission was on track for a lunar orbital insertion on March 3, with a landing attempt scheduled for March 6 at 17:31 UTC. Three minutes before touchdown at 17:27 UTC on 6 March, Athena entered terminal descent. A plume of lunar dust interfered"
  },
  {
    "source": "IM-2.txt",
    "chunk_id": "IM-2.txt_6",
    "chunk": "with its navigation systems, obscuring laser and rangefinder readings while also disrupting radio signals. After a brief period of no communication, mission controllers confirmed that Athena had landed, detected lunar gravity, and was generating power. However, one of its two radio antennas had lost signal, and power generation was lower than expected. The Intuitive Machines team placed Athena in a power-saving \"safe mode,\" but after 38 minutes of troubleshooting, they determined the lander was not receiving sufficient energy. At a 21:00 UTC press conference, Intuitive Machines CEO Steve Altemus said that Athena was not in the correct attitude, meaning its solar panels were not facing the Sun. Despite this, the lander had begun performing scientific experiments, though at limited capacity due to the power constraints. On 7 March, downlinked images confirmed that Athena had come to rest sideways in a shadowed crater where the temperature was −173 °C (−280 °F). With the solar arrays producing only about 100 watts of power – insufficient to operate both the spacecraft’s heaters and its high-gain antenna – mission operators opted to maximize data collection over a 13-hour period rather than run the heaters to extend operations, but with minimal ability to conduct scientific"
  },
  {
    "source": "IM-2.txt",
    "chunk_id": "IM-2.txt_7",
    "chunk": "experiments. During this time, Athena transmitted imagery and data from the Moon’s south pole. The TRIDENT drill was extended but not operated, and private customers, including Nokia, retrieved useful data from their payloads. However the rovers and Micro-Nova were not able to be deployed. On 13 March, Intuitive Machines shared that, like on the IM-1 mission, the Athena's altimeter had failed during landing, leaving its onboard computer without an accurate altitude reading. As a result, the spacecraft struck a plateau, tipped over, and skidded across the lunar surface, rolling once or twice before settling inside a crater. The company's CEO compared it to a baseball player sliding into a base. The impact also kicked up regolith that coated the solar panels in dust, further degrading their performance. On 7 March 2025, at 16:54:21 UTC, the Lunar Reconnaissance Orbiter (LRO) imaged the Athena spacecraft landed within the center of a 20 m (66 ft) wide crater, about 23.5 hours after it touched down the lunar surface. The orbiter subsequently imaged the lander again at a much more oblique angle on 10 March. On 7 March 2025, Intuitive Machines announced that the mission was over after Athena landed on its side in"
  },
  {
    "source": "IM-2.txt",
    "chunk_id": "IM-2.txt_8",
    "chunk": "the Mons Mouton region near the south pole of the Moon. The same day, NASA confirmed that lander operations ended at 1:15 a.m., less than 13 hours after landing."
  },
  {
    "source": "Infrared telescope.txt",
    "chunk_id": "Infrared telescope.txt_0",
    "chunk": "# Infrared telescope An infrared telescope is a telescope that uses infrared light to detect celestial bodies. Infrared light is one of several types of radiation present in the electromagnetic spectrum. All celestial objects with a temperature above absolute zero emit some form of electromagnetic radiation. In order to study the universe, scientists use several different types of telescopes to detect these different types of emitted radiation in the electromagnetic spectrum. Some of these are gamma ray, x-ray, ultra-violet, regular visible light (optical), as well as infrared telescopes. There were several key developments that led to the invention of the infrared telescope: Infrared telescopes may be ground-based, air-borne, or space telescopes. They contain an infrared camera with a special solid-state infrared detector which must be cooled to cryogenic temperatures. Ground-based telescopes were the first to be used to observe outer space in infrared. Their popularity increased in the mid-1960s. Ground-based telescopes have limitations because water vapor in the Earth's atmosphere absorbs infrared radiation. Ground-based infrared telescopes tend to be placed on high mountains and in very dry climates to improve visibility. In the 1960s, scientists used balloons to lift infrared telescopes to higher altitudes. With balloons, they were able to"
  },
  {
    "source": "Infrared telescope.txt",
    "chunk_id": "Infrared telescope.txt_1",
    "chunk": "reach about 25 miles (40 kilometres) up. In 1967, infrared telescopes were placed on rockets. These were the first air-borne infrared telescopes. Since then, aircraft like the Kuiper Airborne Observatory (KAO) have been adapted to carry infrared telescopes. A more recent air-borne infrared telescope to reach the stratosphere was NASA's Stratospheric Observatory for Infrared Astronomy (SOFIA) in May 2010. Together, United States scientists and the German Aerospace Center scientists placed a 17-ton infrared telescope on a Boeing 747 jet airplane. Placing infrared telescopes in space eliminates the interference from the Earth's atmosphere. One of the most significant infrared telescope projects was the Infrared Astronomical Satellite (IRAS) that launched in 1983. It revealed information about other galaxies, as well as information about the center of our galaxy the Milky Way. NASA presently has solar-powered spacecraft in space with an infrared telescope called the James Webb Space Telescope (JWST). It was launched on December 25, 2021. The wavelength of visible light is about 0.4 μm to 0.7 μm, and 0.75 μm to 1000 μm (1 mm) is a typical range for infrared astronomy, far-infrared astronomy, to submillimetre astronomy."
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_0",
    "chunk": "# Interstellar medium The interstellar medium (ISM) is the matter and radiation that exists in the space between the star systems in a galaxy. This matter includes gas in ionic, atomic, and molecular form, as well as dust and cosmic rays. It fills interstellar space and blends smoothly into the surrounding intergalactic medium. The energy that occupies the same volume, in the form of electromagnetic radiation, is the interstellar radiation field. Although the density of atoms in the ISM is usually far below that in the best laboratory vacuums, the mean free path between collisions is short compared to typical interstellar lengths, so on these scales the ISM behaves as a gas (more precisely, as a plasma: it is everywhere at least slightly ionized), responding to pressure forces, and not as a collection of non-interacting particles. The interstellar medium is composed of multiple phases distinguished by whether matter is ionic, atomic, or molecular, and the temperature and density of the matter. The interstellar medium is composed primarily of hydrogen, followed by helium with trace amounts of carbon, oxygen, and nitrogen. The thermal pressures of these phases are in rough equilibrium with one another. Magnetic fields and turbulent motions also provide"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_1",
    "chunk": "pressure in the ISM, and are typically more important, dynamically, than the thermal pressure. In the interstellar medium, matter is primarily in molecular form and reaches number densities of 10 molecules per m (1 trillion molecules per m). In hot, diffuse regions, gas is highly ionized, and the density may be as low as 100 ions per m. Compare this with a number density of roughly 10 molecules per m for air at sea level, and 10 molecules per m (10 quadrillion molecules per m) for a laboratory high-vacuum chamber. Within our galaxy, by mass, 99% of the ISM is gas in any form, and 1% is dust. Of the gas in the ISM, by number 91% of atoms are hydrogen and 8.9% are helium, with 0.1% being atoms of elements heavier than hydrogen or helium, known as \"metals\" in astronomical parlance. By mass this amounts to 70% hydrogen, 28% helium, and 1.5% heavier elements. The hydrogen and helium are primarily a result of primordial nucleosynthesis, while the heavier elements in the ISM are mostly a result of enrichment (due to stellar nucleosynthesis) in the process of stellar evolution. The ISM plays a crucial role in astrophysics precisely because of"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_2",
    "chunk": "its intermediate role between stellar and galactic scales. Stars form within the densest regions of the ISM, which ultimately contributes to molecular clouds and replenishes the ISM with matter and energy through planetary nebulae, stellar winds, and supernovae. This interplay between stars and the ISM helps determine the rate at which a galaxy depletes its gaseous content, and therefore its lifespan of active star formation. Voyager 1 reached the ISM on August 25, 2012, making it the first artificial object from Earth to do so. Interstellar plasma and dust will be studied until the estimated mission end date of 2025. Its twin Voyager 2 entered the ISM on November 5, 2018. Table 1 shows a breakdown of the properties of the components of the ISM of the Milky Way. Field, Goldsmith & Habing (1969) put forward the static two phase equilibrium model to explain the observed properties of the ISM. Their modeled ISM included a cold dense phase (T < 300 K), consisting of clouds of neutral and molecular hydrogen, and a warm intercloud phase (T ~ 10 K), consisting of rarefied neutral and ionized gas. McKee & Ostriker (1977) added a dynamic third phase that represented the very hot"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_3",
    "chunk": "(T ~ 10 K) gas that had been shock heated by supernovae and constituted most of the volume of the ISM. These phases are the temperatures where heating and cooling can reach a stable equilibrium. Their paper formed the basis for further study over the subsequent three decades. However, the relative proportions of the phases and their subdivisions are still not well understood. The basic physics behind these phases can be understood through the behaviour of hydrogen, since this is by far the largest constituent of the ISM. The different phases are roughly in pressure balance over most of the Galactic disk, since regions of excess pressure will expand and cool, and likewise under-pressure regions will be compressed and heated. Therefore, since P = n k T, hot regions (high T) generally have low particle number density n. Coronal gas has low enough density that collisions between particles are rare and so little radiation is produced, hence there is little loss of energy and the temperature can stay high for periods of hundreds of millions of years. In contrast, once the temperature falls to O(10 K) with correspondingly higher density, protons and electrons can recombine to form hydrogen atoms, emitting"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_4",
    "chunk": "photons which take energy out of the gas, leading to runaway cooling. Left to itself this would produce the warm neutral medium. However, OB stars are so hot that some of their photons have energy greater than the Lyman limit, E > 13.6 eV, enough to ionize hydrogen. Such photons will be absorbed by, and ionize, any neutral hydrogen atom they encounter, setting up a dynamic equilibrium between ionization and recombination such that gas close enough to OB stars is almost entirely ionized, with temperature around 8000 K (unless already in the coronal phase), until the distance where all the ionizing photons are used up. This ionization front marks the boundary between the Warm ionized and Warm neutral medium. OB stars, and also cooler ones, produce many more photons with energies below the Lyman limit, which pass through the ionized region almost unabsorbed. Some of these have high enough energy (> 11.3 eV) to ionize carbon atoms, creating a C II (\"ionized carbon\") region outside the (hydrogen) ionization front. In dense regions this may also be limited in size by the availability of photons, but often such photons can penetrate throughout the neutral phase and only get absorbed in the"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_5",
    "chunk": "outer layers of molecular clouds. Photons with E > 4 eV or so can break up molecules such as H2 and CO, creating a photodissociation region (PDR) which is more or less equivalent to the Warm neutral medium. These processes contribute to the heating of the WNM. The distinction between Warm and Cold neutral medium is again due to a range of temperature/density in which runaway cooling occurs. The densest molecular clouds have significantly higher pressure than the interstellar average, since they are bound together by their own gravity. When stars form in such clouds, especially OB stars, they convert the surrounding gas into the warm ionized phase, a temperature increase of several hundred. Initially the gas is still at molecular cloud densities, and so at vastly higher pressure than the ISM average: this is a classical H II region. The large overpressure causes the ionized gas to expand away from the remaining molecular gas (a Champagne flow), and the flow will continue until either the molecular cloud is fully evaporated or the OB stars reach the end of their lives, after a few millions years. At this point the OB stars explode as supernovas, creating blast waves in the"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_6",
    "chunk": "warm gas that increase temperatures to the coronal phase (supernova remnants, SNR). These too expand and cool over several million years until they return to average ISM pressure. Most discussion of the ISM concerns spiral galaxies like the Milky Way, in which nearly all the mass in the ISM is confined to a relatively thin disk, typically with scale height about 100 parsecs (300 light years), which can be compared to a typical disk diameter of 30,000 parsecs. Gas and stars in the disk orbit the galactic centre with typical orbital speeds of 200 km/s. This is much faster than the random motions of atoms in the ISM, but since the orbital motion of the gas is coherent, the average motion does not directly affect structure in the ISM. The vertical scale height of the ISM is set in roughly the same way as the Earth's atmosphere, as a balance between the local gravitation field (dominated by the stars in the disk) and the pressure. Further from the disk plane, the ISM is mainly in the low-density warm and coronal phases, which extend at least several thousand parsecs away from the disk plane. This galactic halo or 'corona' also contains"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_7",
    "chunk": "significant magnetic field and cosmic ray energy density. The rotation of galaxy disks influences ISM structures in several ways. Since the angular velocity declines with increasing distance from the centre, any ISM feature, such as giant molecular clouds or magnetic field lines, that extend across a range of radius are sheared by differential rotation, and so tend to become stretched out in the tangential direction; this tendency is opposed by interstellar turbulence (see below) which tends to randomize the structures. Spiral arms are due to perturbations in the disk orbits - essentially ripples in the disk, that cause orbits to alternately converge and diverge, compressing and then expanding the local ISM. The visible spiral arms are the regions of maximum density, and the compression often triggers star formation in molecular clouds, leading to an abundance of H II regions along the arms. Coriolis force also influences large ISM features. Irregular galaxies such as the Magellanic Clouds have similar interstellar mediums to spirals, but less organized. In elliptical galaxies the ISM is almost entirely in the coronal phase, since there is no coherent disk motion to support cold gas far from the center: instead, the scale height of the ISM must"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_8",
    "chunk": "be comperable to the radius of the galaxy. This is consistent with the observation that there is little sign of current star formation in ellipticals. Some elliptical galaxies do show evidence for a small disk component, with ISM similar to spirals, buried close to their centers. The ISM of lenticular galaxies, as with their other properties, appear intermediate between spirals and ellipticals. Very close to the center of most galaxies (within a few hundred light years at most), the ISM is profoundly modified by the central supermassive black hole: see Galactic Center for the Milky Way, and Active galactic nucleus for extreme examples in other galaxies. The rest of this article will focus on the ISM in the disk plane of spirals, far from the galactic center. Astronomers describe the ISM as turbulent, meaning that the gas has quasi-random motions coherent over a large range of spatial scales. Unlike normal turbulence, in which the fluid motions are highly subsonic, the bulk motions of the ISM are usually larger than the sound speed. Supersonic collisions between gas clouds cause shock waves which compress and heat the gas, increasing the sounds speed so that the flow is locally subsonic; thus supersonic turbulence"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_9",
    "chunk": "has been described as 'a box of shocklets', and is inevitably associated with complex density and temperature structure. In the ISM this is further complicated by the magnetic field, which provides wave modes such as Alfvén waves which are often faster than pure sound waves: if turbulent speeds are supersonic but below the Alfvén wave speed, the behaviour is more like subsonic turbulence. Stars are born deep inside large complexes of molecular clouds, typically a few parsecs in size. During their lives and deaths, stars interact physically with the ISM. Stellar winds from young clusters of stars (often with giant or supergiant HII regions surrounding them) and shock waves created by supernovae inject enormous amounts of energy into their surroundings, which leads to hypersonic turbulence. The resultant structures – of varying sizes – can be observed, such as stellar wind bubbles and superbubbles of hot gas, seen by X-ray satellite telescopes or turbulent flows observed in radio telescope maps. Stars and planets, once formed, are unaffected by pressure forces in the ISM, and so do not take part in the turbulent motions, although stars formed in molecular clouds in a galactic disk share their general orbital motion around the galaxy"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_10",
    "chunk": "center. Thus stars are usually in motion relative to their surrounding ISM. The Sun is currently traveling through the Local Interstellar Cloud, an irregular clump of the warm neutral medium a few parsecs across, within the low-density Local Bubble, a 100-parsec radius region of coronal gas. In October 2020, astronomers reported a significant unexpected increase in density in the space beyond the Solar System as detected by the Voyager 1 and Voyager 2 space probes. According to the researchers, this implies that \"the density gradient is a large-scale feature of the VLISM (very local interstellar medium) in the general direction of the heliospheric nose\". The interstellar medium begins where the interplanetary medium of the Solar System ends. The solar wind slows to subsonic velocities at the termination shock, 90–100 astronomical units from the Sun. In the region beyond the termination shock, called the heliosheath, interstellar matter interacts with the solar wind. Voyager 1, the farthest human-made object from the Earth (after 1998), crossed the termination shock December 16, 2004 and later entered interstellar space when it crossed the heliopause on August 25, 2012, providing the first direct probe of conditions in the ISM (Stone et al. 2005). Dust grains in"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_11",
    "chunk": "the ISM are responsible for extinction and reddening, the decreasing light intensity and shift in the dominant observable wavelengths of light from a star. These effects are caused by scattering and absorption of photons and allow the ISM to be observed with the naked eye in a dark sky. The apparent rifts that can be seen in the band of the Milky Way – a uniform disk of stars – are caused by absorption of background starlight by dust in molecular clouds within a few thousand light years from Earth. This effect decreases rapidly with increasing wavelength (\"reddening\" is caused by greater absorption of blue than red light), and becomes almost negligible at mid-infrared wavelengths (> 5 μm). Extinction provides one of the best ways of mapping the three-dimensional structure of the ISM, especially since the advent of accurate distances to millions of stars from the Gaia mission. The total amount of dust in front of each star is determined from its reddening, and the dust is then located along the line of sight by comparing the dust column density in front of stars projected close together on the sky, but at different distances. By 2022 it was possible to"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_12",
    "chunk": "generate a map of ISM structures within 3 kpc (10,000 light years) of the Sun. Far ultraviolet light is absorbed effectively by the neutral hydrogen gas in the ISM. Specifically, atomic hydrogen absorbs very strongly at about 121.5 nanometers, the Lyman-alpha transition, and also at the other Lyman series lines. Therefore, it is nearly impossible to see light emitted at those wavelengths from a star farther than a few hundred light years from Earth, because most of it is absorbed during the trip to Earth by intervening neutral hydrogen. All photons with wavelength < 91.6 nm, the Lyman limit, can ionize hydrogen and are also very strongly absorbed. The absorption gradually decreases with increasing photon energy, and the ISM begins to become transparent again in soft X-rays, with wavelengths shorter than about 1 nm. The ISM is usually far from thermodynamic equilibrium. Collisions establish a Maxwell–Boltzmann distribution of velocities, and the 'temperature' normally used to describe interstellar gas is the 'kinetic temperature', which describes the temperature at which the particles would have the observed Maxwell–Boltzmann velocity distribution in thermodynamic equilibrium. However, the interstellar radiation field is typically much weaker than a medium in thermodynamic equilibrium; it is most often roughly"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_13",
    "chunk": "that of an A star (surface temperature of ~10,000 K) highly diluted. Therefore, bound levels within an atom or molecule in the ISM are rarely populated according to the Boltzmann formula (Spitzer 1978, § 2.4). Depending on the temperature, density, and ionization state of a portion of the ISM, different heating and cooling mechanisms determine the temperature of the gas. Grain heating by thermal exchange is very important in supernova remnants where densities and temperatures are very high. Gas heating via grain-gas collisions is dominant deep in giant molecular clouds (especially at high densities). Far infrared radiation penetrates deeply due to the low optical depth. Dust grains are heated via this radiation and can transfer thermal energy during collisions with the gas. A measure of efficiency in the heating is given by the accommodation coefficient: α = T 2 − T T d − T {\\displaystyle \\alpha ={\\frac {T_{2}-T}{T_{d}-T}}} where T is the gas temperature, Td the dust temperature, and T2 the post-collision temperature of the gas atom or molecule. This coefficient was measured by (Burke & Hollenbach 1983) as α = 0.35. Despite its extremely low density, photons generated in the ISM are prominent in nearly all bands of"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_14",
    "chunk": "the electromagnetic spectrum. In fact the optical band, on which astronomers relied until well into the 20th century, is the one in which the ISM is least obvious. Radio waves are affected by the plasma properties of the ISM. The lowest frequency radio waves, below ≈ 0.1 MHz, cannot propagate through the ISM since they are below its plasma frequency. At higher frequencies, the plasma has a significant refractive index, decreasing with increasing frequency, and also dependent on the density of free electrons. Random variations in the electron density cause interstellar scintillation, which broadens the apparent size of distant radio sources seen through the ISM, with the broadening decreasing with frequency squared. The variation of refractive index with frequency causes the arrival times of pulses from pulsars and Fast radio bursts to be delayed at lower frequencies (dispersion). The amount of delay is proportional to the column density of free electrons (Dispersion measure, DM), which is useful for both mapping the distribution of ionized gas in the Galaxy and estimating distances to pulsars (more distant ones have larger DM). A second propagation effect is Faraday rotation, which affects linearly polarized radio waves, such as those produced by synchrotron radiation, one"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_15",
    "chunk": "of the most common sources of radio emission in astrophysics. Faraday rotation depends on both the electron density and the magnetic field strength, and so is used as a probe of the interstellar magnetic field. The ISM is generally very transparent to radio waves, allowing unimpeded observations right through the disk of the Galaxy. There are a few exceptions to this rule. The most intense spectral lines in the radio spectrum can become opaque, so that only the surface of the line-emitting cloud is visible. This mainly affects the carbon monoxide lines at millimetre wavelengths that are used to trace molecular clouds, but the 21-cm line from neutral hydrogen can become opaque in the cold neutral medium. Such absorption only affects photons at the line frequencies: the clouds are otherwise transparent. The other significant absorption process occurs in dense ionized regions. These emit photons, including radio waves, via thermal bremsstrahlung. At short wavelengths, typically microwaves, these are quite transparent, but their brightness approaches the black body limit as ∝ λ 2.1 {\\displaystyle \\propto \\lambda ^{2.1}} , and at wavelengths long enough that this limit is reached, they become opaque. Thus metre-wavelength observations show H II regions as cool spots blocking"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_16",
    "chunk": "the bright background emission from Galactic synchrotron radiation, while at decametres the entire galactic plane is absorbed, and the longest radio waves observed, 1 km, can only propagate 10-50 parsecs through the Local Bubble. The frequency at which a particular nebula becomes optically thick depends on its emission measure the column density of squared electron number density. Exceptionally dense nebulae can become optically thick at centimetre wavelengths: these are just-formed and so both rare and small ('Ultra-compact H II regions') The general transparency of the ISM to radio waves, especially microwaves, may seem surprising since radio waves at frequencies > 10 GHz are significantly attenuated by Earth's atmosphere (as seen in the figure). But the column density through the atmosphere is vastly larger than the column through the entire Galaxy, due to the extremely low density of the ISM. The word 'interstellar' (between the stars) was coined by Francis Bacon in the context of the ancient theory of a literal sphere of fixed stars. Later in the 17th century, when the idea that stars were scattered through infinite space became popular, it was debated whether that space was a true vacuum or filled with a hypothetical fluid, sometimes called aether,"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_17",
    "chunk": "as in René Descartes' vortex theory of planetary motions. While vortex theory did not survive the success of Newtonian physics, an invisible luminiferous aether was re-introduced in the early 19th century as the medium to carry light waves; e.g., in 1862 a journalist wrote: \"this efflux occasions a thrill, or vibratory motion, in the ether which fills the interstellar spaces.\" In 1864, William Huggins used spectroscopy to determine that a nebula is made of gas. Huggins had a private observatory with an 8-inch telescope, with a lens by Alvan Clark; but it was equipped for spectroscopy, which enabled breakthrough observations. From around 1889, Edward Barnard pioneered deep photography of the sky, finding many 'holes in the Milky Way'. At first he compared them to sunspots, but by 1899 was prepared to write: \"One can scarcely conceive a vacancy with holes in it, unless there is nebulous matter covering these apparently vacant places in which holes might occur\". These holes are now known as dark nebulae, dusty molecular clouds silhouetted against the background star field of the galaxy; the most prominent are listed in his Barnard Catalogue. The first direct detection of cold diffuse matter in interstellar space came in 1904,"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_18",
    "chunk": "when Johannes Hartmann observed the binary star Mintaka (Delta Orionis) with the Potsdam Great Refractor. Hartmann reported that absorption from the \"K\" line of calcium appeared \"extraordinarily weak, but almost perfectly sharp\" and also reported the \"quite surprising result that the calcium line at 393.4 nanometres does not share in the periodic displacements of the lines caused by the orbital motion of the spectroscopic binary star\". The stationary nature of the line led Hartmann to conclude that the gas responsible for the absorption was not present in the atmosphere of the star, but was instead located within an isolated cloud of matter residing somewhere along the line of sight to this star. This discovery launched the study of the interstellar medium. Interstellar gas was further confirmed by Slipher in 1909, and then by 1912 interstellar dust was confirmed by Slipher. Interstellar sodium was detected by Mary Lea Heger in 1919 through the observation of stationary absorption from the atom's \"D\" lines at 589.0 and 589.6 nanometres towards Delta Orionis and Beta Scorpii. In the series of investigations, Viktor Ambartsumian introduced the now commonly accepted notion that interstellar matter occurs in the form of clouds. Subsequent observations of the \"H\" and"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_19",
    "chunk": "\"K\" lines of calcium by Beals (1936) revealed double and asymmetric profiles in the spectra of Epsilon and Zeta Orionis. These were the first steps in the study of the very complex interstellar sightline towards Orion. Asymmetric absorption line profiles are the result of the superposition of multiple absorption lines, each corresponding to the same atomic transition (for example the \"K\" line of calcium), but occurring in interstellar clouds with different radial velocities. Because each cloud has a different velocity (either towards or away from the observer/Earth), the absorption lines occurring within each cloud are either blue-shifted or red-shifted (respectively) from the lines' rest wavelength through the Doppler Effect. These observations confirming that matter is not distributed homogeneously were the first evidence of multiple discrete clouds within the ISM. The growing evidence for interstellar material led Pickering (1912) to comment: \"While the interstellar absorbing medium may be simply the ether, yet the character of its selective absorption, as indicated by Kapteyn, is characteristic of a gas, and free gaseous molecules are certainly there, since they are probably constantly being expelled by the Sun and stars.\" The same year, Victor Hess's discovery of cosmic rays, highly energetic charged particles that rain"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_20",
    "chunk": "onto the Earth from space, led others to speculate whether they also pervaded interstellar space. The following year, the Norwegian explorer and physicist Kristian Birkeland wrote: \"It seems to be a natural consequence of our points of view to assume that the whole of space is filled with electrons and flying electric ions of all kinds. We have assumed that each stellar system in evolutions throws off electric corpuscles into space. It does not seem unreasonable therefore to think that the greater part of the material masses in the universe is found, not in the solar systems or nebulae, but in 'empty' space\" (Birkeland 1913). Thorndike (1930) noted that \"it could scarcely have been believed that the enormous gaps between the stars are completely void. Terrestrial aurorae are not improbably excited by charged particles emitted by the Sun. If the millions of other stars are also ejecting ions, as is undoubtedly true, no absolute vacuum can exist within the galaxy.\" In September 2012, NASA scientists reported that polycyclic aromatic hydrocarbons (PAHs), subjected to interstellar medium (ISM) conditions, are transformed, through hydrogenation, oxygenation and hydroxylation, to more complex organics, \"a step along the path toward amino acids and nucleotides, the raw"
  },
  {
    "source": "Interstellar medium.txt",
    "chunk_id": "Interstellar medium.txt_21",
    "chunk": "materials of proteins and DNA, respectively\". Further, as a result of these transformations, the PAHs lose their spectroscopic signature, which could be one of the reasons \"for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks.\" In February 2014, NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets. In April 2019, scientists, working with the Hubble Space Telescope, reported the confirmed detection of the large and complex ionized molecules of buckminsterfullerene (C60) (also known as \"buckyballs\") in the interstellar medium spaces between the stars. In September 2020, evidence was presented of solid-state water in the interstellar medium, and particularly, of water ice mixed with silicate grains in cosmic dust grains."
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_0",
    "chunk": "# Iron Iron is a chemical element; it has symbol Fe (from Latin ferrum 'iron') and atomic number 26. It is a metal that belongs to the first transition series and group 8 of the periodic table. It is, by mass, the most common element on Earth, forming much of Earth's outer and inner core. It is the fourth most abundant element in the Earth's crust, being mainly deposited by meteorites in its metallic state. Extracting usable metal from iron ores requires kilns or furnaces capable of reaching 1,500 °C (2,730 °F), about 500 °C (900 °F) higher than that required to smelt copper. Humans started to master that process in Eurasia during the 2nd millennium BC and the use of iron tools and weapons began to displace copper alloys – in some regions, only around 1200 BC. That event is considered the transition from the Bronze Age to the Iron Age. In the modern world, iron alloys, such as steel, stainless steel, cast iron and special steels, are by far the most common industrial metals, due to their mechanical properties and low cost. The iron and steel industry is thus very important economically, and iron is the cheapest metal,"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_1",
    "chunk": "with a price of a few dollars per kilogram or pound. Pristine and smooth pure iron surfaces are a mirror-like silvery-gray. Iron reacts readily with oxygen and water to produce brown-to-black hydrated iron oxides, commonly known as rust. Unlike the oxides of some other metals that form passivating layers, rust occupies more volume than the metal and thus flakes off, exposing more fresh surfaces for corrosion. Chemically, the most common oxidation states of iron are iron(II) and iron(III). Iron shares many properties of other transition metals, including the other group 8 elements, ruthenium and osmium. Iron forms compounds in a wide range of oxidation states, −4 to +7. Iron also forms many coordination complexes; some of them, such as ferrocene, ferrioxalate, and Prussian blue have substantial industrial, medical, or research applications. The body of an adult human contains about 4 grams (0.005% body weight) of iron, mostly in hemoglobin and myoglobin. These two proteins play essential roles in oxygen transport by blood and oxygen storage in muscles. To maintain the necessary levels, human iron metabolism requires a minimum of iron in the diet. Iron is also the metal at the active site of many important redox enzymes dealing with cellular"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_2",
    "chunk": "respiration and oxidation and reduction in plants and animals. At least four allotropes of iron (differing atom arrangements in the solid) are known, conventionally denoted α, γ, δ, and ε. The first three forms are observed at ordinary pressures. As molten iron cools past its freezing point of 1538 °C, it crystallizes into its δ allotrope, which has a body-centered cubic (bcc) crystal structure. As it cools further to 1394 °C, it changes to its γ-iron allotrope, a face-centered cubic (fcc) crystal structure, or austenite. At 912 °C and below, the crystal structure again becomes the bcc α-iron allotrope. The physical properties of iron at very high pressures and temperatures have also been studied extensively, because of their relevance to theories about the cores of the Earth and other planets. Above approximately 10 GPa and temperatures of a few hundred kelvin or less, α-iron changes into another hexagonal close-packed (hcp) structure, which is also known as ε-iron. The higher-temperature γ-phase also changes into ε-iron, but does so at higher pressure. Some controversial experimental evidence exists for a stable β phase at pressures above 50 GPa and temperatures of at least 1500 K. It is supposed to have an orthorhombic or"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_3",
    "chunk": "a double hcp structure. (Confusingly, the term \"β-iron\" is sometimes also used to refer to α-iron above its Curie point, when it changes from being ferromagnetic to paramagnetic, even though its crystal structure has not changed.) The Earth's inner core is generally presumed to consist of an iron-nickel alloy with ε (or β) structure. The melting and boiling points of iron, along with its enthalpy of atomization, are lower than those of the earlier 3d elements from scandium to chromium, showing the lessened contribution of the 3d electrons to metallic bonding as they are attracted more and more into the inert core by the nucleus; however, they are higher than the values for the previous element manganese because that element has a half-filled 3d sub-shell and consequently its d-electrons are not easily delocalized. This same trend appears for ruthenium but not osmium. The melting point of iron is experimentally well defined for pressures less than 50 GPa. For greater pressures, published data (as of 2007) still varies by tens of gigapascals and over a thousand kelvin. Below its Curie point of 770 °C (1,420 °F; 1,040 K), α-iron changes from paramagnetic to ferromagnetic: the spins of the two unpaired electrons"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_4",
    "chunk": "in each atom generally align with the spins of its neighbors, creating an overall magnetic field. This happens because the orbitals of those two electrons (dz and dx − y) do not point toward neighboring atoms in the lattice, and therefore are not involved in metallic bonding. In the absence of an external source of magnetic field, the atoms get spontaneously partitioned into magnetic domains, about 10 micrometers across, such that the atoms in each domain have parallel spins, but some domains have other orientations. Thus a macroscopic piece of iron will have a nearly zero overall magnetic field. Application of an external magnetic field causes the domains that are magnetized in the same general direction to grow at the expense of adjacent ones that point in other directions, reinforcing the external field. This effect is exploited in devices that need to channel magnetic fields to fulfill design function, such as electrical transformers, magnetic recording heads, and electric motors. Impurities, lattice defects, or grain and particle boundaries can \"pin\" the domains in the new positions, so that the effect persists even after the external field is removed – thus turning the iron object into a (permanent) magnet. Similar behavior is"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_5",
    "chunk": "exhibited by some iron compounds, such as the ferrites including the mineral magnetite, a crystalline form of the mixed iron(II,III) oxide Fe3O4 (although the atomic-scale mechanism, ferrimagnetism, is somewhat different). Pieces of magnetite with natural permanent magnetization (lodestones) provided the earliest compasses for navigation. Particles of magnetite were extensively used in magnetic recording media such as core memories, magnetic tapes, floppies, and disks, until they were replaced by cobalt-based materials. Iron has four stable isotopes: Fe (5.845% of natural iron), Fe (91.754%), Fe (2.119%) and Fe (0.282%). Twenty-four artificial isotopes have also been created. Of these stable isotopes, only Fe has a nuclear spin (−1⁄2). The nuclide Fe theoretically can undergo double electron capture to Cr, but the process has never been observed and only a lower limit on the half-life of 4.4×10 years has been established. Fe is an extinct radionuclide of long half-life (2.6 million years). It is not found on Earth, but its ultimate decay product is its granddaughter, the stable nuclide Ni. Much of the past work on isotopic composition of iron has focused on the nucleosynthesis of Fe through studies of meteorites and ore formation. In the last decade, advances in mass spectrometry have allowed"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_6",
    "chunk": "the detection and quantification of minute, naturally occurring variations in the ratios of the stable isotopes of iron. Much of this work is driven by the Earth and planetary science communities, although applications to biological and industrial systems are emerging. In phases of the meteorites Semarkona and Chervony Kut, a correlation between the concentration of Ni, the granddaughter of Fe, and the abundance of the stable iron isotopes provided evidence for the existence of Fe at the time of formation of the Solar System. Possibly the energy released by the decay of Fe, along with that released by Al, contributed to the remelting and differentiation of asteroids after their formation 4.6 billion years ago. The abundance of Ni present in extraterrestrial material may bring further insight into the origin and early history of the Solar System. The most abundant iron isotope Fe is of particular interest to nuclear scientists because it represents the most common endpoint of nucleosynthesis. Since Ni (14 alpha particles) is easily produced from lighter nuclei in the alpha process in nuclear reactions in supernovae (see silicon burning process), it is the endpoint of fusion chains inside extremely massive stars. Although adding more alpha particles is possible,"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_7",
    "chunk": "but nonetheless the sequence does effectively end at Ni because conditions in stellar interiors cause the competition between photodisintegration and the alpha process to favor photodisintegration around Ni. This Ni, which has a half-life of about 6 days, is created in quantity in these stars, but soon decays by two successive positron emissions within supernova decay products in the supernova remnant gas cloud, first to radioactive Co, and then to stable Fe. As such, iron is the most abundant element in the core of red giants, and is the most abundant metal in iron meteorites and in the dense metal cores of planets such as Earth. It is also very common in the universe, relative to other stable metals of approximately the same atomic weight. Iron is the sixth most abundant element in the universe, and the most common refractory element. Although a further tiny energy gain could be extracted by synthesizing Ni, which has a marginally higher binding energy than Fe, conditions in stars are unsuitable for this process. Element production in supernovas greatly favor iron over nickel, and in any case, Fe still has a lower mass per nucleon than Ni due to its higher fraction of lighter"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_8",
    "chunk": "protons. Hence, elements heavier than iron require a supernova for their formation, involving rapid neutron capture by starting Fe nuclei. In the far future of the universe, assuming that proton decay does not occur, cold fusion occurring via quantum tunnelling would cause the light nuclei in ordinary matter to fuse into Fe nuclei. Fission and alpha-particle emission would then make heavy nuclei decay into iron, converting all stellar-mass objects to cold spheres of pure iron. Iron's abundance in rocky planets like Earth is due to its abundant production during the runaway fusion and explosion of type Ia supernovae, which scatters the iron into space. Metallic or native iron is rarely found on the surface of the Earth because it tends to oxidize. However, both the Earth's inner and outer core, which together account for 35% of the mass of the whole Earth, are believed to consist largely of an iron alloy, possibly with nickel. Electric currents in the liquid outer core are believed to be the origin of the Earth's magnetic field. The other terrestrial planets (Mercury, Venus, and Mars) as well as the Moon are believed to have a metallic core consisting mostly of iron. The M-type asteroids are"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_9",
    "chunk": "also believed to be partly or mostly made of metallic iron alloy. The rare iron meteorites are the main form of natural metallic iron on the Earth's surface. Items made of cold-worked meteoritic iron have been found in various archaeological sites dating from a time when iron smelting had not yet been developed; and the Inuit in Greenland have been reported to use iron from the Cape York meteorite for tools and hunting weapons. About 1 in 20 meteorites consist of the unique iron-nickel minerals taenite (35–80% iron) and kamacite (90–95% iron). Native iron is also rarely found in basalts that have formed from magmas that have come into contact with carbon-rich sedimentary rocks, which have reduced the oxygen fugacity sufficiently for iron to crystallize. This is known as telluric iron and is described from a few localities, such as Disko Island in West Greenland, Yakutia in Russia and Bühl in Germany. Ferropericlase (Mg,Fe)O, a solid solution of periclase (MgO) and wüstite (FeO), makes up about 20% of the volume of the lower mantle of the Earth, which makes it the second most abundant mineral phase in that region after silicate perovskite (Mg,Fe)SiO3; it also is the major host for"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_10",
    "chunk": "iron in the lower mantle. At the bottom of the transition zone of the mantle, the reaction γ-(Mg,Fe)2[SiO4] ↔ (Mg,Fe)[SiO3] + (Mg,Fe)O transforms γ-olivine into a mixture of silicate perovskite and ferropericlase and vice versa. In the literature, this mineral phase of the lower mantle is also often called magnesiowüstite. Silicate perovskite may form up to 93% of the lower mantle, and the magnesium iron form, (Mg,Fe)SiO3, is considered to be the most abundant mineral in the Earth, making up 38% of its volume. While iron is the most abundant element on Earth, most of this iron is concentrated in the inner and outer cores. The fraction of iron that is in Earth's crust only amounts to about 5% of the overall mass of the crust and is thus only the fourth most abundant element in that layer (after oxygen, silicon, and aluminium). Most of the iron in the crust is combined with various other elements to form many iron minerals. An important class is the iron oxide minerals such as hematite (Fe2O3), magnetite (Fe3O4), and siderite (FeCO3), which are the major ores of iron. Many igneous rocks also contain the sulfide minerals pyrrhotite and pentlandite. During weathering, iron tends"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_11",
    "chunk": "to leach from sulfide deposits as the sulfate and from silicate deposits as the bicarbonate. Both of these are oxidized in aqueous solution and precipitate in even mildly elevated pH as iron(III) oxide. Large deposits of iron are banded iron formations, a type of rock consisting of repeated thin layers of iron oxides alternating with bands of iron-poor shale and chert. The banded iron formations were laid down in the time between 3,700 million years ago and 1,800 million years ago. Materials containing finely ground iron(III) oxides or oxide-hydroxides, such as ochre, have been used as yellow, red, and brown pigments since pre-historical times. They contribute as well to the color of various rocks and clays, including entire geological formations like the Painted Hills in Oregon and the Buntsandstein (\"colored sandstone\", British Bunter). Through Eisensandstein (a jurassic 'iron sandstone', e.g. from Donzdorf in Germany) and Bath stone in the UK, iron compounds are responsible for the yellowish color of many historical buildings and sculptures. The proverbial red color of the surface of Mars is derived from an iron oxide-rich regolith. Significant amounts of iron occur in the iron sulfide mineral pyrite (FeS2), but it is difficult to extract iron from"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_12",
    "chunk": "it and it is therefore not exploited. In fact, iron is so common that production generally focuses only on ores with very high quantities of it. According to the International Resource Panel's Metal Stocks in Society report, the global stock of iron in use in society is 2,200 kg per capita. More-developed countries differ in this respect from less-developed countries (7,000–14,000 vs 2,000 kg per capita). Ocean science demonstrated the role of the iron in the ancient seas in both marine biota and climate. Iron shows the characteristic chemical properties of the transition metals, namely the ability to form variable oxidation states differing by steps of one and a very large coordination and organometallic chemistry: indeed, it was the discovery of an iron compound, ferrocene, that revolutionalized the latter field in the 1950s. Iron is sometimes considered as a prototype for the entire block of transition metals, due to its abundance and the immense role it has played in the technological progress of humanity. Its 26 electrons are arranged in the configuration [Ar]3d4s, of which the 3d and 4s electrons are relatively close in energy, and thus a number of electrons can be ionized. Iron forms compounds mainly in the"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_13",
    "chunk": "oxidation states +2 (iron(II), \"ferrous\") and +3 (iron(III), \"ferric\"). Iron also occurs in higher oxidation states, e.g., the purple potassium ferrate (K2FeO4), which contains iron in its +6 oxidation state. The anion [FeO4] with iron in its +7 oxidation state, along with an iron(V)-peroxo isomer, has been detected by infrared spectroscopy at 4 K after cocondensation of laser-ablated Fe atoms with a mixture of O2/Ar. Iron(IV) is a common intermediate in many biochemical oxidation reactions. Numerous organoiron compounds contain formal oxidation states of +1, 0, −1, or even −2. The oxidation states and other bonding properties are often assessed using the technique of Mössbauer spectroscopy. Many mixed valence compounds contain both iron(II) and iron(III) centers, such as magnetite and Prussian blue (Fe4(Fe[CN]6)3). The latter is used as the traditional \"blue\" in blueprints. Iron is the first of the transition metals that cannot reach its group oxidation state of +8, although its heavier congeners ruthenium and osmium can, with ruthenium having more difficulty than osmium. Ruthenium exhibits an aqueous cationic chemistry in its low oxidation states similar to that of iron, but osmium does not, favoring high oxidation states in which it forms anionic complexes. In the second half of the"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_14",
    "chunk": "3d transition series, vertical similarities down the groups compete with the horizontal similarities of iron with its neighbors cobalt and nickel in the periodic table, which are also ferromagnetic at room temperature and share similar chemistry. As such, iron, cobalt, and nickel are sometimes grouped together as the iron triad. Unlike many other metals, iron does not form amalgams with mercury. As a result, mercury is traded in standardized 76 pound flasks (34 kg) made of iron. Iron is by far the most reactive element in its group; it is pyrophoric when finely divided and dissolves easily in dilute acids, giving Fe. However, it does not react with concentrated nitric acid and other oxidizing acids due to the formation of an impervious oxide layer, which can nevertheless react with hydrochloric acid. High-purity iron, called electrolytic iron, is considered to be resistant to rust, due to its oxide layer. Iron forms various oxide and hydroxide compounds; the most common are iron(II,III) oxide (Fe3O4), and iron(III) oxide (Fe2O3). Iron(II) oxide also exists, though it is unstable at room temperature. Despite their names, they are actually all non-stoichiometric compounds whose compositions may vary. These oxides are the principal ores for the production of"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_15",
    "chunk": "iron (see bloomery and blast furnace). They are also used in the production of ferrites, useful magnetic storage media in computers, and pigments. The best known sulfide is iron pyrite (FeS2), also known as fool's gold owing to its golden luster. It is not an iron(IV) compound, but is actually an iron(II) polysulfide containing Fe and S2 ions in a distorted sodium chloride structure. The binary ferrous and ferric halides are well-known. The ferrous halides typically arise from treating iron metal with the corresponding hydrohalic acid to give the corresponding hydrated salts. Iron reacts with fluorine, chlorine, and bromine to give the corresponding ferric halides, ferric chloride being the most common. Ferric iodide is an exception, being thermodynamically unstable due to the oxidizing power of Fe and the high reducing power of I: Ferric iodide, a black solid, is not stable in ordinary conditions, but can be prepared through the reaction of iron pentacarbonyl with iodine and carbon monoxide in the presence of hexane and light at the temperature of −20 °C, with oxygen and water excluded. Complexes of ferric iodide with some soft bases are known to be stable compounds. The standard reduction potentials in acidic aqueous solution for"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_16",
    "chunk": "some common iron ions are given below: The red-purple tetrahedral ferrate(VI) anion is such a strong oxidizing agent that it oxidizes ammonia to nitrogen (N2) and water to oxygen: The pale-violet hexaquo complex [Fe(H2O)6] is an acid such that above pH 0 it is fully hydrolyzed: As pH rises above 0 the above yellow hydrolyzed species form and as it rises above 2–3, reddish-brown hydrous iron(III) oxide precipitates out of solution. Although Fe has a d configuration, its absorption spectrum is not like that of Mn with its weak, spin-forbidden d–d bands, because Fe has higher positive charge and is more polarizing, lowering the energy of its ligand-to-metal charge transfer absorptions. Thus, all the above complexes are rather strongly colored, with the single exception of the hexaquo ion – and even that has a spectrum dominated by charge transfer in the near ultraviolet region. On the other hand, the pale green iron(II) hexaquo ion [Fe(H2O)6] does not undergo appreciable hydrolysis. Carbon dioxide is not evolved when carbonate anions are added, which instead results in white iron(II) carbonate being precipitated out. In excess carbon dioxide this forms the slightly soluble bicarbonate, which occurs commonly in groundwater, but it oxidises quickly in"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_17",
    "chunk": "air to form iron(III) oxide that accounts for the brown deposits present in a sizeable number of streams. Due to its electronic structure, iron has a very large coordination and organometallic chemistry. Many coordination compounds of iron are known. A typical six-coordinate anion is hexachloroferrate(III), [FeCl6], found in the mixed salt tetrakis(methylammonium) hexachloroferrate(III) chloride. Complexes with multiple bidentate ligands have geometric isomers. For example, the trans-chlorohydridobis(bis-1,2-(diphenylphosphino)ethane)iron(II) complex is used as a starting material for compounds with the Fe(dppe)2 moiety. The ferrioxalate ion with three oxalate ligands displays helical chirality with its two non-superposable geometries labelled Λ (lambda) for the left-handed screw axis and Δ (delta) for the right-handed screw axis, in line with IUPAC conventions. Potassium ferrioxalate is used in chemical actinometry and along with its sodium salt undergoes photoreduction applied in old-style photographic processes. The dihydrate of iron(II) oxalate has a polymeric structure with co-planar oxalate ions bridging between iron centres with the water of crystallisation located forming the caps of each octahedron, as illustrated below. Iron(III) complexes are quite similar to those of chromium(III) with the exception of iron(III)'s preference for O-donor instead of N-donor ligands. The latter tend to be rather more unstable than iron(II) complexes and"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_18",
    "chunk": "often dissociate in water. Many Fe–O complexes show intense colors and are used as tests for phenols or enols. For example, in the ferric chloride test, used to determine the presence of phenols, iron(III) chloride reacts with a phenol to form a deep violet complex: Among the halide and pseudohalide complexes, fluoro complexes of iron(III) are the most stable, with the colorless [FeF5(H2O)] being the most stable in aqueous solution. Chloro complexes are less stable and favor tetrahedral coordination as in [FeCl4]; [FeBr4] and [FeI4] are reduced easily to iron(II). Thiocyanate is a common test for the presence of iron(III) as it forms the blood-red [Fe(SCN)(H2O)5]. Like manganese(II), most iron(III) complexes are high-spin, the exceptions being those with ligands that are high in the spectrochemical series such as cyanide. An example of a low-spin iron(III) complex is [Fe(CN)6]. Iron shows a great variety of electronic spin states, including every possible spin quantum number value for a d-block element from 0 (diamagnetic) to 5⁄2 (5 unpaired electrons). This value is always half the number of unpaired electrons. Complexes with zero to two unpaired electrons are considered low-spin and those with four or five are considered high-spin. Iron(II) complexes are less stable"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_19",
    "chunk": "than iron(III) complexes but the preference for O-donor ligands is less marked, so that for example [Fe(NH3)6] is known while [Fe(NH3)6] is not. They have a tendency to be oxidized to iron(III) but this can be moderated by low pH and the specific ligands used. Organoiron chemistry is the study of organometallic compounds of iron, where carbon atoms are covalently bound to the metal atom. They are many and varied, including cyanide complexes, carbonyl complexes, sandwich and half-sandwich compounds. Prussian blue or \"ferric ferrocyanide\", Fe4[Fe(CN)6]3, is an old and well-known iron-cyanide complex, extensively used as pigment and in several other applications. Its formation can be used as a simple wet chemistry test to distinguish between aqueous solutions of Fe and Fe as they react (respectively) with potassium ferricyanide and potassium ferrocyanide to form Prussian blue. Another old example of an organoiron compound is iron pentacarbonyl, Fe(CO)5, in which a neutral iron atom is bound to the carbon atoms of five carbon monoxide molecules. The compound can be used to make carbonyl iron powder, a highly reactive form of metallic iron. Thermolysis of iron pentacarbonyl gives triiron dodecacarbonyl, Fe3(CO)12, a complex with a cluster of three iron atoms at its core."
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_20",
    "chunk": "Collman's reagent, disodium tetracarbonylferrate, is a useful reagent for organic chemistry; it contains iron in the −2 oxidation state. Cyclopentadienyliron dicarbonyl dimer contains iron in the rare +1 oxidation state. A landmark in this field was the discovery in 1951 of the remarkably stable sandwich compound ferrocene Fe(C5H5)2, by Pauson and Kealy and independently by Miller and colleagues, whose surprising molecular structure was determined only a year later by Woodward and Wilkinson and Fischer. Ferrocene is still one of the most important tools and models in this class. Iron-centered organometallic species are used as catalysts. The Knölker complex, for example, is a transfer hydrogenation catalyst for ketones. The iron compounds produced on the largest scale in industry are iron(II) sulfate (FeSO4·7H2O) and iron(III) chloride (FeCl3). The former is one of the most readily available sources of iron(II), but is less stable to aerial oxidation than Mohr's salt ((NH4)2Fe(SO4)2·6H2O). Iron(II) compounds tend to be oxidized to iron(III) compounds in the air. Iron is one of the elements undoubtedly known to the ancient world. It has been worked, or wrought, for millennia. However, iron artefacts of great age are much rarer than objects made of gold or silver due to the ease"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_21",
    "chunk": "with which iron corrodes. The technology developed slowly, and even after the discovery of smelting it took many centuries for iron to replace bronze as the metal of choice for tools and weapons. Beads made from meteoric iron in 3500 BC or earlier were found in Gerzeh, Egypt by G. A. Wainwright. The beads contain 7.5% nickel, which is a signature of meteoric origin since iron found in the Earth's crust generally has only minuscule nickel impurities. Meteoric iron was highly regarded due to its origin in the heavens and was often used to forge weapons and tools. For example, a dagger made of meteoric iron was found in the tomb of Tutankhamun, containing similar proportions of iron, cobalt, and nickel to a meteorite discovered in the area, deposited by an ancient meteor shower. Items that were likely made of iron by Egyptians date from 3000 to 2500 BC. Meteoritic iron is comparably soft and ductile and easily cold forged but may get brittle when heated because of the nickel content. The first iron production started in the Middle Bronze Age, but it took several centuries before iron displaced bronze. Samples of smelted iron from Asmar, Mesopotamia and Tall Chagar"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_22",
    "chunk": "Bazaar in northern Syria were made sometime between 3000 and 2700 BC. The Hittites established an empire in north-central Anatolia around 1600 BC. They appear to be the first to understand the production of iron from its ores and regard it highly in their society. The Hittites began to smelt iron between 1500 and 1200 BC and the practice spread to the rest of the Near East after their empire fell in 1180 BC. The subsequent period is called the Iron Age. Artifacts of smelted iron are found in India dating from 1800 to 1200 BC, and in the Levant from about 1500 BC (suggesting smelting in Anatolia or the Caucasus). Alleged references (compare history of metallurgy in South Asia) to iron in the Indian Vedas have been used for claims of a very early usage of iron in India respectively to date the texts as such. The rigveda term ayas (metal) refers to copper, while iron which is called as śyāma ayas, literally \"black copper\", first is mentioned in the post-rigvedic Atharvaveda. Some archaeological evidence suggests iron was smelted in Zimbabwe and southeast Africa as early as the eighth century BC. Iron working was introduced to Greece in the"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_23",
    "chunk": "late 11th century BC, from which it spread quickly throughout Europe. The spread of ironworking in Central and Western Europe is associated with Celtic expansion. According to Pliny the Elder, iron use was common in the Roman era. In the lands of what is now considered China, iron appears approximately 700–500 BC. Iron smelting may have been introduced into China through Central Asia. The earliest evidence of the use of a blast furnace in China dates to the 1st century AD, and cupola furnaces were used as early as the Warring States period (403–221 BC). Usage of the blast and cupola furnace remained widespread during the Tang and Song dynasties. During the Industrial Revolution in Britain, Henry Cort began refining iron from pig iron to wrought iron (or bar iron) using innovative production systems. In 1783 he patented the puddling process for refining iron ore. It was later improved by others, including Joseph Hall. Cast iron was first produced in China during 5th century BC, but was hardly in Europe until the medieval period. The earliest cast iron artifacts were discovered by archaeologists in what is now modern Luhe County, Jiangsu in China. Cast iron was used in ancient China"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_24",
    "chunk": "for warfare, agriculture, and architecture. During the medieval period, means were found in Europe of producing wrought iron from cast iron (in this context known as pig iron) using finery forges. For all these processes, charcoal was required as fuel. Medieval blast furnaces were about 10 feet (3.0 m) tall and made of fireproof brick; forced air was usually provided by hand-operated bellows. Modern blast furnaces have grown much bigger, with hearths fourteen meters in diameter that allow them to produce thousands of tons of iron each day, but essentially operate in much the same way as they did during medieval times. In 1709, Abraham Darby I established a coke-fired blast furnace to produce cast iron, replacing charcoal, although continuing to use blast furnaces. The ensuing availability of inexpensive iron was one of the factors leading to the Industrial Revolution. Toward the end of the 18th century, cast iron began to replace wrought iron for certain purposes, because it was cheaper. Carbon content in iron was not implicated as the reason for the differences in properties of wrought iron, cast iron, and steel until the 18th century. Since iron was becoming cheaper and more plentiful, it also became a major"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_25",
    "chunk": "structural material following the building of the innovative first iron bridge in 1778. This bridge still stands today as a monument to the role iron played in the Industrial Revolution. Following this, iron was used in rails, boats, ships, aqueducts, and buildings, as well as in iron cylinders in steam engines. Railways have been central to the formation of modernity and ideas of progress and various languages refer to railways as iron road (e.g. French chemin de fer, German Eisenbahn, Turkish demiryolu, Russian железная дорога, Chinese, Japanese, and Korean 鐵道, Vietnamese đường sắt). Steel (with smaller carbon content than pig iron but more than wrought iron) was first produced in antiquity by using a bloomery. Blacksmiths in Luristan in western Persia were making good steel by 1000 BC. Then improved versions, Wootz steel by India and Damascus steel were developed around 300 BC and AD 500 respectively. These methods were specialized, and so steel did not become a major commodity until the 1850s. New methods of producing it by carburizing bars of iron in the cementation process were devised in the 17th century. In the Industrial Revolution, new methods of producing bar iron without charcoal were devised and these were"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_26",
    "chunk": "later applied to produce steel. In the late 1850s, Henry Bessemer invented a new steelmaking process, involving blowing air through molten pig iron, to produce mild steel. This made steel much more economical, thereby leading to wrought iron no longer being produced in large quantities. In 1774, Antoine Lavoisier used the reaction of water steam with metallic iron inside an incandescent iron tube to produce hydrogen in his experiments leading to the demonstration of the conservation of mass, which was instrumental in changing chemistry from a qualitative science to a quantitative one. Iron plays a certain role in mythology and has found various usage as a metaphor and in folklore. The Greek poet Hesiod's Works and Days (lines 109–201) lists different ages of man named after metals like gold, silver, bronze and iron to account for successive ages of humanity. The Iron Age was closely related with Rome, and in Ovid's Metamorphoses The Virtues, in despair, quit the earth; and the depravity of man becomes universal and complete. Hard steel succeeded then. An example of the importance of iron's symbolic role may be found in the German Campaign of 1813. Frederick William III commissioned then the first Iron Cross as"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_27",
    "chunk": "military decoration. Berlin iron jewellery reached its peak production between 1813 and 1815, when the Prussian royal family urged citizens to donate gold and silver jewellery for military funding. The inscription Ich gab Gold für Eisen (I gave gold for iron) was used as well in later war efforts. For a few limited purposes when it is needed, pure iron is produced in the laboratory in small quantities by reducing the pure oxide or hydroxide with hydrogen, or forming iron pentacarbonyl and heating it to 250 °C so that it decomposes to form pure iron powder. Another method is electrolysis of ferrous chloride onto an iron cathode. Nowadays, the industrial production of iron or steel consists of two main stages. In the first stage, iron ore is reduced with coke in a blast furnace, and the molten metal is separated from gross impurities such as silicate minerals. This stage yields an alloy – pig iron – that contains relatively large amounts of carbon. In the second stage, the amount of carbon in the pig iron is lowered by oxidation to yield wrought iron, steel, or cast iron. Other metals can be added at this stage to form alloy steels. The"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_28",
    "chunk": "blast furnace is loaded with iron ores, usually hematite Fe2O3 or magnetite Fe3O4, along with coke (coal that has been separately baked to remove volatile components) and flux (limestone or dolomite). \"Blasts\" of air pre-heated to 900 °C (sometimes with oxygen enrichment) is blown through the mixture, in sufficient amount to turn the carbon into carbon monoxide: This reaction raises the temperature to about 2000 °C. The carbon monoxide reduces the iron ore to metallic iron: Some iron in the high-temperature lower region of the furnace reacts directly with the coke: The flux removes silicaceous minerals in the ore, which would otherwise clog the furnace: The heat of the furnace decomposes the carbonates to calcium oxide, which reacts with any excess silica to form a slag composed of calcium silicate CaSiO3 or other products. At the furnace's temperature, the metal and the slag are both molten. They collect at the bottom as two immiscible liquid layers (with the slag on top), that are then easily separated. The slag can be used as a material in road construction or to improve mineral-poor soils for agriculture. Steelmaking thus remains one of the largest industrial contributors of CO2 emissions in the world. The"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_29",
    "chunk": "pig iron produced by the blast furnace process contains up to 4–5% carbon (by mass), with small amounts of other impurities like sulfur, magnesium, phosphorus, and manganese. This high level of carbon makes it relatively weak and brittle. Reducing the amount of carbon to 0.002–2.1% produces steel, which may be up to 1000 times harder than pure iron. A great variety of steel articles can then be made by cold working, hot rolling, forging, machining, etc. Removing the impurities from pig iron, but leaving 2–4% carbon, results in cast iron, which is cast by foundries into articles such as stoves, pipes, radiators, lamp-posts, and rails. Steel products often undergo various heat treatments after they are forged to shape. Annealing consists of heating them to 700–800 °C for several hours and then gradual cooling. It makes the steel softer and more workable. Owing to environmental concerns, alternative methods of processing iron have been developed. \"Direct iron reduction\" reduces iron ore to a ferrous lump called \"sponge\" iron or \"direct\" iron that is suitable for steelmaking. Two main reactions comprise the direct reduction process: Iron ore is then treated with these gases in a furnace, producing solid sponge iron: Ignition of a"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_30",
    "chunk": "mixture of aluminium powder and iron oxide yields metallic iron via the thermite reaction: Alternatively pig iron may be made into steel (with up to about 2% carbon) or wrought iron (commercially pure iron). Various processes have been used for this, including finery forges, puddling furnaces, Bessemer converters, open hearth furnaces, basic oxygen furnaces, and electric arc furnaces. In all cases, the objective is to oxidize some or all of the carbon, together with other impurities. On the other hand, other metals may be added to make alloy steels. Molten oxide electrolysis (MOE) uses electrolysis of molten iron oxide to yield metallic iron. It is studied in laboratory-scale experiments and is proposed as a method for industrial iron production that has no direct emissions of carbon dioxide. It uses a liquid iron cathode, an anode formed from an alloy of chromium, aluminium and iron, and the electrolyte is a mixture of molten metal oxides into which iron ore is dissolved. The current keeps the electrolyte molten and reduces the iron oxide. Oxygen gas is produced in addition to liquid iron. The only carbon dioxide emissions come from any fossil fuel-generated electricity used to heat and reduce the metal. Iron is"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_31",
    "chunk": "the most widely used of all the metals, accounting for over 90% of worldwide metal production. Its low cost and high strength often make it the material of choice to withstand stress or transmit forces, such as the construction of machinery and machine tools, rails, automobiles, ship hulls, concrete reinforcing bars, and the load-carrying framework of buildings. Since pure iron is quite soft, it is most commonly combined with alloying elements to make steel. The mechanical properties of iron and its alloys are extremely relevant to their structural applications. Those properties can be evaluated in various ways, including the Brinell test, the Rockwell test and the Vickers hardness test. The properties of pure iron are often used to calibrate measurements or to compare tests. However, the mechanical properties of iron are significantly affected by the sample's purity: pure, single crystals of iron are actually softer than aluminium, and the purest industrially produced iron (99.99%) has a hardness of 20–30 Brinell. The pure iron (99.9%～99.999%), especially called electrolytic iron, is industrially produced by electrolytic refining. An increase in the carbon content will cause a significant increase in the hardness and tensile strength of iron. Maximum hardness of 65 Rc is achieved"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_32",
    "chunk": "with a 0.6% carbon content, although the alloy has low tensile strength. Because of the softness of iron, it is much easier to work with than its heavier congeners ruthenium and osmium. α-Iron is a fairly soft metal that can dissolve only a small concentration of carbon (no more than 0.021% by mass at 910 °C). Austenite (γ-iron) is similarly soft and metallic but can dissolve considerably more carbon (as much as 2.04% by mass at 1146 °C). This form of iron is used in the type of stainless steel used for making cutlery, and hospital and food-service equipment. Commercially available iron is classified based on purity and the abundance of additives. Pig iron has 3.5–4.5% carbon and contains varying amounts of contaminants such as sulfur, silicon and phosphorus. Pig iron is not a saleable product, but rather an intermediate step in the production of cast iron and steel. The reduction of contaminants in pig iron that negatively affect material properties, such as sulfur and phosphorus, yields cast iron containing 2–4% carbon, 1–6% silicon, and small amounts of manganese. Pig iron has a melting point in the range of 1420–1470 K, which is lower than either of its two main"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_33",
    "chunk": "components, and makes it the first product to be melted when carbon and iron are heated together. Its mechanical properties vary greatly and depend on the form the carbon takes in the alloy. \"White\" cast irons contain their carbon in the form of cementite, or iron carbide (Fe3C). This hard, brittle compound dominates the mechanical properties of white cast irons, rendering them hard, but unresistant to shock. The broken surface of a white cast iron is full of fine facets of the broken iron carbide, a very pale, silvery, shiny material, hence the appellation. Cooling a mixture of iron with 0.8% carbon slowly below 723 °C to room temperature results in separate, alternating layers of cementite and α-iron, which is soft and malleable and is called pearlite for its appearance. Rapid cooling, on the other hand, does not allow time for this separation and creates hard and brittle martensite. The steel can then be tempered by reheating to a temperature in between, changing the proportions of pearlite and martensite. The end product below 0.8% carbon content is a pearlite-αFe mixture, and that above 0.8% carbon content is a pearlite-cementite mixture. In gray iron the carbon exists as separate, fine flakes"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_34",
    "chunk": "of graphite, and also renders the material brittle due to the sharp edged flakes of graphite that produce stress concentration sites within the material. A newer variant of gray iron, referred to as ductile iron, is specially treated with trace amounts of magnesium to alter the shape of graphite to spheroids, or nodules, reducing the stress concentrations and vastly increasing the toughness and strength of the material. Wrought iron contains less than 0.25% carbon but large amounts of slag that give it a fibrous characteristic. Wrought iron is more corrosion resistant than steel. It has been almost completely replaced by mild steel, which corrodes more readily than wrought iron, but is cheaper and more widely available. Carbon steel contains 2.0% carbon or less, with small amounts of manganese, sulfur, phosphorus, and silicon. Alloy steels contain varying amounts of carbon as well as other metals, such as chromium, vanadium, molybdenum, nickel, tungsten, etc. Their alloy content raises their cost, and so they are usually only employed for specialist uses. One common alloy steel, though, is stainless steel. Recent developments in ferrous metallurgy have produced a growing range of microalloyed steels, also termed 'HSLA' or high-strength, low alloy steels, containing tiny additions"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_35",
    "chunk": "to produce high strengths and often spectacular toughness at minimal cost. Alloys with high purity elemental makeups (such as alloys of electrolytic iron) have specifically enhanced properties such as ductility, tensile strength, toughness, fatigue strength, heat resistance, and corrosion resistance. Apart from traditional applications, iron is also used for protection from ionizing radiation. Although it is lighter than another traditional protection material, lead, it is much stronger mechanically. The main disadvantage of iron and steel is that pure iron, and most of its alloys, suffer badly from rust if not protected in some way, a cost amounting to over 1% of the world's economy. Painting, galvanization, passivation, plastic coating and bluing are all used to protect iron from rust by excluding water and oxygen or by cathodic protection. The mechanism of the rusting of iron is as follows: The electrolyte is usually iron(II) sulfate in urban areas (formed when atmospheric sulfur dioxide attacks iron), and salt particles in the atmosphere in seaside areas. Because Fe is inexpensive and nontoxic, much effort has been devoted to the development of Fe-based catalysts and reagents. Iron is however less common as a catalyst in commercial processes than more expensive metals. In biology, Fe-containing"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_36",
    "chunk": "enzymes are pervasive. Iron catalysts are traditionally used in the Haber–Bosch process for the production of ammonia and the Fischer–Tropsch process for conversion of carbon monoxide to hydrocarbons for fuels and lubricants. Powdered iron in an acidic medium is used in the Bechamp reduction, the conversion of nitrobenzene to aniline. Iron(III) oxide mixed with aluminium powder can be ignited to create a thermite reaction, used in welding large iron parts (like rails) and purifying ores. Iron(III) oxide and oxyhydroxide are used as reddish and ocher pigments. Iron(III) chloride finds use in water purification and sewage treatment, in the dyeing of cloth, as a coloring agent in paints, as an additive in animal feed, and as an etchant for copper in the manufacture of printed circuit boards. It can also be dissolved in alcohol to form tincture of iron, which is used as a medicine to stop bleeding in canaries. Iron(II) sulfate is used as a precursor to other iron compounds. It is also used to reduce chromate in cement. It is used to fortify foods and treat iron deficiency anemia. Iron(III) sulfate is used in settling minute sewage particles in tank water. Iron(II) chloride is used as a reducing flocculating"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_37",
    "chunk": "agent, in the formation of iron complexes and magnetic iron oxides, and as a reducing agent in organic synthesis. Sodium nitroprusside is a drug used as a vasodilator. It is on the World Health Organization's List of Essential Medicines. Iron is required for life. The iron–sulfur clusters are pervasive and include nitrogenase, the enzymes responsible for biological nitrogen fixation. Iron-containing proteins participate in transport, storage and use of oxygen. Iron proteins are involved in electron transfer. Examples of iron-containing proteins in higher organisms include hemoglobin, cytochrome (see high-valent iron), and catalase. The average adult human contains about 0.005% body weight of iron, or about four grams, of which three quarters is in hemoglobin—a level that remains constant despite only about one milligram of iron being absorbed each day, because the human body recycles its hemoglobin for the iron content. Microbial growth may be assisted by oxidation of iron(II) or by reduction of iron(III). Iron acquisition poses a problem for aerobic organisms because ferric iron is poorly soluble near neutral pH. Thus, these organisms have developed means to absorb iron as complexes, sometimes taking up ferrous iron before oxidising it back to ferric iron. In particular, bacteria have evolved very high-affinity"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_38",
    "chunk": "sequestering agents called siderophores. After uptake in human cells, iron storage is precisely regulated. A major component of this regulation is the protein transferrin, which binds iron ions absorbed from the duodenum and carries it in the blood to cells. Transferrin contains Fe in the middle of a distorted octahedron, bonded to one nitrogen, three oxygens and a chelating carbonate anion that traps the Fe ion: it has such a high stability constant that it is very effective at taking up Fe ions even from the most stable complexes. At the bone marrow, transferrin is reduced from Fe to Fe and stored as ferritin to be incorporated into hemoglobin. The most commonly known and studied bioinorganic iron compounds (biological iron molecules) are the heme proteins: examples are hemoglobin, myoglobin, and cytochrome P450. These compounds participate in transporting gases, building enzymes, and transferring electrons. Metalloproteins are a group of proteins with metal ion cofactors. Some examples of iron metalloproteins are ferritin and rubredoxin. Many enzymes vital to life contain iron, such as catalase, lipoxygenases, and IRE-BP. Hemoglobin is an oxygen carrier that occurs in red blood cells and contributes their color, transporting oxygen in the arteries from the lungs to the"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_39",
    "chunk": "muscles where it is transferred to myoglobin, which stores it until it is needed for the metabolic oxidation of glucose, generating energy. Here the hemoglobin binds to carbon dioxide, produced when glucose is oxidized, which is transported through the veins by hemoglobin (predominantly as bicarbonate anions) back to the lungs where it is exhaled. In hemoglobin, the iron is in one of four heme groups and has six possible coordination sites; four are occupied by nitrogen atoms in a porphyrin ring, the fifth by an imidazole nitrogen in a histidine residue of one of the protein chains attached to the heme group, and the sixth is reserved for the oxygen molecule it can reversibly bind to. When hemoglobin is not attached to oxygen (and is then called deoxyhemoglobin), the Fe ion at the center of the heme group (in the hydrophobic protein interior) is in a high-spin configuration. It is thus too large to fit inside the porphyrin ring, which bends instead into a dome with the Fe ion about 55 picometers above it. In this configuration, the sixth coordination site reserved for the oxygen is blocked by another histidine residue. When deoxyhemoglobin picks up an oxygen molecule, this histidine"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_40",
    "chunk": "residue moves away and returns once the oxygen is securely attached to form a hydrogen bond with it. This results in the Fe ion switching to a low-spin configuration, resulting in a 20% decrease in ionic radius so that now it can fit into the porphyrin ring, which becomes planar. Additionally, this hydrogen bonding results in the tilting of the oxygen molecule, resulting in a Fe–O–O bond angle of around 120° that avoids the formation of Fe–O–Fe or Fe–O2–Fe bridges that would lead to electron transfer, the oxidation of Fe to Fe, and the destruction of hemoglobin. This results in a movement of all the protein chains that leads to the other subunits of hemoglobin changing shape to a form with larger oxygen affinity. Thus, when deoxyhemoglobin takes up oxygen, its affinity for more oxygen increases, and vice versa. Myoglobin, on the other hand, contains only one heme group and hence this cooperative effect cannot occur. Thus, while hemoglobin is almost saturated with oxygen in the high partial pressures of oxygen found in the lungs, its affinity for oxygen is much lower than that of myoglobin, which oxygenates even at low partial pressures of oxygen found in muscle tissue. As"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_41",
    "chunk": "described by the Bohr effect (named after Christian Bohr, the father of Niels Bohr), the oxygen affinity of hemoglobin diminishes in the presence of carbon dioxide. Carbon monoxide and phosphorus trifluoride are poisonous to humans because they bind to hemoglobin similarly to oxygen, but with much more strength, so that oxygen can no longer be transported throughout the body. Hemoglobin bound to carbon monoxide is known as carboxyhemoglobin. This effect also plays a minor role in the toxicity of cyanide, but there the major effect is by far its interference with the proper functioning of the electron transport protein cytochrome a. The cytochrome proteins also involve heme groups and are involved in the metabolic oxidation of glucose by oxygen. The sixth coordination site is then occupied by either another imidazole nitrogen or a methionine sulfur, so that these proteins are largely inert to oxygen—with the exception of cytochrome a, which bonds directly to oxygen and thus is very easily poisoned by cyanide. Here, the electron transfer takes place as the iron remains in low spin but changes between the +2 and +3 oxidation states. Since the reduction potential of each step is slightly greater than the previous one, the energy"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_42",
    "chunk": "is released step-by-step and can thus be stored in adenosine triphosphate. Cytochrome a is slightly distinct, as it occurs at the mitochondrial membrane, binds directly to oxygen, and transports protons as well as electrons, as follows: Although the heme proteins are the most important class of iron-containing proteins, the iron–sulfur proteins are also very important, being involved in electron transfer, which is possible since iron can exist stably in either the +2 or +3 oxidation states. These have one, two, four, or eight iron atoms that are each approximately tetrahedrally coordinated to four sulfur atoms; because of this tetrahedral coordination, they always have high-spin iron. The simplest of such compounds is rubredoxin, which has only one iron atom coordinated to four sulfur atoms from cysteine residues in the surrounding peptide chains. Another important class of iron–sulfur proteins is the ferredoxins, which have multiple iron atoms. Transferrin does not belong to either of these classes. The ability of sea mussels to maintain their grip on rocks in the ocean is facilitated by their use of organometallic iron-based bonds in their protein-rich cuticles. Based on synthetic replicas, the presence of iron in these structures increased elastic modulus 770 times, tensile strength 58"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_43",
    "chunk": "times, and toughness 92 times. The amount of stress required to permanently damage them increased 76 times. Iron is pervasive, but particularly rich sources of dietary iron include red meat, oysters, beans, poultry, fish, leaf vegetables, watercress, tofu, and blackstrap molasses. Bread and breakfast cereals are sometimes specifically fortified with iron. Iron provided by dietary supplements is often found as iron(II) fumarate, although iron(II) sulfate is cheaper and is absorbed equally well. Elemental iron, or reduced iron, despite being absorbed at only one-third to two-thirds the efficiency (relative to iron sulfate), is often added to foods such as breakfast cereals or enriched wheat flour. Iron is most available to the body when chelated to amino acids and is also available for use as a common iron supplement. Glycine, the least expensive amino acid, is most often used to produce iron glycinate supplements. The U.S. Institute of Medicine (IOM) updated Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for iron in 2001. The current EAR for iron for women ages 14‍–‍18 is 7.9 mg/day, 8.1 mg/day for ages 19‍–‍50 and 5.0 mg/day thereafter (postmenopause). For men, the EAR is 6.0 mg/day for ages 19 and up. The RDA is 15.0"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_44",
    "chunk": "mg/day for women ages 15‍–‍18, 18.0 mg/day for ages 19‍–‍50 and 8.0 mg/day thereafter. For men, 8.0 mg/day for ages 19 and up. RDAs are higher than EARs so as to identify amounts that will cover people with higher-than-average requirements. RDA for pregnancy is 27 mg/day and, for lactation, 9 mg/day. For children ages 1‍–‍3 years 7 mg/day, 10 mg/day for ages 4–8 and 8 mg/day for ages 9‍–‍13. As for safety, the IOM also sets Tolerable upper intake levels (ULs) for vitamins and minerals when evidence is sufficient. In the case of iron, the UL is set at 45 mg/day. Collectively the EARs, RDAs and ULs are referred to as Dietary Reference Intakes. The European Food Safety Authority (EFSA) refers to the collective set of information as Dietary Reference Values, with Population Reference Intake (PRI) instead of RDA, and Average Requirement instead of EAR. AI and UL are defined the same as in the United States. For women the PRI is 13 mg/day ages 15‍–‍17 years, 16 mg/day for women ages 18 and up who are premenopausal and 11 mg/day postmenopausal. For pregnancy and lactation, 16 mg/day. For men the PRI is 11 mg/day ages 15 and older. For"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_45",
    "chunk": "children ages 1 to 14, the PRI increases from 7 to 11 mg/day. The PRIs are higher than the U.S. RDAs, with the exception of pregnancy. The EFSA reviewed the same safety question did not establish a UL. Infants may require iron supplements if they are bottle-fed cow's milk. Frequent blood donors are at risk of low iron levels and are often advised to supplement their iron intake. For U.S. food and dietary supplement labeling purposes, the amount in a serving is expressed as a percent of Daily Value (%DV). For iron labeling purposes, 100% of the Daily Value was 18 mg, and as of May 27, 2016 remained unchanged at 18 mg. A table of the old and new adult daily values is provided at Reference Daily Intake. Iron deficiency is the most common nutritional deficiency in the world. When loss of iron is not adequately compensated by adequate dietary iron intake, a state of latent iron deficiency occurs, which over time leads to iron-deficiency anemia if left untreated, which is characterised by an insufficient number of red blood cells and an insufficient amount of hemoglobin. Children, pre-menopausal women (women of child-bearing age), and people with poor diet are"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_46",
    "chunk": "most susceptible to the disease. Most cases of iron-deficiency anemia are mild, but if not treated can cause problems like fast or irregular heartbeat, complications during pregnancy, and delayed growth in infants and children. The brain is resistant to acute iron deficiency due to the slow transport of iron through the blood brain barrier. Acute fluctuations in iron status (marked by serum ferritin levels) do not reflect brain iron status, but prolonged nutritional iron deficiency is suspected to reduce brain iron concentrations over time. In the brain, iron plays a role in oxygen transport, myelin synthesis, mitochondrial respiration, and as a cofactor for neurotransmitter synthesis and metabolism. Animal models of nutritional iron deficiency report biomolecular changes resembling those seen in Parkinson's and Huntington's disease. However, age-related accumulation of iron in the brain has also been linked to the development of Parkinson's. Iron uptake is tightly regulated by the human body, which has no regulated physiological means of excreting iron. Only small amounts of iron are lost daily due to mucosal and skin epithelial cell sloughing, so control of iron levels is primarily accomplished by regulating uptake. Regulation of iron uptake is impaired in some people as a result of a"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_47",
    "chunk": "genetic defect that maps to the HLA-H gene region on chromosome 6 and leads to abnormally low levels of hepcidin, a key regulator of the entry of iron into the circulatory system in mammals. In these people, excessive iron intake can result in iron overload disorders, known medically as hemochromatosis. Many people have an undiagnosed genetic susceptibility to iron overload, and are not aware of a family history of the problem. For this reason, people should not take iron supplements unless they suffer from iron deficiency and have consulted a doctor. Hemochromatosis is estimated to be the cause of 0.3–0.8% of all metabolic diseases of Caucasians. Overdoses of ingested iron can cause excessive levels of free iron in the blood. High blood levels of free ferrous iron react with peroxides to produce highly reactive free radicals that can damage DNA, proteins, lipids, and other cellular components. Iron toxicity occurs when the cell contains free iron, which generally occurs when iron levels exceed the availability of transferrin to bind the iron. Damage to the cells of the gastrointestinal tract can also prevent them from regulating iron absorption, leading to further increases in blood levels. Iron typically damages cells in the heart,"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_48",
    "chunk": "liver and elsewhere, causing adverse effects that include coma, metabolic acidosis, shock, liver failure, coagulopathy, long-term organ damage, and even death. Humans experience iron toxicity when the iron exceeds 20 milligrams for every kilogram of body mass; 60 milligrams per kilogram is considered a lethal dose. Overconsumption of iron, often the result of children eating large quantities of ferrous sulfate tablets intended for adult consumption, is one of the most common toxicological causes of death in children under six. The Dietary Reference Intake (DRI) sets the Tolerable Upper Intake Level (UL) for adults at 45 mg/day. For children under fourteen years old the UL is 40 mg/day. The medical management of iron toxicity is complicated, and can include use of a specific chelating agent called deferoxamine to bind and expel excess iron from the body. Some research has suggested that low thalamic iron levels may play a role in the pathophysiology of ADHD. Some researchers have found that iron supplementation can be effective especially in the inattentive subtype of the disorder. Some researchers in the 2000s suggested a link between low levels of iron in the blood and ADHD. A 2012 study found no such correlation. The role of iron"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_49",
    "chunk": "in cancer defense can be described as a \"double-edged sword\" because of its pervasive presence in non-pathological processes. People having chemotherapy may develop iron deficiency and anemia, for which intravenous iron therapy is used to restore iron levels. Iron overload, which may occur from high consumption of red meat, may initiate tumor growth and increase susceptibility to cancer onset, particularly for colorectal cancer. Iron plays an essential role in marine systems and can act as a limiting nutrient for planktonic activity. Because of this, too much of a decrease in iron may lead to a decrease in growth rates in phytoplanktonic organisms such as diatoms. Iron can also be oxidized by marine microbes under conditions that are high in iron and low in oxygen. Iron can enter marine systems through adjoining rivers and directly from the atmosphere. Once iron enters the ocean, it can be distributed throughout the water column through ocean mixing and through recycling on the cellular level. In the arctic, sea ice plays a major role in the store and distribution of iron in the ocean, depleting oceanic iron as it freezes in the winter and releasing it back into the water when thawing occurs in the"
  },
  {
    "source": "Iron.txt",
    "chunk_id": "Iron.txt_50",
    "chunk": "summer. The iron cycle can fluctuate the forms of iron from aqueous to particle forms altering the availability of iron to primary producers. Increased light and warmth increases the amount of iron that is in forms that are usable by primary producers."
  },
  {
    "source": "James Edward Keeler.txt",
    "chunk_id": "James Edward Keeler.txt_0",
    "chunk": "# James Edward Keeler James Edward Keeler (September 10, 1857 – August 12, 1900) was an American astronomer. He was an early observer of galaxies using photography, as well as the first to show observationally that the rings of Saturn do not rotate as a solid body. Keeler was born in La Salle Illinois, but grew up and spent the majority of his early life in Mayport, Florida near Jacksonville. His mother's father was a former Governor of Connecticut, Henry Dutton. Keeler worked at Lick Observatory beginning in 1888 but left after being appointed director of the University of Pittsburgh's Allegheny Observatory in 1891. He returned to Lick Observatory as its director in 1898 but died not long after in 1900. Keeler suffered from a heart weakness that went undiagnosed until shortly before his death. On the 12th of August, he died from a sudden stroke. His ashes were interred in a crypt at the base of the 31-inch Keeler Memorial telescope at the Allegheny Observatory. Along with George Hale, Keeler founded and edited the Astrophysical Journal, which remains a major journal of astronomy today. His parents were William F. and Anna (née Dutton) Keeler. He had married in 1891"
  },
  {
    "source": "James Edward Keeler.txt",
    "chunk_id": "James Edward Keeler.txt_1",
    "chunk": "and left a widow and two children. Keeler was the first to observe the gap in Saturn's rings now known as the Encke Gap, using the 36-inch refractor at Lick Observatory on 7 January 1888. After this feature had been named for Johann Encke, who had observed a much broader variation in the brightness of the A Ring, Keeler's contributions were brought to light. The second major gap in the A Ring, discovered by Voyager, was named the Keeler Gap in his honor. In 1895, his spectroscopic study of the rings of Saturn revealed that different parts of the rings reflect light with different Doppler shifts, due to their different rates of orbit around Saturn. This was the first observational confirmation of the theory of James Clerk Maxwell that the rings are made up of countless small objects, each orbiting Saturn at its own rate. These observations were made with a spectrograph attached to the 13-inch Fitz-Clark refracting telescope at Allegheny Observatory. His observations with the Lick Crossley telescope helped establish the importance of large optical reflecting telescopes, and expanded astronomers' understanding of nebulae. After his untimely death, his colleagues at Lick Observatory arranged for the publication of his photographs"
  },
  {
    "source": "James Edward Keeler.txt",
    "chunk_id": "James Edward Keeler.txt_2",
    "chunk": "of nebulae and clusters in a special volume of the Lick Observatory publications. Keeler discovered two minor planets, the Koronis asteroid 452 Hamiltonia in 1899, and the Mars-crosser asteroid (20958) A900 MA in 1900, which became a lost minor planet until its recovery 99 years later. After the discovery of pulsars in 1967, optical images of the Crab Nebula taken by Keeler in 1899 were used to determine the proper motion of the Crab Pulsar. Keeler was awarded the Henry Draper Medal from the National Academy of Sciences in 1899. In 1900 he was elected president of the Astronomical Society of the Pacific. In 1880, Allegheny Observatory director Samuel Pierpont Langley, accompanied by Keeler and others, went on a scientific expedition to the summit of Mount Whitney. The purpose of the expedition was to study how the Sun's radiation was selectively absorbed by the Earth's atmosphere, comparing the results at high altitude with those found at lower levels. As a result of the expedition, a 14,240-ft. peak near Mount Whitney was named the \"Keeler Needle\". In addition to the Keeler gap in Saturn's rings, the Martian crater Keeler, the lunar crater Keeler, as well as the asteroid 2261 Keeler, are"
  },
  {
    "source": "James Edward Keeler.txt",
    "chunk_id": "James Edward Keeler.txt_3",
    "chunk": "named in his honor."
  },
  {
    "source": "Jay Pasachoff.txt",
    "chunk_id": "Jay Pasachoff.txt_0",
    "chunk": "# Jay Pasachoff Jay Myron Pasachoff (July 1, 1943 – November 20, 2022) was an American astronomer. Pasachoff was Field Memorial Professor of Astronomy at Williams College and the author of textbooks and tradebooks in astronomy, physics, mathematics, and other sciences. After the Bronx High School of Science, Pasachoff studied at Harvard, receiving his bachelor's degree in 1963, his master's degree in 1965, and his doctorate in 1969. His doctoral thesis was titled Fine Structure in the Solar Chromosphere. He worked at the Harvard College Observatory and Caltech before going to Williams College in 1972. His sabbaticals and other leaves have been at the University of Hawaii’s Institute for Astronomy, the Institut d'Astrophysique de Paris, the Institute for Advanced Study in Princeton, New Jersey, the Center for Astrophysics | Harvard & Smithsonian in Cambridge, Massachusetts, Caltech in Pasadena, California, and most recently at the Carnegie Observatories, also in Pasadena. He has taken a leading role in the science and history of transits of Mercury and Venus, as an analogue to exoplanet studies, leading up to the transit of Venus, and the 2016 and 2019 transits of Mercury. Jay Pasachoff on solar eclipses: \"Each time is like going to the seventh"
  },
  {
    "source": "Jay Pasachoff.txt",
    "chunk_id": "Jay Pasachoff.txt_1",
    "chunk": "game of the World Series with the score tied in the ninth inning.\" Pasachoff observed with a wide variety of ground-based telescopes and spacecraft, and reported on those activities in writing his texts. Pasachoff carried out extensive scientific work at total solar eclipses, and championed the continued contemporary scientific value of solar eclipse research. His research was sponsored by the National Science Foundation, NASA, and the National Geographic Society. He was Chair of the Working Group on Eclipses of the International Astronomical Union of the Sun and Heliosphere Division and of the Education, Outreach, and Heritage Division. His solar work also included studies of the solar chromosphere, backed by NASA grants, using NASA spacecraft and the 1-m Swedish Solar Telescope on La Palma, Canary Islands, Spain. With Richard Cohen and his sister, Nancy Pasachoff, Pasachoff wrote in 1970 an article for Nature discussing that the belief in the supernatural such as horoscopes impede the growth of science. He collaborated with a professor of art history, Roberta J. M. Olson of the New-York Historical Society, on astronomical images in the art of Renaissance Italy, Great Britain, the U.S. (eclipse oil paintings), and elsewhere. Jay and Naomi Pasachoff wrote a review of"
  },
  {
    "source": "Jay Pasachoff.txt",
    "chunk_id": "Jay Pasachoff.txt_2",
    "chunk": "Alexander Borodin’s solar-inspired opera for Nature produced by the New York Metropolitan Opera in 2014. Also with his wife, Naomi, Pasachoff wrote biographies of Henry Norris Russell, John Pond, Hypatia, and Edward Williams Morley for the Biographical Encyclopedia of Astronomers. Their books and other publications are listed at http://solarcorona.com as links to publishers’ websites. Pasachoff received the 2003 Education Prize of the American Astronomical Society, \"For his eloquent and informative writing of textbooks from junior high through college, For his devotion to teaching generations of students, For sharing with the world the joys of observing eclipses, For his many popular books and articles on astronomy, For his intense advocacy on behalf of science education in various forums, For his willingness to go into educational nooks where no astronomer has gone before, the AAS Education Prize is awarded to Jay M. Pasachoff.\" Asteroid 5100 Pasachoff recognizes Pasachoff's astronomical accomplishments. In addition to his college astronomy texts, Pasachoff wrote Peterson Field Guide to the Stars and Planets, was co-author of Peterson Field Guide to Weather, and was author or coauthor of textbooks in calculus and in physics, as well as several junior-high-school textbooks. Pasachoff received the 2012 Prix-Jules–Janssen from the Société astronomique"
  },
  {
    "source": "Jay Pasachoff.txt",
    "chunk_id": "Jay Pasachoff.txt_3",
    "chunk": "de France,\"for your outstanding research, teaching and popularisation of Astronomy, in the spirit with which Camille Flammarion created the award back in 1897.\" He received the 2017 Richtmyer Memorial Award from the American Association of Physics Teachers \"for outstanding contributions to physics and effectively communicating those contributions to physics educators.\" He received the 2019 Klumpke-Roberts Award of the Astronomical Society of the Pacific \"for his contributions to the public understanding and appreciation of astronomy\", based in part on his role at the times of solar eclipses, when \"Jay becomes astronomy's cheerleader-in-chief, allowing more and more people to become interested and engaged in the field.\" Pasachoff collaborated with scientists from Williams College and MIT to observe the atmospheres of outer planets and their moons, including Pluto, its moon Charon, Neptune’s moon Triton, and other objects in the outer Solar System. He has also used radio astronomy made observations of the interstellar medium with scientists from Hofstra University and elsewhere, concentrating on deuterium. Pasachoff was active in educational and curriculum matters. He was U.S. National Liaison to and was President (2003–2006) Commission on Education and Development, which is now Commission C1 on Astronomy Education and Development of Division C on Education, Outreach,"
  },
  {
    "source": "Jay Pasachoff.txt",
    "chunk_id": "Jay Pasachoff.txt_4",
    "chunk": "and Heritage, of the International Astronomical Union. He was twice Chair of the Astronomy Division of the American Association for the Advancement of Science, and was on the astronomy committees of the American Astronomical Society (and its representative 2004–2013 to the AAAS), the American Physical Society, and the American Association of Physics Teachers. He was on the Council of Advisors of the Astronomy Education Review. He spearheaded a discussion of what should be taught in astronomy courses, championing the position of including and emphasizing contemporary astronomy. He was a Fellow of the American Physical Society, the International Planetarium Society, the American Association for the Advancement of Science, Committee for Skeptical Inquiry, and the Royal Astronomical Society, and he has held a Getty Fellowship. He was elected a Legacy Fellow of the American Astronomical Society in 2020. He lectured widely, including a stint as a Sigma Xi Distinguished Lecturer. He was also Director of the Hopkins Observatory and (in rotation, currently beginning in the fall semester of 2019) Chair of the Astronomy Department at Williams. Pasachoff was Chair of the Historical Astronomy Division of the American Astronomical Society (2013-2015). He was on the Organizing Committee for Commission C.C3 on the History"
  },
  {
    "source": "Jay Pasachoff.txt",
    "chunk_id": "Jay Pasachoff.txt_5",
    "chunk": "of Astronomy of the International Astronomical Union (2015-2018) and on the Johannes Kepler Working Group. A catalogue of the Jay and Naomi Pasachoff rare-book collection—including works by Copernicus, Tycho, Galileo, Kepler, Newton, Fraunhofer, and Einstein—on deposit in the Chapin Library of Williams College (W. Hammond, 2014). In 1974, Pasachoff married Naomi (née Schwartz) in a Jewish ceremony. His wife. Dr. Naomi Pasachoff (1947–), is a writer and the daughter of economist Anna Schwartz. They have two daughters, one of whom, Eloise Pasachoff, is a research Professor of law at Georgetown Law."
  },
  {
    "source": "Jupiter trojan.txt",
    "chunk_id": "Jupiter trojan.txt_0",
    "chunk": "# Jupiter trojan The Jupiter trojans, commonly called trojan asteroids or simply trojans, are a large group of asteroids that share the planet Jupiter's orbit around the Sun. Relative to Jupiter, each trojan librates around one of Jupiter's stable Lagrange points: either L4, existing 60° ahead of the planet in its orbit, or L5, 60° behind. Jupiter trojans are distributed in two elongated, curved regions around these Lagrangian points with an average semi-major axis of about 5.2 AU. The first Jupiter trojan discovered, 588 Achilles, was spotted in 1906 by German astronomer Max Wolf. More than 9,800 Jupiter trojans have been found as of May 2021. By convention, they are each named from Greek mythology after a figure of the Trojan War, hence the name \"trojan\". The total number of Jupiter trojans larger than 1 km in diameter is believed to be about 1 million, approximately equal to the number of asteroids larger than 1 km in the asteroid belt. Like main-belt asteroids, Jupiter trojans form families. As of 2004, many Jupiter trojans showed to observational instruments as dark bodies with reddish, featureless spectra. No firm evidence of the presence of water, or any other specific compound on their surface"
  },
  {
    "source": "Jupiter trojan.txt",
    "chunk_id": "Jupiter trojan.txt_1",
    "chunk": "has been obtained, but it is thought that they are coated in tholins, organic polymers formed by the Sun's radiation. The Jupiter trojans' densities (as measured by studying binaries or rotational lightcurves) vary from 0.8 to 2.5 g·cm. Jupiter trojans are thought to have been captured into their orbits during the early stages of the Solar System's formation or slightly later, during the migration of giant planets. The term \"Trojan Asteroid\" specifically refers to the asteroids co-orbital with Jupiter, but the general term \"trojan\" is sometimes more generally applied to other small Solar System bodies with similar relationships to larger bodies: Mars trojans, Neptune trojans, Uranus trojans and Earth trojans are known to exist. Temporary Venus trojans and Saturn trojans exist, as well as for 1 Ceres and 4 Vesta. The term \"Trojan asteroid\" is normally understood to specifically mean the Jupiter trojans because the first Trojans were discovered near Jupiter's orbit and Jupiter currently has by far the most known Trojans. In 1772, Italian-born mathematician Joseph-Louis Lagrange, in studying the restricted three-body problem, predicted that a small body sharing an orbit with a planet but lying 60° ahead or behind it will be trapped near these points. The trapped"
  },
  {
    "source": "Jupiter trojan.txt",
    "chunk_id": "Jupiter trojan.txt_2",
    "chunk": "body will librate slowly around the point of equilibrium in a tadpole or horseshoe orbit. These leading and trailing points are called the L4 and L5 Lagrange points. The first asteroids trapped in Lagrange points were observed more than a century after Lagrange's hypothesis. Those associated with Jupiter were the first to be discovered. E. E. Barnard made the first recorded observation of a trojan, (12126) 1999 RM11 (identified as A904 RD at the time), in 1904, but neither he nor others appreciated its significance at the time. Barnard believed he had seen the recently discovered Saturnian satellite Phoebe, which was only two arc-minutes away in the sky at the time, or possibly an asteroid. The object's identity was not understood until its orbit was calculated in 1999. The first accepted discovery of a trojan occurred in February 1906, when astronomer Max Wolf of Heidelberg-Königstuhl State Observatory discovered an asteroid at the L4 Lagrangian point of the Sun–Jupiter system, later named 588 Achilles. In 1906–1907 two more Jupiter trojans were found by fellow German astronomer August Kopff (624 Hektor and 617 Patroclus). Hektor, like Achilles, belonged to the L4 swarm (\"ahead\" of the planet in its orbit), whereas Patroclus was"
  },
  {
    "source": "Jupiter trojan.txt",
    "chunk_id": "Jupiter trojan.txt_3",
    "chunk": "the first asteroid known to reside at the L5 Lagrangian point (\"behind\" the planet). By 1938, 11 Jupiter trojans had been detected. This number increased to 14 only in 1961. As instruments improved, the rate of discovery grew rapidly: by January 2000, a total of 257 had been discovered; by May 2003, the number had grown to 1,600. As of October 2018 there are 4,601 known Jupiter trojans at L4 and 2,439 at L5. The custom of naming all asteroids in Jupiter's L4 and L5 points after famous heroes of the Trojan War was suggested by Johann Palisa of Vienna, who was the first to accurately calculate their orbits. Asteroids in the leading (L4) orbit are named after Greek heroes (the \"Greek node or camp\" or \"Achilles group\"), and those at the trailing (L5) orbit are named after the heroes of Troy (the \"Trojan node or camp\"). The asteroids 617 Patroclus and 624 Hektor were named before the Greece/Troy rule was devised, resulting in a \"Greek spy\", Patroclus, in the Trojan node and a \"Trojan spy\", Hector, in the Greek node. In 2018, at its 30th General Assembly in Vienna, the International Astronomical Union amended the naming convention for Jupiter"
  },
  {
    "source": "Jupiter trojan.txt",
    "chunk_id": "Jupiter trojan.txt_4",
    "chunk": "trojans, allowing for asteroids with H larger than 12 (that is, a mean diameter smaller than approximately 22 kilometers, for an assumed albedo of 0.057) to be named after Olympic athletes, because there are now far more known Jupiter trojans than available names of Greek and Trojan warriors that fought in the Trojan war. Estimates of the total number of Jupiter trojans are based on deep surveys of limited areas of the sky. The L4 swarm is believed to hold between 160,000 and 240,000 asteroids with diameters larger than 2 km and about 600,000 with diameters larger than 1 km. If the L5 swarm contains a comparable number of objects, there are more than 1 million Jupiter trojans 1 km in size or larger. For the objects brighter than absolute magnitude 9.0 the population is probably complete. These numbers are similar to that of comparable asteroids in the asteroid belt. The total mass of the Jupiter trojans is estimated at 0.0001 of the mass of Earth or one-fifth of the mass of the asteroid belt. Two more recent studies indicate that the above numbers may overestimate the number of Jupiter trojans by several-fold. This overestimate is caused by (1) the"
  },
  {
    "source": "Jupiter trojan.txt",
    "chunk_id": "Jupiter trojan.txt_5",
    "chunk": "assumption that all Jupiter trojans have a low albedo of about 0.04, whereas small bodies may have an average albedo as high as 0.12; (2) an incorrect assumption about the distribution of Jupiter trojans in the sky. According to the new estimates, the total number of Jupiter trojans with a diameter larger than 2 km is 6,300 ± 1,000 and 3,400 ± 500 in the L4 and L5 swarms, respectively. These numbers would be reduced by a factor of 2 if small Jupiter trojans are more reflective than large ones. The number of Jupiter trojans observed in the L4 swarm is slightly larger than that observed in L5. Because the brightest Jupiter trojans show little variation in numbers between the two populations, this disparity is probably due to observational bias. Some models indicate that the L4 swarm may be slightly more stable than the L5 swarm. The largest Jupiter trojan is 624 Hektor, which has a mean diameter of 203 ± 3.6 km. There are few large Jupiter trojans in comparison to the overall population. With decreasing size, the number of Jupiter trojans grows very quickly down to 84 km, much more so than in the asteroid belt. A diameter"
  },
  {
    "source": "Jupiter trojan.txt",
    "chunk_id": "Jupiter trojan.txt_6",
    "chunk": "of 84 km corresponds to an absolute magnitude of 9.5, assuming an albedo of 0.04. Within the 4.4–40 km range the Jupiter trojans' size distribution resembles that of the main-belt asteroids. Nothing is known about the masses of the smaller Jupiter trojans. The size distribution suggests that the smaller Trojans may be the products of collisions by larger Jupiter trojans. Jupiter trojans have orbits with radii between 5.05 and 5.35 AU (the mean semi-major axis is 5.2 ± 0.15 AU), and are distributed throughout elongated, curved regions around the two Lagrangian points; each swarm stretches for about 26° along the orbit of Jupiter, amounting to a total distance of about 2.5 AU. The width of the swarms approximately equals two Hill's radii, which in the case of Jupiter amounts to about 0.6 AU. Many of Jupiter trojans have large orbital inclinations relative to Jupiter's orbital plane—up to 40°. Jupiter trojans do not maintain a fixed separation from Jupiter. They slowly librate around their respective equilibrium points, periodically moving closer to Jupiter or farther from it. Jupiter trojans generally follow paths called tadpole orbits around the Lagrangian points; the average period of their libration is about 150 years. The amplitude of"
  },
  {
    "source": "Jupiter trojan.txt",
    "chunk_id": "Jupiter trojan.txt_7",
    "chunk": "the libration (along the Jovian orbit) varies from 0.6° to 88°, with the average being about 33°. Simulations show that Jupiter trojans can follow even more complicated trajectories when moving from one Lagrangian point to another—these are called horseshoe orbits (currently no Jupiter Trojan with such an orbit is known, though one is known for Neptune). Discerning dynamical families within the Jupiter trojan population is more difficult than it is in the asteroid belt, because the Jupiter trojans are locked within a far narrower range of possible positions. This means that clusters tend to overlap and merge with the overall swarm. By 2003 roughly a dozen dynamical families were identified. Jupiter-trojan families are much smaller in size than families in the asteroid belt; the largest identified family, the Menelaus group, consists of only eight members. In 2001, 617 Patroclus was the first Jupiter trojan to be identified as a binary asteroid. The binary's orbit is extremely close, at 650 km, compared to 35,000 km for the primary's Hill sphere. The largest Jupiter trojan—624 Hektor— is probably a contact binary with a moonlet. Jupiter trojans are dark bodies of irregular shape. Their geometric albedos generally vary between 3 and 10%. The"
  },
  {
    "source": "Jupiter trojan.txt",
    "chunk_id": "Jupiter trojan.txt_8",
    "chunk": "average value is 0.056 ± 0.003 for the objects larger than 57 km, and 0.121 ± 0.003 (R-band) for those smaller than 25 km. The asteroid 4709 Ennomos has the highest albedo (0.18) of all known Jupiter trojans. Little is known about the masses, chemical composition, rotation or other physical properties of the Jupiter trojans. The rotational properties of Jupiter trojans are not well known. Analysis of the rotational light curves of 72 Jupiter trojans gave an average rotational period of about 11.2 hours, whereas the average period of the control sample of asteroids in the asteroid belt was 10.6 hours. The distribution of the rotational periods of Jupiter trojans appeared to be well approximated by a Maxwellian function, whereas the distribution for main-belt asteroids was found to be non-Maxwellian, with a deficit of periods in the range 8–10 hours. The Maxwellian distribution of the rotational periods of Jupiter trojans may indicate that they have undergone a stronger collisional evolution compared to the asteroid belt. In 2008 a team from Calvin College examined the light curves of a debiased sample of ten Jupiter trojans, and found a median spin period of 18.9 hours. This value was significantly higher than that"
  },
  {
    "source": "Jupiter trojan.txt",
    "chunk_id": "Jupiter trojan.txt_9",
    "chunk": "for main-belt asteroids of similar size (11.5 hours). The difference could mean that the Jupiter trojans possess a lower average density, which may imply that they formed in the Kuiper belt (see below). Spectroscopically, the Jupiter trojans mostly are D-type asteroids, which predominate in the outer regions of the asteroid belt. A small number are classified as P or C-type asteroids. Their spectra are red (meaning that they reflect more light at longer wavelengths) or neutral and featureless. No firm evidence of water, organics or other chemical compounds has been obtained as of 2007. 4709 Ennomos has an albedo slightly higher than the Jupiter-trojan average, which may indicate the presence of water ice. Some other Jupiter Trojans, such as 911 Agamemnon and 617 Patroclus, have shown very weak absorptions at 1.7 and 2.3 μm, which might indicate the presence of organics. The Jupiter trojans' spectra are similar to those of the irregular moons of Jupiter and, to a certain extent, comet nuclei, though Jupiter trojans are spectrally very different from the redder Kuiper belt objects. A Jupiter trojan's spectrum can be matched to a mixture of water ice, a large amount of carbon-rich material (charcoal), and possibly magnesium-rich silicates. The"
  },
  {
    "source": "Jupiter trojan.txt",
    "chunk_id": "Jupiter trojan.txt_10",
    "chunk": "composition of the Jupiter trojan population appears to be markedly uniform, with little or no differentiation between the two swarms. A team from the Keck Observatory in Hawaii announced in 2006 that it had measured the density of the binary Jupiter trojan 617 Patroclus as being less than that of water ice (0.8 g/cm), suggesting that the pair, and possibly many other Trojan objects, more closely resemble comets or Kuiper belt objects in composition—water ice with a layer of dust—than they do the main-belt asteroids. Countering this argument, the density of Hektor as determined from its rotational lightcurve (2.480 g/cm) is significantly higher than that of 617 Patroclus. Such a difference in densities suggests that density may not be a good indicator of asteroid origin. Two main theories have emerged to explain the formation and evolution of the Jupiter trojans. The first suggests that the Jupiter trojans formed in the same part of the Solar System as Jupiter and entered their orbits while it was forming. The last stage of Jupiter's formation involved runaway growth of its mass through the accretion of large amounts of hydrogen and helium from the protoplanetary disk; during this growth, which lasted for only about"
  },
  {
    "source": "Jupiter trojan.txt",
    "chunk_id": "Jupiter trojan.txt_11",
    "chunk": "10,000 years, the mass of Jupiter increased by a factor of ten. The planetesimals that had approximately the same orbits as Jupiter were caught by the increased gravity of the planet. The capture mechanism was very efficient—about 50% of all remaining planetesimals were trapped. This hypothesis has two major problems: the number of trapped bodies exceeds the observed population of Jupiter trojans by four orders of magnitude, and the present Jupiter trojan asteroids have larger orbital inclinations than are predicted by the capture model. Simulations of this scenario show that such a mode of formation also would inhibit the creation of similar trojans for Saturn, and this has been borne out by observation: to date no trojans have been found near Saturn. In a variation of this theory Jupiter captures trojans during its initial growth then migrates as it continues to grow. During Jupiter's migration the orbits of objects in horseshoe orbits are distorted causing the L4 side of these orbits to be over occupied. As a result, an excess of trojans is trapped on the L4 side when the horseshoe orbits shift to tadpole orbits as Jupiter grows. This model also leaves the Jupiter trojan population 3–4 orders of"
  },
  {
    "source": "Jupiter trojan.txt",
    "chunk_id": "Jupiter trojan.txt_12",
    "chunk": "magnitude too large. The second theory proposes that the Jupiter trojans were captured during the migration of the giant planets described in the Nice model. In the Nice model the orbits of the giant planets became unstable 500–600 million years after the Solar System's formation when Jupiter and Saturn crossed their 1:2 mean-motion resonance. Encounters between planets resulted in Uranus and Neptune being scattered outward into the primordial Kuiper belt, disrupting it and throwing millions of objects inward. When Jupiter and Saturn were near their 1:2 resonance the orbits of pre-existing Jupiter trojans became unstable during a secondary resonance with Jupiter and Saturn. This occurred when the period of the trojans' libration about their Lagrangian point had a 3:1 ratio to the period at which the position where Jupiter passes Saturn circulated relative to its perihelion. This process was also reversible allowing a fraction of the numerous objects scattered inward by Uranus and Neptune to enter this region and be captured as Jupiter's and Saturn's orbits separated. These new trojans had a wide range of inclinations, the result of multiple encounters with the giant planets before being captured. This process can also occur later when Jupiter and Saturn cross weaker"
  },
  {
    "source": "Jupiter trojan.txt",
    "chunk_id": "Jupiter trojan.txt_13",
    "chunk": "resonances. In a revised version of the Nice model Jupiter trojans are captured when Jupiter encounters an ice giant during the instability. In this version of the Nice model one of the ice giants (Uranus, Neptune, or a lost fifth planet) is scattered inward onto a Jupiter-crossing orbit and is scattered outward by Jupiter causing the orbits of Jupiter and Saturn to quickly separate. When Jupiter's semi-major axis jumps during these encounters existing Jupiter trojans can escape and new objects with semi-major axes similar to Jupiter's new semi-major axis are captured. Following its last encounter the ice giant can pass through one of the libration points and perturb their orbits leaving this libration point depleted relative to the other. After the encounters end some of these Jupiter trojans are lost and others captured when Jupiter and Saturn are near weak mean motion resonances such as the 3:7 resonance via the mechanism of the original Nice model. The long-term future of the Jupiter trojans is open to question, because multiple weak resonances with Jupiter and Saturn cause them to behave chaotically over time. Collisional shattering slowly depletes the Jupiter trojan population as fragments are ejected. Ejected Jupiter trojans could become temporary"
  },
  {
    "source": "Jupiter trojan.txt",
    "chunk_id": "Jupiter trojan.txt_14",
    "chunk": "satellites of Jupiter or Jupiter-family comets. Simulations show that the orbits of up to 17% of Jupiter trojans are unstable over the age of the Solar System. Levison et al. believe that roughly 200 ejected Jupiter trojans greater than 1 km in diameter might be travelling the Solar System, with a few possibly on Earth-crossing orbits. Some of the escaped Jupiter trojans may become Jupiter-family comets as they approach the Sun and their surface ice begins evaporating. On 4 January 2017 NASA announced that Lucy was selected as one of their next two Discovery Program missions. Lucy is set to explore seven Jupiter trojans. It was launched on October 16, 2021, and will arrive at the L4 Trojan cloud in 2027 after two Earth gravity assists and a fly-by of a main-belt asteroid. It will then return to the vicinity of Earth for another gravity assist to take it to Jupiter's L5 Trojan cloud where it will visit 617 Patroclus."
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_0",
    "chunk": "# Kepler's laws of planetary motion In astronomy, Kepler's laws of planetary motion, published by Johannes Kepler in 1609 (except the third law, which was fully published in 1619), describe the orbits of planets around the Sun. These laws replaced circular orbits and epicycles in the heliocentric theory of Nicolaus Copernicus with elliptical orbits and explained how planetary velocities vary. The three laws state that: The elliptical orbits of planets were indicated by calculations of the orbit of Mars. From this, Kepler inferred that other bodies in the Solar System, including those farther away from the Sun, also have elliptical orbits. The second law establishes that when a planet is closer to the Sun, it travels faster. The third law expresses that the farther a planet is from the Sun, the longer its orbital period. Isaac Newton showed in 1687 that relationships like Kepler's would apply in the Solar System as a consequence of his own laws of motion and law of universal gravitation. A more precise historical approach is found in Astronomia nova and Epitome Astronomiae Copernicanae. Johannes Kepler's laws improved the model of Copernicus. According to Copernicus: Despite being correct in saying that the planets revolved around the"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_1",
    "chunk": "Sun, Copernicus was incorrect in defining their orbits. Introducing physical explanations for movement in space beyond just geometry, Kepler correctly defined the orbit of planets as follows: The eccentricity of the orbit of the Earth makes the time from the March equinox to the September equinox, around 186 days, unequal to the time from the September equinox to the March equinox, around 179 days. A diameter would cut the orbit into equal parts, but the plane through the Sun parallel to the equator of the Earth cuts the orbit into two parts with areas in a 186 to 179 ratio, so the eccentricity of the orbit of the Earth is approximately which is close to the correct value (0.016710218). The accuracy of this calculation requires that the two dates chosen be along the elliptical orbit's minor axis and that the midpoints of each half be along the major axis. As the two dates chosen here are equinoxes, this will be correct when perihelion, the date the Earth is closest to the Sun, falls on a solstice. The current perihelion, near January 4, is fairly close to the solstice of December 21 or 22. It took nearly two centuries for the"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_2",
    "chunk": "current formulation of Kepler's work to take on its settled form. Voltaire's Eléments de la philosophie de Newton (Elements of Newton's Philosophy) of 1738 was the first publication to use the terminology of \"laws\". The Biographical Encyclopedia of Astronomers in its article on Kepler (p. 620) states that the terminology of scientific laws for these discoveries was current at least from the time of Joseph de Lalande. It was the exposition of Robert Small, in An account of the astronomical discoveries of Kepler (1814) that made up the set of three laws, by adding in the third. Small also claimed, against the history, that these were empirical laws, based on inductive reasoning. Further, the current usage of \"Kepler's second law\" is something of a misnomer. Kepler had two versions, related in a qualitative sense: the \"distance law\" and the \"area law\". The \"area law\" is what became the second law in the set of three; but Kepler did himself not privilege it in that way. Kepler published his first two laws about planetary motion in 1609, having found them by analyzing the astronomical observations of Tycho Brahe. Kepler's third law was published in 1619. Kepler had believed in the Copernican"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_3",
    "chunk": "model of the Solar System, which called for circular orbits, but he could not reconcile Brahe's highly precise observations with a circular fit to Mars' orbit – Mars coincidentally having the highest eccentricity of all planets except Mercury. His first law reflected this discovery. In 1621, Kepler noted that his third law applies to the four brightest moons of Jupiter. Godefroy Wendelin also made this observation in 1643. The second law, in the \"area law\" form, was contested by Nicolaus Mercator in a book from 1664, but by 1670 his Philosophical Transactions were in its favour. As the century proceeded it became more widely accepted. The reception in Germany changed noticeably between 1688, the year in which Newton's Principia was published and was taken to be basically Copernican, and 1690, by which time work of Gottfried Leibniz on Kepler had been published. Newton was credited with understanding that the second law is not special to the inverse square law of gravitation, being a consequence just of the radial nature of that law, whereas the other laws do depend on the inverse square form of the attraction. Carl Runge and Wilhelm Lenz much later identified a symmetry principle in the phase"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_4",
    "chunk": "space of planetary motion (the orthogonal group O(4) acting) which accounts for the first and third laws in the case of Newtonian gravitation, as conservation of angular momentum does via rotational symmetry for the second law. The mathematical model of the kinematics of a planet subject to the laws allows a large range of further calculations. where p {\\displaystyle p} is the semi-latus rectum, ε is the eccentricity of the ellipse, r is the distance from the Sun to the planet, and θ is the angle to the planet's current position from its closest approach, as seen from the Sun. So (r, θ) are polar coordinates. For an ellipse 0 < ε < 1 ; in the limiting case ε = 0, the orbit is a circle with the Sun at the centre (i.e. where there is zero eccentricity). At θ = 180°, aphelion, the distance is maximum (by definition, aphelion is – invariably – perihelion plus 180°) The special case of a circle is ε = 0, resulting in r = p = rmin = rmax = a = b and A = πr. A line joining a planet and the Sun sweeps out equal areas during equal intervals"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_5",
    "chunk": "of time. The orbital radius and angular velocity of the planet in the elliptical orbit will vary. This is shown in the animation: the planet travels faster when closer to the Sun, then slower when farther from the Sun. Kepler's second law states that the blue sector has constant area. Kepler notably arrived at this law through assumptions that were either only approximately true or outright false and can be outlined as follows: Nevertheless, the result of the second law is exactly true, as it is logically equivalent to the conservation of angular momentum, which is true for any body experiencing a radially symmetric force. A correct proof can be shown through this. Since the cross product of two vectors gives the area of a parallelogram possessing sides of those vectors, the triangular area dA swept out in a short period of time is given by half the cross product of the r and dx vectors, for some short piece of the orbit, dx. d A = 1 2 ( r → × d x → ) = 1 2 ( r → × v → d t ) {\\displaystyle dA={\\frac {1}{2}}({\\vec {r}}\\times {\\vec {dx}})={\\frac {1}{2}}({\\vec {r}}\\times {\\vec {v}}dt)} for"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_6",
    "chunk": "a small piece of the orbit dx and time to cover it dt. Thus d A d t = 1 2 ( r → × v → ) . {\\displaystyle {\\frac {dA}{dt}}={\\frac {1}{2}}({\\vec {r}}\\times {\\vec {v}}).} d A d t = 1 m 1 2 ( r → × p → ) . {\\displaystyle {\\frac {dA}{dt}}={\\frac {1}{m}}{\\frac {1}{2}}({\\vec {r}}\\times {\\vec {p}}).} Since the final expression is proportional to the total angular momentum ( r → × p → ) {\\displaystyle ({\\vec {r}}\\times {\\vec {p}})} , Kepler's equal area law will hold for any system that conserves angular momentum. Since any radial force will produce no torque on the planet's motion, angular momentum will be conserved. In a small time d t {\\displaystyle dt} the planet sweeps out a small triangle having base line r {\\displaystyle r} and height r d θ {\\displaystyle r\\,d\\theta } and area d A = 1 2 ⋅ r ⋅ r d θ {\\textstyle dA={\\frac {1}{2}}\\cdot r\\cdot r\\,d\\theta } , so the constant areal velocity is d A d t = r 2 2 d θ d t . {\\displaystyle {\\frac {dA}{dt}}={\\frac {r^{2}}{2}}{\\frac {d\\theta }{dt}}.} The area enclosed by the elliptical orbit is π a b"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_7",
    "chunk": "{\\displaystyle \\pi ab} . So the period T {\\displaystyle T} satisfies And so, d A d t = a b n 2 = π a b T . {\\displaystyle {\\frac {dA}{dt}}={\\frac {abn}{2}}={\\frac {\\pi ab}{T}}.} The ratio of the square of an object's orbital period with the cube of the semi-major axis of its orbit is the same for all objects orbiting the same primary. This captures the relationship between the distance of planets from the Sun, and their orbital periods. Kepler enunciated in 1619 this third law in a laborious attempt to determine what he viewed as the \"music of the spheres\" according to precise laws, and express it in terms of musical notation. It was therefore known as the harmonic law. The original form of this law (referring to not the semi-major axis, but rather a \"mean distance\") holds true only for planets with small eccentricities near zero. Using Newton's law of gravitation (published 1687), this relation can be found in the case of a circular orbit by setting the centripetal force equal to the gravitational force: Then, expressing the angular velocity ω in terms of the orbital period T {\\displaystyle {T}} and then rearranging, results in Kepler's third"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_8",
    "chunk": "law: A more detailed derivation can be done with general elliptical orbits, instead of circles, as well as orbiting the center of mass, instead of just the large mass. This results in replacing a circular radius, r {\\displaystyle r} , with the semi-major axis, a {\\displaystyle a} , of the elliptical relative motion of one mass relative to the other, as well as replacing the large mass M {\\displaystyle M} with M + m {\\displaystyle M+m} . However, with planet masses being so much smaller than the Sun, this correction is often ignored. The full corresponding formula is: where M {\\displaystyle M} is the mass of the Sun, m {\\displaystyle m} is the mass of the planet, G {\\displaystyle G} is the gravitational constant, T {\\displaystyle T} is the orbital period and a {\\displaystyle a} is the elliptical semi-major axis, and AU {\\displaystyle {\\text{AU}}} is the astronomical unit, the average distance from earth to the sun. Kepler became aware of John Napier's recent invention of logarithms and log-log graphs before he discovered the pattern. I first believed I was dreaming... But it is absolutely certain and exact that the ratio which exists between the period times of any two planets"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_9",
    "chunk": "is precisely the ratio of the 3/2th power of the mean distance. Isaac Newton computed in his Philosophiæ Naturalis Principia Mathematica the acceleration of a planet moving according to Kepler's first and second laws. This implies that the Sun may be the physical cause of the acceleration of planets. However, Newton states in his Principia that he considers forces from a mathematical point of view, not a physical, thereby taking an instrumentalist view. Moreover, he does not assign a cause to gravity. Newton defined the force acting on a planet to be the product of its mass and the acceleration (see Newton's laws of motion). So: The Sun plays an unsymmetrical part, which is unjustified. So he assumed, in Newton's law of universal gravitation: As the planets have small masses compared to that of the Sun, the orbits conform approximately to Kepler's laws. Newton's model improves upon Kepler's model, and fits actual observations more accurately. (See two-body problem.) Below comes the detailed calculation of the acceleration of a planet moving according to Kepler's first and second laws. From the heliocentric point of view consider the vector to the planet r = r r ^ {\\displaystyle \\mathbf {r} =r{\\hat {\\mathbf {r}"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_10",
    "chunk": "}}} where r {\\displaystyle r} is the distance to the planet and r ^ {\\displaystyle {\\hat {\\mathbf {r} }}} is a unit vector pointing towards the planet. d r ^ d t = r ^ ˙ = θ ˙ θ ^ , d θ ^ d t = θ ^ ˙ = − θ ˙ r ^ {\\displaystyle {\\frac {d{\\hat {\\mathbf {r} }}}{dt}}={\\dot {\\hat {\\mathbf {r} }}}={\\dot {\\theta }}{\\hat {\\boldsymbol {\\theta }}},\\qquad {\\frac {d{\\hat {\\boldsymbol {\\theta }}}}{dt}}={\\dot {\\hat {\\boldsymbol {\\theta }}}}=-{\\dot {\\theta }}{\\hat {\\mathbf {r} }}} where θ ^ {\\displaystyle {\\hat {\\boldsymbol {\\theta }}}} is the unit vector whose direction is 90 degrees counterclockwise of r ^ {\\displaystyle {\\hat {\\mathbf {r} }}} , and θ {\\displaystyle \\theta } is the polar angle, and where a dot on top of the variable signifies differentiation with respect to time. Differentiate the position vector twice to obtain the velocity vector and the acceleration vector: r ˙ = r ˙ r ^ + r r ^ ˙ = r ˙ r ^ + r θ ˙ θ ^ , r ¨ = ( r ¨ r ^ + r ˙ r ^ ˙ ) + ( r ˙ θ ˙ θ ^ + r"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_11",
    "chunk": "θ ¨ θ ^ + r θ ˙ θ ^ ˙ ) = ( r ¨ − r θ ˙ 2 ) r ^ + ( r θ ¨ + 2 r ˙ θ ˙ ) θ ^ . {\\displaystyle {\\begin{aligned}{\\dot {\\mathbf {r} }}&={\\dot {r}}{\\hat {\\mathbf {r} }}+r{\\dot {\\hat {\\mathbf {r} }}}={\\dot {r}}{\\hat {\\mathbf {r} }}+r{\\dot {\\theta }}{\\hat {\\boldsymbol {\\theta }}},\\\\{\\ddot {\\mathbf {r} }}&=\\left({\\ddot {r}}{\\hat {\\mathbf {r} }}+{\\dot {r}}{\\dot {\\hat {\\mathbf {r} }}}\\right)+\\left({\\dot {r}}{\\dot {\\theta }}{\\hat {\\boldsymbol {\\theta }}}+r{\\ddot {\\theta }}{\\hat {\\boldsymbol {\\theta }}}+r{\\dot {\\theta }}{\\dot {\\hat {\\boldsymbol {\\theta }}}}\\right)=\\left({\\ddot {r}}-r{\\dot {\\theta }}^{2}\\right){\\hat {\\mathbf {r} }}+\\left(r{\\ddot {\\theta }}+2{\\dot {r}}{\\dot {\\theta }}\\right){\\hat {\\boldsymbol {\\theta }}}.\\end{aligned}}} So r ¨ = a r r ^ + a θ θ ^ {\\displaystyle {\\ddot {\\mathbf {r} }}=a_{r}{\\hat {\\boldsymbol {r}}}+a_{\\theta }{\\hat {\\boldsymbol {\\theta }}}} where the radial acceleration is a r = r ¨ − r θ ˙ 2 {\\displaystyle a_{r}={\\ddot {r}}-r{\\dot {\\theta }}^{2}} and the transversal acceleration is a θ = r θ ¨ + 2 r ˙ θ ˙ . {\\displaystyle a_{\\theta }=r{\\ddot {\\theta }}+2{\\dot {r}}{\\dot {\\theta }}.} Kepler's second law says that r 2 θ ˙ = n a b {\\displaystyle r^{2}{\\dot {\\theta }}=nab} is constant. The transversal acceleration a θ {\\displaystyle a_{\\theta"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_12",
    "chunk": "}} is zero: d ( r 2 θ ˙ ) d t = r ( 2 r ˙ θ ˙ + r θ ¨ ) = r a θ = 0. {\\displaystyle {\\frac {d\\left(r^{2}{\\dot {\\theta }}\\right)}{dt}}=r\\left(2{\\dot {r}}{\\dot {\\theta }}+r{\\ddot {\\theta }}\\right)=ra_{\\theta }=0.} So the acceleration of a planet obeying Kepler's second law is directed towards the Sun. The radial acceleration a r {\\displaystyle a_{\\text{r}}} is a r = r ¨ − r θ ˙ 2 = r ¨ − r ( n a b r 2 ) 2 = r ¨ − n 2 a 2 b 2 r 3 . {\\displaystyle a_{\\text{r}}={\\ddot {r}}-r{\\dot {\\theta }}^{2}={\\ddot {r}}-r\\left({\\frac {nab}{r^{2}}}\\right)^{2}={\\ddot {r}}-{\\frac {n^{2}a^{2}b^{2}}{r^{3}}}.} Kepler's first law states that the orbit is described by the equation: p r = 1 + ε cos ⁡ ( θ ) . {\\displaystyle {\\frac {p}{r}}=1+\\varepsilon \\cos(\\theta ).} Differentiating with respect to time − p r ˙ r 2 = − ε sin ⁡ ( θ ) θ ˙ {\\displaystyle -{\\frac {p{\\dot {r}}}{r^{2}}}=-\\varepsilon \\sin(\\theta )\\,{\\dot {\\theta }}} or p r ˙ = n a b ε sin ⁡ ( θ ) . {\\displaystyle p{\\dot {r}}=nab\\,\\varepsilon \\sin(\\theta ).} Differentiating once more p r ¨ = n a b ε cos"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_13",
    "chunk": "⁡ ( θ ) θ ˙ = n a b ε cos ⁡ ( θ ) n a b r 2 = n 2 a 2 b 2 r 2 ε cos ⁡ ( θ ) . {\\displaystyle p{\\ddot {r}}=nab\\varepsilon \\cos(\\theta )\\,{\\dot {\\theta }}=nab\\varepsilon \\cos(\\theta )\\,{\\frac {nab}{r^{2}}}={\\frac {n^{2}a^{2}b^{2}}{r^{2}}}\\varepsilon \\cos(\\theta ).} The radial acceleration a r {\\displaystyle a_{\\text{r}}} satisfies p a r = n 2 a 2 b 2 r 2 ε cos ⁡ ( θ ) − p n 2 a 2 b 2 r 3 = n 2 a 2 b 2 r 2 ( ε cos ⁡ ( θ ) − p r ) . {\\displaystyle pa_{\\text{r}}={\\frac {n^{2}a^{2}b^{2}}{r^{2}}}\\varepsilon \\cos(\\theta )-p{\\frac {n^{2}a^{2}b^{2}}{r^{3}}}={\\frac {n^{2}a^{2}b^{2}}{r^{2}}}\\left(\\varepsilon \\cos(\\theta )-{\\frac {p}{r}}\\right).} Substituting the equation of the ellipse gives p a r = n 2 a 2 b 2 r 2 ( p r − 1 − p r ) = − n 2 a 2 r 2 b 2 . {\\displaystyle pa_{\\text{r}}={\\frac {n^{2}a^{2}b^{2}}{r^{2}}}\\left({\\frac {p}{r}}-1-{\\frac {p}{r}}\\right)=-{\\frac {n^{2}a^{2}}{r^{2}}}b^{2}.} The relation b 2 = p a {\\displaystyle b^{2}=pa} gives the simple final result a r = − n 2 a 3 r 2 . {\\displaystyle a_{\\text{r}}=-{\\frac {n^{2}a^{3}}{r^{2}}}.} This means that the acceleration vector r ¨ {\\displaystyle \\mathbf"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_14",
    "chunk": "{\\ddot {r}} } of any planet obeying Kepler's first and second law satisfies the inverse square law r ¨ = − α r 2 r ^ {\\displaystyle \\mathbf {\\ddot {r}} =-{\\frac {\\alpha }{r^{2}}}{\\hat {\\mathbf {r} }}} where α = n 2 a 3 {\\displaystyle \\alpha =n^{2}a^{3}} is a constant, and r ^ {\\displaystyle {\\hat {\\mathbf {r} }}} is the unit vector pointing from the Sun towards the planet, and r {\\displaystyle r\\,} is the distance between the planet and the Sun. Since mean motion n = 2 π T {\\displaystyle n={\\frac {2\\pi }{T}}} where T {\\displaystyle T} is the period, according to Kepler's third law, α {\\displaystyle \\alpha } has the same value for all the planets. So the inverse square law for planetary accelerations applies throughout the entire Solar System. The inverse square law is a differential equation. The solutions to this differential equation include the Keplerian motions, as shown, but they also include motions where the orbit is a hyperbola or parabola or a straight line. (See Kepler orbit.) By Newton's second law, the gravitational force that acts on the planet is: F = m planet r ¨ = − m planet α r − 2 r ^"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_15",
    "chunk": "{\\displaystyle \\mathbf {F} =m_{\\text{planet}}\\mathbf {\\ddot {r}} =-m_{\\text{planet}}\\alpha r^{-2}{\\hat {\\mathbf {r} }}} where m planet {\\displaystyle m_{\\text{planet}}} is the mass of the planet and α {\\displaystyle \\alpha } has the same value for all planets in the Solar System. According to Newton's third law, the Sun is attracted to the planet by a force of the same magnitude. Since the force is proportional to the mass of the planet, under the symmetric consideration, it should also be proportional to the mass of the Sun, m Sun {\\displaystyle m_{\\text{Sun}}} . So α = G m Sun {\\displaystyle \\alpha =Gm_{\\text{Sun}}} where G {\\displaystyle G} is the gravitational constant. The acceleration of Solar System body number i is, according to Newton's laws: r ¨ i = G ∑ j ≠ i m j r i j − 2 r ^ i j {\\displaystyle \\mathbf {\\ddot {r}} _{i}=G\\sum _{j\\neq i}m_{j}r_{ij}^{-2}{\\hat {\\mathbf {r} }}_{ij}} where m j {\\displaystyle m_{j}} is the mass of body j, r i j {\\displaystyle r_{ij}} is the distance between body i and body j, r ^ i j {\\displaystyle {\\hat {\\mathbf {r} }}_{ij}} is the unit vector from body i towards body j, and the vector summation is over all bodies"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_16",
    "chunk": "in the Solar System, besides i itself. In the special case where there are only two bodies in the Solar System, Earth and Sun, the acceleration becomes r ¨ Earth = G m Sun r Earth , Sun − 2 r ^ Earth , Sun {\\displaystyle \\mathbf {\\ddot {r}} _{\\text{Earth}}=Gm_{\\text{Sun}}r_{{\\text{Earth}},{\\text{Sun}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Earth}},{\\text{Sun}}}} which is the acceleration of the Kepler motion. So this Earth moves around the Sun according to Kepler's laws. If the two bodies in the Solar System are Moon and Earth the acceleration of the Moon becomes r ¨ Moon = G m Earth r Moon , Earth − 2 r ^ Moon , Earth {\\displaystyle \\mathbf {\\ddot {r}} _{\\text{Moon}}=Gm_{\\text{Earth}}r_{{\\text{Moon}},{\\text{Earth}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Moon}},{\\text{Earth}}}} So in this approximation, the Moon moves around the Earth according to Kepler's laws. In the three-body case the accelerations are r ¨ Sun = G m Earth r Sun , Earth − 2 r ^ Sun , Earth + G m Moon r Sun , Moon − 2 r ^ Sun , Moon r ¨ Earth = G m Sun r Earth , Sun − 2 r ^ Earth , Sun + G m Moon r Earth , Moon − 2 r ^"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_17",
    "chunk": "Earth , Moon r ¨ Moon = G m Sun r Moon , Sun − 2 r ^ Moon , Sun + G m Earth r Moon , Earth − 2 r ^ Moon , Earth {\\displaystyle {\\begin{aligned}\\mathbf {\\ddot {r}} _{\\text{Sun}}&=Gm_{\\text{Earth}}r_{{\\text{Sun}},{\\text{Earth}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Sun}},{\\text{Earth}}}+Gm_{\\text{Moon}}r_{{\\text{Sun}},{\\text{Moon}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Sun}},{\\text{Moon}}}\\\\\\mathbf {\\ddot {r}} _{\\text{Earth}}&=Gm_{\\text{Sun}}r_{{\\text{Earth}},{\\text{Sun}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Earth}},{\\text{Sun}}}+Gm_{\\text{Moon}}r_{{\\text{Earth}},{\\text{Moon}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Earth}},{\\text{Moon}}}\\\\\\mathbf {\\ddot {r}} _{\\text{Moon}}&=Gm_{\\text{Sun}}r_{{\\text{Moon}},{\\text{Sun}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Moon}},{\\text{Sun}}}+Gm_{\\text{Earth}}r_{{\\text{Moon}},{\\text{Earth}}}^{-2}{\\hat {\\mathbf {r} }}_{{\\text{Moon}},{\\text{Earth}}}\\end{aligned}}} These accelerations are not those of Kepler orbits, and the three-body problem is complicated. But Keplerian approximation is the basis for perturbation calculations. (See Lunar theory.) Kepler used his two first laws to compute the position of a planet as a function of time. His method involves the solution of a transcendental equation called Kepler's equation. The procedure for calculating the heliocentric polar coordinates (r,θ) of a planet as a function of the time t since perihelion, is the following five steps: The position polar coordinates (r,θ) can now be written as a Cartesian vector p = r ⟨ cos ⁡ θ , sin ⁡ θ ⟩ {\\displaystyle \\mathbf {p} =r\\left\\langle \\cos {\\theta },\\sin {\\theta }\\right\\rangle } and the Cartesian velocity vector can then be calculated as v = μ a r ⟨ − sin"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_18",
    "chunk": "⁡ E , 1 − ε 2 cos ⁡ E ⟩ {\\displaystyle \\mathbf {v} ={\\frac {\\sqrt {\\mu a}}{r}}\\left\\langle -\\sin {E},{\\sqrt {1-\\varepsilon ^{2}}}\\cos {E}\\right\\rangle } , where μ {\\displaystyle \\mu } is the standard gravitational parameter. The important special case of circular orbit, ε = 0, gives θ = E = M. Because the uniform circular motion was considered to be normal, a deviation from this motion was considered an anomaly. The problem is to compute the polar coordinates (r,θ) of the planet from the time since perihelion, t. It is solved in steps. Kepler considered the circle with the major axis as a diameter, and The sector areas are related by | z s p | = b a ⋅ | z s x | . {\\displaystyle |zsp|={\\frac {b}{a}}\\cdot |zsx|.} The circular sector area | z c y | = a 2 M 2 . {\\displaystyle |zcy|={\\frac {a^{2}M}{2}}.} The area swept since perihelion, | z s p | = b a ⋅ | z s x | = b a ⋅ | z c y | = b a ⋅ a 2 M 2 = a b M 2 , {\\displaystyle |zsp|={\\frac {b}{a}}\\cdot |zsx|={\\frac {b}{a}}\\cdot |zcy|={\\frac {b}{a}}\\cdot {\\frac {a^{2}M}{2}}={\\frac {abM}{2}},}"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_19",
    "chunk": "is by Kepler's second law proportional to time since perihelion. So the mean anomaly, M, is proportional to time since perihelion, t. M = n t , {\\displaystyle M=nt,} where n is the mean motion. When the mean anomaly M is computed, the goal is to compute the true anomaly θ. The function θ = f(M) is, however, not elementary. Kepler's solution is to use E = ∠ z c x , {\\displaystyle E=\\angle zcx,} x as seen from the centre, the eccentric anomaly as an intermediate variable, and first compute E as a function of M by solving Kepler's equation below, and then compute the true anomaly θ from the eccentric anomaly E. Here are the details. | z c y | = | z s x | = | z c x | − | s c x | w i t h | s c x | = | c s | . | d x | 2 a 2 M 2 = a 2 E 2 − a ε ⋅ a sin ⁡ E 2 {\\displaystyle {\\begin{aligned}|zcy|&=|zsx|=|zcx|-|scx|\\\\with|scx|&={\\frac {|cs|.|dx|}{2}}\\\\{\\frac {a^{2}M}{2}}&={\\frac {a^{2}E}{2}}-{\\frac {a\\varepsilon \\cdot a\\sin E}{2}}\\end{aligned}}} Division by a/2 gives Kepler's equation M = E − ε sin"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_20",
    "chunk": "⁡ E . {\\displaystyle M=E-\\varepsilon \\sin E.} This equation gives M as a function of E. Determining E for a given M is the inverse problem. Iterative numerical algorithms are commonly used. Having computed the eccentric anomaly E, the next step is to calculate the true anomaly θ. But note: Cartesian position coordinates with reference to the center of ellipse are (a cos E, b sin E) With reference to the Sun (with coordinates (c,0) = (ae,0) ), r = (a cos E – ae, b sin E) Note from the figure that | c d | = | c s | + | s d | {\\displaystyle |cd|=|cs|+|sd|} so that a cos ⁡ E = a ε + r cos ⁡ θ . {\\displaystyle a\\cos E=a\\varepsilon +r\\cos \\theta .} Dividing by a {\\displaystyle a} and inserting from Kepler's first law r a = 1 − ε 2 1 + ε cos ⁡ θ {\\displaystyle {\\frac {r}{a}}={\\frac {1-\\varepsilon ^{2}}{1+\\varepsilon \\cos \\theta }}} to get cos ⁡ E = ε + 1 − ε 2 1 + ε cos ⁡ θ cos ⁡ θ = ε ( 1 + ε cos ⁡ θ ) + ( 1 − ε 2 )"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_21",
    "chunk": "cos ⁡ θ 1 + ε cos ⁡ θ = ε + cos ⁡ θ 1 + ε cos ⁡ θ . {\\displaystyle \\cos E=\\varepsilon +{\\frac {1-\\varepsilon ^{2}}{1+\\varepsilon \\cos \\theta }}\\cos \\theta ={\\frac {\\varepsilon (1+\\varepsilon \\cos \\theta )+\\left(1-\\varepsilon ^{2}\\right)\\cos \\theta }{1+\\varepsilon \\cos \\theta }}={\\frac {\\varepsilon +\\cos \\theta }{1+\\varepsilon \\cos \\theta }}.} The result is a usable relationship between the eccentric anomaly E and the true anomaly θ. A computationally more convenient form follows by substituting into the trigonometric identity: tan 2 ⁡ x 2 = 1 − cos ⁡ x 1 + cos ⁡ x . {\\displaystyle \\tan ^{2}{\\frac {x}{2}}={\\frac {1-\\cos x}{1+\\cos x}}.} Get tan 2 ⁡ E 2 = 1 − cos ⁡ E 1 + cos ⁡ E = 1 − ε + cos ⁡ θ 1 + ε cos ⁡ θ 1 + ε + cos ⁡ θ 1 + ε cos ⁡ θ = ( 1 + ε cos ⁡ θ ) − ( ε + cos ⁡ θ ) ( 1 + ε cos ⁡ θ ) + ( ε + cos ⁡ θ ) = 1 − ε 1 + ε ⋅ 1 − cos ⁡ θ 1 + cos ⁡ θ = 1"
  },
  {
    "source": "Kepler's laws of planetary motion.txt",
    "chunk_id": "Kepler's laws of planetary motion.txt_22",
    "chunk": "− ε 1 + ε tan 2 ⁡ θ 2 . {\\displaystyle {\\begin{aligned}\\tan ^{2}{\\frac {E}{2}}&={\\frac {1-\\cos E}{1+\\cos E}}={\\frac {1-{\\frac {\\varepsilon +\\cos \\theta }{1+\\varepsilon \\cos \\theta }}}{1+{\\frac {\\varepsilon +\\cos \\theta }{1+\\varepsilon \\cos \\theta }}}}\\\\[8pt]&={\\frac {(1+\\varepsilon \\cos \\theta )-(\\varepsilon +\\cos \\theta )}{(1+\\varepsilon \\cos \\theta )+(\\varepsilon +\\cos \\theta )}}={\\frac {1-\\varepsilon }{1+\\varepsilon }}\\cdot {\\frac {1-\\cos \\theta }{1+\\cos \\theta }}={\\frac {1-\\varepsilon }{1+\\varepsilon }}\\tan ^{2}{\\frac {\\theta }{2}}.\\end{aligned}}} Multiplying by 1 + ε gives the result ( 1 − ε ) tan 2 ⁡ θ 2 = ( 1 + ε ) tan 2 ⁡ E 2 {\\displaystyle (1-\\varepsilon )\\tan ^{2}{\\frac {\\theta }{2}}=(1+\\varepsilon )\\tan ^{2}{\\frac {E}{2}}} The fourth step is to compute the heliocentric distance r from the true anomaly θ by Kepler's first law: r ( 1 + ε cos ⁡ θ ) = a ( 1 − ε 2 ) {\\displaystyle r(1+\\varepsilon \\cos \\theta )=a\\left(1-\\varepsilon ^{2}\\right)} Using the relation above between θ and E the final equation for the distance r is: r = a ( 1 − ε cos ⁡ E ) . {\\displaystyle r=a(1-\\varepsilon \\cos E).}"
  },
  {
    "source": "Kepler-11.txt",
    "chunk_id": "Kepler-11.txt_0",
    "chunk": "# Kepler-11 Kepler-11, also designated as 2MASS J19482762+4154328, is a Sun-like star slightly larger than the Sun in the constellation Cygnus, located some 2,110 light years from Earth. It is located within the field of vision of the Kepler space telescope, the satellite that NASA's Kepler Mission uses to detect planets that may be transiting their stars. Announced on February 2, 2011, the star system is among the most compact and flattest systems yet discovered. It is the first discovered case of a star system with six transiting planets. All discovered planets are larger than Earth, with the larger ones being about Neptune's size. Kepler-11 and its planets were discovered by NASA's Kepler Mission, a mission tasked with discovering planets in transit around their stars. The transit method that Kepler uses involves detecting dips in brightness in stars. These dips in brightness can be interpreted as planets whose orbits move in front of their stars from the perspective of Earth. Kepler-11 is the first discovered exoplanetary system with more than three transiting planets. Kepler-11 is named for the Kepler Mission: it is the 11th star with confirmed planets discovered in the Kepler field of view. The planets are named alphabetically,"
  },
  {
    "source": "Kepler-11.txt",
    "chunk_id": "Kepler-11.txt_1",
    "chunk": "starting with the innermost: b, c, d, e, f, and g, distinguishers that are tagged onto the name of their home star. Kepler-11 is a G-type star that is approximately 104% the mass of and 102% the radius of the Sun. It has a surface temperature of about 5836 K and is estimated to have an age of around 3.2 billion years. In comparison, the Sun is about 4.6 billion years old and has a surface temperature of 5778 K. With an apparent magnitude of 14.2, it is too faint to be seen with the naked eye. All known planets transit the star; this means that all six planets' orbits appear to cross in front of their star as viewed from the Earth's perspective. Their inclinations relative to Earth's line of sight, or how far above or below the plane of sight they are, vary by a little more than a degree. This allows direct measurements of the planets' periods and relative diameters (compared to the host star) by monitoring each planet's transit of the star. Simulations suggest that the mean mutual inclinations of the planetary orbits are about 1°, meaning the system is probably more coplanar (flatter) than the"
  },
  {
    "source": "Kepler-11.txt",
    "chunk_id": "Kepler-11.txt_2",
    "chunk": "Solar System, where the corresponding figure is 2.3°. The estimated masses of planets b - f fall in the range between those of Earth and Neptune. Their estimated densities, all lower than that of Earth, imply that none of them have an Earth-like composition; a significant hydrogen/helium atmosphere is predicted for planets c, d, e, f, and g, while planet b may be surrounded by a steam atmosphere or perhaps by a hydrogen atmosphere. The low densities likely result from high-volume extended atmospheres that surround cores of iron, rock, and possibly H2O. The inner constituents of the Kepler-11 system were, at the time of their discoveries, the most comprehensively understood extrasolar planets smaller than Neptune. Currently, observations do not place a firm constraint on the mass of planet g (<25 ME). However, formation and evolution studies indicate that the mass of planet g is not much greater than about 7 ME. Kepler-11 planets may have formed in situ (i.e., at their observed orbital locations) or ex situ, that is, they may have started their formation farther away from the star while migrating inward through gravitational interactions with a gaseous protoplanetary disk. This second scenario predicts that a substantial fraction of"
  },
  {
    "source": "Kepler-11.txt",
    "chunk_id": "Kepler-11.txt_3",
    "chunk": "the planets' mass is in H2O. Regardless of the formation scenario, the gaseous component of the planets accounts for less than about 20% of their masses but for ≈40 to ≈60% of their radii. In 2014, the dynamical simulation shown what the Kepler-11 planetary system have likely to undergone a substantial inward migration in the past, producing an observed pattern of lower-mass planets on tightest orbits. Additional yet unobserved gas giant planets on wider orbit are likely necessary for migration of smaller planets to proceed that far inward. The system is among the most compact known; the orbits of planets b - f would easily fit inside the orbit of Mercury, with g only slightly outside it. Despite this close packing of the orbits, dynamical integrations indicate the Kepler-11 system has the potential to be stable on a time scale of billions of years. However, it may be approaching instability due to a secular resonance involving b and c. If this happens, b will most likely become eccentric enough that it collides with c. None of the planets are in low-ratio orbital resonances, in which multiple planets gravitationally tug on and stabilize each other's orbits, resulting in simple ratios of"
  },
  {
    "source": "Kepler-11.txt",
    "chunk_id": "Kepler-11.txt_4",
    "chunk": "their orbital periods. However, b and c are close to a 5:4 ratio. There could conceivably be other planets in the system that do not transit the star, but they would only be detectable by the effects of their gravity on the motion of the visible planets (much as how Neptune was discovered). The presence of additional gas giant planets is currently excluded up to orbital radius of 30 AU."
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_0",
    "chunk": "# Lambda-CDM model The Lambda-CDM, Lambda cold dark matter, or ΛCDM model is a mathematical model of the Big Bang theory with three major components: It is the current standard model of Big Bang cosmology, as it is the simplest model that provides a reasonably good account of: The model assumes that general relativity is the correct theory of gravity on cosmological scales. It emerged in the late 1990s as a concordance cosmology, after a period when disparate observed properties of the universe appeared mutually inconsistent, and there was no consensus on the makeup of the energy density of the universe. The ΛCDM model has been successful in modeling broad collection of astronomical observations over decades. Remaining issues challenge the assumptions of the ΛCDM model and have led to many alternative models. This combination greatly simplifies the equations of general relativity into a form called the Friedmann equations. These equations specify the evolution of the scale factor of the universe in terms of the pressure and density of a perfect fluid. The evolving density is composed of different kinds of energy and matter, each with its own role in affecting the scale factor. For example, a model might include baryons,"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_1",
    "chunk": "photons, neutrinos, and dark matter. These component densities become parameters extracted when the model is constrained to match astrophysical observations. The model aims to describe the observable universe from approximately 0.1 s to the present. The most accurate observations which are sensitive to the component densities are consequences of statistical inhomogeneity called \"perturbations\" in the early universe. Since the Friedmann equations assume homogeneity, additional theory must be added before comparison to experiments. Inflation is a simple model producing perturbations by postulating an extremely rapid expansion early in the universe that separates quantum fluctuations before they can equilibrate. The perturbations are characterized by additional parameters also determined by matching observations. Finally, the light which will become astronomical observations must pass through the universe. The latter part of that journey will pass through ionized space, where the electrons can scatter the light, altering the anisotropies. This effect is characterized by one additional parameter. The ΛCDM model includes an expansion of metric space that is well documented, both as the redshift of prominent spectral absorption or emission lines in the light from distant galaxies, and as the time dilation in the light decay of supernova luminosity curves. Both effects are attributed to a"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_2",
    "chunk": "Doppler shift in electromagnetic radiation as it travels across expanding space. Although this expansion increases the distance between objects that are not under shared gravitational influence, it does not increase the size of the objects (e.g. galaxies) in space. Also, since it originates from ordinary general relativity, it, like general relativity, allows for distant galaxies to recede from each other at speeds greater than the speed of light; local expansion is less than the speed of light, but expansion summed across great distances can collectively exceed the speed of light. The letter Λ (lambda) represents the cosmological constant, which is associated with a vacuum energy or dark energy in empty space that is used to explain the contemporary accelerating expansion of space against the attractive effects of gravity. A cosmological constant has negative pressure, p = − ρ c 2 {\\displaystyle p=-\\rho c^{2}} , which contributes to the stress–energy tensor that, according to the general theory of relativity, causes accelerating expansion. The fraction of the total energy density of our (flat or almost flat) universe that is dark energy, Ω Λ {\\displaystyle \\Omega _{\\Lambda }} , is estimated to be 0.669 ± 0.038 based on the 2018 Dark Energy Survey"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_3",
    "chunk": "results using Type Ia supernovae or 0.6847±0.0073 based on the 2018 release of Planck satellite data, or more than 68.3% (2018 estimate) of the mass–energy density of the universe. Dark matter is postulated in order to account for gravitational effects observed in very large-scale structures (the \"non-keplerian\" rotation curves of galaxies; the gravitational lensing of light by galaxy clusters; and the enhanced clustering of galaxies) that cannot be accounted for by the quantity of observed matter. The ΛCDM model proposes specifically cold dark matter, hypothesized as: Dark matter constitutes about 26.5% of the mass–energy density of the universe. The remaining 4.9% comprises all ordinary matter observed as atoms, chemical elements, gas and plasma, the stuff of which visible planets, stars and galaxies are made. The great majority of ordinary matter in the universe is unseen, since visible stars and gas inside galaxies and clusters account for less than 10% of the ordinary matter contribution to the mass–energy density of the universe. The model includes a single originating event, the \"Big Bang\", which was not an explosion but the abrupt appearance of expanding spacetime containing radiation at temperatures of around 10 K. This was immediately (within 10 seconds) followed by an"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_4",
    "chunk": "exponential expansion of space by a scale multiplier of 10 or more, known as cosmic inflation. The early universe remained hot (above 10 000 K) for several hundred thousand years, a state that is detectable as a residual cosmic microwave background, or CMB, a very low-energy radiation emanating from all parts of the sky. The \"Big Bang\" scenario, with cosmic inflation and standard particle physics, is the only cosmological model consistent with the observed continuing expansion of space, the observed distribution of lighter elements in the universe (hydrogen, helium, and lithium), and the spatial texture of minute irregularities (anisotropies) in the CMB radiation. Cosmic inflation also addresses the \"horizon problem\" in the CMB; indeed, it seems likely that the universe is larger than the observable particle horizon. The expansion of the universe is parameterized by a dimensionless scale factor a = a ( t ) {\\displaystyle a=a(t)} (with time t {\\displaystyle t} counted from the birth of the universe), defined relative to the present time, so a 0 = a ( t 0 ) = 1 {\\displaystyle a_{0}=a(t_{0})=1} ; the usual convention in cosmology is that subscript 0 denotes present-day values, so t 0 {\\displaystyle t_{0}} denotes the age of"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_5",
    "chunk": "the universe. The scale factor is related to the observed redshift z {\\displaystyle z} of the light emitted at time t e m {\\displaystyle t_{\\mathrm {em} }} by a ( t em ) = 1 1 + z . {\\displaystyle a(t_{\\text{em}})={\\frac {1}{1+z}}\\,.} The expansion rate is described by the time-dependent Hubble parameter, H ( t ) {\\displaystyle H(t)} , defined as H ( t ) ≡ a ˙ a , {\\displaystyle H(t)\\equiv {\\frac {\\dot {a}}{a}},} where a ˙ {\\displaystyle {\\dot {a}}} is the time-derivative of the scale factor. The first Friedmann equation gives the expansion rate in terms of the matter+radiation density ρ {\\displaystyle \\rho } , the curvature k {\\displaystyle k} , and the cosmological constant Λ {\\displaystyle \\Lambda } , H 2 = ( a ˙ a ) 2 = 8 π G 3 ρ − k c 2 a 2 + Λ c 2 3 , {\\displaystyle H^{2}=\\left({\\frac {\\dot {a}}{a}}\\right)^{2}={\\frac {8\\pi G}{3}}\\rho -{\\frac {kc^{2}}{a^{2}}}+{\\frac {\\Lambda c^{2}}{3}},} where, as usual c {\\displaystyle c} is the speed of light and G {\\displaystyle G} is the gravitational constant. A critical density ρ c r i t {\\displaystyle \\rho _{\\mathrm {crit} }} is the present-day density, which gives zero curvature k"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_6",
    "chunk": "{\\displaystyle k} , assuming the cosmological constant Λ {\\displaystyle \\Lambda } is zero, regardless of its actual value. Substituting these conditions to the Friedmann equation gives ρ c r i t = 3 H 0 2 8 π G = 1.878 47 ( 23 ) × 10 − 26 h 2 k g ⋅ m − 3 , {\\displaystyle \\rho _{\\mathrm {crit} }={\\frac {3H_{0}^{2}}{8\\pi G}}=1.878\\;47(23)\\times 10^{-26}\\;h^{2}\\;\\mathrm {kg{\\cdot }m^{-3}} ,} where h ≡ H 0 / ( 100 k m ⋅ s − 1 ⋅ M p c − 1 ) {\\displaystyle h\\equiv H_{0}/(100\\;\\mathrm {km{\\cdot }s^{-1}{\\cdot }Mpc^{-1}} )} is the reduced Hubble constant. If the cosmological constant were actually zero, the critical density would also mark the dividing line between eventual recollapse of the universe to a Big Crunch, or unlimited expansion. For the Lambda-CDM model with a positive cosmological constant (as observed), the universe is predicted to expand forever regardless of whether the total density is slightly above or below the critical density; though other outcomes are possible in extended models where the dark energy is not constant but actually time-dependent. The present-day density parameter Ω x {\\displaystyle \\Omega _{x}} for various species is defined as the dimensionless ratio Ω"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_7",
    "chunk": "x ≡ ρ x ( t = t 0 ) ρ c r i t = 8 π G ρ x ( t = t 0 ) 3 H 0 2 {\\displaystyle \\Omega _{x}\\equiv {\\frac {\\rho _{x}(t=t_{0})}{\\rho _{\\mathrm {crit} }}}={\\frac {8\\pi G\\rho _{x}(t=t_{0})}{3H_{0}^{2}}}} where the subscript x {\\displaystyle x} is one of b {\\displaystyle \\mathrm {b} } for baryons, c {\\displaystyle \\mathrm {c} } for cold dark matter, r a d {\\displaystyle \\mathrm {rad} } for radiation (photons plus relativistic neutrinos), and Λ {\\displaystyle \\Lambda } for dark energy. Since the densities of various species scale as different powers of a {\\displaystyle a} , e.g. a − 3 {\\displaystyle a^{-3}} for matter etc., the Friedmann equation can be conveniently rewritten in terms of the various density parameters as H ( a ) ≡ a ˙ a = H 0 ( Ω c + Ω b ) a − 3 + Ω r a d a − 4 + Ω k a − 2 + Ω Λ a − 3 ( 1 + w ) , {\\displaystyle H(a)\\equiv {\\frac {\\dot {a}}{a}}=H_{0}{\\sqrt {(\\Omega _{\\rm {c}}+\\Omega _{\\rm {b}})a^{-3}+\\Omega _{\\mathrm {rad} }a^{-4}+\\Omega _{k}a^{-2}+\\Omega _{\\Lambda }a^{-3(1+w)}}},} where w {\\displaystyle w} is the equation of state"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_8",
    "chunk": "parameter of dark energy, and assuming negligible neutrino mass (significant neutrino mass requires a more complex equation). The various Ω {\\displaystyle \\Omega } parameters add up to 1 {\\displaystyle 1} by construction. In the general case this is integrated by computer to give the expansion history a ( t ) {\\displaystyle a(t)} and also observable distance–redshift relations for any chosen values of the cosmological parameters, which can then be compared with observations such as supernovae and baryon acoustic oscillations. In the minimal 6-parameter Lambda-CDM model, it is assumed that curvature Ω k {\\displaystyle \\Omega _{k}} is zero and w = − 1 {\\displaystyle w=-1} , so this simplifies to H ( a ) = H 0 Ω m a − 3 + Ω r a d a − 4 + Ω Λ {\\displaystyle H(a)=H_{0}{\\sqrt {\\Omega _{\\rm {m}}a^{-3}+\\Omega _{\\mathrm {rad} }a^{-4}+\\Omega _{\\Lambda }}}} Observations show that the radiation density is very small today, Ω rad ∼ 10 − 4 {\\displaystyle \\Omega _{\\text{rad}}\\sim 10^{-4}} ; if this term is neglected the above has an analytic solution a ( t ) = ( Ω m / Ω Λ ) 1 / 3 sinh 2 / 3 ⁡ ( t / t Λ )"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_9",
    "chunk": "{\\displaystyle a(t)=(\\Omega _{\\rm {m}}/\\Omega _{\\Lambda })^{1/3}\\,\\sinh ^{2/3}(t/t_{\\Lambda })} where t Λ ≡ 2 / ( 3 H 0 Ω Λ ) ; {\\displaystyle t_{\\Lambda }\\equiv 2/(3H_{0}{\\sqrt {\\Omega _{\\Lambda }}})\\ ;} this is fairly accurate for a > 0.01 {\\displaystyle a>0.01} or t > 10 {\\displaystyle t>10} million years. Solving for a ( t ) = 1 {\\displaystyle a(t)=1} gives the present age of the universe t 0 {\\displaystyle t_{0}} in terms of the other parameters. It follows that the transition from decelerating to accelerating expansion (the second derivative a ¨ {\\displaystyle {\\ddot {a}}} crossing zero) occurred when a = ( Ω m / 2 Ω Λ ) 1 / 3 , {\\displaystyle a=(\\Omega _{\\rm {m}}/2\\Omega _{\\Lambda })^{1/3},} which evaluates to a ∼ 0.6 {\\displaystyle a\\sim 0.6} or z ∼ 0.66 {\\displaystyle z\\sim 0.66} for the best-fit parameters estimated from the Planck spacecraft. Multiple variants of the ΛCDM model are used with some differences in parameters. One such set is outlined in the table below. The Planck collaboration version of the ΛCDM model is based on six parameters: baryon density parameter; dark matter density parameter; scalar spectral index; two parameters related to curvature fluctuation amplitude; and the probability that photons from"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_10",
    "chunk": "the early universe will be scattered once on route (called reionization optical depth). Six is the smallest number of parameters needed to give an acceptable fit to the observations; other possible parameters are fixed at \"natural\" values, e.g. total density parameter = 1.00, dark energy equation of state = −1. The parameter values, and uncertainties, are estimated using computer searches to locate the region of parameter space providing an acceptable match to cosmological observations. From these six parameters, the other model values, such as the Hubble constant and the dark energy density, can be calculated. The discovery of the cosmic microwave background (CMB) in 1964 confirmed a key prediction of the Big Bang cosmology. From that point on, it was generally accepted that the universe started in a hot, dense state and has been expanding over time. The rate of expansion depends on the types of matter and energy present in the universe, and in particular, whether the total density is above or below the so-called critical density. During the 1970s, most attention focused on pure-baryonic models, but there were serious challenges explaining the formation of galaxies, given the small anisotropies in the CMB (upper limits at that time). In"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_11",
    "chunk": "the early 1980s, it was realized that this could be resolved if cold dark matter dominated over the baryons, and the theory of cosmic inflation motivated models with critical density. During the 1980s, most research focused on cold dark matter with critical density in matter, around 95% CDM and 5% baryons: these showed success at forming galaxies and clusters of galaxies, but problems remained; notably, the model required a Hubble constant lower than preferred by observations, and observations around 1988–1990 showed more large-scale galaxy clustering than predicted. These difficulties sharpened with the discovery of CMB anisotropy by the Cosmic Background Explorer in 1992, and several modified CDM models, including ΛCDM and mixed cold and hot dark matter, came under active consideration through the mid-1990s. The ΛCDM model then became the leading model following the observations of accelerating expansion in 1998, and was quickly supported by other observations: in 2000, the BOOMERanG microwave background experiment measured the total (matter–energy) density to be close to 100% of critical, whereas in 2001 the 2dFGRS galaxy redshift survey measured the matter density to be near 25%; the large difference between these values supports a positive Λ or dark energy. Much more precise spacecraft measurements"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_12",
    "chunk": "of the microwave background from WMAP in 2003–2010 and Planck in 2013–2015 have continued to support the model and pin down the parameter values, most of which are constrained below 1 percent uncertainty. Among all cosmological models, the ΛCDM model has been the most successful; it describes a wide range of astronomical observations with remarkable accuracy. The notable successes include: In addition to explaining many pre-2000 observations, the model has made a number of successful predictions: notably the existence of the baryon acoustic oscillation feature, discovered in 2005 in the predicted location; and the statistics of weak gravitational lensing, first observed in 2000 by several teams. The polarization of the CMB, discovered in 2002 by DASI, has been successfully predicted by the model: in the 2015 Planck data release, there are seven observed peaks in the temperature (TT) power spectrum, six peaks in the temperature–polarization (TE) cross spectrum, and five peaks in the polarization (EE) spectrum. The six free parameters can be well constrained by the TT spectrum alone, and then the TE and EE spectra can be predicted theoretically to few-percent precision with no further adjustments allowed. Despite the widespread success of ΛCDM in matching observations of our universe,"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_13",
    "chunk": "cosmologists believe that the model may be an approximation of a more fundamental model. Extensive searches for dark matter particles have so far shown no well-agreed detection, while dark energy may be almost impossible to detect in a laboratory, and its value is extremely small compared to vacuum energy theoretical predictions. The ΛCDM model, like all models built on the Friedmann–Lemaître–Robertson–Walker metric, assume that the universe looks the same in all directions (isotropy) and from every location (homogeneity) if you look at a large enough scale: \"the universe looks the same whoever and wherever you are.\" This cosmological principle allows a metric, Friedmann–Lemaître–Robertson–Walker metric, to be derived and developed into a theory to compare to experiments. Without the principle, a metric would need to be extracted from astronomical data, which may not be possible. The assumptions were carried over into the ΛCDM model. However, some findings suggested violations of the cosmological principle. Evidence from galaxy clusters, quasars, and type Ia supernovae suggest that isotropy is violated on large scales. Data from the Planck Mission shows hemispheric bias in the cosmic microwave background in two respects: one with respect to average temperature (i.e. temperature fluctuations), the second with respect to larger"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_14",
    "chunk": "variations in the degree of perturbations (i.e. densities). The European Space Agency (the governing body of the Planck Mission) has concluded that these anisotropies in the CMB are, in fact, statistically significant and can no longer be ignored. Already in 1967, Dennis Sciama predicted that the cosmic microwave background has a significant dipole anisotropy. In recent years, the CMB dipole has been tested, and the results suggest our motion with respect to distant radio galaxies and quasars differs from our motion with respect to the cosmic microwave background. The same conclusion has been reached in recent studies of the Hubble diagram of Type Ia supernovae and quasars. This contradicts the cosmological principle. The CMB dipole is hinted at through a number of other observations. First, even within the cosmic microwave background, there are curious directional alignments and an anomalous parity asymmetry that may have an origin in the CMB dipole. Separately, the CMB dipole direction has emerged as a preferred direction in studies of alignments in quasar polarizations, scaling relations in galaxy clusters, strong lensing time delay, Type Ia supernovae, and quasars and gamma-ray bursts as standard candles. The fact that all these independent observables, based on different physics, are"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_15",
    "chunk": "tracking the CMB dipole direction suggests that the Universe is anisotropic in the direction of the CMB dipole. Nevertheless, some authors have stated that the universe around Earth is isotropic at high significance by studies of the combined cosmic microwave background temperature and polarization maps. The homogeneity of the universe needed for the ΛCDM applies to very large volumes of space. N-body simulations in ΛCDM show that the spatial distribution of galaxies is statistically homogeneous if averaged over scales 260/h Mpc or more. Numerous claims of large-scale structures reported to be in conflict with the predicted scale of homogeneity for ΛCDM do not withstand statistical analysis. El Gordo is a massive interacting galaxy cluster in the early Universe ( z = 0.87 {\\displaystyle z=0.87} ). The extreme properties of El Gordo in terms of its redshift, mass, and the collision velocity leads to strong ( 6.16 σ {\\displaystyle 6.16\\sigma } ) tension with the ΛCDM model. The properties of El Gordo are however consistent with cosmological simulations in the framework of MOND due to more rapid structure formation. The KBC void is an immense, comparatively empty region of space containing the Milky Way approximately 2 billion light-years (600 megaparsecs, Mpc)"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_16",
    "chunk": "in diameter. Some authors have said the existence of the KBC void violates the assumption that the CMB reflects baryonic density fluctuations at z = 1100 {\\displaystyle z=1100} or Einstein's theory of general relativity, either of which would violate the ΛCDM model, while other authors have claimed that supervoids as large as the KBC void are consistent with the ΛCDM model. Statistically significant differences remain in values of the Hubble constant derived by matching the ΛCDM model to data from the \"early universe\", like the cosmic background radiation, compared to values derived from stellar distance measurements, called the \"late universe\". While systematic error in the measurements remains a possibility, many different kinds of observations agree with one of these two values of the constant. This difference, called the Hubble tension, widely acknowledged to be a major problem for the ΛCDM model. Dozens of proposals for modifications of ΛCDM or completely new models have been published to explain the Hubble tension. Among these models are many that modify the properties of dark energy or of dark matter over time, interactions between dark energy and dark matter, unified dark energy and matter, other forms of dark radiation like sterile neutrinos, modifications to"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_17",
    "chunk": "the properties of gravity, or the modification of the effects of inflation, changes to the properties of elementary particles in the early universe, among others. None of these models can simultaneously explain the breadth of other cosmological data as well as ΛCDM. The \" S 8 {\\displaystyle S_{8}} tension\" is a name for another question mark for the ΛCDM model. The S 8 {\\displaystyle S_{8}} parameter in the ΛCDM model quantifies the amplitude of matter fluctuations in the late universe and is defined as S 8 ≡ σ 8 Ω m / 0.3 {\\displaystyle S_{8}\\equiv \\sigma _{8}{\\sqrt {\\Omega _{\\rm {m}}/0.3}}} Early- (e.g. from CMB data collected using the Planck observatory) and late-time (e.g. measuring weak gravitational lensing events) facilitate increasingly precise values of S 8 {\\displaystyle S_{8}} . However, these two categories of measurement differ by more standard deviations than their uncertainties. This discrepancy is called the S 8 {\\displaystyle S_{8}} tension. The name \"tension\" reflects that the disagreement is not merely between two data sets: the many sets of early- and late-time measurements agree well within their own categories, but there is an unexplained difference between values obtained from different points in the evolution of the universe. Such a"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_18",
    "chunk": "tension indicates that the ΛCDM model may be incomplete or in need of correction. Some values for S 8 {\\displaystyle S_{8}} are 0.832±0.013 (2020 Planck), 0.766+0.020−0.014 (2021 KIDS), 0.776±0.017 (2022 DES), 0.790+0.018−0.014 (2023 DES+KIDS), 0.769+0.031−0.034 – 0.776+0.032−0.033 (2023 HSC-SSP), 0.86±0.01 (2024 EROSITA). Values have also obtained using peculiar velocities, 0.637±0.054 (2020) and 0.776±0.033 (2020), among other methods. The \"axis of evil\" is a name given to a purported correlation between the plane of the Solar System and aspects of the cosmic microwave background (CMB). Such a correlation would give the plane of the Solar System and hence the location of Earth a greater significance than might be expected by chance, a result which has been claimed to be evidence of a departure from the Copernican principle. However, a 2016 study compared isotropic and anisotropic cosmological models against WMAP and Planck data and found no evidence for anisotropy. The actual observable amount of lithium in the universe is less than the calculated amount from the ΛCDM model by a factor of 3–4. If every calculation is correct, then solutions beyond the existing ΛCDM model might be needed. The ΛCDM model assumes that the shape of the universe is of zero curvature"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_19",
    "chunk": "(is flat) and has an undetermined topology. In 2019, interpretation of Planck data suggested that the curvature of the universe might be positive (often called \"closed\"), which would contradict the ΛCDM model. Some authors have suggested that the Planck data detecting a positive curvature could be evidence of a local inhomogeneity in the curvature of the universe rather than the universe actually being globally a 3-manifold of positive curvature. The ΛCDM model assumes that the strong equivalence principle is true. However, in 2020 a group of astronomers analyzed data from the Spitzer Photometry and Accurate Rotation Curves (SPARC) sample, together with estimates of the large-scale external gravitational field from an all-sky galaxy catalog. They concluded that there was highly statistically significant evidence of violations of the strong equivalence principle in weak gravitational fields in the vicinity of rotationally supported galaxies. They observed an effect inconsistent with tidal effects in the ΛCDM model. These results have been challenged as failing to consider inaccuracies in the rotation curves and correlations between galaxy properties and clustering strength. and as inconsistent with similar analysis of other galaxies. Several discrepancies between the predictions of cold dark matter in the ΛCDM model and observations of galaxies"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_20",
    "chunk": "and their clustering have arisen. Some of these problems have proposed solutions, but it remains unclear whether they can be solved without abandoning the ΛCDM model. Milgrom, McGaugh, and Kroupa have criticized the dark matter portions of the theory from the perspective of galaxy formation models and supporting the alternative modified Newtonian dynamics (MOND) theory, which requires a modification of the Einstein field equations and the Friedmann equations as seen in proposals such as modified gravity theory (MOG theory) or tensor–vector–scalar gravity theory (TeVeS theory). Other proposals by theoretical astrophysicists of cosmological alternatives to Einstein's general relativity that attempt to account for dark energy or dark matter include f(R) gravity, scalar–tensor theories such as galileon [ko] theories (see Galilean invariance), brane cosmologies, the DGP model, and massive gravity and its extensions such as bimetric gravity. The density distributions of dark matter halos in cold dark matter simulations (at least those that do not include the impact of baryonic feedback) are much more peaked than what is observed in galaxies by investigating their rotation curves. Cold dark matter simulations predict large numbers of small dark matter halos, more numerous than the number of small dwarf galaxies that are observed around galaxies"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_21",
    "chunk": "like the Milky Way. Dwarf galaxies around the Milky Way and Andromeda galaxies are observed to be orbiting in thin, planar structures whereas the simulations predict that they should be distributed randomly about their parent galaxies. However, latest research suggests this seemingly bizarre alignment is just a quirk which will dissolve over time. Galaxies in the NGC 3109 association are moving away too rapidly to be consistent with expectations in the ΛCDM model. In this framework, NGC 3109 is too massive and distant from the Local Group for it to have been flung out in a three-body interaction involving the Milky Way or Andromeda Galaxy. If galaxies grew hierarchically, then massive galaxies required many mergers. Major mergers inevitably create a classical bulge. On the contrary, about 80% of observed galaxies give evidence of no such bulges, and giant pure-disc galaxies are commonplace. The tension can be quantified by comparing the observed distribution of galaxy shapes today with predictions from high-resolution hydrodynamical cosmological simulations in the ΛCDM framework, revealing a highly significant problem that is unlikely to be solved by improving the resolution of the simulations. The high bulgeless fraction was nearly constant for 8 billion years. If galaxies were embedded"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_22",
    "chunk": "within massive halos of cold dark matter, then the bars that often develop in their central regions would be slowed down by dynamical friction with the halo. This is in serious tension with the fact that observed galaxy bars are typically fast. Comparison of the model with observations may have some problems on sub-galaxy scales, possibly predicting too many dwarf galaxies and too much dark matter in the innermost regions of galaxies. This problem is called the \"small scale crisis\". These small scales are harder to resolve in computer simulations, so it is not yet clear whether the problem is the simulations, non-standard properties of dark matter, or a more radical error in the model. Observations from the James Webb Space Telescope have resulted in various galaxies confirmed by spectroscopy at high redshift, such as JADES-GS-z13-0 at cosmological redshift of 13.2. Other candidate galaxies which have not been confirmed by spectroscopy include CEERS-93316 at cosmological redshift of 16.4. Existence of surprisingly massive galaxies in the early universe challenges the preferred models describing how dark matter halos drive galaxy formation. It remains to be seen whether a revision of the Lambda-CDM model with parameters given by Planck Collaboration is necessary to"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_23",
    "chunk": "resolve this issue. The discrepancies could also be explained by particular properties (stellar masses or effective volume) of the candidate galaxies, yet unknown force or particle outside of the Standard Model through which dark matter interacts, more efficient baryonic matter accumulation by the dark matter halos, early dark energy models, or the hypothesized long-sought Population III stars. Massimo Persic and Paolo Salucci first estimated the baryonic density today present in ellipticals, spirals, groups and clusters of galaxies. They performed an integration of the baryonic mass-to-light ratio over luminosity (in the following M b / L {\\textstyle M_{\\rm {b}}/L} ), weighted with the luminosity function ϕ ( L ) {\\textstyle \\phi (L)} over the previously mentioned classes of astrophysical objects: ρ b = ∑ ∫ L ϕ ( L ) M b L d L . {\\displaystyle \\rho _{\\rm {b}}=\\sum \\int L\\phi (L){\\frac {M_{\\rm {b}}}{L}}\\,dL.} The result was: Ω b = Ω ∗ + Ω gas = 2.2 × 10 − 3 + 1.5 × 10 − 3 h − 1.3 ≃ 0.003 , {\\displaystyle \\Omega _{\\rm {b}}=\\Omega _{*}+\\Omega _{\\text{gas}}=2.2\\times 10^{-3}+1.5\\times 10^{-3}\\;h^{-1.3}\\simeq 0.003,} where h ≃ 0.72 {\\displaystyle h\\simeq 0.72} . Note that this value is much lower than the prediction"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_24",
    "chunk": "of standard cosmic nucleosynthesis Ω b ≃ 0.0486 {\\displaystyle \\Omega _{\\rm {b}}\\simeq 0.0486} , so that stars and gas in galaxies and in galaxy groups and clusters account for less than 10% of the primordially synthesized baryons. This issue is known as the problem of the \"missing baryons\". The missing baryon problem is claimed to be resolved. Using observations of the kinematic Sunyaev–Zel'dovich effect spanning more than 90% of the lifetime of the Universe, in 2021 astrophysicists found that approximately 50% of all baryonic matter is outside dark matter haloes, filling the space between galaxies. Together with the amount of baryons inside galaxies and surrounding them, the total amount of baryons in the late time Universe is compatible with early Universe measurements. It has been argued that the ΛCDM model has adopted conventionalist stratagems, rendering it unfalsifiable in the sense defined by Karl Popper. When faced with new data not in accord with a prevailing model, the conventionalist will find ways to adapt the theory rather than declare it false. Thus dark matter was added after the observations of anomalous galaxy rotation rates. Thomas Kuhn viewed the process differently, as \"problem solving\" within the existing paradigm. Extended models allow one"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_25",
    "chunk": "or more of the \"fixed\" parameters above to vary, in addition to the basic six; so these models join smoothly to the basic six-parameter model in the limit that the additional parameter(s) approach the default values. For example, possible extensions of the simplest ΛCDM model allow for spatial curvature ( Ω tot {\\displaystyle \\Omega _{\\text{tot}}} may be different from 1); or quintessence rather than a cosmological constant where the equation of state of dark energy is allowed to differ from −1. Cosmic inflation predicts tensor fluctuations (gravitational waves). Their amplitude is parameterized by the tensor-to-scalar ratio (denoted r {\\displaystyle r} ), which is determined by the unknown energy scale of inflation. Other modifications allow hot dark matter in the form of neutrinos more massive than the minimal value, or a running spectral index; the latter is generally not favoured by simple cosmic inflation models. Allowing additional variable parameter(s) will generally increase the uncertainties in the standard six parameters quoted above, and may also shift the central values slightly. The table below shows results for each of the possible \"6+1\" scenarios with one additional variable parameter; this indicates that, as of 2015, there is no convincing evidence that any additional parameter"
  },
  {
    "source": "Lambda-CDM model.txt",
    "chunk_id": "Lambda-CDM model.txt_26",
    "chunk": "is different from its default value. Some researchers have suggested that there is a running spectral index, but no statistically significant study has revealed one. Theoretical expectations suggest that the tensor-to-scalar ratio r {\\displaystyle r} should be between 0 and 0.3, and the latest results are within those limits."
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_0",
    "chunk": "# Lens A lens is a transmissive optical device that focuses or disperses a light beam by means of refraction. A simple lens consists of a single piece of transparent material, while a compound lens consists of several simple lenses (elements), usually arranged along a common axis. Lenses are made from materials such as glass or plastic and are ground, polished, or molded to the required shape. A lens can focus light to form an image, unlike a prism, which refracts light without focusing. Devices that similarly focus or disperse waves and radiation other than visible light are also called \"lenses\", such as microwave lenses, electron lenses, acoustic lenses, or explosive lenses. Lenses are used in various imaging devices such as telescopes, binoculars, and cameras. They are also used as visual aids in glasses to correct defects of vision such as myopia and hypermetropia. The word lens comes from lēns, the Latin name of the lentil (a seed of a lentil plant), because a double-convex lens is lentil-shaped. The lentil also gives its name to a geometric figure. Some scholars argue that the archeological evidence indicates that there was widespread use of lenses in antiquity, spanning several millennia. The so-called"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_1",
    "chunk": "Nimrud lens is a rock crystal artifact dated to the 7th century BCE which may or may not have been used as a magnifying glass, or a burning glass. Others have suggested that certain Egyptian hieroglyphs depict \"simple glass meniscal lenses\". The oldest certain reference to the use of lenses is from Aristophanes' play The Clouds (424 BCE) mentioning a burning-glass. Pliny the Elder (1st century) confirms that burning-glasses were known in the Roman period. Pliny also has the earliest known reference to the use of a corrective lens when he mentions that Nero was said to watch the gladiatorial games using an emerald (presumably concave to correct for nearsightedness, though the reference is vague). Both Pliny and Seneca the Younger (3 BC–65 AD) described the magnifying effect of a glass globe filled with water. Ptolemy (2nd century) wrote a book on Optics, which however survives only in the Latin translation of an incomplete and very poor Arabic translation. The book was, however, received by medieval scholars in the Islamic world, and commented upon by Ibn Sahl (10th century), who was in turn improved upon by Alhazen (Book of Optics, 11th century). The Arabic translation of Ptolemy's Optics became available"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_2",
    "chunk": "in Latin translation in the 12th century (Eugenius of Palermo 1154). Between the 11th and 13th century \"reading stones\" were invented. These were primitive plano-convex lenses initially made by cutting a glass sphere in half. The medieval (11th or 12th century) rock crystal Visby lenses may or may not have been intended for use as burning glasses. Spectacles were invented as an improvement of the \"reading stones\" of the high medieval period in Northern Italy in the second half of the 13th century. This was the start of the optical industry of grinding and polishing lenses for spectacles, first in Venice and Florence in the late 13th century, and later in the spectacle-making centres in both the Netherlands and Germany. Spectacle makers created improved types of lenses for the correction of vision based more on empirical knowledge gained from observing the effects of the lenses (probably without the knowledge of the rudimentary optical theory of the day). The practical development and experimentation with lenses led to the invention of the compound optical microscope around 1595, and the refracting telescope in 1608, both of which appeared in the spectacle-making centres in the Netherlands. With the invention of the telescope and microscope"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_3",
    "chunk": "there was a great deal of experimentation with lens shapes in the 17th and early 18th centuries by those trying to correct chromatic errors seen in lenses. Opticians tried to construct lenses of varying forms of curvature, wrongly assuming errors arose from defects in the spherical figure of their surfaces. Optical theory on refraction and experimentation was showing no single-element lens could bring all colours to a focus. This led to the invention of the compound achromatic lens by Chester Moore Hall in England in 1733, an invention also claimed by fellow Englishman John Dollond in a 1758 patent. Developments in transatlantic commerce were the impetus for the construction of modern lighthouses in the 18th century, which utilize a combination of elevated sightlines, lighting sources, and lenses to provide navigational aid overseas. With maximal distance of visibility needed in lighthouses, conventional convex lenses would need to be significantly sized which would negatively affect the development of lighthouses in terms of cost, design, and implementation. Fresnel lens were developed that considered these constraints by featuring less material through their concentric annular sectioning. They were first fully implemented into a lighthouse in 1823. Most lenses are spherical lenses: their two surfaces are"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_4",
    "chunk": "parts of the surfaces of spheres. Each surface can be convex (bulging outwards from the lens), concave (depressed into the lens), or planar (flat). The line joining the centres of the spheres making up the lens surfaces is called the axis of the lens. Typically the lens axis passes through the physical centre of the lens, because of the way they are manufactured. Lenses may be cut or ground after manufacturing to give them a different shape or size. The lens axis may then not pass through the physical centre of the lens. Toric or sphero-cylindrical lenses have surfaces with two different radii of curvature in two orthogonal planes. They have a different focal power in different meridians. This forms an astigmatic lens. An example is eyeglass lenses that are used to correct astigmatism in someone's eye. Lenses are classified by the curvature of the two optical surfaces. A lens is biconvex (or double convex, or just convex) if both surfaces are convex. If both surfaces have the same radius of curvature, the lens is equiconvex. A lens with two concave surfaces is biconcave (or just concave). If one of the surfaces is flat, the lens is plano-convex or plano-concave"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_5",
    "chunk": "depending on the curvature of the other surface. A lens with one convex and one concave side is convex-concave or meniscus. Convex-concave lenses are most commonly used in corrective lenses, since the shape minimizes some aberrations. For a biconvex or plano-convex lens in a lower-index medium, a collimated beam of light passing through the lens converges to a spot (a focus) behind the lens. In this case, the lens is called a positive or converging lens. For a thin lens in air, the distance from the lens to the spot is the focal length of the lens, which is commonly represented by f in diagrams and equations. An extended hemispherical lens is a special type of plano-convex lens, in which the lens's curved surface is a full hemisphere and the lens is much thicker than the radius of curvature. Another extreme case of a thick convex lens is a ball lens, whose shape is completely round. When used in novelty photography it is often called a \"lensball\". A ball-shaped lens has the advantage of being omnidirectional, but for most optical glass types, its focal point lies close to the ball's surface. Because of the ball's curvature extremes compared to the"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_6",
    "chunk": "lens size, optical aberration is much worse than thin lenses, with the notable exception of chromatic aberration. For a biconcave or plano-concave lens in a lower-index medium, a collimated beam of light passing through the lens is diverged (spread); the lens is thus called a negative or diverging lens. The beam, after passing through the lens, appears to emanate from a particular point on the axis in front of the lens. For a thin lens in air, the distance from this point to the lens is the focal length, though it is negative with respect to the focal length of a converging lens. The behavior reverses when a lens is placed in a medium with higher refractive index than the material of the lens. In this case a biconvex or plano-convex lens diverges light, and a biconcave or plano-concave one converges it. Convex-concave (meniscus) lenses can be either positive or negative, depending on the relative curvatures of the two surfaces. A negative meniscus lens has a steeper concave surface (with a shorter radius than the convex surface) and is thinner at the centre than at the periphery. Conversely, a positive meniscus lens has a steeper convex surface (with a shorter"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_7",
    "chunk": "radius than the concave surface) and is thicker at the centre than at the periphery. An ideal thin lens with two surfaces of equal curvature (also equal in the sign) would have zero optical power (as its focal length becomes infinity as shown in the lensmaker's equation), meaning that it would neither converge nor diverge light. All real lenses have a nonzero thickness, however, which makes a real lens with identical curved surfaces slightly positive. To obtain exactly zero optical power, a meniscus lens must have slightly unequal curvatures to account for the effect of the lens' thickness. For a single refraction for a circular boundary, the relation between object and its image in the paraxial approximation is given by n 1 u + n 2 v = n 2 − n 1 R {\\displaystyle {\\frac {n_{1}}{u}}+{\\frac {n_{2}}{v}}={\\frac {n_{2}-n_{1}}{R}}} where R is the radius of the spherical surface, n2 is the refractive index of the material of the surface, n1 is the refractive index of medium (the medium other than the spherical surface material), u {\\textstyle u} is the on-axis (on the optical axis) object distance from the line perpendicular to the axis toward the refraction point on the surface"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_8",
    "chunk": "(which height is h), and v {\\textstyle v} is the on-axis image distance from the line. Due to paraxial approximation where the line of h is close to the vertex of the spherical surface meeting the optical axis on the left, u {\\textstyle u} and v {\\textstyle v} are also considered distances with respect to the vertex. Moving v {\\textstyle v} toward the right infinity leads to the first or object focal length f 0 {\\textstyle f_{0}} for the spherical surface. Similarly, u {\\textstyle u} toward the left infinity leads to the second or image focal length f i {\\displaystyle f_{i}} . f 0 = n 1 n 2 − n 1 R , f i = n 2 n 2 − n 1 R {\\displaystyle {\\begin{aligned}f_{0}&={\\frac {n_{1}}{n_{2}-n_{1}}}R,\\\\f_{i}&={\\frac {n_{2}}{n_{2}-n_{1}}}R\\end{aligned}}} Applying this equation on the two spherical surfaces of a lens and approximating the lens thickness to zero (so a thin lens) leads to the lensmaker's formula. Applying Snell's law on the spherical surface, n 1 sin ⁡ i = n 2 sin ⁡ r . {\\displaystyle n_{1}\\sin i=n_{2}\\sin r\\,.} Also in the diagram, tan ⁡ ( i − θ ) = h u tan ⁡ ( θ − r )"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_9",
    "chunk": "= h v sin ⁡ θ = h R {\\displaystyle {\\begin{aligned}\\tan(i-\\theta )&={\\frac {h}{u}}\\\\\\tan(\\theta -r)&={\\frac {h}{v}}\\\\\\sin \\theta &={\\frac {h}{R}}\\end{aligned}}} , and using small angle approximation (paraxial approximation) and eliminating i, r, and θ, n 2 v + n 1 u = n 2 − n 1 R . {\\displaystyle {\\frac {n_{2}}{v}}+{\\frac {n_{1}}{u}}={\\frac {n_{2}-n_{1}}{R}}\\,.} The (effective) focal length f {\\displaystyle f} of a spherical lens in air or vacuum for paraxial rays can be calculated from the lensmaker's equation: 1 f = ( n − 1 ) [ 1 R 1 − 1 R 2 + ( n − 1 ) d n R 1 R 2 ] , {\\displaystyle {\\frac {1}{\\ f\\ }}=\\left(n-1\\right)\\left[\\ {\\frac {1}{\\ R_{1}\\ }}-{\\frac {1}{\\ R_{2}\\ }}+{\\frac {\\ \\left(n-1\\right)\\ d~}{\\ n\\ R_{1}\\ R_{2}\\ }}\\ \\right]\\ ,} where The focal length f {\\textstyle \\ f\\ } is with respect to the principal planes of the lens, and the locations of the principal planes h 1 {\\textstyle \\ h_{1}\\ } and h 2 {\\textstyle \\ h_{2}\\ } with respect to the respective lens vertices are given by the following formulas, where it is a positive value if it is right to the respective vertex. h 1 = − ( n"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_10",
    "chunk": "− 1 ) f d n R 2 {\\displaystyle \\ h_{1}=-\\ {\\frac {\\ \\left(n-1\\right)f\\ d~}{\\ n\\ R_{2}\\ }}\\ } h 2 = − ( n − 1 ) f d n R 1 {\\displaystyle \\ h_{2}=-\\ {\\frac {\\ \\left(n-1\\right)f\\ d~}{\\ n\\ R_{1}\\ }}\\ } The focal length f {\\displaystyle \\ f\\ } is positive for converging lenses, and negative for diverging lenses. The reciprocal of the focal length, 1 f , {\\textstyle \\ {\\tfrac {1}{\\ f\\ }}\\ ,} is the optical power of the lens. If the focal length is in metres, this gives the optical power in dioptres (reciprocal metres). Lenses have the same focal length when light travels from the back to the front as when light goes from the front to the back. Other properties of the lens, such as the aberrations are not the same in both directions. The signs of the lens' radii of curvature indicate whether the corresponding surfaces are convex or concave. The sign convention used to represent this varies, but in this article a positive R indicates a surface's center of curvature is further along in the direction of the ray travel (right, in the accompanying diagrams), while negative R means that"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_11",
    "chunk": "rays reaching the surface have already passed the center of curvature. Consequently, for external lens surfaces as diagrammed above, R1 > 0 and R2 < 0 indicate convex surfaces (used to converge light in a positive lens), while R1 < 0 and R2 > 0 indicate concave surfaces. The reciprocal of the radius of curvature is called the curvature. A flat surface has zero curvature, and its radius of curvature is infinite. This convention is used in this article. Other conventions such as the Cartesian sign convention change the form of the equations. If d is small compared to R1 and R2 then the thin lens approximation can be made. For a lens in air, f is then given by 1 f ≈ ( n − 1 ) [ 1 R 1 − 1 R 2 ] . {\\displaystyle \\ {\\frac {1}{\\ f\\ }}\\approx \\left(n-1\\right)\\left[\\ {\\frac {1}{\\ R_{1}\\ }}-{\\frac {1}{\\ R_{2}\\ }}\\ \\right]~.} The spherical thin lens equation in paraxial approximation is derived here with respect to the right figure. The 1st spherical lens surface (which meets the optical axis at V 1 {\\textstyle \\ V_{1}\\ } as its vertex) images an on-axis object point O to the virtual image"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_12",
    "chunk": "I, which can be described by the following equation, n 1 u + n 2 v ′ = n 2 − n 1 R 1 . {\\displaystyle \\ {\\frac {\\ n_{1}\\ }{\\ u\\ }}+{\\frac {\\ n_{2}\\ }{\\ v'\\ }}={\\frac {\\ n_{2}-n_{1}\\ }{\\ R_{1}\\ }}~.} For the imaging by second lens surface, by taking the above sign convention, u ′ = − v ′ + d {\\textstyle \\ u'=-v'+d\\ } and n 2 − v ′ + d + n 1 v = n 1 − n 2 R 2 . {\\displaystyle \\ {\\frac {n_{2}}{\\ -v'+d\\ }}+{\\frac {\\ n_{1}\\ }{\\ v\\ }}={\\frac {\\ n_{1}-n_{2}\\ }{\\ R_{2}\\ }}~.} Adding these two equations yields n 1 u + n 1 v = ( n 2 − n 1 ) ( 1 R 1 − 1 R 2 ) + n 2 d ( v ′ − d ) v ′ . {\\displaystyle \\ {\\frac {\\ n_{1}\\ }{u}}+{\\frac {\\ n_{1}\\ }{v}}=\\left(n_{2}-n_{1}\\right)\\left({\\frac {1}{\\ R_{1}\\ }}-{\\frac {1}{\\ R_{2}\\ }}\\right)+{\\frac {\\ n_{2}\\ d\\ }{\\ \\left(\\ v'-d\\ \\right)\\ v'\\ }}~.} For the thin lens approximation where d → 0 , {\\displaystyle \\ d\\rightarrow 0\\ ,} the 2nd term of the RHS (Right Hand Side) is gone, so n"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_13",
    "chunk": "1 u + n 1 v = ( n 2 − n 1 ) ( 1 R 1 − 1 R 2 ) . {\\displaystyle \\ {\\frac {\\ n_{1}\\ }{u}}+{\\frac {\\ n_{1}\\ }{v}}=\\left(n_{2}-n_{1}\\right)\\left({\\frac {1}{\\ R_{1}\\ }}-{\\frac {1}{\\ R_{2}\\ }}\\right)~.} The focal length f {\\displaystyle \\ f\\ } of the thin lens is found by limiting u → − ∞ , {\\displaystyle \\ u\\rightarrow -\\infty \\ ,} n 1 f = ( n 2 − n 1 ) ( 1 R 1 − 1 R 2 ) → 1 f = ( n 2 n 1 − 1 ) ( 1 R 1 − 1 R 2 ) . {\\displaystyle \\ {\\frac {\\ n_{1}\\ }{\\ f\\ }}=\\left(n_{2}-n_{1}\\right)\\left({\\frac {1}{\\ R_{1}\\ }}-{\\frac {1}{\\ R_{2}\\ }}\\right)\\rightarrow {\\frac {1}{\\ f\\ }}=\\left({\\frac {\\ n_{2}\\ }{\\ n_{1}\\ }}-1\\right)\\left({\\frac {1}{\\ R_{1}\\ }}-{\\frac {1}{\\ R_{2}\\ }}\\right)~.} 1 u + 1 v = 1 f . {\\displaystyle \\ {\\frac {1}{\\ u\\ }}+{\\frac {1}{\\ v\\ }}={\\frac {1}{\\ f\\ }}~.} For the thin lens in air or vacuum where n 1 = 1 {\\textstyle \\ n_{1}=1\\ } can be assumed, f {\\textstyle \\ f\\ } becomes 1 f = ( n − 1 ) ( 1 R 1 − 1 R 2"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_14",
    "chunk": ") {\\displaystyle \\ {\\frac {1}{\\ f\\ }}=\\left(n-1\\right)\\left({\\frac {1}{\\ R_{1}\\ }}-{\\frac {1}{\\ R_{2}\\ }}\\right)\\ } where the subscript of 2 in n 2 {\\textstyle \\ n_{2}\\ } is dropped. As mentioned above, a positive or converging lens in air focuses a collimated beam travelling along the lens axis to a spot (known as the focal point) at a distance f from the lens. Conversely, a point source of light placed at the focal point is converted into a collimated beam by the lens. These two cases are examples of image formation in lenses. In the former case, an object at an infinite distance (as represented by a collimated beam of waves) is focused to an image at the focal point of the lens. In the latter, an object at the focal length distance from the lens is imaged at infinity. The plane perpendicular to the lens axis situated at a distance f from the lens is called the focal plane. For paraxial rays, if the distances from an object to a spherical thin lens (a lens of negligible thickness) and from the lens to the image are S1 and S2 respectively, the distances are related by the (Gaussian) thin lens formula:"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_15",
    "chunk": "1 f = 1 S 1 + 1 S 2 . {\\displaystyle {1 \\over f}={1 \\over S_{1}}+{1 \\over S_{2}}\\,.} The right figure shows how the image of an object point can be found by using three rays; the first ray parallelly incident on the lens and refracted toward the second focal point of it, the second ray crossing the optical center of the lens (so its direction does not change), and the third ray toward the first focal point and refracted to the direction parallel to the optical axis. This is a simple ray tracing method easily used. Two rays among the three are sufficient to locate the image point. By moving the object along the optical axis, it is shown that the second ray determines the image size while other rays help to locate the image location. where x 1 = S 1 − f {\\displaystyle x_{1}=S_{1}-f} and x 2 = S 2 − f . {\\displaystyle x_{2}=S_{2}-f\\,.} x 1 {\\textstyle x_{1}} is positive if it is left to the front focal point F 1 {\\textstyle F_{1}} , and x 2 {\\textstyle x_{2}} is positive if it is right to the rear focal point F 2 {\\textstyle F_{2}} ."
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_16",
    "chunk": "Because f 2 {\\textstyle f^{2}} is positive, an object point and the corresponding imaging point made by a lens are always in opposite sides with respect to their respective focal points. ( x 1 {\\textstyle x_{1}} and x 2 {\\textstyle x_{2}} are either positive or negative.) This Newtonian form of the lens equation can be derived by using a similarity between triangles P1PO1F1 and L3L2F1 and another similarity between triangles L1L2F2 and P2P02F2 in the right figure. The similarities give the following equations and combining these results gives the Newtonian form of the lens equation. y 1 x 1 = | y 2 | f y 1 f = | y 2 | x 2 {\\displaystyle {\\begin{array}{lcr}{\\frac {y_{1}}{x_{1}}}={\\frac {\\left\\vert y_{2}\\right\\vert }{f}}\\\\{\\frac {y_{1}}{f}}={\\frac {\\left\\vert y_{2}\\right\\vert }{x_{2}}}\\end{array}}} The above equations also hold for thick lenses (including a compound lens made by multiple lenses, that can be treated as a thick lens) in air or vacuum (which refractive index can be treated as 1) if S 1 {\\textstyle S_{1}} , S 2 {\\textstyle S_{2}} , and f {\\textstyle f} are with respect to the principal planes of the lens ( f {\\textstyle f} is the effective focal length in this case). This is"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_17",
    "chunk": "because of triangle similarities like the thin lens case above; similarity between triangles P1PO1F1 and L3H1F1 and another similarity between triangles L1'H2F2 and P2P02F2 in the right figure. If distances S1 or S2 pass through a medium other than air or vacuum, then a more complicated analysis is required. If an object is placed at a distance S1 > f from a positive lens of focal length f, we will find an image at a distance S2 according to this formula. If a screen is placed at a distance S2 on the opposite side of the lens, an image is formed on it. This sort of image, which can be projected onto a screen or image sensor, is known as a real image. This is the principle of the camera, and also of the human eye, in which the retina serves as the image sensor. The focusing adjustment of a camera adjusts S2, as using an image distance different from that required by this formula produces a defocused (fuzzy) image for an object at a distance of S1 from the camera. Put another way, modifying S2 causes objects at a different S1 to come into perfect focus. In some cases,"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_18",
    "chunk": "S2 is negative, indicating that the image is formed on the opposite side of the lens from where those rays are being considered. Since the diverging light rays emanating from the lens never come into focus, and those rays are not physically present at the point where they appear to form an image, this is called a virtual image. Unlike real images, a virtual image cannot be projected on a screen, but appears to an observer looking through the lens as if it were a real object at the location of that virtual image. Likewise, it appears to a subsequent lens as if it were an object at that location, so that second lens could again focus that light into a real image, S1 then being measured from the virtual image location behind the first lens to the second lens. This is exactly what the eye does when looking through a magnifying glass. The magnifying glass creates a (magnified) virtual image behind the magnifying glass, but those rays are then re-imaged by the lens of the eye to create a real image on the retina. Using a positive lens of focal length f, a virtual image results when S1 <"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_19",
    "chunk": "f, the lens thus being used as a magnifying glass (rather than if S1 ≫ f as for a camera). Using a negative lens (f < 0) with a real object (S1 > 0) can only produce a virtual image (S2 < 0), according to the above formula. It is also possible for the object distance S1 to be negative, in which case the lens sees a so-called virtual object. This happens when the lens is inserted into a converging beam (being focused by a previous lens) before the location of its real image. In that case even a negative lens can project a real image, as is done by a Barlow lens. For a given lens with the focal length f, the minimum distance between an object and the real image is 4f (S1 = S2 = 2f). This is derived by letting L = S1 + S2, expressing S2 in terms of S1 by the lens equation (or expressing S1 in terms of S2), and equating the derivative of L with respect to S1 (or S2) to zero. (Note that L has no limit in increasing so its extremum is only the minimum, at which the derivate of"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_20",
    "chunk": "L is zero.) M = − S 2 S 1 = f f − S 1 = − f x 1 {\\displaystyle M=-{\\frac {S_{2}}{S_{1}}}={\\frac {f}{f-S_{1}}}\\ =-{\\frac {f}{x_{1}}}} where M is the magnification factor defined as the ratio of the size of an image compared to the size of the object. The sign convention here dictates that if M is negative, as it is for real images, the image is upside-down with respect to the object. For virtual images M is positive, so the image is upright. This magnification formula provides two easy ways to distinguish converging (f > 0) and diverging (f < 0) lenses: For an object very close to the lens (0 < S1 < |f|), a converging lens would form a magnified (bigger) virtual image, whereas a diverging lens would form a demagnified (smaller) image; For an object very far from the lens (S1 > |f| > 0), a converging lens would form an inverted image, whereas a diverging lens would form an upright image. Linear magnification M is not always the most useful measure of magnifying power. For instance, when characterizing a visual telescope or binoculars that produce only a virtual image, one would be more"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_21",
    "chunk": "concerned with the angular magnification—which expresses how much larger a distant object appears through the telescope compared to the naked eye. In the case of a camera one would quote the plate scale, which compares the apparent (angular) size of a distant object to the size of the real image produced at the focus. The plate scale is the reciprocal of the focal length of the camera lens; lenses are categorized as long-focus lenses or wide-angle lenses according to their focal lengths. Using an inappropriate measurement of magnification can be formally correct but yield a meaningless number. For instance, using a magnifying glass of 5 cm focal length, held 20 cm from the eye and 5 cm from the object, produces a virtual image at infinity of infinite linear size: M = ∞. But the angular magnification is 5, meaning that the object appears 5 times larger to the eye than without the lens. When taking a picture of the moon using a camera with a 50 mm lens, one is not concerned with the linear magnification M ≈ −50 mm / 380000 km = −1.3×10. Rather, the plate scale of the camera is about 1°/mm, from which one can"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_22",
    "chunk": "conclude that the 0.5 mm image on the film corresponds to an angular size of the moon seen from earth of about 0.5°. In the extreme case where an object is an infinite distance away, S1 = ∞, S2 = f and M = −f/∞ = 0, indicating that the object would be imaged to a single point in the focal plane. In fact, the diameter of the projected spot is not actually zero, since diffraction places a lower limit on the size of the point spread function. This is called the diffraction limit. Lenses do not form perfect images, and always introduce some degree of distortion or aberration that makes the image an imperfect replica of the object. Careful design of the lens system for a particular application minimizes the aberration. Several types of aberration affect image quality, including spherical aberration, coma, and chromatic aberration. Spherical aberration occurs because spherical surfaces are not the ideal shape for a lens, but are by far the simplest shape to which glass can be ground and polished, and so are often used. Spherical aberration causes beams parallel to, but laterally distant from, the lens axis to be focused in a slightly different"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_23",
    "chunk": "place than beams close to the axis. This manifests itself as a blurring of the image. Spherical aberration can be minimised with normal lens shapes by carefully choosing the surface curvatures for a particular application. For instance, a plano-convex lens, which is used to focus a collimated beam, produces a sharper focal spot when used with the convex side towards the beam source. Coma, or comatic aberration, derives its name from the comet-like appearance of the aberrated image. Coma occurs when an object off the optical axis of the lens is imaged, where rays pass through the lens at an angle to the axis θ. Rays that pass through the centre of a lens of focal length f are focused at a point with distance f tan θ from the axis. Rays passing through the outer margins of the lens are focused at different points, either further from the axis (positive coma) or closer to the axis (negative coma). In general, a bundle of parallel rays passing through the lens at a fixed distance from the centre of the lens are focused to a ring-shaped image in the focal plane, known as a comatic circle (see each circle of the"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_24",
    "chunk": "image in the below figure). The sum of all these circles results in a V-shaped or comet-like flare. As with spherical aberration, coma can be minimised (and in some cases eliminated) by choosing the curvature of the two lens surfaces to match the application. Lenses in which both spherical aberration and coma are minimised are called bestform lenses. Chromatic aberration is caused by the dispersion of the lens material—the variation of its refractive index, n, with the wavelength of light. Since, from the formulae above, f is dependent upon n, it follows that light of different wavelengths is focused to different positions. Chromatic aberration of a lens is seen as fringes of colour around the image. It can be minimised by using an achromatic doublet (or achromat) in which two materials with differing dispersion are bonded together to form a single lens. This reduces the amount of chromatic aberration over a certain range of wavelengths, though it does not produce perfect correction. The use of achromats was an important step in the development of the optical microscope. An apochromat is a lens or lens system with even better chromatic aberration correction, combined with improved spherical aberration correction. Apochromats are much"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_25",
    "chunk": "more expensive than achromats. Different lens materials may also be used to minimise chromatic aberration, such as specialised coatings or lenses made from the crystal fluorite. This naturally occurring substance has the highest known Abbe number, indicating that the material has low dispersion. Other kinds of aberration include field curvature, barrel and pincushion distortion, and astigmatism. Even if a lens is designed to minimize or eliminate the aberrations described above, the image quality is still limited by the diffraction of light passing through the lens' finite aperture. A diffraction-limited lens is one in which aberrations have been reduced to the point where the image quality is primarily limited by diffraction under the design conditions. Simple lenses are subject to the optical aberrations discussed above. In many cases these aberrations can be compensated for to a great extent by using a combination of simple lenses with complementary aberrations. A compound lens is a collection of simple lenses of different shapes and made of materials of different refractive indices, arranged one after the other with a common axis. In a multiple-lens system, if the purpose of the system is to image an object, then the system design can be such that each"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_26",
    "chunk": "lens treats the image made by the previous lens as an object, and produces the new image of it, so the imaging is cascaded through the lenses. As shown above, the Gaussian lens equation for a spherical lens is derived such that the 2nd surface of the lens images the image made by the 1st lens surface. For multi-lens imaging, 3rd lens surface (the front surface of the 2nd lens) can image the image made by the 2nd surface, and 4th surface (the back surface of the 2nd lens) can also image the image made by the 3rd surface. This imaging cascade by each lens surface justifies the imaging cascade by each lens. For a two-lens system the object distances of each lens can be denoted as s o 1 {\\textstyle s_{o1}} and s o 2 {\\textstyle s_{o2}} , and the image distances as and s i 1 {\\textstyle s_{i1}} and s i 2 {\\textstyle s_{i2}} . If the lenses are thin, each satisfies the thin lens formula 1 f j = 1 s o j + 1 s i j , {\\displaystyle {\\frac {1}{f_{j}}}={\\frac {1}{s_{oj}}}+{\\frac {1}{s_{ij}}},} If the distance between the two lenses is d {\\displaystyle d} , then"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_27",
    "chunk": "s o 2 = d − s i 1 {\\textstyle s_{o2}=d-s_{i1}} . (The 2nd lens images the image of the first lens.) FFD (Front Focal Distance) is defined as the distance between the front (left) focal point of an optical system and its nearest optical surface vertex. If an object is located at the front focal point of the system, then its image made by the system is located infinitely far way to the right (i.e., light rays from the object is collimated after the system). To do this, the image of the 1st lens is located at the focal point of the 2nd lens, i.e., s i 1 = d − f 2 {\\displaystyle s_{i1}=d-f_{2}} . So, the thin lens formula for the 1st lens becomes 1 f 1 = 1 F F D + 1 d − f 2 → F F D = f 1 ( d − f 2 ) d − ( f 1 + f 2 ) . {\\displaystyle {\\frac {1}{f_{1}}}={\\frac {1}{FFD}}+{\\frac {1}{d-f_{2}}}\\rightarrow FFD={\\frac {f_{1}(d-f_{2})}{d-(f_{1}+f_{2})}}.} BFD (Back Focal Distance) is similarly defined as the distance between the back (right) focal point of an optical system and its nearest optical surface vertex. If an object"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_28",
    "chunk": "is located infinitely far away from the system (to the left), then its image made by the system is located at the back focal point. In this case, the 1st lens images the object at its focal point. So, the thin lens formula for the 2nd lens becomes 1 f 2 = 1 B F D + 1 d − f 1 → B F D = f 2 ( d − f 1 ) d − ( f 1 + f 2 ) . {\\displaystyle {\\frac {1}{f_{2}}}={\\frac {1}{BFD}}+{\\frac {1}{d-f_{1}}}\\rightarrow BFD={\\frac {f_{2}(d-f_{1})}{d-(f_{1}+f_{2})}}.} A simplest case is where thin lenses are placed in contact ( d = 0 {\\displaystyle d=0} ). Then the combined focal length f of the lenses is given by 1 f = 1 f 1 + 1 f 2 . {\\displaystyle {\\frac {1}{f}}={\\frac {1}{f_{1}}}+{\\frac {1}{f_{2}}}\\,.} Since 1/f is the power of a lens with focal length f, it can be seen that the powers of thin lenses in contact are additive. The general case of multiple thin lenses in contact is 1 f = ∑ k = 1 N 1 f k {\\displaystyle {\\frac {1}{f}}=\\sum _{k=1}^{N}{\\frac {1}{f_{k}}}} If two thin lenses are separated in air by some"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_29",
    "chunk": "distance d, then the focal length for the combined system is given by 1 f = 1 f 1 + 1 f 2 − d f 1 f 2 . {\\displaystyle {\\frac {1}{f}}={\\frac {1}{f_{1}}}+{\\frac {1}{f_{2}}}-{\\frac {d}{f_{1}f_{2}}}\\,.} As d tends to zero, the focal length of the system tends to the value of f given for thin lenses in contact. It can be shown that the same formula works for thick lenses if d is taken as the distance between their principal planes. If the separation distance between two lenses is equal to the sum of their focal lengths (d = f1 + f2), then the FFD and BFD are infinite. This corresponds to a pair of lenses that transforms a parallel (collimated) beam into another collimated beam. This type of system is called an afocal system, since it produces no net convergence or divergence of the beam. Two lenses at this separation form the simplest type of optical telescope. Although the system does not alter the divergence of a collimated beam, it does alter the (transverse) width of the beam. The magnification of such a telescope is given by which is the ratio of the output beam width to the"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_30",
    "chunk": "input beam width. Note the sign convention: a telescope with two convex lenses (f1 > 0, f2 > 0) produces a negative magnification, indicating an inverted image. A convex plus a concave lens (f1 > 0 > f2) produces a positive magnification and the image is upright. For further information on simple optical telescopes, see Refracting telescope § Refracting telescope designs. Cylindrical lenses have curvature along only one axis. They are used to focus light into a line, or to convert the elliptical light from a laser diode into a round beam. They are also used in motion picture anamorphic lenses. Aspheric lenses have at least one surface that is neither spherical nor cylindrical. The more complicated shapes allow such lenses to form images with less aberration than standard simple lenses, but they are more difficult and expensive to produce. These were formerly complex to make and often extremely expensive, but advances in technology have greatly reduced the manufacturing cost for such lenses. A Fresnel lens has its optical surface broken up into narrow rings, allowing the lens to be much thinner and lighter than conventional lenses. Durable Fresnel lenses can be molded from plastic and are inexpensive. Lenticular lenses"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_31",
    "chunk": "are arrays of microlenses that are used in lenticular printing to make images that have an illusion of depth or that change when viewed from different angles. Bifocal lens has two or more, or a graduated, focal lengths ground into the lens. A gradient index lens has flat optical surfaces, but has a radial or axial variation in index of refraction that causes light passing through the lens to be focused. An axicon has a conical optical surface. It images a point source into a line along the optic axis, or transforms a laser beam into a ring. Superlenses are made from negative index metamaterials and claim to produce images at spatial resolutions exceeding the diffraction limit. The first superlenses were made in 2004 using such a metamaterial for microwaves. Improved versions have been made by other researchers. As of 2014 the superlens has not yet been demonstrated at visible or near-infrared wavelengths. A single convex lens mounted in a frame with a handle or stand is a magnifying glass. Lenses are used as prosthetics for the correction of refractive errors such as myopia, hypermetropia, presbyopia, and astigmatism. (See corrective lens, contact lens, eyeglasses, intraocular lens.) Most lenses used for"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_32",
    "chunk": "other purposes have strict axial symmetry; eyeglass lenses are only approximately symmetric. They are usually shaped to fit in a roughly oval, not circular, frame; the optical centres are placed over the eyeballs; their curvature may not be axially symmetric to correct for astigmatism. Sunglasses' lenses are designed to attenuate light; sunglass lenses that also correct visual impairments can be custom made. Other uses are in imaging systems such as monoculars, binoculars, telescopes, microscopes, cameras and projectors. Some of these instruments produce a virtual image when applied to the human eye; others produce a real image that can be captured on photographic film or an optical sensor, or can be viewed on a screen. In these devices lenses are sometimes paired up with curved mirrors to make a catadioptric system where the lens's spherical aberration corrects the opposite aberration in the mirror (such as Schmidt and meniscus correctors). Convex lenses produce an image of an object at infinity at their focus; if the sun is imaged, much of the visible and infrared light incident on the lens is concentrated into the small image. A large lens creates enough intensity to burn a flammable object at the focal point. Since ignition"
  },
  {
    "source": "Lens.txt",
    "chunk_id": "Lens.txt_33",
    "chunk": "can be achieved even with a poorly made lens, lenses have been used as burning-glasses for at least 2400 years. A modern application is the use of relatively large lenses to concentrate solar energy on relatively small photovoltaic cells, harvesting more energy without the need to use larger and more expensive cells. Radio astronomy and radar systems often use dielectric lenses, commonly called a lens antenna to refract electromagnetic radiation into a collector antenna. Lenses can become scratched and abraded. Abrasion-resistant coatings are available to help control this."
  },
  {
    "source": "Leonard Susskind.txt",
    "chunk_id": "Leonard Susskind.txt_0",
    "chunk": "# Leonard Susskind Leonard Susskind (/ˈsʌskɪnd/; born June 16, 1940) is an American theoretical physicist, Professor of theoretical physics at Stanford University and founding director of the Stanford Institute for Theoretical Physics. His research interests are string theory, quantum field theory, quantum statistical mechanics and quantum cosmology. He is a member of the US National Academy of Sciences, and the American Academy of Arts and Sciences, an associate member of the faculty of Canada's Perimeter Institute for Theoretical Physics, and a distinguished professor of the Korea Institute for Advanced Study. Susskind is widely regarded as one of the fathers of string theory. He was the first to give a precise string-theoretic interpretation of the holographic principle in 1995 and the first to introduce the idea of the string theory landscape in 2003. Susskind was awarded the 1998 J. J. Sakurai Prize, the 2018 Oskar Klein Medal, and the Dirac Medal of the International Centre for Theoretical Physics in 2023. Leonard Susskind was born to a Jewish family from the South Bronx in New York City. He began working as a plumber at the age of 16, taking over from his father who had become ill. Later, he enrolled in the"
  },
  {
    "source": "Leonard Susskind.txt",
    "chunk_id": "Leonard Susskind.txt_1",
    "chunk": "City College of New York as an engineering student and had planned to study mechanical engineering but he changed his mind and later graduated with a B.S. in physics in 1962. In an interview in the Los Angeles Times, Susskind recalls a discussion with his father that changed his career path: \"When I told my father I wanted to be a physicist, he said, 'Hell no, you ain't going to work in a drug store.' I said, 'No, not a pharmacist.' I said, 'Like Einstein.' He poked me in the chest with a piece of plumbing pipe. 'You ain't going to be no engineer,' he said. 'You're going to be Einstein.'\" Susskind then studied at Cornell University under Peter A. Carruthers, where he earned his Ph.D. in 1965. Susskind was an assistant professor of physics, then an associate professor at Yeshiva University (1966–1970), after which he went for a year to the Tel Aviv University (1971–72), returning to Yeshiva to become a professor of physics (1970–1979). Since 1979 he has been professor of physics at Stanford University, and since 2000 has held the Felix Bloch professorship of physics. Susskind was awarded the 1998 J. J. Sakurai Prize for his \"pioneering"
  },
  {
    "source": "Leonard Susskind.txt",
    "chunk_id": "Leonard Susskind.txt_2",
    "chunk": "contributions to hadronic string models, lattice gauge theories, quantum chromodynamics, and dynamical symmetry breaking.\" Susskind's hallmark, according to colleagues, has been the application of \"brilliant imagination and originality to the theoretical study of the nature of the elementary particles and forces that make up the physical world.\" In 2007, Susskind joined the faculty of Perimeter Institute for Theoretical Physics in Waterloo, Ontario, Canada, as an associate member. He has been elected to the National Academy of Sciences and the American Academy of Arts and Sciences. He is also a distinguished professor at Korea Institute for Advanced Study. Susskind was one of at least three physicists, alongside Yoichiro Nambu and Holger Bech Nielsen, who independently discovered during or around 1970 that Gabriele Veneziano's dual resonance model of strong interactions could be described by a quantum mechanical model of oscillating strings, and was the first to propose the idea of the string theory landscape. Susskind has also made important contributions in the following areas of physics: The Cosmic Landscape: String Theory and the Illusion of Intelligent Design is Susskind's first popular science book, published by Little, Brown and Company on December 12, 2005. It is Susskind's attempt to bring his idea of"
  },
  {
    "source": "Leonard Susskind.txt",
    "chunk_id": "Leonard Susskind.txt_3",
    "chunk": "the anthropic landscape of string theory to the general public. In the book, Susskind describes how the string theory landscape was an almost inevitable consequence of several factors, one of which was Steven Weinberg's prediction of the cosmological constant in 1987. The question addressed here is why our universe is fine-tuned for our existence. Susskind explains that Weinberg calculated that if the cosmological constant was just a little different, our universe would cease to exist. The Black Hole War: My Battle with Stephen Hawking to Make the World Safe for Quantum Mechanics is Susskind's second popular science book, published by Little, Brown, and Company on July 7, 2008. The book is his most famous work and explains what he thinks would happen to the information and matter stored in a black hole when it evaporates. The book sparked from a debate that started in 1981, when there was a meeting of physicists to try to decode some of the mysteries about how particles of particular elemental compounds function. During this discussion Stephen Hawking stated that the information inside a black hole is lost forever as the black hole evaporates. It took 28 years for Leonard Susskind to formulate his theory"
  },
  {
    "source": "Leonard Susskind.txt",
    "chunk_id": "Leonard Susskind.txt_4",
    "chunk": "that would prove Hawking wrong. He then published his theory in his book, The Black Hole War. Like The Cosmic Landscape, The Black Hole War is aimed at the lay reader. He writes: \"The real tools for understanding the quantum universe are abstract mathematics: infinite dimensional Hilbert spaces, projection operators, unitary matrices and a lot of other advanced principles that take a few years to learn. But let's see how we do in just a few pages\". Susskind co-authored a series of companion books to his lecture series The Theoretical Minimum. The first of these, The Theoretical Minimum: What You Need to Know to Start Doing Physics, was published in 2013 and presents the modern formulations of classical mechanics. The second of these, Quantum Mechanics: The Theoretical Minimum, was published in February 2014. The third book, Special Relativity and Classical Field Theory: The Theoretical Minimum (September 26, 2017), introduces readers to Einstein's special relativity and Maxwell's classical field theory. The fourth book in the series, General Relativity: The Theoretical Minimum was published in January 2023. Susskind teaches a series of Stanford Continuing Studies courses about modern physics referred to as The Theoretical Minimum. The title of the series is a"
  },
  {
    "source": "Leonard Susskind.txt",
    "chunk_id": "Leonard Susskind.txt_5",
    "chunk": "clear reference to Landau's famous comprehensive exam called the \"Theoretical Minimum\" which students were expected to pass before admission to his school. The Theoretical Minimum lectures later formed the basis for the books of the same name. The goal of the courses is to teach the basic but rigorous theoretical foundations required to study certain areas of physics. The sequence covers classical mechanics, relativity, quantum mechanics, statistical mechanics, and cosmology, including the physics of black holes. These courses are available on The Theoretical Minimum website, on iTunes, and on YouTube. The courses are intended for the mathematically literate public as well as physical science/mathematics students. Susskind aims the courses at people with prior exposure to algebra, and calculus. Homework and study outside of class is otherwise unnecessary. Susskind explains most of the mathematics used, which form the basis of the lectures. Susskind gave 3 lectures \"The Birth of the Universe and the Origin of Laws of Physics\" April 28-May 1, 2014 in the Cornell Messenger Lecture series which are posted on a Cornell website. The Smolin–Susskind debate refers to the series of intense postings in 2004 between Lee Smolin and Susskind, concerning Smolin's argument that the \"anthropic principle cannot yield"
  },
  {
    "source": "Leonard Susskind.txt",
    "chunk_id": "Leonard Susskind.txt_6",
    "chunk": "any falsifiable predictions, and therefore cannot be a part of science\". It began on July 26, 2004, with Smolin's publication of \"scientific alternatives to the anthropic principle\". Smolin e-mailed Susskind asking for a comment. Having not had the chance to read the paper, Susskind requested a summarization of his arguments. Smolin obliged, and on July 28, 2004, Susskind responded, saying that the logic Smolin followed \"can lead to ridiculous conclusions\". The next day, Smolin responded, saying that \"If a large body of our colleagues feels comfortable believing a theory that cannot be proved wrong, then the progress of science could get stuck, leading to a situation in which false, but unfalsifiable theories dominate the attention of our field.\" This was followed by another paper by Susskind which made a few comments about Smolin's theory of \"cosmic natural selection\". The Smolin–Susskind debate finally ended with each of them agreeing to write a final letter which would be posted on the edge.org website, with three conditions attached: He has been married twice, first in 1960, and he has four children. Susskind is a great-grandfather. The insistence on unitarity in the presence of black holes led 't Hooft (1993) and Susskind (1995b) to"
  },
  {
    "source": "Leonard Susskind.txt",
    "chunk_id": "Leonard Susskind.txt_7",
    "chunk": "embrace a more radical, holographic interpretation of ..."
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_0",
    "chunk": "# Library of Congress The Library of Congress (LOC) is a research library in Washington, D.C., serving as the library and research service for the United States Congress and the de facto national library of the United States. It also administers copyright law through the United States Copyright Office. Founded in 1800, the Library of Congress is the oldest federal cultural institution in the United States. It is housed in three buildings on Capitol Hill, adjacent to the United States Capitol, along with the National Audio-Visual Conservation Center in Culpeper, Virginia, and additional storage facilities at Fort George G. Meade and Cabin Branch in Hyattsville, Maryland. The library's functions are overseen by the librarian of Congress, and its buildings are maintained by the architect of the Capitol. The LOC is one of the largest libraries in the world, containing approximately 173 million items and employing over 3,000 staff. Its collections are \"universal, not limited by subject, format, or national boundary, and include research materials from all parts of the world and in more than 470 languages\". When Congress moved to Washington in November 1800, a small congressional library was housed in the Capitol. Much of the original collection was lost"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_1",
    "chunk": "in the August 1814 Burning of Washington by the British during the War of 1812. Congress accepted former president Thomas Jefferson's offer to sell his entire personal collection of 6,487 books to restore the library. The collection grew slowly and suffered another major fire in 1851, which destroyed two-thirds of Jefferson's original books. The Library of Congress faced space shortages, understaffing, and lack of funding, until the American Civil War increased the importance of legislative research to meet the demands of a growing federal government. In 1870, the library gained the right to receive two copies of every copyrightable work printed in the United States; it also built its collections through acquisitions and donations. Between 1890 and 1897, a new library building, now the Thomas Jefferson Building, was constructed. Two additional buildings, the John Adams Building (opened in 1939) and the James Madison Memorial Building (opened in 1980), were later added. The LOC's primary mission is to inform legislation, which it carries out through the Congressional Research Service. The library is open to the public for research, although only members of Congress, their staff, and library employees may borrow materials for use outside the library. In 1783, James Madison, a"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_2",
    "chunk": "Founding Father and the nation's fourth president, proposed creating a congressional library, but failed to gain necessary support for the idea. After the American Revolutionary War, however, the Philadelphia Library Company and New York Society Library served as surrogate congressional libraries when Congress convened in those cities. On April 24, 1800, the Library of Congress was established when John Adams, the nation's second president, signed an act of Congress, which appropriated $5,000 \"for the purchase of such books as may be necessary for the use of Congress...and for fitting up a suitable apartment for containing them.\" Books were ordered from London, forming a collection of 740 books and three maps housed in the new United States Capitol. Adams' successor as U.S. President, Thomas Jefferson, also played a crucial role in shaping development of the Library of Congress. On January 26, 1802, Jefferson signed a bill allowing the president to appoint the librarian of Congress and establishing a Joint Committee on the Library to oversee it. The law also extended borrowing privileges to the president and vice president. In August 1814, British forces occupied Washington, D.C. and, in retaliation for American acts in Canada, burned several government buildings, including the Library"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_3",
    "chunk": "of Congress, resulting in the destructing of over 3,000 of its volumes. These volumes were held in the Senate wing of the Capitol; one surviving volume was a government account book from 1810. This volume was taken by British commander George Cockburn as a souvenir, which was later returned to the U.S. over a century later, in 1940, by his family. Within a month, Jefferson offered to sell his large personal library as a replacement. He had reconstituted his own collection after losing part of it to a fire. Congress accepted the offer in January 1815, appropriating $23,950 to purchase his 6,487 books. Some House members, including New Hampshire representative Daniel Webster, opposed the purchase, wanting to exclude \"books of an atheistical, irreligious, and immoral tendency\". Jefferson's collection, gathered over 50 years, covered various subjects and languages, including topics not typically found in a legislative library. He believed all subjects had a place in the Library of Congress, stating: I do not know that it contains any branch of science which Congress would wish to exclude from their collection; there is, in fact, no subject to which a Member of Congress may not have occasion to refer. Jefferson's library was"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_4",
    "chunk": "a working collection for a scholar, not for display. It doubled the size of the original library, transforming it from a specialist's library to a more general one. He organized his books based on Francis Bacon's organization of knowledge, grouping them into Memory, Reason, and Imagination with 44 subdivisions. The library used this scheme until the late 19th century when librarian Herbert Putnam introduced the Library of Congress Classification, now applying to over 138 million items. A February 24, 1824, report from the Committee of Ways and Means recommended a $5,000 appropriation for the Library of Congress, noting the need to improve its collections in \"Law, Politics, Commerce, History, and Geography,\" which were crucial for Congress. On December 24, 1851, the largest fire in the library's history destroyed 35,000 books, two-thirds of the library's collection, and two-thirds of Thomas Jefferson's original transfer. Congress appropriated $168,700 to replace the lost books in 1852 but not to acquire new materials. (By 2008, the librarians of Congress had found replacements for all but 300 of the works that had been documented as being in Jefferson's original collection.) This marked the start of a conservative period in the library's administration by librarian John Silva"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_5",
    "chunk": "Meehan and joint committee chairman James A. Pearce, who restricted the library's activities. Meehan and Pearce's views about a restricted scope for the Library of Congress reflected those shared by members of Congress. While Meehan was a librarian, he supported and perpetuated the notion that \"the congressional library should play a limited role on the national scene and that its collections, by and large, should emphasize American materials of obvious use to the U.S. Congress.\" In 1859, Congress transferred the library's public document distribution activities to the Department of the Interior and its international book exchange program to the Department of State. During the 1850s, Smithsonian Institution librarian Charles Coffin Jewett aggressively tried to develop the Smithsonian as the United States national library. His efforts were rejected by Smithsonian secretary Joseph Henry, who advocated a focus on scientific research and publication. To reinforce his intentions for the Smithsonian, Henry established laboratories, developed a robust physical sciences library, and started the Smithsonian Contributions to Knowledge, the first of many publications intended to disseminate research results. For Henry, the Library of Congress was the obvious choice as the national library. Unable to resolve the conflict, Henry dismissed Jewett in July 1854. In"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_6",
    "chunk": "1865, the Smithsonian building, also called the Castle due to its Norman architectural style, was severely damaged by fire. This incident presented Henry with an opportunity related to the Smithsonian's non-scientific library. Around this time, the Library of Congress was planning to build and relocate to the new Thomas Jefferson Building, designed to be fireproof. Authorized by an act of Congress, Henry transferred the Smithsonian's non-scientific library of 40,000 volumes to the Library of Congress in 1866. In 1861, President Abraham Lincoln appointed John G. Stephenson as Librarian of Congress; the appointment is regarded as the most political to date. Stephenson was a physician and spent equal time serving as librarian and as a physician in the Union Army. He could manage this division of interest because he hired Ainsworth Rand Spofford as his assistant. Despite his new job, Stephenson focused on the war. Three weeks into his term as Librarian of Congress, he left Washington, D.C., to serve as a volunteer aide-de-camp at the battles of Chancellorsville and Gettysburg during the American Civil War. Stephenson's hiring of Spofford, who directed the library in his absence, may have been his most significant achievement. Librarian Ainsworth Rand Spofford, who directed the"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_7",
    "chunk": "Library of Congress from 1865 to 1897, built broad bipartisan support to develop it as a national library and a legislative resource. He was aided by expansion of the federal government after the war and a favorable political climate. He began comprehensively collecting Americana and American literature, led the construction of a new building to house the library, and transformed the librarian of Congress position into one of strength and independence. Between 1865 and 1870, Congress appropriated funds for the construction of the Thomas Jefferson Building, placed all copyright registration and deposit activities under the library's control, and restored the international book exchange. The library also acquired the vast libraries of the Smithsonian and of historian Peter Force, strengthening its scientific and Americana collections significantly. By 1876, the Library of Congress had 300,000 volumes; it was tied with the Boston Public Library as the nation's largest library. It moved from the Capitol building to its new headquarters in 1897 with more than 840,000 volumes, 40 percent of which had been acquired through copyright deposit. A year before the library's relocation, the Joint Library Committee held hearings to assess the condition of the library and plan for its future growth and"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_8",
    "chunk": "possible reorganization. Spofford and six experts sent by the American Library Association testified that the library should continue its expansion to become a true national library. Based on the hearings, Congress authorized a budget that allowed the library to more than double its staff, from 42 to 108 persons. Senators Justin Morrill of Vermont and Daniel W. Voorhees of Indiana were particularly helpful in gaining this support. The library also established new administrative units for all aspects of the collection. In its bill, Congress strengthened the role of librarian of Congress: it became responsible for governing the library and making staff appointments. As with presidential Cabinet appointments, the Senate was required to approve presidential appointees to the position. In 1893, Elizabeth Dwyer became the first woman to be appointed to the staff of the library. With this support and the 1897 reorganization upon moving into its new home, the Library of Congress began to grow and develop more rapidly. Librarian Spofford's successor John Russell Young overhauled the library's bureaucracy, used his connections as a former diplomat to acquire more materials from around the world, and established the library's first assistance programs for the blind and physically disabled, with the establishment"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_9",
    "chunk": "of the National Library Service for the Blind and Print Disabled. Librarian Young's successor Herbert Putnam held the office for forty years of the 20th century from 1899 to 1939. Two years after he took office, the library became the first in the United States to hold one million volumes. Putnam focused his efforts to make the library more accessible and useful for the public and for other libraries. He instituted the interlibrary loan service, transforming the Library of Congress into what he referred to as a \"library of last resort\". Putnam also expanded library access to \"scientific investigators and duly qualified individuals\", and began publishing primary sources for the benefit of scholars. During Putnam's tenure, the library broadened the diversity of its acquisitions. In 1903, Putnam persuaded President Theodore Roosevelt to use an executive order to transfer the papers of the Founding Fathers from the State Department to the Library of Congress. Putnam also expanded foreign acquisitions, including the 1904 purchase of a 4,000-volume library of Indica, the 1906 purchase of G. V. Yudin's 80,000-volume Russian library, the 1908 Schatz collection of early opera librettos, and the early 1930s purchase of the Russian Imperial Collection, consisting of 2,600 volumes"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_10",
    "chunk": "from the library of the Romanov family on a variety of topics. Collections of Hebraica, Chinese, and Japanese works were also acquired. On one occasion, Congress initiated an acquisition: in 1929 Congressman Ross Collins (D-Mississippi) gained approval for the library to purchase Otto Vollbehr's collection of incunabula for $1.5 million. This collection included one of three remaining perfect vellum copies of the Gutenberg Bible. Putnam established the Legislative Reference Service (LRS) in 1914 as a separative administrative unit of the library. Based on the Progressive Era's philosophy of science to be used to solve problems, and modeled after successful research branches of state legislatures, the LRS would provide informed answers to Congressional research inquiries on almost any topic. Congress passed in 1925 an act allowing the Library of Congress to establish a trust fund board to accept donations and endowments, giving the library a role as a patron of the arts. The library received donations and endowments by such prominent wealthy individuals as John D. Rockefeller, James B. Wilbur, and Archer M. Huntington. Gertrude Clarke Whittall donated five Stradivarius violins to the library. Elizabeth Sprague Coolidge's donations paid for a concert hall to be constructed within the Library of Congress"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_11",
    "chunk": "building and an honorarium established for the Music Division to pay live performers for concerts. A number of chairs and consultantships were established from the donations, the most well-known of which is the Poet Laureate Consultant. The library's expansion eventually filled the library's Main Building, although it used shelving expansions in 1910 and 1927. The library needed to expand into a new structure. Congress acquired nearby land in 1928 and approved construction of the Annex Building (later known as the John Adams Building) in 1930. Although delayed during the Depression years, it was completed in 1938 and opened to the public in 1939. In 1939, following Putnam's retirement, President Franklin D. Roosevelt appointed poet and writer Archibald MacLeish as his successor. Occupying the post from 1939 to 1944 during the height of World War II, MacLeish became the most widely known librarian of Congress in the library's history. MacLeish encouraged librarians to oppose totalitarianism on behalf of democracy; dedicated the South Reading Room of the Adams Building to Thomas Jefferson, and commissioned artist Ezra Winter to paint four themed murals for the room. He established a \"democracy alcove\" in the Main Reading Room of the Jefferson Building for essential documents"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_12",
    "chunk": "such as the Declaration of Independence, the Constitution, and The Federalist Papers. The Library of Congress assisted during the war effort, ranging from storage of the Declaration of Independence and the United States Constitution in Fort Knox for safekeeping to researching weather data on the Himalayas for Air Force pilots. MacLeish resigned in 1944 when appointed as Assistant Secretary of State. President Harry Truman appointed Luther H. Evans as Librarian of Congress. Evans, who served until 1953, expanded the library's acquisitions, cataloging, and bibliographic services. But he is best known for creating Library of Congress Missions worldwide. Missions played a variety of roles in the postwar world: the mission in San Francisco assisted participants in the meeting that established the United Nations, the mission in Europe acquired European publications for the Library of Congress and other American libraries, and the mission in Japan aided in the creation of the National Diet Library. Evans' successor Lawrence Quincy Mumford took over in 1953. During his tenure, lasting until 1974, Mumford directed the initiation of construction of the James Madison Memorial Building, the third Library of Congress building on Capitol Hill. Mumford led the library during the government's increased educational spending. The library"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_13",
    "chunk": "was able to establish new acquisition centers abroad, including in Cairo and New Delhi. In 1967, the library began experimenting with book preservation techniques through a Preservation Office. This has developed as the most extensive library research and conservation effort in the United States. During Mumford's administration, the last significant public debate occurred about the Library of Congress's role as both a legislative and national library. Asked by Joint Library Committee chairman Senator Claiborne Pell (D-RI) to assess operations and make recommendations, Douglas Bryant of Harvard University Library proposed several institutional reforms. These included expanding national activities and services and various organizational changes, all of which would emphasize the library's federal role rather than its legislative role. Bryant suggested changing the name of the Library of Congress, a recommendation rebuked by Mumford as \"unspeakable violence to tradition.\" The debate continued within the library community for some time. The Legislative Reorganization Act of 1970 renewed emphasis for the library on its legislative roles, requiring a greater focus on research for Congress and congressional committees, and renaming the Legislative Reference Service as the Congressional Research Service. After Mumford retired in 1974, President Gerald Ford appointed historian Daniel J. Boorstin as a librarian."
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_14",
    "chunk": "Boorstin's first challenge was to manage the relocation of some sections to the new Madison Building, which took place between 1980 and 1982. With this accomplished, Boorstin focused on other areas of library administration, such as acquisitions and collections. Taking advantage of steady budgetary growth, from $116 million in 1975 to over $250 million by 1987, Boorstin enhanced institutional and staff ties with scholars, authors, publishers, cultural leaders, and the business community. His activities changed the post of librarian of Congress so that by the time he retired in 1987, The New York Times called this office \"perhaps the leading intellectual public position in the nation.\" In 1987, President Ronald Reagan nominated historian James H. Billington as the 13th librarian of Congress, and the U.S. Senate unanimously confirmed the appointment. Under Billington's leadership, the library doubled the size of its analog collections from 85.5 million items in 1987 to more than 160 million items in 2014. At the same time, it established new programs and employed new technologies to \"get the champagne out of the bottle\". These included: Since 1988, the library has administered the National Film Preservation Board. Established by congressional mandate, it selects twenty-five American films annually for"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_15",
    "chunk": "preservation and inclusion in the National Film Registry, a collection of American films, for which the Library of Congress accepts nominations each year. There also exists a National Recording Registry administered by the National Recording Preservation Board that serves a similar purpose for music and sound recordings. The library has made some of these available on the Internet for free streaming and additionally has provided brief essays on the films that have been added to the registry. By 2015, the librarian had named 650 films to the registry. The films in the collection date from the earliest period to ones produced more than ten years ago; they are selected from nominations submitted to the board. Further programs included: During Billington's tenure, the library acquired General Lafayette's papers in 1996 from a castle at La Grange, France; they had previously been inaccessible. It also acquired the only copy of the 1507 Waldseemüller world map (\"America's birth certificate\") in 2003; it is on permanent display in the library's Thomas Jefferson Building. Using privately raised funds, the Library of Congress has created a reconstruction of Thomas Jefferson's original library. This has been on permanent display in the Jefferson building since 2008. Under Billington,"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_16",
    "chunk": "public spaces of the Jefferson Building were enlarged and technologically enhanced to serve as a national exhibition venue. It has hosted more than 100 exhibitions. These included exhibits on the Vatican Library and the Bibliothèque Nationale de France, several on the Civil War and Lincoln, on African-American culture, on Religion and the founding of the American Republic, the Early Americas (the Kislak Collection became a permanent display), on the global celebration commemorating the 800th anniversary of Magna Carta, and on early American printing, featuring the Rubenstein Bay Psalm Book. Onsite access to the Library of Congress has been increased. Billington gained an underground connection between the new U.S. Capitol Visitors Center and the library in 2008 to increase both congressional usage and public tours of the library's Thomas Jefferson Building. In 2001, the library began a mass deacidification program, to extend the lifespan of almost 4 million volumes and 12 million manuscript sheets. In 2002, a new storage facility was completed at Fort Meade, Maryland, where a collection of storage modules have preserved and made accessible more than 4 million items from the library's analog collections. Billington established the Library Collections Security Oversight Committee in 1992 to improve protection of"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_17",
    "chunk": "the collections, and also the Library of Congress Congressional Caucus in 2008 to draw attention to the library's curators and collections. He created the library's first Young Readers Center in the Jefferson Building in 2009, and the first large-scale summer intern (Junior Fellows) program for university students in 1991. Under Billington, the library sponsored the Gateway to Knowledge in 2010 to 2011, a mobile exhibition to ninety sites, covering all states east of the Mississippi, in a specially designed eighteen-wheel truck. This increased public access to library collections off-site, particularly for rural populations, and helped raise awareness of what was also available online. Billington raised more than half a billion dollars of private support to supplement Congressional appropriations for library collections, programs, and digital outreach. These private funds helped the library to continue its growth and outreach in the face of a 30% decrease in staffing, caused mainly by legislative appropriations cutbacks. He created the library's first development office for private fundraising in 1987. In 1990, he established the James Madison Council, the library's first national private sector donor-support group. In 1987, Billington also asked the Government Accountability Office (GAO) to conduct the first library-wide audit. He created the first"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_18",
    "chunk": "Office of the Inspector General at the library to provide regular, independent reviews of library operations. This precedent has resulted in regular annual financial audits at the library; it has received unmodified (\"clean\") opinions from 1995 onward. In April 2010, the library announced plans to archive all public communication on Twitter, including all communication since Twitter's launch in March 2006. As of 2015, the Twitter archive remains unfinished. Before retiring in 2015, after 28 years of service, Billington had come \"under pressure\" as librarian of Congress. This followed a GAO report that described a \"work environment lacking central oversight\" and faulted Billington for \"ignoring repeated calls to hire a chief information officer, as required by law.\" When Billington announced his plans to retire in 2015, commentator George Weigel described the Library of Congress as \"one of the last refuges in Washington of serious bipartisanship and calm, considered conversation\", and \"one of the world's greatest cultural centers\". Carla Hayden was sworn in as the 14th librarian of Congress on September 14, 2016, the first woman and the first African American to hold the position. In 2017, the library announced the Librarian-in-Residence program, which aims to support the future generation of librarians"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_19",
    "chunk": "by giving them the opportunity to gain work experience in five different areas of librarianship, including: Acquisitions/Collection Development, Cataloging/Metadata, and Collection Preservation. On January 6, 2021, at 1:11 pm EST, the Library's Madison Building and the Cannon House Office Building were the first buildings in the Capitol Complex to be ordered to evacuate as rioters breached security perimeters before storming the Capitol building. Hayden clarified two days later that rioters did not breach any of the Library's buildings or collections and all staff members were safely evacuated. On February 14, 2023, the Library announced that the Lilly Endowment gifted $2.5 million, five-year grant to \"launch programs that foster greater understanding of religious cultures in Africa, Central Asia and the Middle East\". The Library plans to leverage the donation in these areas: The collections of the Library of Congress include more than 32 million catalogued books and other print materials in 470 languages; more than 61 million manuscripts; the largest rare book collection in North America, including the rough draft of the Declaration of Independence, a Gutenberg Bible (originating from the Saint Blaise Abbey, Black Forest—one of only three perfect vellum copies known to exist); over 1 million U.S. government publications;"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_20",
    "chunk": "1 million issues of world newspapers spanning the past three centuries; 33,000 bound newspaper volumes; 500,000 microfilm reels; U.S. and foreign comic books—over 12,000 titles in all, totaling more than 140,000 issues; 1.9 million moving images (as of 2020); 5.3 million maps; 6 million works of sheet music; 3 million sound recordings; more than 14.7 million prints and photographic images including fine and popular art pieces and architectural drawings; the Betts Stradivarius; and the Cassavetti Stradivarius. The library developed a system of book classification called Library of Congress Classification (LCC), which is used by most U.S. research and university libraries. The library serves as a legal repository for copyright protection and copyright registration, and as the base for the United States Copyright Office. Regardless of whether they register their copyright, all publishers are required to submit two complete copies of their published works to the library—this requirement is known as mandatory deposit. Nearly 15,000 new items published in the U.S. arrive every business day at the library. Contrary to popular belief, however, the library does not retain all of these works in its permanent collection, although it does add an average of 12,000 items per day. Rejected items are used"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_21",
    "chunk": "in trades with other libraries around the world, distributed to federal agencies, or donated to schools, communities, and other organizations within the United States. As is true of many similar libraries, the Library of Congress retains copies of every publication in the English language that is deemed significant. The Library of Congress states that its collection fills about 838 mi (1,349 km) of bookshelves and holds more than 167 million items with over 39 million books and other print materials. A 2000 study by information scientists Peter Lyman and Hal Varian suggested that the amount of uncompressed textual data represented by the 26 million books then in the collection was 10 terabytes. The library also administers the National Library Service for the Blind and Physically Handicapped, an audio book and braille library program provided to more than 766,000 Americans. The library's first digitization project, American Memory, was launched in 1990, and was initially planned to choose 160 million objects from its collection to make digitally available on LaserDiscs and CDs, which were distributed to schools and libraries. After realizing that this plan would be too expensive and inefficient, and with the rise of the Internet, the library decided to instead"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_22",
    "chunk": "make digitized material available over the Internet. This project was made official in the National Digital Library Program (NDLP), created in October 1994. By 1999, the NDLP had succeeded in digitizing over 5 million objects and had a budget of $12 million. The library has kept the \"American Memory\" name for its public domain website, which today contains 15 million digital objects, comprising over 7 petabytes of data. American Memory is a source for public domain image resources, as well as audio, video, and archived Web content. Nearly all of the lists of holdings, the catalogs of the library, can be consulted directly on its website. Librarians all over the world consult these catalogs, through the Web or through other media better suited to their needs, when they need to catalog for their collection a book published in the United States. They use the Library of Congress Control Number to make sure of the exact identity of the book. Digital images are also available at Snapshots of the Past, which provides archival prints. The library has a budget of $6–8 million each year for digitization, meaning that not all works can be digitized. It makes determinations about what objects to"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_23",
    "chunk": "prioritize based on what is especially important to Congress or potentially interesting for the public. The 15 million digitized items represent less than 10% of the library's total 160-million-item collection. The library has chosen not to participate in other digital library projects such as Google Books and the Digital Public Library of America, although it has supported the Internet Archive project. In 1995, the Library of Congress established THOMAS, an online archive of the proceedings of the United States Congress, which included the full text of proposed legislation, bill summaries, and statuses, Congressional Record text, and an index of the Congressional Record. In 2005 and again in 2010, the THOMAS system received major updates. A migration to a more modernized Web system, Congress.gov, began in 2012, and the THOMAS system was retired in 2016. Congress.gov is a joint project of the Library of Congress, the House, the Senate, and the Government Publishing Office. The Library of Congress is physically housed in three buildings on Capitol Hill and a conservation center in rural Virginia. The library's Capitol Hill buildings are all connected by underground passageways, so that a library user need pass through security only once in a single visit. The"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_24",
    "chunk": "library also has off-site storage facilities in Maryland for less commonly requested materials. The Thomas Jefferson Building is located between Independence Avenue and East Capitol Street on First Street SE. Construction began in 1890 with granite supplied by New England Granite Works, owned by James G. Batterson. The building opened in 1897 as the main building of the library and is the oldest of the three buildings. Known originally as the Library of Congress Building or Main Building, it took its present name on June 13, 1980. The John Adams Building is located between Independence Avenue and East Capitol Street on 2nd Street SE, the block adjacent to the Jefferson Building. The building was originally known as The Annex to the Main Building, which had run out of space. It opened its doors to the public on January 3, 1939. Initially, it also housed the U.S. Copyright Office which moved to the Madison building in the 1970s. The James Madison Memorial Building is located between First and Second Streets on Independence Avenue SE. The building was constructed from 1971 to 1976, and serves as the official memorial to James Madison, a Founding Father and the fourth President of the United"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_25",
    "chunk": "States The Madison Building is also home to the United States Copyright Office and to the Mary Pickford Theater, the \"motion picture and television reading room\" of the Library of Congress. The theater hosts regular free screenings of classic and contemporary movies and television shows. Founded in 2007 and located in Culpeper, Virginia in Northern Virginia, the National Audio-Visual Conservation Center is the Library of Congress's newest building. It was constructed out of a former Federal Reserve storage center and Cold War bunker. The campus is designed to act as a single site to store all of the library's movie, television, and sound collections. It is named in honor of David Woodley Packard, whose Packard Humanities Institute oversaw the design and construction of the facility. The centerpiece of the complex is a reproduction Art Deco movie theater that presents free movie screenings to the public on a semi-weekly basis. The Library of Congress, through both the librarian of Congress and the register of copyrights, is responsible for authorizing exceptions to Section 1201 of Title 17 of the United States Code as part of the Digital Millennium Copyright Act. This process is done every three years, with the register receiving proposals"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_26",
    "chunk": "from the public and acting as an advisor to the librarian, who issues a ruling on what is exempt. After three years have passed, the ruling is no longer valid and a new ruling on exemptions must be made. The library is open for academic research to anyone with a Reader Identification Card. One may not remove library items from the reading rooms or the library buildings. Most of the library's general collection of books and journals are in the closed stacks of the Jefferson and Adams Buildings; specialized collections of books and other materials are in closed stacks in all three main library buildings, or are stored off-site. Access to the closed stacks is not permitted under any circumstances, except to authorized library staff, and occasionally, to dignitaries. Only the reading room reference collections are on open shelves. Since 1902, American libraries have been able to request books and other items through interlibrary loan from the Library of Congress if these items are not readily available elsewhere. Through this system, the Library of Congress has served as a \"library of last resort\", according to Herbert Putnam, the Librarian of Congress from 1899 to 1939. The Library of Congress lends"
  },
  {
    "source": "Library of Congress.txt",
    "chunk_id": "Library of Congress.txt_27",
    "chunk": "books to other libraries with the stipulation that they be used only inside the borrowing library. In 2017, the Library of Congress began development on a reader's card for children under the age of sixteen. In addition to its library services, the Library of Congress is actively involved in various standard activities in areas related to bibliographical and search and retrieval standards. Areas of work include MARC standards, Metadata Encoding and Transmission Standard (METS), Metadata Object Description Schema (MODS), Z39.50 and Search/Retrieve Web Service (SRW), and Search/Retrieve via URL (SRU). The Law Library of Congress \"seeks to further legal scholarship by providing opportunities for scholars and practitioners to conduct significant legal research. Individuals are invited to apply for projects which would further the multi-faceted mission of the law library in serving the U.S. Congress, other governmental agencies, and the public.\""
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_0",
    "chunk": "# Light Light, visible light, or visible radiation is electromagnetic radiation that can be perceived by the human eye. Visible light spans the visible spectrum and is usually defined as having wavelengths in the range of 400–700 nanometres (nm), corresponding to frequencies of 750–420 terahertz. The visible band sits adjacent to the infrared (with longer wavelengths and lower frequencies) and the ultraviolet (with shorter wavelengths and higher frequencies), called collectively optical radiation. In physics, the term \"light\" may refer more broadly to electromagnetic radiation of any wavelength, whether visible or not. In this sense, gamma rays, X-rays, microwaves and radio waves are also light. The primary properties of light are intensity, propagation direction, frequency or wavelength spectrum, and polarization. Its speed in vacuum, 299792458 m/s, is one of the fundamental constants of nature. All electromagnetic radiation exhibits some properties of both particles and waves. Single, massless elementary particles, or quanta, of light called photons can be detected with specialized equipment; phenomena like interference are described by waves. Most everyday interactions with light can be understood using geometrical optics; quantum optics, is an important research area in modern physics. The main source of natural light on Earth is the Sun. Historically,"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_1",
    "chunk": "another important source of light for humans has been fire, from ancient campfires to modern kerosene lamps. With the development of electric lights and power systems, electric lighting has effectively replaced firelight. Generally, electromagnetic radiation (EMR) is classified by wavelength into radio waves, microwaves, infrared, the visible spectrum that we perceive as light, ultraviolet, X-rays and gamma rays. The designation \"radiation\" excludes static electric, magnetic and near fields. The behavior of EMR depends on its wavelength. Higher frequencies have shorter wavelengths and lower frequencies have longer wavelengths. When EMR interacts with single atoms and molecules, its behavior depends on the amount of energy per quantum it carries. EMR in the visible light region consists of quanta (called photons) that are at the lower end of the energies that are capable of causing electronic excitation within molecules, which leads to changes in the bonding or chemistry of the molecule. At the lower end of the visible light spectrum, EMR becomes invisible to humans (infrared) because its photons no longer have enough individual energy to cause a lasting molecular change (a change in conformation) in the visual molecule retinal in the human retina, which change triggers the sensation of vision. There exist"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_2",
    "chunk": "animals that are sensitive to various types of infrared, but not by means of quantum-absorption. Infrared sensing in snakes depends on a kind of natural thermal imaging, in which tiny packets of cellular water are raised in temperature by the infrared radiation. EMR in this range causes molecular vibration and heating effects, which is how these animals detect it. Above the range of visible light, ultraviolet light becomes invisible to humans, mostly because it is absorbed by the cornea below 360 nm and the internal lens below 400 nm. Furthermore, the rods and cones located in the retina of the human eye cannot detect the very short (below 360 nm) ultraviolet wavelengths and are in fact damaged by ultraviolet. Many animals with eyes that do not require lenses (such as insects and shrimp) are able to detect ultraviolet, by quantum photon-absorption mechanisms, in much the same chemical way that humans detect visible light. Various sources define visible light as narrowly as 420–680 nm to as broadly as 380–800 nm. Under ideal laboratory conditions, people can see infrared up to at least 1,050 nm; children and young adults may perceive ultraviolet wavelengths down to about 310–313 nm. Plant growth is also"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_3",
    "chunk": "affected by the colour spectrum of light, a process known as photomorphogenesis. The speed of light in vacuum is defined to be exactly 299792458 m/s (approximately 186,282 miles per second). The fixed value of the speed of light in SI units results from the fact that the metre is now defined in terms of the speed of light. All forms of electromagnetic radiation move at exactly this same speed in vacuum. Different physicists have attempted to measure the speed of light throughout history. Galileo attempted to measure the speed of light in the seventeenth century. An early experiment to measure the speed of light was conducted by Ole Rømer, a Danish physicist, in 1676. Using a telescope, Rømer observed the motions of Jupiter and one of its moons, Io. Noting discrepancies in the apparent period of Io's orbit, he calculated that light takes about 22 minutes to traverse the diameter of Earth's orbit. However, its size was not known at that time. If Rømer had known the diameter of the Earth's orbit, he would have calculated a speed of 227000000 m/s. Another more accurate measurement of the speed of light was performed in Europe by Hippolyte Fizeau in 1849. Fizeau"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_4",
    "chunk": "directed a beam of light at a mirror several kilometers away. A rotating cog wheel was placed in the path of the light beam as it traveled from the source, to the mirror and then returned to its origin. Fizeau found that at a certain rate of rotation, the beam would pass through one gap in the wheel on the way out and the next gap on the way back. Knowing the distance to the mirror, the number of teeth on the wheel and the rate of rotation, Fizeau was able to calculate the speed of light as 313000000 m/s. Léon Foucault carried out an experiment which used rotating mirrors to obtain a value of 298000000 m/s in 1862. Albert A. Michelson conducted experiments on the speed of light from 1877 until his death in 1931. He refined Foucault's methods in 1926 using improved rotating mirrors to measure the time it took light to make a round trip from Mount Wilson to Mount San Antonio in California. The precise measurements yielded a speed of 299796000 m/s. The effective velocity of light in various transparent substances containing ordinary matter, is less than in vacuum. For example, the speed of light in"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_5",
    "chunk": "water is about 3/4 of that in vacuum. Two independent teams of physicists were said to bring light to a \"complete standstill\" by passing it through a Bose–Einstein condensate of the element rubidium, one team at Harvard University and the Rowland Institute for Science in Cambridge, Massachusetts and the other at the Harvard–Smithsonian Center for Astrophysics, also in Cambridge. However, the popular description of light being \"stopped\" in these experiments refers only to light being stored in the excited states of atoms, then re-emitted at an arbitrary later time, as stimulated by a second laser pulse. During the time it had \"stopped\", it had ceased to be light. The study of light and the interaction of light and matter is termed optics. Optics has different forms appropriate to different circumstances. Geometrical optics, appropriate for understanding things like eyes, lenses, cameras, fiber optics, and mirrors, works well when the wavelength of light is small in comparison to the objects it interacts with. Physical optics incorporates wave properties and is needed understand diffraction and interference. Quantum optics applies when studying individual photons interacting with matter. A transparent object allows light to transmit or pass through. Conversely, an opaque object does not allow"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_6",
    "chunk": "light to transmit through and instead reflecting or absorbing the light it receives. Most objects do not reflect or transmit light specularly and to some degree scatters the incoming light, which is called glossiness. Surface scattering is caused by the surface roughness of the reflecting surfaces, and internal scattering is caused by the difference of refractive index between the particles and medium inside the object. Like transparent objects, translucent objects allow light to transmit through, but translucent objects also scatter certain wavelength of light via internal scattering. Refraction is the bending of light rays when passing through a surface between one transparent material and another. It is described by Snell's Law: where θ1 is the angle between the ray and the surface normal in the first medium, θ2 is the angle between the ray and the surface normal in the second medium and n1 and n2 are the indices of refraction, n = 1 in a vacuum and n > 1 in a transparent substance. When a beam of light crosses the boundary between a vacuum and another medium, or between two different media, the wavelength of the light changes, but the frequency remains constant. If the beam of light"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_7",
    "chunk": "is not orthogonal (or rather normal) to the boundary, the change in wavelength results in a change in the direction of the beam. This change of direction is known as refraction. The refractive quality of lenses is frequently used to manipulate light in order to change the apparent size of images. Magnifying glasses, spectacles, contact lenses, microscopes and refracting telescopes are all examples of this manipulation. There are many sources of light. A body at a given temperature emits a characteristic spectrum of black-body radiation. A simple thermal source is sunlight, the radiation emitted by the chromosphere of the Sun at around 6,000 K (5,730 °C; 10,340 °F). Solar radiation peaks in the visible region of the electromagnetic spectrum when plotted in wavelength units, and roughly 44% of the radiation that reaches the ground is visible. Another example is incandescent light bulbs, which emit only around 10% of their energy as visible light and the remainder as infrared. A common thermal light source in history is the glowing solid particles in flames, but these also emit most of their radiation in the infrared and only a fraction in the visible spectrum. The peak of the black-body spectrum is in the"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_8",
    "chunk": "deep infrared, at about 10 micrometre wavelength, for relatively cool objects like human beings. As the temperature increases, the peak shifts to shorter wavelengths, producing first a red glow, then a white one and finally a blue-white colour as the peak moves out of the visible part of the spectrum and into the ultraviolet. These colours can be seen when metal is heated to \"red hot\" or \"white hot\". Blue-white thermal emission is not often seen, except in stars (the commonly seen pure-blue colour in a gas flame or a welder's torch is in fact due to molecular emission, notably by CH radicals emitting a wavelength band around 425 nm and is not seen in stars or pure thermal radiation). Atoms emit and absorb light at characteristic energies. This produces \"emission lines\" in the spectrum of each atom. Emission can be spontaneous, as in light-emitting diodes, gas discharge lamps (such as neon lamps and neon signs, mercury-vapor lamps, etc.) and flames (light from the hot gas itself—so, for example, sodium in a gas flame emits characteristic yellow light). Emission can also be stimulated, as in a laser or a microwave maser. Deceleration of a free charged particle, such as an"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_9",
    "chunk": "electron, can produce visible radiation: cyclotron radiation, synchrotron radiation and bremsstrahlung radiation are all examples of this. Particles moving through a medium faster than the speed of light in that medium can produce visible Cherenkov radiation. Certain chemicals produce visible radiation by chemoluminescence. In living things, this process is called bioluminescence. For example, fireflies produce light by this means and boats moving through water can disturb plankton which produce a glowing wake. Certain substances produce light when they are illuminated by more energetic radiation, a process known as fluorescence. Some substances emit light slowly after excitation by more energetic radiation. This is known as phosphorescence. Phosphorescent materials can also be excited by bombarding them with subatomic particles. Cathodoluminescence is one example. This mechanism is used in cathode-ray tube television sets and computer monitors. When the concept of light is intended to include very-high-energy photons (gamma rays), additional generation mechanisms include: Light is measured with two main alternative sets of units: radiometry consists of measurements of light power at all wavelengths, while photometry measures light with wavelength weighted with respect to a standardized model of human brightness perception. Photometry is useful, for example, to quantify Illumination (lighting) intended for human use."
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_10",
    "chunk": "The photometry units are different from most systems of physical units in that they take into account how the human eye responds to light. The cone cells in the human eye are of three types which respond differently across the visible spectrum and the cumulative response peaks at a wavelength of around 555 nm. Therefore, two sources of light which produce the same intensity (W/m) of visible light do not necessarily appear equally bright. The photometry units are designed to take this into account and therefore are a better representation of how \"bright\" a light appears to be than raw intensity. They relate to raw power by a quantity called luminous efficacy and are used for purposes like determining how to best achieve sufficient illumination for various tasks in indoor and outdoor settings. The illumination measured by a photocell sensor does not necessarily correspond to what is perceived by the human eye and without filters which may be costly, photocells and charge-coupled devices (CCD) tend to respond to some infrared, ultraviolet or both. Light exerts physical pressure on objects in its path, a phenomenon which can be deduced by Maxwell's equations, but can be more easily explained by the particle"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_11",
    "chunk": "nature of light: photons strike and transfer their momentum. Light pressure is equal to the power of the light beam divided by c, the speed of light. Due to the magnitude of c, the effect of light pressure is negligible for everyday objects. For example, a one-milliwatt laser pointer exerts a force of about 3.3 piconewtons on the object being illuminated; thus, one could lift a U.S. penny with laser pointers, but doing so would require about 30 billion 1-mW laser pointers. However, in nanometre-scale applications such as nanoelectromechanical systems (NEMS), the effect of light pressure is more significant and exploiting light pressure to drive NEMS mechanisms and to flip nanometre-scale physical switches in integrated circuits is an active area of research. At larger scales, light pressure can cause asteroids to spin faster, acting on their irregular shapes as on the vanes of a windmill. The possibility of making solar sails that would accelerate spaceships in space is also under investigation. Although the motion of the Crookes radiometer was originally attributed to light pressure, this interpretation is incorrect; the characteristic Crookes rotation is the result of a partial vacuum. This should not be confused with the Nichols radiometer, in which"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_12",
    "chunk": "the (slight) motion caused by torque (though not enough for full rotation against friction) is directly caused by light pressure. As a consequence of light pressure, Einstein in 1909 predicted the existence of \"radiation friction\" which would oppose the movement of matter. He wrote, \"radiation will exert pressure on both sides of the plate. The forces of pressure exerted on the two sides are equal if the plate is at rest. However, if it is in motion, more radiation will be reflected on the surface that is ahead during the motion (front surface) than on the back surface. The backwardacting force of pressure exerted on the front surface is thus larger than the force of pressure acting on the back. Hence, as the resultant of the two forces, there remains a force that counteracts the motion of the plate and that increases with the velocity of the plate. We will call this resultant 'radiation friction' in brief.\" Usually light momentum is aligned with its direction of motion. However, for example in evanescent waves momentum is transverse to direction of propagation. In the fifth century BC, Empedocles postulated that everything was composed of four elements; fire, air, earth and water. He"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_13",
    "chunk": "believed that goddess Aphrodite made the human eye out of the four elements and that she lit the fire in the eye which shone out from the eye making sight possible. If this were true, then one could see during the night just as well as during the day, so Empedocles postulated an interaction between rays from the eyes and rays from a source such as the sun. In about 300 BC, Euclid wrote Optica, in which he studied the properties of light. Euclid postulated that light travelled in straight lines and he described the laws of reflection and studied them mathematically. He questioned that sight is the result of a beam from the eye, for he asks how one sees the stars immediately, if one closes one's eyes, then opens them at night. If the beam from the eye travels infinitely fast this is not a problem. In 55 BC, Lucretius, a Roman who carried on the ideas of earlier Greek atomists, wrote that \"The light & heat of the sun; these are composed of minute atoms which, when they are shoved off, lose no time in shooting right across the interspace of air in the direction imparted by"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_14",
    "chunk": "the shove.\" (from On the nature of the Universe). Despite being similar to later particle theories, Lucretius's views were not generally accepted. Ptolemy (c. second century) wrote about the refraction of light in his book Optics. In ancient India, the Hindu schools of Samkhya and Vaisheshika, from around the early centuries AD developed theories on light. According to the Samkhya school, light is one of the five fundamental \"subtle\" elements (tanmatra) out of which emerge the gross elements. The atomicity of these elements is not specifically mentioned and it appears that they were actually taken to be continuous. The Vishnu Purana refers to sunlight as \"the seven rays of the sun\". The Indian Buddhists, such as Dignāga in the fifth century and Dharmakirti in the seventh century, developed a type of atomism that is a philosophy about reality being composed of atomic entities that are momentary flashes of light or energy. They viewed light as being an atomic entity equivalent to energy. René Descartes (1596–1650) held that light was a mechanical property of the luminous body, rejecting the \"forms\" of Ibn al-Haytham and Witelo as well as the \"species\" of Roger Bacon, Robert Grosseteste and Johannes Kepler. In 1637 he"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_15",
    "chunk": "published a theory of the refraction of light that assumed, incorrectly, that light travelled faster in a denser medium than in a less dense medium. Descartes arrived at this conclusion by analogy with the behaviour of sound waves. Although Descartes was incorrect about the relative speeds, he was correct in assuming that light behaved like a wave and in concluding that refraction could be explained by the speed of light in different media. Descartes is not the first to use the mechanical analogies but because he clearly asserts that light is only a mechanical property of the luminous body and the transmitting medium, Descartes's theory of light is regarded as the start of modern physical optics. Pierre Gassendi (1592–1655), an atomist, proposed a particle theory of light which was published posthumously in the 1660s. Isaac Newton studied Gassendi's work at an early age and preferred his view to Descartes's theory of the plenum. He stated in his Hypothesis of Light of 1675 that light was composed of corpuscles (particles of matter) which were emitted in all directions from a source. One of Newton's arguments against the wave nature of light was that waves were known to bend around obstacles, while"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_16",
    "chunk": "light travelled only in straight lines. He did, however, explain the phenomenon of the diffraction of light (which had been observed by Francesco Grimaldi) by allowing that a light particle could create a localised wave in the aether. Newton's theory could be used to predict the reflection of light, but could only explain refraction by incorrectly assuming that light accelerated upon entering a denser medium because the gravitational pull was greater. Newton published the final version of his theory in his Opticks of 1704. His reputation helped the particle theory of light to hold sway during the eighteenth century. The particle theory of light led Pierre-Simon Laplace to argue that a body could be so massive that light could not escape from it. In other words, it would become what is now called a black hole. Laplace withdrew his suggestion later, after a wave theory of light became firmly established as the model for light (as has been explained, neither a particle or wave theory is fully correct). A translation of Newton's essay on light appears in The large scale structure of space-time, by Stephen Hawking and George F. R. Ellis. The fact that light could be polarized was for"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_17",
    "chunk": "the first time qualitatively explained by Newton using the particle theory. Étienne-Louis Malus in 1810 created a mathematical particle theory of polarization. Jean-Baptiste Biot in 1812 showed that this theory explained all known phenomena of light polarization. At that time the polarization was considered as the proof of the particle theory. To explain the origin of colours, Robert Hooke (1635–1703) developed a \"pulse theory\" and compared the spreading of light to that of waves in water in his 1665 work Micrographia (\"Observation IX\"). In 1672 Hooke suggested that light's vibrations could be perpendicular to the direction of propagation. Christiaan Huygens (1629–1695) worked out a mathematical wave theory of light in 1678 and published it in his Treatise on Light in 1690. He proposed that light was emitted in all directions as a series of waves in a medium called the luminiferous aether. As waves are not affected by gravity, it was assumed that they slowed down upon entering a denser medium. Another supporter of the wave theory was Leonhard Euler. He argued in Nova theoria lucis et colorum (1746) that diffraction could more easily be explained by a wave theory. The wave theory predicted that light waves could interfere with"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_18",
    "chunk": "each other like sound waves (as noted around 1800 by Thomas Young). Young showed by means of a diffraction experiment that light behaved as waves. He first publicly stated his \"general law\" of interference in January 1802, in his book A Syllabus of a Course of Lectures on Natural and Experimental Philosophy: But the general law, by which all these appearances are governed, may be very easily deduced from the interference of two coincident undulations, which either cooperate, or destroy each other, in the same manner as two musical notes produce an alternate intension and remission, in the beating of an imperfect unison. He also proposed that different colours were caused by different wavelengths of light and explained colour vision in terms of three-coloured receptors in the eye. In 1816 André-Marie Ampère gave Augustin-Jean Fresnel an idea that the polarization of light can be explained by the wave theory if light were a transverse wave. Later, Fresnel independently worked out his own wave theory of light and presented it to the Académie des Sciences in 1817. Siméon Denis Poisson added to Fresnel's mathematical work to produce a convincing argument in favor of the wave theory, helping to overturn Newton's corpuscular"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_19",
    "chunk": "theory. By the year 1821, Fresnel was able to show via mathematical methods that polarization could be explained by the wave theory of light if and only if light was entirely transverse, with no longitudinal vibration whatsoever. The weakness of the wave theory was that light waves, like sound waves, would need a medium for transmission. The existence of the hypothetical substance luminiferous aether proposed by Huygens in 1678 was cast into strong doubt in the late nineteenth century by the Michelson–Morley experiment. Newton's corpuscular theory implied that light would travel faster in a denser medium, while the wave theory of Huygens and others implied the opposite. At that time, the speed of light could not be measured accurately enough to decide which theory was correct. The first to make a sufficiently accurate measurement was Léon Foucault, in 1850. His result supported the wave theory, and the classical particle theory was finally abandoned (only to partly re-emerge in the twentieth century as photons in quantum theory). In 1845, Michael Faraday discovered that the plane of polarization of linearly polarized light is rotated when the light rays travel along the magnetic field direction in the presence of a transparent dielectric, an"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_20",
    "chunk": "effect now known as Faraday rotation. This was the first evidence that light was related to electromagnetism. In 1846 he speculated that light might be some form of disturbance propagating along magnetic field lines. Faraday proposed in 1847 that light was a high-frequency electromagnetic vibration, which could propagate even in the absence of a medium such as the ether. Faraday's work inspired James Clerk Maxwell to study electromagnetic radiation and light. Maxwell discovered that self-propagating electromagnetic waves would travel through space at a constant speed, which happened to be equal to the previously measured speed of light. From this, Maxwell concluded that light was a form of electromagnetic radiation: he first stated this result in 1862 in On Physical Lines of Force. In 1873, he published A Treatise on Electricity and Magnetism, which contained a full mathematical description of the behavior of electric and magnetic fields, still known as Maxwell's equations. Soon after, Heinrich Hertz confirmed Maxwell's theory experimentally by generating and detecting radio waves in the laboratory and demonstrating that these waves behaved exactly like visible light, exhibiting properties such as reflection, refraction, diffraction and interference. Maxwell's theory and Hertz's experiments led directly to the development of modern radio,"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_21",
    "chunk": "radar, television, electromagnetic imaging and wireless communications. In the quantum theory, photons are seen as wave packets of the waves described in the classical theory of Maxwell. The quantum theory was needed to explain effects even with visual light that Maxwell's classical theory could not (such as spectral lines). In 1900 Max Planck, attempting to explain black-body radiation, suggested that although light was a wave, these waves could gain or lose energy only in finite amounts related to their frequency. Planck called these \"lumps\" of light energy \"quanta\" (from a Latin word for \"how much\"). In 1905, Albert Einstein used the idea of light quanta to explain the photoelectric effect and suggested that these light quanta had a \"real\" existence. In 1923 Arthur Holly Compton showed that the wavelength shift seen when low intensity X-rays scattered from electrons (so called Compton scattering) could be explained by a particle-theory of X-rays, but not a wave theory. In 1926 Gilbert N. Lewis named these light quanta particles photons. Eventually quantum mechanics came to picture light as (in some sense) both a particle and a wave, and (in another sense) as a phenomenon which is neither a particle nor a wave (which actually"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_22",
    "chunk": "are macroscopic phenomena, such as baseballs or ocean waves). Instead, under some approximations light can be described sometimes with mathematics appropriate to one type of macroscopic metaphor (particles) and sometimes another macroscopic metaphor (waves). As in the case for radio waves and the X-rays involved in Compton scattering, physicists have noted that electromagnetic radiation tends to behave more like a classical wave at lower frequencies, but more like a classical particle at higher frequencies, but never completely loses all qualities of one or the other. Visible light, which occupies a middle ground in frequency, can easily be shown in experiments to be describable using either a wave or particle model, or sometimes both. In 1924–1925, Satyendra Nath Bose showed that light followed different statistics from that of classical particles. With Einstein, they generalized this result for a whole set of integer spin particles called bosons (after Bose) that follow Bose–Einstein statistics. The photon is a massless boson of spin 1. In 1927, Paul Dirac quantized the electromagnetic field. Pascual Jordan and Vladimir Fock generalized this process to treat many-body systems as excitations of quantum fields, a process with the misnomer of second quantization. And at the end of the 1940s"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_23",
    "chunk": "a full theory of quantum electrodynamics was developed using quantum fields based on the works of Julian Schwinger, Richard Feynman, Freeman Dyson, and Shinichiro Tomonaga. John R. Klauder, George Sudarshan, Roy J. Glauber, and Leonard Mandel applied quantum theory to the electromagnetic field in the 1950s and 1960s to gain a more detailed understanding of photodetection and the statistics of light (see degree of coherence). This led to the introduction of the coherent state as a concept which addressed variations between laser light, thermal light, exotic squeezed states, etc. as it became understood that light cannot be fully described just referring to the electromagnetic fields describing the waves in the classical picture. In 1977, H. Jeff Kimble et al. demonstrated a single atom emitting one photon at a time, further compelling evidence that light consists of photons. Previously unknown quantum states of light with characteristics unlike classical states, such as squeezed light were subsequently discovered. Development of short and ultrashort laser pulses—created by Q switching and modelocking techniques—opened the way to the study of what became known as ultrafast processes. Applications for solid state research (e.g. Raman spectroscopy) were found, and mechanical forces of light on matter were studied. The"
  },
  {
    "source": "Light.txt",
    "chunk_id": "Light.txt_24",
    "chunk": "latter led to levitating and positioning clouds of atoms or even small biological samples in an optical trap or optical tweezers by laser beam. This, along with Doppler cooling and Sisyphus cooling, was the crucial technology needed to achieve the celebrated Bose–Einstein condensation. Other remarkable results are the demonstration of quantum entanglement, quantum teleportation, and quantum logic gates. The latter are of much interest in quantum information theory, a subject which partly emerged from quantum optics, partly from theoretical computer science. Sunlight provides the energy that green plants use to create sugars mostly in the form of starches, which release energy into the living things that digest them. This process of photosynthesis provides virtually all the energy used by living things. Some species of animals generate their own light, a process called bioluminescence. For example, fireflies use light to locate mates and vampire squid use it to hide themselves from prey."
  },
  {
    "source": "Linda Sparke.txt",
    "chunk_id": "Linda Sparke.txt_0",
    "chunk": "# Linda Sparke Linda Siobhan Sparke is a British astronomer known for her research on the structure and dynamics of galaxies. She is a professor emerita of astronomy at the University of Wisconsin–Madison, and Explorers Program Scientist in the NASA Astrophysics Division. Sparke was born in London, and read mathematics as an undergraduate at the University of Cambridge. She completed a Ph.D. in astronomy in 1981 from the University of California, Berkeley; her dissertation was Swirling Gas Flows in Elliptical Galaxies. After postdoctoral research at the Institute for Advanced Study, the University of Cambridge, and the Kapteyn Astronomical Institute, she became a faculty member at the University of Wisconsin–Madison. She retired in 2010 to become a professor emeritus, served as a program manager at the National Science Foundation for two years, and became research program manager in astrophysics at NASA and later Explorers Program Scientist at NASA. With John Gallagher III, Sparke is the author of the undergraduate textbook Galaxies in the Universe: an Introduction (Cambridge University Press, 2000; 2nd ed., 2006). Sparke was elected as a Fellow of the American Physical Society in 2002, after a nomination from the APS Division of Astrophysics. The fellowship citation noted her \"studies"
  },
  {
    "source": "Linda Sparke.txt",
    "chunk_id": "Linda Sparke.txt_1",
    "chunk": "of the structure and dynamics of galaxies, using orbital motions to probe both time-steady and time-varying gravitational potentials, and the distribution of dark matter\". In 2020 she was named a Fellow of the American Astronomical Society (AAS), as one of 200 Legacy Fellows named to start the AAS Fellows program. Her book Galaxies in the Universe: an Introduction won the Chambliss Astronomical Writing Award of the American Astronomical Society in 2008."
  },
  {
    "source": "List of astronomical observatories.txt",
    "chunk_id": "List of astronomical observatories.txt_0",
    "chunk": "# List of astronomical observatories This is a partial list of astronomical observatories ordered by name, along with initial dates of operation (where an accurate date is available) and location. The list also includes a final year of operation for many observatories that are no longer in operation. While other sciences, such as volcanology and meteorology, also use facilities called observatories for research and observations, this list is limited to observatories that are used to observe celestial objects. Astronomical observatories are mainly divided into four categories: space-based, airborne, ground-based, and underground-based. Many modern telescopes and observatories are located in space to observe astronomical objects in wavelengths of the electromagnetic spectrum that cannot penetrate the Earth's atmosphere (such as ultraviolet radiation, X-rays, and gamma rays) and are thus impossible to observe using ground-based telescopes. Being above the atmosphere, these space observatories can also avoid the effects of atmospheric turbulence that plague ground based telescopes, although new generations of adaptive optics telescopes have since then dramatically improved the situation on the ground. The space high vacuum environment also frees the detectors from the ancestral diurnal cycle due to the atmospheric blue light background of the sky, thereby increasing significantly the observation time."
  },
  {
    "source": "List of astronomical observatories.txt",
    "chunk_id": "List of astronomical observatories.txt_1",
    "chunk": "An intermediate variant is the airborne observatory, specialised in the infrared wavelengths of the electromagnetic spectrum, that conduct observations above the part of the atmosphere containing water vapor that absorbs them, in the stratosphere. Historically, astronomical observatories consisted generally in a building or group of buildings where observations of astronomical objects such as sunspots, planets, asteroids, comets, stars, nebulae, and galaxies in the visible wavelengths of the electromagnetic spectrum were conducted. At first, for millennia, astronomical observations have been made with naked eyes. Then with the discovery of optics, with the help of different types of refractor telescopes and later with reflector telescopes. Their use allowed to dramatically increase both the collecting power and limit of resolution, thus the brightness, level of detail and apparent angular size of distant celestial objects allowing them to be better studied and understood. Following the development of modern physics, new ground-based facilities have been constructed to conduct research in the radio and microwave wavelengths of the electromagnetic spectrum, with radio telescopes and dedicated microwave telescopes. Modern astrophysics has extended the field of study of celestial bodies to non-electromagnetic vectors, such as neutrinos, neutrons and cosmic rays or gravitational waves. Thus, new types of observatories"
  },
  {
    "source": "List of astronomical observatories.txt",
    "chunk_id": "List of astronomical observatories.txt_2",
    "chunk": "have been developed. Interferometers are at the core of gravitational wave detectors. In order to limit the natural or artificial background noise, most particle detector based observatories are built deep underground."
  },
  {
    "source": "List of brightest natural objects in the sky.txt",
    "chunk_id": "List of brightest natural objects in the sky.txt_0",
    "chunk": "# List of brightest natural objects in the sky This list contains all natural objects with an apparent magnitude of 3.5 or above. All objects are listed by their visual magnitudes, and objects too close together to be distinguished are listed jointly. Objects are listed by their proper names or their most commonly used stellar designation. This list does not include transient objects such as comets, or supernovae."
  },
  {
    "source": "List of gravitational wave observations.txt",
    "chunk_id": "List of gravitational wave observations.txt_0",
    "chunk": "# List of gravitational wave observations Direct observation of gravitational waves, which commenced with the detection of an event by LIGO in 2015, plays a key role in gravitational wave astronomy. LIGO has been involved in all subsequent detections to date, with Virgo joining in August 2017. Joint observation runs of LIGO and VIRGO, designated \"O1, O2, etc.\" span many months, with months of maintenance and upgrades in-between designed to increase the instruments sensitivity and range. Within these run periods, the instruments are capable of detecting gravitational waves. The first run, O1, ran from September 12, 2015, to January 19, 2016, and succeeded in its first gravitational wave detection. O2 ran for a greater duration, from November 30, 2016, to August 25, 2017. O3 began on April 1, 2019, which was briefly suspended on September 30, 2019, for maintenance and upgrades, thus O3a. O3b marks resuming of the run and began on November 1, 2019. Due to the COVID-19 pandemic O3 was forced to end prematurely. O4 began on May 24, 2023; initially planned for March, the project needed more time to stabilize the instruments. The O4 observing run has been extended from one year to 18 months, following plans"
  },
  {
    "source": "List of gravitational wave observations.txt",
    "chunk_id": "List of gravitational wave observations.txt_1",
    "chunk": "to make further upgrades for the O5 run. Updated observing plans are published on the official website, containing the latest information on these runs. There is a two month commissioning break planned from January to March 2024, after which observations will resume for the remainder of O4. Gravitational wave events are named starting with the prefix GW, while observations that trigger an event alert but have not (yet) been confirmed are named starting with the prefix S. Six digits then indicate the date of the event, with the two first digits representing the year, the two middle digits the month and two final digits the day of observation. This is similar to the systematic naming for other kinds of astronomical event observations, such as those of gamma-ray bursts. Probable detections that are not confidently identified as gravitational wave events are designated LVT (\"LIGO-Virgo trigger\"). Known gravitational wave events come from the merger of two black holes (BH), two neutron stars (NS), or a black hole and a neutron star (BHNS). Some objects are in the mass gap between the largest predicted neutron star masses (Tolman–Oppenheimer–Volkoff limit) and the smallest known black holes. There is possible detection of nanohertz waves by"
  },
  {
    "source": "List of gravitational wave observations.txt",
    "chunk_id": "List of gravitational wave observations.txt_2",
    "chunk": "observation of the timing of pulsars, but they have not been confirmed at the 5 sigma level of confidence, as of 2023. In addition to well-constrained detections listed above, a number of low-significance detections of possible signals were made by LIGO and Virgo. Their characteristics are listed below, only including detections with a <50% chance of being noise: From observation run O3/2019 on, observations are published as Open Public Alerts to facilitate multi-messenger observations of events. Candidate event records can be directly accessed at the Gravitational-Wave Candidate Event Database (GraceDB). On 1 April 2019, the start of the third observation run was announced with a circular published in the public alerts tracker. The first O3/2019 binary black hole detection alert was broadcast on 8 April 2019. A significant percentage of O3 candidate events detected by LIGO are accompanied by corresponding triggers at Virgo. False alarm rates are mixed, with more than half of events assigned false alarm rates greater than 1 per 20 years, contingent on presence of glitches around signal, foreground electromagnetic instability, seismic activity, and operational status of any one of the three LIGO-Virgo instruments. For instance, events S190421ar and S190425z weren't detected by Virgo and LIGO's Hanford"
  },
  {
    "source": "List of gravitational wave observations.txt",
    "chunk_id": "List of gravitational wave observations.txt_3",
    "chunk": "site, respectively. The LIGO/Virgo collaboration took a short break from observing during the month of October 2019 to improve performance and prepare for future plans, with no signals detected in that month as a result. The Kamioka Gravitational Wave Detector (KAGRA) in Japan became operational on 25 February 2020, likely improving the detection and localization of future gravitational wave signals. However, KAGRA does not report their signals in real-time on GraceDB as LIGO and Virgo do, so the results of their observation run will likely not be published until the end of O3. The LIGO-Virgo collaboration ended the O3 run early on March 27, 2020, due to health concerns from the COVID-19 pandemic. On 15 June 2022, LIGO announced to start the O4 observing run in March 2023. As the date got closer, engineering challenges delayed the observing run to May 2023. An engineering run to assess the sensitivity of LIGO, Virgo, and KAGRA began in April, with the Hanford detector's first operations beginning on April 29, and the Livingston and Virgo detectors' first operations beginning on May 5. On March 7, 2023, a gamma-ray burst compatible with a neutron star merger was detected by the Fermi telescope and named"
  },
  {
    "source": "List of gravitational wave observations.txt",
    "chunk_id": "List of gravitational wave observations.txt_4",
    "chunk": "GRB 230307A. The burst, identified as being from a host galaxy approximately 296 Mpc away, would likely have only been marginally detected at best by LIGO if it had been operating at the time, as the detectors would only later achieve a sensitivity of 160 Mpc for neutron star mergers by O4's beginning, 3 months later. Near the end of the engineering run on 15 May 2023, LIGO announced that O4 would be beginning on 24 May 2023, running for 20 months with up to 2 months of maintenance. The LIGO detectors initially failed to achieve the hoped for 160-190 Mpc sensitivity for neutron star mergers, but did achieve an improved 130-150 Mpc sensitivity over O3's 100-140 Mpc, later improving to nearly 160 Mpc for both detectors by late 2023. Virgo was found to have both a damaged mirror and other new, unknown noise sources, limiting its sensitivity to just 31-35 Mpc (similar to its performance during O2 in 2017, and lower than O3's 40-50 Mpc.) As a result, Virgo spent most of 2023 in commissioning, with a deadline of March 2024 to improve its sensitivity before joining O4. KAGRA achieved its planned 1 Mpc sensitivity before returning to commissioning"
  },
  {
    "source": "List of gravitational wave observations.txt",
    "chunk_id": "List of gravitational wave observations.txt_5",
    "chunk": "in July, with plans to rejoin at an improved 10 Mpc sensitivity by early 2024. However, the Mw7.5 2024 Noto earthquake occurred on 1 January 2024 only 103 kilometres (64 mi) from KAGRA, damaging the detector's sensitive instruments and delaying its development by at least several months. On 18 May 2023, near the end of the engineering run and shortly before O4 proper, the first candidate gravitational wave event was detected. Four more were detected before the official beginning of the run. In October, LIGO announced a planned pause between January and March 2024, for a mid-run commissioning break intended to reduce noise and improve the uptime of the detectors. The O4b run began in April 2024 with the addition of the Virgo detector at a sensitivity of 55 Mpc. The Livingston detector achieved an increased sensitivity of 170-175 Mpc, while the Hanford detector maintained its pre-break sensitivity of 155-160 Mpc. Due to a variety of factors including delays in technologies required for O5, the decision was made in June 2024 to extend O4 by several months to June 2025, with O5 expected to begin in late 2027 or early 2028."
  },
  {
    "source": "List of largest optical telescopes historically.txt",
    "chunk_id": "List of largest optical telescopes historically.txt_0",
    "chunk": "# List of largest optical telescopes historically Telescopes have grown in size since they first appeared around 1608. The following tables list the increase in size over the years. Different technologies can and have been used to build telescopes, which are used to magnify distant views and gather light (especially important in astronomy). The following is a list of largest single mount optical telescopes sorted by total objective diameter (aperture), including segmented and multi-mirror configurations. It is a historical list, with the instruments listed in chronological succession by objective size. By itself, the diameter of the primary optics can be a poor measure of a telescope's historical or scientific significance; for example, William Parsons, 3rd Earl of Rosse's 72-inch (1.8 m) reflecting telescope did not perform as well (i.e. gather as much light) as the smaller silvered glass mirror telescopes that succeeded it because of the poor performance of its speculum metal mirror. Chronological list of optical telescopes by historical significance, which reflects the overall technological progression and not only the primary mirror's diameter (as shown in table above)."
  },
  {
    "source": "List of smallest known stars.txt",
    "chunk_id": "List of smallest known stars.txt_0",
    "chunk": "# List of smallest known stars This is a list of stars, neutron stars, white dwarfs and brown dwarfs which are the least voluminous known (the smallest stars by volume). This is a list of small stars that are notable for characteristics that are not separately listed. Red dwarfs are considered the smallest star known that are active fusion stars, and are the smallest stars possible that is not a brown dwarf."
  },
  {
    "source": "List of solar telescopes.txt",
    "chunk_id": "List of solar telescopes.txt_0",
    "chunk": "# List of solar telescopes Ground-based solar telescopes are specialized telescopes used to observe the Sun from Earth's surface. Solar telescopes often have multiple focal lengths, and use a various combination of mirrors such as coelostats, lenses, and tubes for instruments including spectrographs, cameras, or coronagraphs. There are many types of instruments that have been designed to observe Earth's Sun, for example, in the 20th century solar towers were common. Telescopes for the Sun have existed for hundreds of years, this list is not complete and only goes back to 1900. There are much smaller commercial and/or amateur telescopes such as Coronado Filters from founder and designer David Lunt, bought by Meade Instruments in 2004 and sells SolarMax solar telescopes up to 8 cm Most solar observatories observe optically at visible, UV, and near infrared wavelengths, but other things can be observed."
  },
  {
    "source": "List of space telescopes.txt",
    "chunk_id": "List of space telescopes.txt_0",
    "chunk": "# List of space telescopes This list of space telescopes (astronomical space observatories) is grouped by major frequency ranges: gamma ray, x-ray, ultraviolet, visible, infrared, microwave, and radio. Telescopes that work in multiple frequency bands are included in all of the appropriate sections. Space telescopes that collect particles, such as cosmic ray nuclei and/or electrons, as well as instruments that aim to detect gravitational waves, are also listed. Missions with specific targets within the Solar System (e.g., the Sun and its planets), are excluded; see List of Solar System probes for these, and List of Earth observation satellites for missions targeting Earth. Two values are provided for the dimensions of the initial orbit. For telescopes in Earth orbit, the minimum and maximum altitude are given in kilometers. For telescopes in solar orbit, the minimum distance (periapsis) and the maximum distance (apoapsis) between the telescope and the center of mass of the Sun are given in astronomical units (AU). Gamma-ray telescopes collect and measure individual, high energy gamma rays from astrophysical sources. These are absorbed by the atmosphere, requiring that observations are done by high-altitude balloons or space missions. Gamma rays can be generated by supernovae, neutron stars, pulsars and black"
  },
  {
    "source": "List of space telescopes.txt",
    "chunk_id": "List of space telescopes.txt_1",
    "chunk": "holes. Gamma ray bursts, with extremely high energies, have also been detected but have yet to be identified. X-ray telescopes measure high-energy photons called X-rays. These can not travel a long distance through the atmosphere, meaning that they can only be observed high in the atmosphere or in space. Several types of astrophysical objects emit X-rays, from galaxy clusters, through black holes in active galactic nuclei to galactic objects such as supernova remnants, stars, and binary stars containing a white dwarf (cataclysmic variable stars), neutron star or black hole (X-ray binaries). Some Solar System bodies emit X-rays, the most notable being the Moon, although most of the X-ray brightness of the Moon arises from reflected solar X-rays. A combination of many unresolved X-ray sources is thought to produce the observed X-ray background. Ultraviolet telescopes make observations at ultraviolet wavelengths, i.e. between approximately 10 and 320 nm. Light at these wavelengths is absorbed by the Earth's atmosphere, so observations at these wavelengths must be performed from the upper atmosphere or from space. Objects emitting ultraviolet radiation include the Sun, other stars and galaxies. The oldest form of astronomy, optical or visible-light astronomy, observes wavelengths of light from approximately 400 to 700"
  },
  {
    "source": "List of space telescopes.txt",
    "chunk_id": "List of space telescopes.txt_2",
    "chunk": "nm. Positioning an optical telescope in space eliminates the distortions and limitations that hamper that ground-based optical telescopes (see Astronomical seeing), providing higher resolution images. Optical telescopes are used to look at planets, stars, galaxies, planetary nebulae and protoplanetary disks, amongst many other things. Infrared light is of lower energy than visible light, hence is emitted by sources that are either cooler, or moving away from the observer (in present context: Earth) at high speed. As such, the following can be viewed in the infrared: cool stars (including brown dwarves), nebulae, and redshifted galaxies. Microwave space telescopes have primarily been used to measure cosmological parameters from the Cosmic Microwave Background. They also measure synchrotron radiation, free-free emission and spinning dust from the Milky Way Galaxy, as well as extragalactic compact sources and galaxy clusters through the Sunyaev-Zel'dovich effect. As the atmosphere is transparent for radio waves, radio telescopes in space are most useful for Very Long Baseline Interferometry: doing simultaneous observations of a source with both a satellite and a ground-based telescope and by correlating their signals to simulate a radio telescope the size of the separation between the two telescopes. Typical targets for observations include supernova remnants, masers, gravitational"
  },
  {
    "source": "List of space telescopes.txt",
    "chunk_id": "List of space telescopes.txt_3",
    "chunk": "lenses, and starburst galaxies. Spacecraft and space-based modules that do particle detection, looking for cosmic rays and electrons. These can be emitted by the Sun (Solar Energetic Particles), the Milky Way galaxy (Galactic cosmic rays) and extragalactic sources (Extragalactic cosmic rays). There are also Ultra-high-energy cosmic rays from active galactic nuclei, those can be detected by ground-based detectors via their particle showers. A type of telescope that detects gravitational waves; ripples in space-time generated by colliding neutron stars or black holes."
  },
  {
    "source": "List of telescope parts and construction.txt",
    "chunk_id": "List of telescope parts and construction.txt_0",
    "chunk": "# List of telescope parts and construction Subsequent (sometimes optional) components realign, segment, or in some way modify the light of an incoming image:"
  },
  {
    "source": "Lists of planets.txt",
    "chunk_id": "Lists of planets.txt_0",
    "chunk": "# Lists of planets These are lists of planets. A planet is a large, rounded astronomical body that is neither a star nor its remnant. The best available theory of planet formation is the nebular hypothesis, which posits that an interstellar cloud collapses out of a nebula to create a young protostar orbited by a protoplanetary disk. There are eight planets within the Solar System; planets outside of the solar system are also known as exoplanets. As of 1 May 2025, there are 5,889 confirmed exoplanets in 4,395 planetary systems, with 986 systems having more than one planet. Most of these were discovered by the Kepler space telescope. There are an additional 1,980 potential exoplanets from Kepler's first mission yet to be confirmed, as well as 976 from its \"Second Light\" mission and 4,684 from the Transiting Exoplanet Survey Satellite (TESS) mission."
  },
  {
    "source": "Lunar observation.txt",
    "chunk_id": "Lunar observation.txt_0",
    "chunk": "# Lunar observation The Moon is the largest natural satellite of and the closest major astronomical object to Earth. The Moon may be observed by using a variety of optical instruments, ranging from the naked eye to large telescopes. The Moon is the only celestial body upon which surface features can be discerned with the unaided eyes of most people. Contrary to popular belief, the Moon should ideally not be viewed at its full phase. During a full moon, rays of sunlight are hitting the visible portion of the Moon perpendicular to the surface. As a result, there is less surface detail visible during a full moon than during other phases (such as the quarter and crescent phases) when sunlight hits the Moon at a much shallower angle. The brightness of a full moon as compared to a phase where a smaller percentage of the surface is illuminated tends to wash out substantial amounts of detail and can actually leave an afterimage on an observer's eye that can persist for several minutes. First quarter (six to nine days past new moon) is generally considered the best time to observe the Moon for the average stargazer. Shadows and detail are most"
  },
  {
    "source": "Lunar observation.txt",
    "chunk_id": "Lunar observation.txt_1",
    "chunk": "pronounced along the \"terminator\", the dividing line between the illuminated (day side) and dark (night side) of the Moon. It is a common misconception that the moon is not visible during the daytime, although if the moon is a thin crescent or close to the Sun, viewing can require using a telescope. A person must be very cautious if they use a telescope where the Sun is nearby if they do not have appropriate filtering on that telescope, for the Sun's light is strong enough to very quickly blind a person through a telescope that does not have sufficient filters. Generally, the Moon can be viewed even with the naked eye, however it may be more enjoyable with optical instruments. The primary lunar surface features detectable to the naked eye are the lunar maria or \"seas\", large basaltic plains which form imaginary figures as the traditional \"Moon Rabbit\" or familiar \"Man in the Moon\". The maria cover about 35% of the surface. The contrast between the less reflective dark gray maria and the more reflective gray/white lunar highlands is easily visible without optical aid. Under good viewing conditions, those with keen eyesight may also be able to see some of"
  },
  {
    "source": "Lunar observation.txt",
    "chunk_id": "Lunar observation.txt_2",
    "chunk": "the following features: Another interesting phenomenon visible with the naked eye is Earthshine. Best visible shortly before or after a new moon (during the waning and waxing crescent phases respectively), Earthshine is the faint glow of the non-illuminated (night) side of the Moon caused by sunlight reflecting off the surface of Earth (which would appear nearly full to an observer situated on the Moon at this time) and onto the night side of the Moon. By the time the Moon reaches first its quarter however, the sunlight illuminated portion of the Moon becomes far too bright for Earthshine to be seen with the naked eye, however it can still be observed telescopically. Binoculars are commonly used by those just beginning to observe the Moon, and many experienced amateur astronomers prefer the view through binoculars over that through higher-power telescopes due to the larger field of view. Their high level of portability makes them the simplest device used to see more detail on the lunar surface than what is visible to the naked eye. The primary disadvantage of binoculars is that they cannot be held as steadily unless one utilizes a commercial or homemade binocular tripod. The recent introduction of image-stabilized"
  },
  {
    "source": "Lunar observation.txt",
    "chunk_id": "Lunar observation.txt_3",
    "chunk": "binoculars has changed this to some extent; however, cost is still an issue. A 10× pair of binoculars will magnify the Moon approximately as much as a 200mm camera lens can. The photos below were shot with a 200mm lens. The first photo was taken on 13 November 2016 at 6:20pm PST, observing the full Moon just hours before it would officially become the largest supermoon since 1948. The second photo was shot 24 hours later, and the contrast was enhanced to bring out details such as mountainous terrain. The next supermoon will not occur this large until the year 2034. To some it may be more desirable to utilize a telescope in which case far more options for observing the Moon exist. Even a small, well-made telescope will show the observer much greater detail than is visible with the naked eye or small binoculars. As the aperture of the telescope mirror (in the case of a reflecting telescope) or lens (in the case of a refracting telescope) increases, smaller and smaller features will begin to appear. With large amateur telescopes, features as small as 0.6 miles (1 km) in diameter can be observed depending on atmospheric conditions. Most astronomers"
  },
  {
    "source": "Lunar observation.txt",
    "chunk_id": "Lunar observation.txt_4",
    "chunk": "use different kinds of filters in order to bring out the contrast of certain surface features. Simple neutral density filters are also common as they can cut down the amount of light reaching the eye by 60–95%, something that is helpful especially when observing a full or gibbous moon so the surface does not appear as washed out. An occultation is an astronomical event where a celestial object appears completely hidden by another, closer body (with a greater angular diameter) due to the passage of the closer object directly between the more distant object and the observer. Due to the large apparent size of the Moon, lunar occultations are quite common and when a bright celestial object is involved, the result is an event that can be easily observed using the naked eye. The Moon almost constantly occults faint stars as it orbits the Earth but because even a young Moon appears immensely brighter than these stars, these events are difficult to observe using amateur telescopes. However, the Moon does frequently occult brighter stars and even planets due to its close proximity to the ecliptic. Four first magnitude stars, Regulus, Spica, Antares, and Aldebaran, are sufficiently close to the ecliptic"
  },
  {
    "source": "Lunar observation.txt",
    "chunk_id": "Lunar observation.txt_5",
    "chunk": "that they may be occulted by the Moon. In addition, two star clusters visible to the naked eye, the Beehive Cluster and the Pleiades, are often occulted. Depending on one's location on the Earth, there are usually several occultations involving naked eye objects every year and many more that can be observed using binoculars or a telescope. Accurate timings (accuracy at least +/-0.02 seconds) of lunar occultations are scientifically useful in fields such as lunar topography, astrometry, and binary star studies and are collected by the International Occultation Timing Association - IOTA. The archive of lunar occultations observations, (1623 to the present day) are maintained at VizieR. A transient lunar phenomenon (TLP) or \"Lunar Transient Phenomena\" (LTP), refers to short-lived lights, colors, or changes in appearance of the lunar surface. Claims of these phenomena go back at least 1,000 years, with some having been observed independently by multiple witnesses or some in the scientific community. Nevertheless, the majority of transient lunar phenomena reports are irreproducible and do not possess adequate control experiments that could be used to distinguish among alternative hypotheses. Few reports concerning these phenomena are ever published in peer reviewed scientific journals, and rightfully or wrongfully, the lunar"
  },
  {
    "source": "Lunar observation.txt",
    "chunk_id": "Lunar observation.txt_6",
    "chunk": "scientific community rarely discusses these observations. Most lunar scientists will acknowledge that transient events such as outgassing and impact cratering do occur over geologic time: the controversy lies in the frequency of such events. A number of astronomical societies around the world have implemented their own TLP watch programs and TLP alert networks. A number of observers employ different colored filters to determine colored transient events on the Moon. By quickly alternating filters of opposing colors in the telescopic light path, faintly colored areas on the Moon can stand out more by appearing to flicker on and off. A red area will appear brighter when viewed through a red filter and darker when seen through a blue filter. It is possible to alternate the filters manually however, this requires a certain dexterity of the hand and good coordination. A purpose built filter wheel is much more viable alternative, and this can be motorized, so the observer can devote all of their concentration to what is going on through the eyepiece. There are, however a number of features on the Moon that will appear to blink naturally, among them being the southwestern part of Fracastorius (crater), and a section of the"
  },
  {
    "source": "Lunar observation.txt",
    "chunk_id": "Lunar observation.txt_7",
    "chunk": "western wall of Plato (crater). A special filter wheel called a \"crater extinction device\" is capable of measuring the brightness of an individual lunar feature to be measured according to the point where it ceases to be visible. During the first two weeks, the Moon is called 'crescent' (when the illuminated portion increases) while it is 'falling' for the next two weeks. For two weeks, the crescent Moon wanes before and waxes after new moon, or \"change of Moon\". The Moon when other than crescent or dark, is called a gibbous, waxing before and waning after full moon. Because the Moon is so bright, it is especially interesting to see objects \"superimposed\" on it. One particular point of interest is an ISS (International Space Station) transit."
  },
  {
    "source": "Magnetosphere.txt",
    "chunk_id": "Magnetosphere.txt_0",
    "chunk": "# Magnetosphere In astronomy and planetary science, a magnetosphere is a region of space surrounding an astronomical object in which charged particles are affected by that object's magnetic field. It is created by a celestial body with an active interior dynamo. In the space environment close to a planetary body with a dipole magnetic field such as Earth, the field lines resemble a simple magnetic dipole. Farther out, field lines can be significantly distorted by the flow of electrically conducting plasma, as emitted from the Sun (i.e., the solar wind) or a nearby star. Planets having active magnetospheres, like the Earth, are capable of mitigating or blocking the effects of solar radiation or cosmic radiation. Interactions of particles and atmospheres with magnetospheres are studied under the specialized scientific subjects of plasma physics, space physics, and aeronomy. Study of Earth's magnetosphere began in 1600, when William Gilbert discovered that the magnetic field on the surface of Earth resembled that of a terrella, a small, magnetized sphere. In the 1940s, Walter M. Elsasser proposed the model of dynamo theory, which attributes Earth's magnetic field to the motion of Earth's iron outer core. Through the use of magnetometers, scientists were able to study"
  },
  {
    "source": "Magnetosphere.txt",
    "chunk_id": "Magnetosphere.txt_1",
    "chunk": "the variations in Earth's magnetic field as functions of both time and latitude and longitude. Beginning in the late 1940s, rockets were used to study cosmic rays. In 1958, Explorer 1, the first of the Explorer series of space missions, was launched to study the intensity of cosmic rays above the atmosphere and measure the fluctuations in this activity. This mission observed the existence of the Van Allen radiation belt (located in the inner region of Earth's magnetosphere), with the follow-up Explorer 3 later that year definitively proving its existence. Also during 1958, Eugene Parker proposed the idea of the solar wind, with the term 'magnetosphere' being proposed by Thomas Gold in 1959 to explain how the solar wind interacted with the Earth's magnetic field. The later mission of Explorer 12 in 1961 led by the Cahill and Amazeen observation in 1963 of a sudden decrease in magnetic field strength near the noon-time meridian, later was named the magnetopause. By 1983, the International Cometary Explorer observed the magnetotail, or the distant magnetic field. The structure of magnetospheres are dependent on several factors: the type of astronomical object, the nature of sources of plasma and momentum, the period of the object's"
  },
  {
    "source": "Magnetosphere.txt",
    "chunk_id": "Magnetosphere.txt_2",
    "chunk": "spin, the nature of the axis about which the object spins, the axis of the magnetic dipole, and the magnitude and direction of the flow of solar wind. The planetary distance where the magnetosphere can withstand the solar wind pressure is called the Chapman–Ferraro distance. This is usefully modeled by the formula wherein R P {\\displaystyle R_{\\rm {P}}} represents the radius of the planet, B s u r f {\\displaystyle B_{\\rm {surf}}} represents the magnetic field on the surface of the planet at the equator, V S W {\\displaystyle V_{\\rm {SW}}} represents the velocity of the solar wind, ρ {\\displaystyle \\rho } is the particle density of solar wind, and μ 0 {\\displaystyle \\mu _{0}} is the vacuum permeability constant: A magnetosphere is classified as \"intrinsic\" when R C F ≫ R P {\\displaystyle R_{\\rm {CF}}\\gg R_{\\rm {P}}} , or when the primary opposition to the flow of solar wind is the magnetic field of the object. Mercury, Earth, Jupiter, Ganymede, Saturn, Uranus, and Neptune, for example, exhibit intrinsic magnetospheres. A magnetosphere is classified as \"induced\" when R C F ≪ R P {\\displaystyle R_{\\rm {CF}}\\ll R_{\\rm {P}}} , or when the solar wind is not opposed by the object's"
  },
  {
    "source": "Magnetosphere.txt",
    "chunk_id": "Magnetosphere.txt_3",
    "chunk": "magnetic field. In this case, the solar wind interacts with the atmosphere or ionosphere of the planet (or surface of the planet, if the planet has no atmosphere). Venus has an induced magnetic field, which means that because Venus appears to have no internal dynamo effect, the only magnetic field present is that formed by the solar wind's wrapping around the physical obstacle of Venus (see also Venus' induced magnetosphere). When R C F ≈ R P {\\displaystyle R_{\\rm {CF}}\\approx R_{\\rm {P}}} , the planet itself and its magnetic field both contribute. It is possible that Mars is of this type. When viewed from the Sun, a celestial body's orbital motion can compress its otherwise symmetrical magnetosphere slightly, and stretch it out in the direction opposite its motion (in Earth's example, from west to east). This is known as dawn-dusk asymmetry. The bow shock forms the outermost layer of the magnetosphere; the boundary between the magnetosphere and the surrounding medium. For stars, this is usually the boundary between the stellar wind and interstellar medium; for planets, the speed of the solar wind there decreases as it approaches the magnetopause. Due to interactions with the bow shock, the stellar wind plasma"
  },
  {
    "source": "Magnetosphere.txt",
    "chunk_id": "Magnetosphere.txt_4",
    "chunk": "gains a substantial anisotropy, leading to various plasma instabilities upstream and downstream of the bow shock. The magnetosheath is the region of the magnetosphere between the bow shock and the magnetopause. It is formed mainly from shocked solar wind, though it contains a small amount of plasma from the magnetosphere. It is an area exhibiting high particle energy flux, where the direction and magnitude of the magnetic field varies erratically. This is caused by the collection of solar wind gas that has effectively undergone thermalization. It acts as a cushion that transmits the pressure from the flow of the solar wind and the barrier of the magnetic field from the object. The magnetopause is the area of the magnetosphere wherein the pressure from the planetary magnetic field is balanced with the pressure from the solar wind. It is the convergence of the shocked solar wind from the magnetosheath with the magnetic field of the object and plasma from the magnetosphere. Because both sides of this convergence contain magnetized plasma, the interactions between them are complex. The structure of the magnetopause depends upon the Mach number and beta ratio of the plasma, as well as the magnetic field. The magnetopause changes"
  },
  {
    "source": "Magnetosphere.txt",
    "chunk_id": "Magnetosphere.txt_5",
    "chunk": "size and shape as the pressure from the solar wind fluctuates. Opposite the compressed magnetic field is the magnetotail, where the magnetosphere extends far beyond the astronomical object. It contains two lobes, referred to as the northern and southern tail lobes. Magnetic field lines in the northern tail lobe point towards the object while those in the southern tail lobe point away. The tail lobes are almost empty, with few charged particles opposing the flow of the solar wind. The two lobes are separated by a plasma sheet, an area where the magnetic field is weaker, and the density of charged particles is higher. Over Earth's equator, the magnetic field lines become almost horizontal, then return to reconnect at high latitudes. However, at high altitudes, the magnetic field is significantly distorted by the solar wind and its solar magnetic field. On the dayside of Earth, the magnetic field is significantly compressed by the solar wind to a distance of approximately 65,000 kilometers (40,000 mi). Earth's bow shock is about 17 kilometers (11 mi) thick and located about 90,000 kilometers (56,000 mi) from Earth. The magnetopause exists at a distance of several hundred kilometers above Earth's surface. Earth's magnetopause has been"
  },
  {
    "source": "Magnetosphere.txt",
    "chunk_id": "Magnetosphere.txt_6",
    "chunk": "compared to a sieve because it allows solar wind particles to enter. Kelvin–Helmholtz instabilities occur when large swirls of plasma travel along the edge of the magnetosphere at different velocities from the magnetosphere, causing the plasma to slip past. This results in magnetic reconnection, and as the magnetic field lines break and reconnect, solar wind particles are able to enter the magnetosphere. On Earth's nightside, the magnetic field extends in the magnetotail, which lengthwise exceeds 6,300,000 kilometers (3,900,000 mi). Earth's magnetotail is the primary source of the polar aurora. Also, NASA scientists have suggested that Earth's magnetotail might cause \"dust storms\" on the Moon by creating a potential difference between the day side and the night side. Many astronomical objects generate and maintain magnetospheres. In the Solar System this includes the Sun, Mercury, Earth, Jupiter, Saturn, Uranus, Neptune, and Ganymede. The magnetosphere of Jupiter is the largest planetary magnetosphere in the Solar System, extending up to 7,000,000 kilometers (4,300,000 mi) on the dayside and almost to the orbit of Saturn on the nightside. Jupiter's magnetosphere is stronger than Earth's by an order of magnitude, and its magnetic moment is approximately 18,000 times larger. Venus, Mars, and Pluto, on the other"
  },
  {
    "source": "Magnetosphere.txt",
    "chunk_id": "Magnetosphere.txt_7",
    "chunk": "hand, have no intrinsic magnetic field. This may have had significant effects on their geological history. It is hypothesized that Venus and Mars may have lost their primordial water to photodissociation and the solar wind. A strong magnetosphere, were it present, would greatly slow down this process. Magnetospheres generated by exoplanets are thought to be common, though the first discoveries did not come until the 2010s. In 2014, a magnetic field around HD 209458 b was inferred from the way hydrogen was evaporating from the planet. In 2019, the strength of the surface magnetic fields of 4 hot Jupiters were estimated and ranged between 20 and 120 gauss compared to Jupiter's surface magnetic field of 4.3 gauss. In 2020, a radio emission in the 14-30 MHz band was detected from the Tau Boötis system, likely associated with cyclotron radiation from the poles of Tau Boötis b which might be a signature of a planetary magnetic field. In 2021 a magnetic field generated by the hot Neptune HAT-P-11b became the first to be confirmed. The first unconfirmed detection of a magnetic field generated by a terrestrial exoplanet was found in 2023 on YZ Ceti b."
  },
  {
    "source": "Marc Buie.txt",
    "chunk_id": "Marc Buie.txt_0",
    "chunk": "# Marc Buie Marc William Buie (/ˈbuːi/; born September 17, 1958) is an American astronomer and prolific discoverer of minor planets who works at the Southwest Research Institute in Boulder, Colorado in the Space Science Department. Formerly he worked at the Lowell Observatory in Flagstaff, Arizona, and was the Sentinel Space Telescope Mission Scientist for the B612 Foundation, which is dedicated to protecting Earth from asteroid impact events. Buie grew up in Baton Rouge, Louisiana, and received his B.Sc. in physics from Louisiana State University in 1980. He then switched fields and earned his Ph.D. in Planetary Science from the University of Arizona in 1984. Buie was a post-doctoral fellow at the University of Hawaii from 1985 to 1988. From 1988 to 1991, he worked at the Space Telescope Science Institute where he played a key role in the planning and scheduling of the first planetary observations ever made by the Hubble Space Telescope. Buie joined the staff at Lowell Observatory in 1991. Since 1983, Pluto has been a central theme of research done by Buie, who has published over 85 scientific papers and journal articles. His first result was to prove that the methane visible on Pluto was on"
  },
  {
    "source": "Marc Buie.txt",
    "chunk_id": "Marc Buie.txt_1",
    "chunk": "its surface and not part of its atmosphere. Since then he has worked on albedo maps of the surface, composition maps of Pluto and Charon, refinement of the orbits of Charon in addition to the much more recently discovered satellites, measurements of the structure of the atmosphere, and other measurements of the properties of the surfaces of Pluto and Charon. He is also one of the co-discoverers of Pluto's moons, Nix and Hydra. He has been working with the Deep Ecliptic Survey team who have been responsible for the discovery of over 1,000 Kuiper belt objects (KBOs). Beyond the work of just locating these objects, he additionally seeks to develop a better picture of the structure and nature of them. A spin-off project from this endeavor was his participation in the project to locate a Kuiper belt object that was within the range of NASA's New Horizons mission after it passed by Pluto. This search led to the discovery of over 50 new KBOs, including 486958 Arrokoth, the object that New Horizons would eventually perform a close fly-by of on 1 January 2019. In the lead up to the fly-by, Buie also led a successful occultation campaign in Argentina and"
  },
  {
    "source": "Marc Buie.txt",
    "chunk_id": "Marc Buie.txt_2",
    "chunk": "South Africa to observe Arrokoth as it passed in front of a distant star to refine the estimates of its size, shape, and orbit. Jim Green, NASA's director of planetary science at the time, called the effort \"the most historic occultation on the face of the Earth.\" In addition to his research into all aspects of Pluto and the Kuiper belt, Buie also works on studying transitional objects like 2060 Chiron and 5145 Pholus and occasionally comets, such as the recent Deep impact mission that went to Comet Tempel 1. In an effort closer to home, he also studies near-Earth asteroids to try to understand more about these potentially dangerous solar system neighbors. Most of these research efforts involve the use of Lowell Observatory telescopes in addition to occasional use of the Hubble and Spitzer Space Telescopes. He is also active in the development of state-of-the-art astronomical instrumentation having just completed the construction of an infrared imaging spectrograph, Mimir, in collaboration with Dan Clemens of Boston University. Buie is a member of the American Astronomical Society (AAS) and its Division for Planetary Sciences (DPS), the American Geophysical Union (AGU), the International Astronomical Union (IAU), and the International Dark-Sky Association. The"
  },
  {
    "source": "Marc Buie.txt",
    "chunk_id": "Marc Buie.txt_3",
    "chunk": "inner main-belt asteroid 7553 Buie was named in the astronomer's honor on 28 July 1999 (M.P.C. 35486). He is also profiled as part of an article on Pluto in Air & Space Smithsonian magazine."
  },
  {
    "source": "Marin Mersenne.txt",
    "chunk_id": "Marin Mersenne.txt_0",
    "chunk": "# Marin Mersenne Marin Mersenne, OM (also known as Marinus Mersennus or le Père Mersenne; French: [maʁɛ̃ mɛʁsɛn]; 8 September 1588 – 1 September 1648) was a French polymath whose works touched a wide variety of fields. He is perhaps best known today among mathematicians for Mersenne prime numbers, those written in the form Mn = 2 − 1 for some integer n. He also developed Mersenne's laws, which describe the harmonics of a vibrating string (such as may be found on guitars and pianos), and his seminal work on music theory, Harmonie universelle, for which he is referred to as the \"father of acoustics\". Mersenne, an ordained Catholic priest, had many contacts in the scientific world and has been called \"the center of the world of science and mathematics during the first half of the 1600s\" and, because of his ability to make connections between people and ideas, \"the post-box of Europe\". He was also a member of the ascetical Minim religious order and wrote and lectured on theology and philosophy. Mersenne was born of Jeanne Moulière, wife of Julien Mersenne, peasants who lived near Oizé, County of Maine (present-day Sarthe, France). He was educated at Le Mans and"
  },
  {
    "source": "Marin Mersenne.txt",
    "chunk_id": "Marin Mersenne.txt_1",
    "chunk": "at the Jesuit College of La Flèche. On 17 July 1611, he joined the Minim Friars and, after studying theology and Hebrew in Paris, was ordained a priest in 1613. Between 1614 and 1618, he taught theology and philosophy at Nevers, but he returned to Paris and settled at the convent of L'Annonciade in 1620. There he studied mathematics and music and met with other kindred spirits such as René Descartes, Étienne Pascal, Pierre Petit, Gilles de Roberval, Thomas Hobbes, and Nicolas-Claude Fabri de Peiresc. He corresponded with Giovanni Doni, Jacques Alexandre Le Tenneur, Constantijn Huygens, Galileo Galilei, and other scholars in Italy, England and the Dutch Republic. He was a staunch defender of Galileo, assisting him in translations of some of his mechanical works. For four years, Mersenne devoted himself entirely to philosophic and theological writing, and published Quaestiones celeberrimae in Genesim (Celebrated Questions on the Book of Genesis) (1623); L'Impieté des déistes (The Impiety of the Deists) (1624); La Vérité des sciences (Truth of the Sciences Against the Sceptics, 1624). It is sometimes incorrectly stated that he was a Jesuit. He was educated by Jesuits, but he never joined the Society of Jesus. He taught theology and philosophy"
  },
  {
    "source": "Marin Mersenne.txt",
    "chunk_id": "Marin Mersenne.txt_2",
    "chunk": "at Nevers and Paris. In 1635 he set up the informal Académie Parisienne (Academia Parisiensis), which had nearly 140 correspondents, including astronomers and philosophers as well as mathematicians, and was the precursor of the Académie des sciences established by Jean-Baptiste Colbert in 1666. He was not afraid to cause disputes among his learned friends in order to compare their views, notable among which were disputes between Descartes, Pierre de Fermat, and Jean de Beaugrand. Peter L. Bernstein, in his book Against the Gods: The Remarkable Story of Risk, wrote, \"The Académie des Sciences in Paris and the Royal Society in London, which were founded about twenty years after Mersenne's death, were direct descendants of Mersenne's activities.\" In 1635 Mersenne met with Tommaso Campanella but concluded that he could \"teach nothing in the sciences ... but still he has a good memory and a fertile imagination.\" Mersenne asked if Descartes wanted Campanella to come to Holland to meet him, but Descartes declined. He visited Italy fifteen times, in 1640, 1641 and 1645. In 1643–1644 Mersenne also corresponded with the German Socinian Marcin Ruar concerning the Copernican ideas of Pierre Gassendi, finding Ruar already a supporter of Gassendi's position. Among his correspondents"
  },
  {
    "source": "Marin Mersenne.txt",
    "chunk_id": "Marin Mersenne.txt_3",
    "chunk": "were Descartes, Galileo, Roberval, Pascal, Beeckman and other scientists. Quaestiones celeberrimae in Genesim was written as a commentary on the Book of Genesis and comprises uneven sections headed by verses from the first three chapters of that book. At first sight the book appears to be a collection of treatises on various miscellaneous topics. However Robert Lenoble has shown that the principle of unity in the work is a polemic against magical and divinatory arts, cabalism, and animistic and pantheistic philosophies. Mersenne was concerned with the teachings of some Italian naturalists that all things happened naturally and determined astrologically; for example, the nomological determinism of Lucilio Vanini (\"God acts on sublunary beings (humans) using the sky as a tool\"), and Gerolamo Cardano's idea that martyrs and heretic were compelled to self-harm by the stars; Historian of science William Ashworth explains \"Miracles, for example, were endangered by the naturalists, because in a world filled with sympathies and occult forces—with what Lenoble calls a \"spontanéité indéfinie\"—anything could happen naturally\". Mersenne mentions Martin Del Rio's Investigations into Magic and criticises Marsilio Ficino for claiming power for images and characters. He condemns astral magic and astrology and the anima mundi, a concept popular amongst"
  },
  {
    "source": "Marin Mersenne.txt",
    "chunk_id": "Marin Mersenne.txt_4",
    "chunk": "Renaissance neo-platonists. Whilst allowing for a mystical interpretation of the Cabala, he wholeheartedly condemned its magical application, particularly angelology. He also criticises Pico della Mirandola, Cornelius Agrippa, Francesco Giorgio and Robert Fludd, his main target. Harmonie universelle is perhaps Mersenne's most influential work. It is one of the earliest comprehensive works on music theory, touching on a wide range of musical concepts, and especially the mathematical relationships involved in music. The work contains the earliest formulation of what has become known as Mersenne's laws, which describe the frequency of oscillation of a stretched string. This frequency is: where f is the frequency [Hz], L is the length [m], F is the force [N] and μ is the mass per unit length [kg/m]. In this book, Mersenne also introduced several innovative concepts that can be considered the basis of modern reflecting telescopes: Because of criticism that he encountered, especially from Descartes, Mersenne made no attempt to build a telescope of his own. Mersenne is also remembered today thanks to his association with the Mersenne primes. The Mersenne Twister, named for Mersenne primes, is frequently used in computer engineering and in related fields such as cryptography. However, Mersenne was not primarily a"
  },
  {
    "source": "Marin Mersenne.txt",
    "chunk_id": "Marin Mersenne.txt_5",
    "chunk": "mathematician; he wrote about music theory and other subjects. He edited works of Euclid, Apollonius, Archimedes, and other Greek mathematicians. But perhaps his most important contribution to the advance of learning was his extensive correspondence (in Latin) with mathematicians and other scientists in many countries. At a time when the scientific journal had not yet come into being, Mersenne was the centre of a network for exchange of information. It has been argued that Mersenne used his lack of mathematical specialty, his ties to the print world, his legal acumen, and his friendship with the French mathematician and philosopher René Descartes (1596–1650) to manifest his international network of mathematicians. Mersenne's philosophical works are characterized by wide scholarship and the narrowest theological orthodoxy. His greatest service to philosophy was his enthusiastic defence of Descartes, whose agent he was in Paris and whom he visited in exile in the Netherlands. He submitted to various eminent Parisian thinkers a manuscript copy of the Meditations on First Philosophy, and defended its orthodoxy against numerous clerical critics. In later life, he gave up speculative thought and turned to scientific research, especially in mathematics, physics and astronomy. In this connection, his best known work is Harmonie"
  },
  {
    "source": "Marin Mersenne.txt",
    "chunk_id": "Marin Mersenne.txt_6",
    "chunk": "universelle of 1636, dealing with the theory of music and musical instruments. It is regarded as a source of information on 17th-century music, especially French music and musicians, to rival even the works of Pietro Cerone. as the ratio for an equally-tempered semitone ( 2 12 {\\displaystyle {\\sqrt[{12}]{2}}} ). It was more accurate (0.44 cents sharp) than Vincenzo Galilei's 18/17 (1.05 cents flat), and could be constructed using straightedge and compass. Mersenne's description in the 1636 Harmonie universelle of the first absolute determination of the frequency of an audible tone (at 84 Hz) implies that he had already demonstrated that the absolute-frequency ratio of two vibrating strings, radiating a musical tone and its octave, is 1 : 2. The perceived harmony (consonance) of two such notes would be explained if the ratio of the air oscillation frequencies is also 1 : 2, which in turn is consistent with the source-air-motion-frequency-equivalence hypothesis. He also performed extensive experiments to determine the acceleration of falling objects by comparing them with the swing of pendulums, reported in his Cogitata Physico-Mathematica in 1644. He was the first to measure the length of the seconds pendulum, that is a pendulum whose swing takes one second, and"
  },
  {
    "source": "Marin Mersenne.txt",
    "chunk_id": "Marin Mersenne.txt_7",
    "chunk": "the first to observe that a pendulum's swings are not isochronous as Galileo thought, but that large swings take longer than small swings. Two German pamphlets that circulated around Europe in 1614–15, Fama fraternitatis and Confessio Fraternitatis, claimed to be manifestos of a highly select, secret society of alchemists and sages called the Brotherhood of Rosicrucians. The books were allegories, but were obviously written by a small group who were reasonably knowledgeable about the sciences of the day, and their main theme was to promote educational reform (they were anti-Aristotelian). These pamphlets also promoted an occult view of science containing elements of Paracelsian philosophy, neo-Platonism, Christian Cabala and Hermeticism. In effect, they sought to establish a new form of scientific religion with some pre-Christian elements. Mersenne led the fight against acceptance of these ideas, particularly those of Rosicrucian promoter Robert Fludd, who had a lifelong battle of words with Johannes Kepler. Fludd responded with Sophia cum moria certamen (1626), wherein he discusses his involvement with the Rosicrucians. The anonymous Summum bonum (1629), another critique of Mersenne, is a Rosicrucian-themed text. The cabalist Jacques Gaffarel joined Fludd's side, while Pierre Gassendi defended Mersenne. The Rosicrucian ideas were defended by many prominent"
  },
  {
    "source": "Marin Mersenne.txt",
    "chunk_id": "Marin Mersenne.txt_8",
    "chunk": "men of learning, and some members of the European scholarly community boosted their own prestige by claiming to be among the selected members of the Brotherhood. However, it is now generally agreed among historians that there is no evidence that an order of Rosicrucians existed at the time, with later Rosicrucian Orders drawing on the name, with no relation to the writers of the Rosicrucian Manifestoes. During the mid-1630s Mersenne gave up the search for physical causes in the Aristotelian sense (rejecting the idea of essences, which were still favoured by the scholastic philosophers) and taught that true physics could be only a descriptive science of motions (Mécanisme), which was the direction set by Galileo Galilei. Mersenne had been a regular correspondent with Galileo and had extended the work on vibrating strings originally developed by his father, Vincenzo Galilei. An air attributed to Mersenne was used by Ottorino Respighi in his second suite of Ancient Airs and Dances"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_0",
    "chunk": "# Mass Mass is an intrinsic property of a body. It was traditionally believed to be related to the quantity of matter in a body, until the discovery of the atom and particle physics. It was found that different atoms and different elementary particles, theoretically with the same amount of matter, have nonetheless different masses. Mass in modern physics has multiple definitions which are conceptually distinct, but physically equivalent. Mass can be experimentally defined as a measure of the body's inertia, meaning the resistance to acceleration (change of velocity) when a net force is applied. The object's mass also determines the strength of its gravitational attraction to other bodies. The SI base unit of mass is the kilogram (kg). In physics, mass is not the same as weight, even though mass is often determined by measuring the object's weight using a spring scale, rather than balance scale comparing it directly with known masses. An object on the Moon would weigh less than it does on Earth because of the lower gravity, but it would still have the same mass. This is because weight is a force, while mass is the property that (along with gravity) determines the strength of this"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_1",
    "chunk": "force. In the Standard Model of physics, the mass of elementary particles is believed to be a result of their coupling with the Higgs boson in what is known as the Brout–Englert–Higgs mechanism. There are several distinct phenomena that can be used to measure mass. Although some theorists have speculated that some of these phenomena could be independent of each other, current experiments have found no difference in results regardless of how it is measured: The mass of an object determines its acceleration in the presence of an applied force. The inertia and the inertial mass describe this property of physical bodies at the qualitative and quantitative level respectively. According to Newton's second law of motion, if a body of fixed mass m is subjected to a single force F, its acceleration a is given by F/m. A body's mass also determines the degree to which it generates and is affected by a gravitational field. If a first body of mass mA is placed at a distance r (center of mass to center of mass) from a second body of mass mB, each body is subject to an attractive force Fg = GmAmB/r, where G = 6.67×10 N⋅kg⋅m is the"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_2",
    "chunk": "\"universal gravitational constant\". This is sometimes referred to as gravitational mass. Repeated experiments since the 17th century have demonstrated that inertial and gravitational mass are identical; since 1915, this observation has been incorporated a priori in the equivalence principle of general relativity. The International System of Units (SI) unit of mass is the kilogram (kg). The kilogram is 1000 grams (g), and was first defined in 1795 as the mass of one cubic decimetre of water at the melting point of ice. However, because precise measurement of a cubic decimetre of water at the specified temperature and pressure was difficult, in 1889 the kilogram was redefined as the mass of a metal object, and thus became independent of the metre and the properties of water, this being a copper prototype of the grave in 1793, the platinum Kilogramme des Archives in 1799, and the platinum–iridium International Prototype of the Kilogram (IPK) in 1889. However, the mass of the IPK and its national copies have been found to drift over time. The re-definition of the kilogram and several other units came into effect on 20 May 2019, following a final vote by the CGPM in November 2018. The new definition uses"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_3",
    "chunk": "only invariant quantities of nature: the speed of light, the caesium hyperfine frequency, the Planck constant and the elementary charge. The grain was the earliest unit of mass and is the smallest unit in the apothecary, avoirdupois, Tower, and troy systems. The early unit was a grain of wheat or barleycorn used to weigh the precious metals silver and gold. Larger units preserved in stone standards were developed that were used as both units of mass and of monetary currency. The pound was derived from the mina (unit) used by ancient civilizations. A smaller unit was the shekel, and a larger unit was the talent. The magnitude of these units varied from place to place. The Babylonians and Sumerians had a system in which there were 60 shekels in a mina and 60 minas in a talent. The Roman talent consisted of 100 libra (pound) which were smaller in magnitude than the mina. The troy pound (~373.2 g) used in England and the United States for monetary purposes, like the Roman pound, was divided into 12 ounces, but the Roman uncia (ounce) was smaller. The carat is a unit for measuring gemstones that had its origin in the carob seed,"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_4",
    "chunk": "which later was standardized at 1/144 ounce and then 0.2 gram. Goods of commerce were originally traded by number or volume. When weighing of goods began, units of mass based on a volume of grain or water were developed. The diverse magnitudes of units having the same name, which still appear today in our dry and liquid measures, could have arisen from the various commodities traded. The larger avoirdupois pound for goods of commerce might have been based on volume of water which has a higher bulk density than grain. In physical science, one may distinguish conceptually between at least seven different aspects of mass, or seven physical notions that involve the concept of mass. Every experiment to date has shown these seven values to be proportional, and in some cases equal, and this proportionality gives rise to the abstract concept of mass. There are a number of ways mass can be measured or operationally defined: In everyday usage, mass and \"weight\" are often used interchangeably. For instance, a person's weight may be stated as 75 kg. In a constant gravitational field, the weight of an object is proportional to its mass, and it is unproblematic to use the same"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_5",
    "chunk": "unit for both concepts. But because of slight differences in the strength of the Earth's gravitational field at different places, the distinction becomes important for measurements with a precision better than a few percent, and for places far from the surface of the Earth, such as in space or on other planets. Conceptually, \"mass\" (measured in kilograms) refers to an intrinsic property of an object, whereas \"weight\" (measured in newtons) measures an object's resistance to deviating from its current course of free fall, which can be influenced by the nearby gravitational field. No matter how strong the gravitational field, objects in free fall are weightless, though they still have mass. The force known as \"weight\" is proportional to mass and acceleration in all situations where the mass is accelerated away from free fall. For example, when a body is at rest in a gravitational field (rather than in free fall), it must be accelerated by a force from a scale or the surface of a planetary body such as the Earth or the Moon. This force keeps the object from going into free fall. Weight is the opposing force in such circumstances and is thus determined by the acceleration of"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_6",
    "chunk": "free fall. On the surface of the Earth, for example, an object with a mass of 50 kilograms weighs 491 newtons, which means that 491 newtons is being applied to keep the object from going into free fall. By contrast, on the surface of the Moon, the same object still has a mass of 50 kilograms but weighs only 81.5 newtons, because only 81.5 newtons is required to keep this object from going into a free fall on the moon. Restated in mathematical terms, on the surface of the Earth, the weight W of an object is related to its mass m by W = mg, where g = 9.80665 m/s is the acceleration due to Earth's gravitational field, (expressed as the acceleration experienced by a free-falling object). For other situations, such as when objects are subjected to mechanical accelerations from forces other than the resistance of a planetary surface, the weight force is proportional to the mass of an object multiplied by the total acceleration away from free fall, which is called the proper acceleration. Through such mechanisms, objects in elevators, vehicles, centrifuges, and the like, may experience weight forces many times those caused by resistance to the effects"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_7",
    "chunk": "of gravity on objects, resulting from planetary surfaces. In such cases, the generalized equation for weight W of an object is related to its mass m by the equation W = –ma, where a is the proper acceleration of the object caused by all influences other than gravity. (Again, if gravity is the only influence, such as occurs when an object falls freely, its weight will be zero). Although inertial mass, passive gravitational mass and active gravitational mass are conceptually distinct, no experiment has ever unambiguously demonstrated any difference between them. In classical mechanics, Newton's third law implies that active and passive gravitational mass must always be identical (or at least proportional), but the classical theory offers no compelling reason why the gravitational mass has to equal the inertial mass. That it does is merely an empirical fact. Albert Einstein developed his general theory of relativity starting with the assumption that the inertial and passive gravitational masses are the same. This is known as the equivalence principle. The particular equivalence often referred to as the \"Galilean equivalence principle\" or the \"weak equivalence principle\" has the most important consequence for freely falling objects. Suppose an object has inertial and gravitational masses"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_8",
    "chunk": "m and M, respectively. If the only force acting on the object comes from a gravitational field g, the force on the object is: Given this force, the acceleration of the object can be determined by Newton's second law: This says that the ratio of gravitational to inertial mass of any object is equal to some constant K if and only if all objects fall at the same rate in a given gravitational field. This phenomenon is referred to as the \"universality of free-fall\". In addition, the constant K can be taken as 1 by defining our units appropriately. The first experiments demonstrating the universality of free-fall were—according to scientific 'folklore'—conducted by Galileo obtained by dropping objects from the Leaning Tower of Pisa. This is most likely apocryphal: he is more likely to have performed his experiments with balls rolling down nearly frictionless inclined planes to slow the motion and increase the timing accuracy. Increasingly precise experiments have been performed, such as those performed by Loránd Eötvös, using the torsion balance pendulum, in 1889. As of 2008, no deviation from universality, and thus from Galilean equivalence, has ever been found, at least to the precision 10. More precise experimental efforts"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_9",
    "chunk": "are still being carried out. The universality of free-fall only applies to systems in which gravity is the only acting force. All other forces, especially friction and air resistance, must be absent or at least negligible. For example, if a hammer and a feather are dropped from the same height through the air on Earth, the feather will take much longer to reach the ground; the feather is not really in free-fall because the force of air resistance upwards against the feather is comparable to the downward force of gravity. On the other hand, if the experiment is performed in a vacuum, in which there is no air resistance, the hammer and the feather should hit the ground at exactly the same time (assuming the acceleration of both objects towards each other, and of the ground towards both objects, for its own part, is negligible). This can easily be done in a high school laboratory by dropping the objects in transparent tubes that have the air removed with a vacuum pump. It is even more dramatic when done in an environment that naturally has a vacuum, as David Scott did on the surface of the Moon during Apollo 15. A"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_10",
    "chunk": "stronger version of the equivalence principle, known as the Einstein equivalence principle or the strong equivalence principle, lies at the heart of the general theory of relativity. Einstein's equivalence principle states that within sufficiently small regions of spacetime, it is impossible to distinguish between a uniform acceleration and a uniform gravitational field. Thus, the theory postulates that the force acting on a massive object caused by a gravitational field is a result of the object's tendency to move in a straight line (in other words its inertia) and should therefore be a function of its inertial mass and the strength of the gravitational field. In theoretical physics, a mass generation mechanism is a theory which attempts to explain the origin of mass from the most fundamental laws of physics. To date, a number of different models have been proposed which advocate different views of the origin of mass. The problem is complicated by the fact that the notion of mass is strongly related to the gravitational interaction but a theory of the latter has not been yet reconciled with the currently popular model of particle physics, known as the Standard Model. The concept of amount is very old and predates"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_11",
    "chunk": "recorded history. The concept of \"weight\" would incorporate \"amount\" and acquire a double meaning that was not clearly recognized as such. What we now know as mass was until the time of Newton called “weight.” ... A goldsmith believed that an ounce of gold was a quantity of gold. ... But the ancients believed that a beam balance also measured “heaviness” which they recognized through their muscular senses. ... Mass and its associated downward force were believed to be the same thing. Humans, at some early era, realized that the weight of a collection of similar objects was directly proportional to the number of objects in the collection: where W is the weight of the collection of similar objects and n is the number of objects in the collection. Proportionality, by definition, implies that two values have a constant ratio: An early use of this relationship is a balance scale, which balances the force of one object's weight against the force of another object's weight. The two sides of a balance scale are close enough that the objects experience similar gravitational fields. Hence, if they have similar masses then their weights will also be similar. This allows the scale, by"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_12",
    "chunk": "comparing weights, to also compare masses. Consequently, historical weight standards were often defined in terms of amounts. The Romans, for example, used the carob seed (carat or siliqua) as a measurement standard. If an object's weight was equivalent to 1728 carob seeds, then the object was said to weigh one Roman pound. If, on the other hand, the object's weight was equivalent to 144 carob seeds then the object was said to weigh one Roman ounce (uncia). The Roman pound and ounce were both defined in terms of different sized collections of the same common mass standard, the carob seed. The ratio of a Roman ounce (144 carob seeds) to a Roman pound (1728 carob seeds) was: In 1600 AD, Johannes Kepler sought employment with Tycho Brahe, who had some of the most precise astronomical data available. Using Brahe's precise observations of the planet Mars, Kepler spent the next five years developing his own method for characterizing planetary motion. In 1609, Johannes Kepler published his three laws of planetary motion, explaining how the planets orbit the Sun. In Kepler's final planetary model, he described planetary orbits as following elliptical paths with the Sun at a focal point of the ellipse."
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_13",
    "chunk": "Kepler discovered that the square of the orbital period of each planet is directly proportional to the cube of the semi-major axis of its orbit, or equivalently, that the ratio of these two values is constant for all planets in the Solar System. On 25 August 1609, Galileo Galilei demonstrated his first telescope to a group of Venetian merchants, and in early January 1610, Galileo observed four dim objects near Jupiter, which he mistook for stars. However, after a few days of observation, Galileo realized that these \"stars\" were in fact orbiting Jupiter. These four objects (later named the Galilean moons in honor of their discoverer) were the first celestial bodies observed to orbit something other than the Earth or Sun. Galileo continued to observe these moons over the next eighteen months, and by the middle of 1611, he had obtained remarkably accurate estimates for their periods. Sometime prior to 1638, Galileo turned his attention to the phenomenon of objects in free fall, attempting to characterize these motions. Galileo was not the first to investigate Earth's gravitational field, nor was he the first to accurately describe its fundamental characteristics. However, Galileo's reliance on scientific experimentation to establish physical principles would"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_14",
    "chunk": "have a profound effect on future generations of scientists. It is unclear if these were just hypothetical experiments used to illustrate a concept, or if they were real experiments performed by Galileo, but the results obtained from these experiments were both realistic and compelling. A biography by Galileo's pupil Vincenzo Viviani stated that Galileo had dropped balls of the same material, but different masses, from the Leaning Tower of Pisa to demonstrate that their time of descent was independent of their mass. In support of this conclusion, Galileo had advanced the following theoretical argument: He asked if two bodies of different masses and different rates of fall are tied by a string, does the combined system fall faster because it is now more massive, or does the lighter body in its slower fall hold back the heavier body? The only convincing resolution to this question is that all bodies must fall at the same rate. A later experiment was described in Galileo's Two New Sciences published in 1638. One of Galileo's fictional characters, Salviati, describes an experiment using a bronze ball and a wooden ramp. The wooden ramp was \"12 cubits long, half a cubit wide and three finger-breadths thick\""
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_15",
    "chunk": "with a straight, smooth, polished groove. The groove was lined with \"parchment, also smooth and polished as possible\". And into this groove was placed \"a hard, smooth and very round bronze ball\". The ramp was inclined at various angles to slow the acceleration enough so that the elapsed time could be measured. The ball was allowed to roll a known distance down the ramp, and the time taken for the ball to move the known distance was measured. The time was measured using a water clock described as follows: Galileo found that for an object in free fall, the distance that the object has fallen is always proportional to the square of the elapsed time: Galileo had shown that objects in free fall under the influence of the Earth's gravitational field have a constant acceleration, and Galileo's contemporary, Johannes Kepler, had shown that the planets follow elliptical paths under the influence of the Sun's gravitational mass. However, Galileo's free fall motions and Kepler's planetary motions remained distinct during Galileo's lifetime. According to K. M. Browne: \"Kepler formed a [distinct] concept of mass ('amount of matter' (copia materiae)), but called it 'weight' as did everyone at that time.\" Finally, in 1686,"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_16",
    "chunk": "Newton gave this distinct concept its own name. In the first paragraph of Principia, Newton defined quantity of matter as “density and bulk conjunctly”, and mass as quantity of matter. The quantity of matter is the measure of the same, arising from its density and bulk conjunctly. ... It is this quantity that I mean hereafter everywhere under the name of body or mass. And the same is known by the weight of each body; for it is proportional to the weight. Robert Hooke had published his concept of gravitational forces in 1674, stating that all celestial bodies have an attraction or gravitating power towards their own centers, and also attract all the other celestial bodies that are within the sphere of their activity. He further stated that gravitational attraction increases by how much nearer the body wrought upon is to its own center. In correspondence with Isaac Newton from 1679 and 1680, Hooke conjectured that gravitational forces might decrease according to the double of the distance between the two bodies. Hooke urged Newton, who was a pioneer in the development of calculus, to work through the mathematical details of Keplerian orbits to determine if Hooke's hypothesis was correct. Newton's"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_17",
    "chunk": "own investigations verified that Hooke was correct, but due to personal differences between the two men, Newton chose not to reveal this to Hooke. Isaac Newton kept quiet about his discoveries until 1684, at which time he told a friend, Edmond Halley, that he had solved the problem of gravitational orbits, but had misplaced the solution in his office. After being encouraged by Halley, Newton decided to develop his ideas about gravity and publish all of his findings. In November 1684, Isaac Newton sent a document to Edmund Halley, now lost but presumed to have been titled De motu corporum in gyrum (Latin for \"On the motion of bodies in an orbit\"). Halley presented Newton's findings to the Royal Society of London, with a promise that a fuller presentation would follow. Newton later recorded his ideas in a three-book set, entitled Philosophiæ Naturalis Principia Mathematica (English: Mathematical Principles of Natural Philosophy). The first was received by the Royal Society on 28 April 1685–86; the second on 2 March 1686–87; and the third on 6 April 1686–87. The Royal Society published Newton's entire collection at their own expense in May 1686–87. Isaac Newton had bridged the gap between Kepler's gravitational mass"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_18",
    "chunk": "and Galileo's gravitational acceleration, resulting in the discovery of the following relationship which governed both of these: where g is the apparent acceleration of a body as it passes through a region of space where gravitational fields exist, μ is the gravitational mass (standard gravitational parameter) of the body causing gravitational fields, and R is the radial coordinate (the distance between the centers of the two bodies). By finding the exact relationship between a body's gravitational mass and its gravitational field, Newton provided a second method for measuring gravitational mass. The mass of the Earth can be determined using Kepler's method (from the orbit of Earth's Moon), or it can be determined by measuring the gravitational acceleration on the Earth's surface, and multiplying that by the square of the Earth's radius. The mass of the Earth is approximately three-millionths of the mass of the Sun. To date, no other accurate method for measuring gravitational mass has been discovered. Newton's cannonball was a thought experiment used to bridge the gap between Galileo's gravitational acceleration and Kepler's elliptical orbits. It appeared in Newton's 1728 book A Treatise of the System of the World. According to Galileo's concept of gravitation, a dropped stone"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_19",
    "chunk": "falls with constant acceleration down towards the Earth. However, Newton explains that when a stone is thrown horizontally (meaning sideways or perpendicular to Earth's gravity) it follows a curved path. \"For a stone projected is by the pressure of its own weight forced out of the rectilinear path, which by the projection alone it should have pursued, and made to describe a curve line in the air; and through that crooked way is at last brought down to the ground. And the greater the velocity is with which it is projected, the farther it goes before it falls to the Earth.\" Newton further reasons that if an object were \"projected in an horizontal direction from the top of a high mountain\" with sufficient velocity, \"it would reach at last quite beyond the circumference of the Earth, and return to the mountain from which it was projected.\" In contrast to earlier theories (e.g. celestial spheres) which stated that the heavens were made of entirely different material, Newton's theory of mass was groundbreaking partly because it introduced universal gravitational mass: every object has gravitational mass, and therefore, every object generates a gravitational field. Newton further assumed that the strength of each object's"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_20",
    "chunk": "gravitational field would decrease according to the square of the distance to that object. If a large collection of small objects were formed into a giant spherical body such as the Earth or Sun, Newton calculated the collection would create a gravitational field proportional to the total mass of the body, and inversely proportional to the square of the distance to the body's center. For example, according to Newton's theory of universal gravitation, each carob seed produces a gravitational field. Therefore, if one were to gather an immense number of carob seeds and form them into an enormous sphere, then the gravitational field of the sphere would be proportional to the number of carob seeds in the sphere. Hence, it should be theoretically possible to determine the exact number of carob seeds that would be required to produce a gravitational field similar to that of the Earth or Sun. In fact, by unit conversion it is a simple matter of abstraction to realize that any traditional mass unit can theoretically be used to measure gravitational mass. Measuring gravitational mass in terms of traditional mass units is simple in principle, but extremely difficult in practice. According to Newton's theory, all objects"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_21",
    "chunk": "produce gravitational fields and it is theoretically possible to collect an immense number of small objects and form them into an enormous gravitating sphere. However, from a practical standpoint, the gravitational fields of small objects are extremely weak and difficult to measure. Newton's books on universal gravitation were published in the 1680s, but the first successful measurement of the Earth's mass in terms of traditional mass units, the Cavendish experiment, did not occur until 1797, over a hundred years later. Henry Cavendish found that the Earth's density was 5.448 ± 0.033 times that of water. As of 2009, the Earth's mass in kilograms is only known to around five digits of accuracy, whereas its gravitational mass is known to over nine significant figures. Given two objects A and B, of masses MA and MB, separated by a displacement RAB, Newton's law of gravitation states that each object exerts a gravitational force on the other, of magnitude where G is the universal gravitational constant. The above statement may be reformulated in the following way: if g is the magnitude at a given location in a gravitational field, then the gravitational force on an object with gravitational mass M is This is"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_22",
    "chunk": "the basis by which masses are determined by weighing. In simple spring scales, for example, the force F is proportional to the displacement of the spring beneath the weighing pan, as per Hooke's law, and the scales are calibrated to take g into account, allowing the mass M to be read off. Assuming the gravitational field is equivalent on both sides of the balance, a balance measures relative weight, giving the relative gravitation mass of each object. Mass was traditionally believed to be a measure of the quantity of matter in a physical body, equal to the \"amount of matter\" in an object. For example, Barre´ de Saint-Venant argued in 1851 that every object contains a number of \"points\" (basically, interchangeable elementary particles), and that mass is proportional to the number of points the object contains. (In practice, this \"amount of matter\" definition is adequate for most of classical mechanics, and sometimes remains in use in basic education, if the priority is to teach the difference between mass from weight.) This traditional \"amount of matter\" belief was contradicted by the fact that different atoms (and, later, different elementary particles) can have different masses, and was further contradicted by Einstein's theory"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_23",
    "chunk": "of relativity (1905), which showed that the measurable mass of an object increases when energy is added to it (for example, by increasing its temperature or forcing it near an object that electrically repels it.) This motivates a search for a different definition of mass that is more accurate than the traditional definition of \"the amount of matter in an object\". Inertial mass is the mass of an object measured by its resistance to acceleration. This definition has been championed by Ernst Mach and has since been developed into the notion of operationalism by Percy W. Bridgman. The simple classical mechanics definition of mass differs slightly from the definition in the theory of special relativity, but the essential meaning is the same. In classical mechanics, according to Newton's second law, we say that a body has a mass m if, at any instant of time, it obeys the equation of motion where F is the resultant force acting on the body and a is the acceleration of the body's centre of mass. For the moment, we will put aside the question of what \"force acting on the body\" actually means. This equation illustrates how mass relates to the inertia of"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_24",
    "chunk": "a body. Consider two objects with different masses. If we apply an identical force to each, the object with a bigger mass will experience a smaller acceleration, and the object with a smaller mass will experience a bigger acceleration. We might say that the larger mass exerts a greater \"resistance\" to changing its state of motion in response to the force. However, this notion of applying \"identical\" forces to different objects brings us back to the fact that we have not really defined what a force is. We can sidestep this difficulty with the help of Newton's third law, which states that if one object exerts a force on a second object, it will experience an equal and opposite force. To be precise, suppose we have two objects of constant inertial masses m1 and m2. We isolate the two objects from all other physical influences, so that the only forces present are the force exerted on m1 by m2, which we denote F12, and the force exerted on m2 by m1, which we denote F21. Newton's second law states that where a1 and a2 are the accelerations of m1 and m2, respectively. Suppose that these accelerations are non-zero, so that"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_25",
    "chunk": "the forces between the two objects are non-zero. This occurs, for example, if the two objects are in the process of colliding with one another. Newton's third law then states that If |a1| is non-zero, the fraction is well-defined, which allows us to measure the inertial mass of m1. In this case, m2 is our \"reference\" object, and we can define its mass m as (say) 1 kilogram. Then we can measure the mass of any other object in the universe by colliding it with the reference object and measuring the accelerations. The primary difficulty with Mach's definition of mass is that it fails to take into account the potential energy (or binding energy) needed to bring two masses sufficiently close to one another to perform the measurement of mass. This is most vividly demonstrated by comparing the mass of the proton in the nucleus of deuterium, to the mass of the proton in free space (which is greater by about 0.239%—this is due to the binding energy of deuterium). Thus, for example, if the reference weight m2 is taken to be the mass of the neutron in free space, and the relative accelerations for the proton and neutron in"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_26",
    "chunk": "deuterium are computed, then the above formula over-estimates the mass m1 (by 0.239%) for the proton in deuterium. At best, Mach's formula can only be used to obtain ratios of masses, that is, as m1 / m2 = |a2| / |a1|. An additional difficulty was pointed out by Henri Poincaré, which is that the measurement of instantaneous acceleration is impossible: unlike the measurement of time or distance, there is no way to measure acceleration with a single measurement; one must make multiple measurements (of position, time, etc.) and perform a computation to obtain the acceleration. Poincaré termed this to be an \"insurmountable flaw\" in the Mach definition of mass. Typically, the mass of objects is measured in terms of the kilogram, which since 2019 is defined in terms of fundamental constants of nature. The mass of an atom or other particle can be compared more precisely and more conveniently to that of another atom, and thus scientists developed the dalton (also known as the unified atomic mass unit). By definition, 1 Da (one dalton) is exactly one-twelfth of the mass of a carbon-12 atom, and thus, a carbon-12 atom has a mass of exactly 12 Da. In some frameworks of"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_27",
    "chunk": "special relativity, physicists have used different definitions of the term. In these frameworks, two kinds of mass are defined: rest mass (invariant mass), and relativistic mass (which increases with velocity). Rest mass is the Newtonian mass as measured by an observer moving along with the object. Relativistic mass is the total quantity of energy in a body or system divided by c. The two are related by the following equation: The invariant mass of systems is the same for observers in all inertial frames, while the relativistic mass depends on the observer's frame of reference. In order to formulate the equations of physics such that mass values do not change between observers, it is convenient to use rest mass. The rest mass of a body is also related to its energy E and the magnitude of its momentum p by the relativistic energy-momentum equation: So long as the system is closed with respect to mass and energy, both kinds of mass are conserved in any given frame of reference. The conservation of mass holds even as some types of particles are converted to others. Matter particles (such as atoms) may be converted to non-matter particles (such as photons of light),"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_28",
    "chunk": "but this does not affect the total amount of mass or energy. Although things like heat may not be matter, all types of energy still continue to exhibit mass. Thus, mass and energy do not change into one another in relativity; rather, both are names for the same thing, and neither mass nor energy appear without the other. Both rest and relativistic mass can be expressed as an energy by applying the well-known relationship E = mc, yielding rest energy and \"relativistic energy\" (total system energy) respectively: The \"relativistic\" mass and energy concepts are related to their \"rest\" counterparts, but they do not have the same value as their rest counterparts in systems where there is a net momentum. Because the relativistic mass is proportional to the energy, it has gradually fallen into disuse among physicists. There is disagreement over whether the concept remains useful pedagogically. In bound systems, the binding energy must often be subtracted from the mass of the unbound system, because binding energy commonly leaves the system at the time it is bound. The mass of the system changes in this process merely because the system was not closed during the binding process, so the energy escaped."
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_29",
    "chunk": "For example, the binding energy of atomic nuclei is often lost in the form of gamma rays when the nuclei are formed, leaving nuclides which have less mass than the free particles (nucleons) of which they are composed. Mass–energy equivalence also holds in macroscopic systems. For example, if one takes exactly one kilogram of ice, and applies heat, the mass of the resulting melt-water will be more than a kilogram: it will include the mass from the thermal energy (latent heat) used to melt the ice; this follows from the conservation of energy. This number is small but not negligible: about 3.7 nanograms. It is given by the latent heat of melting ice (334 kJ/kg) divided by the speed of light squared (c ≈ 9×10 m/s). In general relativity, the equivalence principle is the equivalence of gravitational and inertial mass. At the core of this assertion is Albert Einstein's idea that the gravitational force as experienced locally while standing on a massive body (such as the Earth) is the same as the pseudo-force experienced by an observer in a non-inertial (i.e. accelerated) frame of reference. However, it turns out that it is impossible to find an objective general definition for"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_30",
    "chunk": "the concept of invariant mass in general relativity. At the core of the problem is the non-linearity of the Einstein field equations, making it impossible to write the gravitational field energy as part of the stress–energy tensor in a way that is invariant for all observers. For a given observer, this can be achieved by the stress–energy–momentum pseudotensor. In classical mechanics, the inert mass of a particle appears in the Euler–Lagrange equation as a parameter m: After quantization, replacing the position vector x with a wave function, the parameter m appears in the kinetic energy operator: In the ostensibly covariant (relativistically invariant) Dirac equation, and in natural units, this becomes: where the \"mass\" parameter m is now simply a constant associated with the quantum described by the wave function ψ. In the Standard Model of particle physics as developed in the 1960s, this term arises from the coupling of the field ψ to an additional field Φ, the Higgs field. In the case of fermions, the Higgs mechanism results in the replacement of the term mψ in the Lagrangian with G ψ ψ ¯ ϕ ψ {\\displaystyle G_{\\psi }{\\overline {\\psi }}\\phi \\psi } . This shifts the explanandum of the"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_31",
    "chunk": "value for the mass of each elementary particle to the value of the unknown coupling constant Gψ. A tachyonic field, or simply tachyon, is a quantum field with an imaginary mass. Although tachyons (particles that move faster than light) are a purely hypothetical concept not generally believed to exist, fields with imaginary mass have come to play an important role in modern physics and are discussed in popular books on physics. Under no circumstances do any excitations ever propagate faster than light in such theories—the presence or absence of a tachyonic mass has no effect whatsoever on the maximum velocity of signals (there is no violation of causality). While the field may have imaginary mass, any physical particles do not; the \"imaginary mass\" shows that the system becomes unstable, and sheds the instability by undergoing a type of phase transition called tachyon condensation (closely related to second order phase transitions) that results in symmetry breaking in current models of particle physics. The term \"tachyon\" was coined by Gerald Feinberg in a 1967 paper, but it was soon realized that Feinberg's model in fact did not allow for superluminal speeds. Instead, the imaginary mass creates an instability in the configuration:- any"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_32",
    "chunk": "configuration in which one or more field excitations are tachyonic will spontaneously decay, and the resulting configuration contains no physical tachyons. This process is known as tachyon condensation. Well known examples include the condensation of the Higgs boson in particle physics, and ferromagnetism in condensed matter physics. Although the notion of a tachyonic imaginary mass might seem troubling because there is no classical interpretation of an imaginary mass, the mass is not quantized. Rather, the scalar field is; even for tachyonic quantum fields, the field operators at spacelike separated points still commute (or anticommute), thus preserving causality. Therefore, information still does not propagate faster than light, and solutions grow exponentially, but not superluminally (there is no violation of causality). Tachyon condensation drives a physical system that has reached a local limit and might naively be expected to produce physical tachyons, to an alternate stable state where no physical tachyons exist. Once the tachyonic field reaches the minimum of the potential, its quanta are not tachyons any more but rather are ordinary particles with a positive mass-squared. This is a special case of the general rule, where unstable massive particles are formally described as having a complex mass, with the real"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_33",
    "chunk": "part being their mass in the usual sense, and the imaginary part being the decay rate in natural units. However, in quantum field theory, a particle (a \"one-particle state\") is roughly defined as a state which is constant over time; i.e., an eigenvalue of the Hamiltonian. An unstable particle is a state which is only approximately constant over time; If it exists long enough to be measured, it can be formally described as having a complex mass, with the real part of the mass greater than its imaginary part. If both parts are of the same magnitude, this is interpreted as a resonance appearing in a scattering process rather than a particle, as it is considered not to exist long enough to be measured independently of the scattering process. In the case of a tachyon, the real part of the mass is zero, and hence no concept of a particle can be attributed to it. In a Lorentz invariant theory, the same formulas that apply to ordinary slower-than-light particles (sometimes called \"bradyons\" in discussions of tachyons) must also apply to tachyons. In particular the energy–momentum relation: (where p is the relativistic momentum of the bradyon and m is its rest"
  },
  {
    "source": "Mass.txt",
    "chunk_id": "Mass.txt_34",
    "chunk": "mass) should still apply, along with the formula for the total energy of a particle: This equation shows that the total energy of a particle (bradyon or tachyon) contains a contribution from its rest mass (the \"rest mass–energy\") and a contribution from its motion, the kinetic energy. When v is larger than c, the denominator in the equation for the energy is \"imaginary\", as the value under the radical is negative. Because the total energy must be real, the numerator must also be imaginary: i.e. the rest mass m must be imaginary, as a pure imaginary number divided by another pure imaginary number is a real number."
  },
  {
    "source": "Melt inclusion.txt",
    "chunk_id": "Melt inclusion.txt_0",
    "chunk": "# Melt inclusion A melt inclusion is a small parcel or \"blobs\" of melt(s) that is entrapped by crystals growing in magma and eventually forming igneous rocks. In many respects it is analogous to a fluid inclusion within magmatic hydrothermal systems. Melt inclusions tend to be microscopic in size and can be analyzed for volatile contents that are used to interpret trapping pressures of the melt at depth. Melt inclusions are generally small - most are less than 80 micrometres across (a micrometre is one thousandth of a millimeter, or about 0.00004 inches). They may contain a number of different constituents, including glass (which represents melt that has been quenched by rapid cooling), small crystals and a separate vapour-rich bubble. They occur in the crystals that can be found in igneous rocks, such as for example quartz, feldspar, olivine, pyroxene, nepheline, magnetite, perovskite and apatite. Melt inclusions can be found in both volcanic and plutonic rocks. In addition, melt inclusions can contain immiscible (non-miscible) melt phases and their study is an exceptional way to find direct evidence for presence of two or more melts at entrapment. Although they are small, melt inclusions can provide an abundance of useful information. Using"
  },
  {
    "source": "Melt inclusion.txt",
    "chunk_id": "Melt inclusion.txt_1",
    "chunk": "microscopic observations and a range of chemical microanalysis techniques geochemists and igneous petrologists can obtain a range of unique information from melt inclusions. There are various techniques used in analyzing melt inclusion H2O and CO2 contents, major, minor and trace elements including double-sided FTIR micro transmittance, single-sided FTIR micro reflectance, Raman spectroscopy, microthermometry, Secondary Ion Mass Spectroscopy (SIMS), Laser Ablation-Inductively Coupled Plasma Mass Spectrometry (LA-ICPMS), Scanning Electron Microscopy (SEM) and electron microprobe analysis (EMPA). If there is a vapor bubble present within the melt inclusion, analysis of the vapor bubble must be taken into consideration when reconstructing the total volatile budget of the melt inclusion. Microthermometry is the process of reheating a melt inclusion to its original melt temperature and then rapidly quenching to form a homogenous glass phase free of daughter minerals or vapor bubbles that may have been originally contained within the melt inclusion. Stage heating is the process of heating a melt inclusion on a microscope-mounted stage and flowing either helium gas (Vernadsky stage) or argon gas (Linkam TS1400XY) over the stage and then rapidly quenching the melt inclusion after it has reached its original melt temperature to form a homogenous glass phase. Use of a heating"
  },
  {
    "source": "Melt inclusion.txt",
    "chunk_id": "Melt inclusion.txt_2",
    "chunk": "stage allows for observation of changing phases of the melt inclusion as it is reheated to its original melt temperature. This process allows for reheating of one or more melt inclusions in a furnace held at a constant pressure of one atmosphere to their original melt temperatures and then rapidly quenching in water to produce a homogenous glass phase. FTIR is an analytical method which uses an infrared laser focused on a spot on the glass phase of the melt inclusion to determine an absorption (or extinction) coefficient for either H2O and CO2 associated with wavelengths for each species depending on the parent lithology that contained the melt inclusion. Raman spectroscopy is similar to FTIR in using a focused laser on the glass phase of the melt inclusion or a vapor bubble that may be contained in the melt inclusion to identify wavelengths associated with the Raman vibrating bands of volatiles, such as H2O and CO2. Raman spectroscopy can also be used to determine the density of CO2 contained in a vapor bubble if present at a high enough concentration within a melt inclusion. SIMS is used to determine volatile and trace element concentrations by aiming an ion beam (O"
  },
  {
    "source": "Melt inclusion.txt",
    "chunk_id": "Melt inclusion.txt_3",
    "chunk": "or Cs) at the melt inclusion to produce secondary ions that can be measured by a mass spectrometer. LA-ICP-MS can determine major and trace elements, however, with LA-ICPMS, the melt inclusion and any accompanying materials within the melt inclusion are ionized, thus destroying the melt inclusion, and then analyzed with a mass spectrometer. Scanning electron microscopy is a useful tool to employ before any of the above analyses that may result in loss of the original material since it can be used to check for daughter minerals or vapor bubbles and help determine the best technique that should be chosen for melt inclusion analysis. Electron microprobe analysis is ubiquitous in the analysis of major and minor elements in melt inclusions and provide oxide concentrations used in determining parental magma types of the melt inclusions and phenocryst hosts. Melt inclusions have been imaged in three dimensions using X-ray microtomography. This method can be used to determine the dimensions of different phases present in melt inclusions more precisely than by using visible light microscopy. Melt inclusions can be used to determine the composition, compositional evolution and volatile components of magmas that existed in the history of magma systems. This is because melt"
  },
  {
    "source": "Melt inclusion.txt",
    "chunk_id": "Melt inclusion.txt_4",
    "chunk": "inclusions act as a tiny pressure vessel that isolates and preserves the ambient melt surrounding the crystal before they are modified by later processes, such as post-entrapment crystallization. Given that melt inclusions form at varying pressures (P) and temperatures (T), they can also provide important information about the entrapping conditions (P-T) at depth and their volatile contents (H2O, CO2, S, Cl and F) that drive volcanic eruptions. The presence of a vapor bubble adds an additional component for analysis given that the vapor bubble could contain a significant proportion of the H2O and CO2 originally in the melt sampled by the melt inclusion. If the vapor bubble is composed primarily of CO2, Raman spectroscopy can be used to determine the density of CO2 present. Major and minor element concentrations are generally determined using EPMA and common element compositions include Si, Ti, Al, Cr, Fe, Mn, Mg, Ca, Ni, Na, K, P, Cl, F and S. Knowledge of the oxide concentrations related to these major and minor elements can help to determine the composition of the parental magma, the melt inclusion and the phenocryst hosts. Trace element concentrations can be measured by SIMS analysis with resolution in some cases as low"
  },
  {
    "source": "Melt inclusion.txt",
    "chunk_id": "Melt inclusion.txt_5",
    "chunk": "as 1 ppm. LA-ICPMS analyses can also be used to determine trace element concentrations, however lower resolution compared to SIMS does not provide determination of concentrations as low as 1 ppm. Henry Clifton Sorby, in 1858, was the first to document microscopic melt inclusions in crystals. The study of melt inclusions has been driven more recently by the development of sophisticated chemical analysis techniques. Scientists from the former Soviet Union lead the study of melt inclusions in the decades after World War II, and developed methods for heating melt inclusions under a microscope, so changes could be directly observed. A.T. Anderson explored analysis of melt inclusions from basaltic magmas from Kilauea Volcano in Hawaii to determine initial volatile concentrations of magma at depth."
  },
  {
    "source": "Michel Mayor.txt",
    "chunk_id": "Michel Mayor.txt_0",
    "chunk": "# Michel Mayor Michel Gustave Édouard Mayor (French pronunciation: [miʃɛl majɔʁ]; born 12 January 1942) is a Swiss astrophysicist and professor emeritus at the University of Geneva's Department of Astronomy. He formally retired in 2007, but remains active as a researcher at the Observatory of Geneva. He is co-laureate of the 2019 Nobel Prize in Physics along with Jim Peebles and Didier Queloz, and the winner of the 2010 Viktor Ambartsumian International Prize and the 2015 Kyoto Prize. Together with Didier Queloz in 1995, he discovered 51 Pegasi b, the first extrasolar planet orbiting a sun-like star, 51 Pegasi. For this achievement, they were awarded the 2019 Nobel Prize in Physics \"for the discovery of an exoplanet orbiting a solar-type star\" resulting in \"contributions to our understanding of the evolution of the universe and Earth’s place in the cosmos\". Related to the discovery, Mayor noted that humans will never migrate to such exoplanets since they are \"much, much too far away ... [and would take] hundreds of millions of days using the means we have available today\". However, due to discoveries by Mayor, searching for extraterrestrial communications from exoplanets may now be a more practical consideration than thought earlier. Mayor"
  },
  {
    "source": "Michel Mayor.txt",
    "chunk_id": "Michel Mayor.txt_1",
    "chunk": "obtained an MS degree in Physics from the University of Lausanne (1966) and a PhD in Astronomy from the Geneva Observatory in 1971. He was a researcher at the Institute of Astronomy at the University of Cambridge in 1971. Subsequently, he spent sabbatical semesters at the European Southern Observatory (ESO) in northern Chile and at the Institute for Astronomy of the University of Hawaiʻi System. From 1971 to 1984, Mayor worked as a research associate at the Observatory of Geneva, which is home to the astronomy department of the University of Geneva. He became an associate professor at the university in 1984. In 1988, the university named him a full professor, a position he held until his retirement in 2007. Mayor was director of the Observatory of Geneva from 1998 to 2004. He is a professor emeritus at the University of Geneva. Mayor's research interests include extrasolar planets (also known as exoplanets), instrumentation, statistical properties of double stars, globular cluster dynamics, galactic structure and kinematics. Mayor's doctoral thesis at the University of Geneva was devoted to the spiral structure of galaxies. During his time as a research associate, there had been strong interest in developing photoelectric-based Doppler spectrometers to obtain"
  },
  {
    "source": "Michel Mayor.txt",
    "chunk_id": "Michel Mayor.txt_2",
    "chunk": "more accurate measurements of radial velocities of stellar objects compared to existing photographic methods. Following preliminary work by Roger Griffin in 1967 to show the feasibility of photoelectric measurements of radial velocities, Mayor worked with André Baranne at the Marseille Observatory to develop CORAVEL, a photoelectric spectrometer capable of highly accurate radial velocity measurements, which allow measurement of star movements, orbital periods of binary stars, and even the rotational speed of stars. This research led to various fields of interest, including the study of statistical characteristics of solar-type binary stars. With fellow researcher Antoine Duquennoy, they examined the radial velocities of several systems believed to be binary stars in 1991. Their results found that a subset of these may in fact be single star systems with substellar secondary objects. Desiring more accurate radial velocity measurements, Mayor, along with Baranne at Marseille, and with graduate student Didier Queloz, developed ELODIE, a new spectrograph based on the work of CORAVEL, which was estimated to have an accuracy of 15 m/s for bright stars, improving upon the 1 km/s from CORAVEL. ELODIE was developed with the specific intent to determine if the substellar secondary objects were brown dwarf stars or potentially giant planets."
  },
  {
    "source": "Michel Mayor.txt",
    "chunk_id": "Michel Mayor.txt_3",
    "chunk": "By 1994, ELODIE was operational at Geneva and Mayor and Queloz began their survey of Sun-like systems with suspected substellar secondary objects. In July 1995, the pair's survey of 51 Pegasi affirmed that there was an exoplanet orbiting it, identified as 51 Pegasi b, which was later classified as a hot-Jupiter–type planet. This was the first exoplanet to be found orbiting a main-sequence star, as opposed to planets that orbited the remains of a star. Mayor's and Queloz's discovery of an exoplanet launched great interest is searching for other exoplanets since. On 21 March 2022, the 5000th exoplanet beyond the Solar System was confirmed. Mayor's work focused more on improving instrumentation for radial velocity measurements to improve detecting exoplanets and measuring their properties. Mayor led a team to further improve ELODIE to increase velocity measurement accuracy to 1 m/s via the High Accuracy Radial Velocity Planet Searcher (HARPS) installed on the ESO 3.6 m Telescope at La Silla Observatory in Chile by 2003. Mayor led the team that used HARPS to seek out other exoplanets. In 2007, Mayor was one of 11 European scientists who discovered Gliese 581c, the first extrasolar planet in a star's habitable zone, from the ESO"
  },
  {
    "source": "Michel Mayor.txt",
    "chunk_id": "Michel Mayor.txt_4",
    "chunk": "telescope. In 2009, Mayor and his team discovered the lightest exoplanet ever detected around a main sequence star: Gliese 581e. Nonetheless, Mayor noted that humans will never migrate to such exoplanets since they are \"much, much too far away ... [and would take] hundreds of millions of days using the means we have available today\". However, due to discoveries by Mayor, searching for extraterrestrial communications from exoplanets may now be a more practical consideration than thought earlier. In 1998, Mayor was awarded the Swiss Marcel Benoist Prize in recognition of his work and its significance for human life. As of 2003, he was a member of the board of trustees. He received the Prix Jules Janssen from the Société astronomique de France (French Astronomical Society) in 1998. In 2000, he was awarded the Balzan Prize. Four years later, he was awarded the Albert Einstein Medal. In 2005, he received the Shaw Prize in Astronomy, along with American astrophysicist Geoffrey Marcy. Mayor was made a knight of the French Legion d'Honneur in 2004. In collaboration with Pierre-Yves Frei, Mayor wrote a book in French called Les Nouveaux mondes du Cosmos (Seuil, 260 pages), which was awarded the Livre de l'astronomie 2001"
  },
  {
    "source": "Michel Mayor.txt",
    "chunk_id": "Michel Mayor.txt_5",
    "chunk": "prize by the 17th Astronomy Festival Haute Maurienne. Mayor has received honorary doctorate degrees from eight universities: Katholieke Universiteit Leuven (Belgium), 2001; École Polytechnique Fédérale de Lausanne (EPFL) (Lausanne, Switzerland) (2002); Federal University of Rio Grande do Norte (Brazil), 2006; Uppsala University (Sweden), 2007; Paris Observatory (France), 2008; Université Libre de Bruxelles (Belgium), 2009; University of Provence (Marseille, France), 2011, and Université Joseph Fourier (Grenoble, France), 2014. Mayor has received the 2011 BBVA Foundation Frontiers of Knowledge Award of Basic Sciences (together with his former student Didier Queloz) for developing new astronomical instruments and experimental techniques that led to the first observation of planets around Sun-like stars. Asteroid 125076 Michelmayor, discovered by Swiss amateur astronomer Michel Ory at the Jura Observatory in 2001, was named in his honor. The official naming citation was published by the Minor Planet Center on 21 August 2013 (M.P.C. 84674). In 2015, he was awarded the Gold Medal of the Royal Astronomical Society, and the Kyoto Prize in Basic Sciences. In 2017, he received the Wolf Prize in Physics. He and Didier Queloz (also from Switzerland) were awarded one half of the 2019 Nobel Prize in Physics for the discovery of the exoplanet 51 Pegasi"
  },
  {
    "source": "Michel Mayor.txt",
    "chunk_id": "Michel Mayor.txt_6",
    "chunk": "b."
  },
  {
    "source": "Monogamy of entanglement.txt",
    "chunk_id": "Monogamy of entanglement.txt_0",
    "chunk": "# Monogamy of entanglement In quantum physics, monogamy is the property of quantum entanglement that restrict entanglement from being freely shared between arbitrarily many parties. In order for two qubits A and B to be maximally entangled, they must not be entangled with any third qubit C whatsoever. Even if A and B are not maximally entangled, the degree of entanglement between them constrains the degree to which either can be entangled with C. In full generality, for n ≥ 3 {\\displaystyle n\\geq 3} qubits A 1 , … , A n {\\displaystyle A_{1},\\ldots ,A_{n}} , monogamy is characterized by the Coffman–Kundu–Wootters (CKW) inequality, which states that where ρ A 1 A k {\\displaystyle \\rho _{A_{1}A_{k}}} is the density matrix of the substate consisting of qubits A 1 {\\displaystyle A_{1}} and A k {\\displaystyle A_{k}} and τ {\\displaystyle \\tau } is the \"tangle\", a quantification of bipartite entanglement equal to the square of the concurrence. Monogamy, which is closely related to the no-cloning property, is purely a feature of quantum correlations, and has no classical analogue. Supposing that two classical random variables X and Y are correlated, we can copy, or \"clone\", X to create arbitrarily many random variables that"
  },
  {
    "source": "Monogamy of entanglement.txt",
    "chunk_id": "Monogamy of entanglement.txt_1",
    "chunk": "all share precisely the same correlation with Y. If we let X and Y be entangled quantum states instead, then X cannot be cloned, and this sort of \"polygamous\" outcome is impossible. The monogamy of entanglement has broad implications for applications of quantum mechanics ranging from black hole physics to quantum cryptography, where it plays a pivotal role in the security of quantum key distribution. The monogamy of bipartite entanglement was established for tripartite systems in terms of concurrence by Valerie Coffman, Joydip Kundu, and William Wootters in 2000. In 2006, Tobias Osborne and Frank Verstraete extended this result to the multipartite case, proving the CKW inequality. For the sake of illustration, consider the three-qubit state | ψ ⟩ ∈ ( C 2 ) ⊗ 3 {\\displaystyle |\\psi \\rangle \\in (\\mathbb {C} ^{2})^{\\otimes 3}} consisting of qubits A, B, and C. Suppose that A and B form a (maximally entangled) EPR pair. We will show that: for some valid quantum state | ϕ ⟩ C {\\displaystyle |\\phi \\rangle _{C}} . By the definition of entanglement, this implies that C must be completely disentangled from A and B. When measured in the standard basis, A and B collapse to the states"
  },
  {
    "source": "Monogamy of entanglement.txt",
    "chunk_id": "Monogamy of entanglement.txt_2",
    "chunk": "| 00 ⟩ {\\displaystyle |00\\rangle } and | 11 ⟩ {\\displaystyle |11\\rangle } with probability 1 2 {\\displaystyle {\\frac {1}{2}}} each. It follows that: for some α 0 , α 1 , β 0 , β 1 ∈ C {\\displaystyle \\alpha _{0},\\alpha _{1},\\beta _{0},\\beta _{1}\\in \\mathbb {C} } such that | α 0 | 2 + | α 1 | 2 = | β 0 | 2 + | β 1 | 2 = 1 2 {\\displaystyle |\\alpha _{0}|^{2}+|\\alpha _{1}|^{2}=|\\beta _{0}|^{2}+|\\beta _{1}|^{2}={\\frac {1}{2}}} . We can rewrite the states of A and B in terms of diagonal basis vectors | + ⟩ {\\displaystyle |+\\rangle } and | − ⟩ {\\displaystyle |-\\rangle } : Being maximally entangled, A and B collapse to one of the two states | + + ⟩ {\\displaystyle |++\\rangle } or | − − ⟩ {\\displaystyle |--\\rangle } when measured in the diagonal basis. The probability of observing outcomes | + − ⟩ {\\displaystyle |+-\\rangle } or | − + ⟩ {\\displaystyle |-+\\rangle } is zero. Therefore, according to the equation above, it must be the case that α 0 − β 0 = 0 {\\displaystyle \\alpha _{0}-\\beta _{0}=0} and α 1 − β 1 = 0"
  },
  {
    "source": "Monogamy of entanglement.txt",
    "chunk_id": "Monogamy of entanglement.txt_3",
    "chunk": "{\\displaystyle \\alpha _{1}-\\beta _{1}=0} . It follows immediately that α 0 = β 0 {\\displaystyle \\alpha _{0}=\\beta _{0}} and α 1 = β 1 {\\displaystyle \\alpha _{1}=\\beta _{1}} . We can rewrite our expression for | ψ ⟩ {\\displaystyle |\\psi \\rangle } accordingly: This shows that the original state can be written as a product of a pure state in AB and a pure state in C, which means that the EPR state in qubits A and B is not entangled with the qubit C."
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_0",
    "chunk": "# Moon The Moon is Earth's only natural satellite. It orbits Earth at an average distance of 384399 km (238,854 mi; about 30 times Earth's diameter). The Moon's orbital period (lunar month) and rotation period (lunar day) are synchronized by Earth's gravitational pull at 29.5 Earth days, making the same side of the Moon always face Earth. The Moon's pull on Earth is the main driver of Earth's tides. In geophysical terms, the Moon is a planetary-mass object or satellite planet. Its mass is 1.2% that of the Earth, and its diameter is 3,474 km (2,159 mi), roughly one-quarter of Earth's (about as wide as the contiguous United States). Within the Solar System, it is the largest and most massive satellite in relation to its parent planet, the fifth-largest and fifth-most massive moon overall, and larger and more massive than all known dwarf planets. Its surface gravity is about one-sixth of Earth's, about half that of Mars, and the second-highest among all moons in the Solar System, after Jupiter's moon Io. The body of the Moon is differentiated and terrestrial, with no significant hydrosphere, atmosphere, or magnetic field. The lunar surface is covered in lunar dust and marked by mountains,"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_1",
    "chunk": "impact craters, their ejecta, ray-like streaks, rilles and, mostly on the near side of the Moon, by dark maria ('seas'), which are plains of cooled lava. These maria were formed when molten lava flowed into ancient impact basins. The Moon formed 4.51 billion years ago, not long after Earth's formation, out of the debris from a giant impact between Earth and a hypothesized Mars-sized body called Theia. The Moon is, except when passing through Earth's shadow during a lunar eclipse, always illuminated by the Sun, but from Earth the visible illumination shifts during its orbit, producing the lunar phases. The Moon is the brightest celestial object in Earth's night sky. This is mainly due to its large angular diameter, while the reflectance of the lunar surface is comparable to that of asphalt. The apparent size is nearly the same as that of the Sun, allowing it to cover the Sun completely during a total solar eclipse. From Earth about 59% of the lunar surface is visible due to cyclical shifts in perspective (libration), making parts of the far side of the Moon visible. The Moon has been an important source of inspiration and knowledge for humans, having been crucial to"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_2",
    "chunk": "cosmography, mythology, religion, art, time keeping, natural science, and spaceflight. The first human-made objects to fly to an extraterrestrial body were sent to the Moon, starting in 1959 with the flyby of the Soviet Union's Luna 1 and the intentional impact of Luna 2. In 1966, the first soft landing (by Luna 9) and orbital insertion (by Luna 10) followed. On July 20, 1969, humans for the first time stepped on an extraterrestrial body, landing on the Moon at Mare Tranquillitatis with the lander Eagle of the United States' Apollo 11 mission. Five more crews were sent between then and 1972, each with two men landing on the surface. The longest stay was 75 hours by the Apollo 17 crew. Since then, exploration of the Moon has continued robotically, and crewed missions are being planned to return beginning in the late 2020s. The English proper name for Earth's natural satellite is typically written as Moon, with a capital M. The noun moon is derived from Old English mōna, which stems from Proto-Germanic *mēnōn, which in turn comes from Proto-Indo-European *mēnsis 'month' (from earlier *mēnōt, genitive *mēneses) which may be related to the verb 'measure' (of time). Occasionally, the name Luna"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_3",
    "chunk": "/ˈluːnə/ is used in scientific writing and especially in science fiction to distinguish the Earth's moon from others, while in poetry \"Luna\" has been used to denote personification of the Moon. Cynthia /ˈsɪnθiə/ is a rare poetic name for the Moon personified as a goddess, while Selene /səˈliːniː/ (literally 'Moon') is the Greek goddess of the Moon. The English adjective pertaining to the Moon is lunar, derived from the Latin word for the Moon, lūna. Selenian /səliːniən/ is an adjective used to describe the Moon as a world, rather than as a celestial object, but its use is rare. It is derived from σελήνη selēnē, the Greek word for the Moon, and its cognate selenic was originally a rare synonym but now nearly always refers to the chemical element selenium. The element name selenium and the prefix seleno- (as in selenography, the study of the physical features of the Moon) come from this Greek word. Artemis, the Greek goddess of the wilderness and the hunt, also came to be identified with Selene, and was sometimes called Cynthia after her birthplace on Mount Cynthus. Her Roman equivalent is Diana. The names Luna, Cynthia, and Selene are reflected in technical terms for"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_4",
    "chunk": "lunar orbits such as apolune, pericynthion and selenocentric. The astronomical symbols for the Moon are the crescent and decrescent , for example in M☾ 'lunar mass'. The lunar geological periods are named after their characteristic features, from most impact craters outside the dark mare, to the mare and later craters, and finally the young, still bright and therefore readily visible craters with ray systems like Copernicus or Tycho. Isotope dating of lunar samples suggests the Moon formed around 50 million years after the origin of the Solar System. Historically, several formation mechanisms have been proposed, but none satisfactorily explains the features of the Earth–Moon system. A fission of the Moon from Earth's crust through centrifugal force would require too great an initial rotation rate of Earth. Gravitational capture of a pre-formed Moon depends on an unfeasibly extended atmosphere of Earth to dissipate the energy of the passing Moon. A co-formation of Earth and the Moon together in the primordial accretion disk does not explain the depletion of metals in the Moon. None of these hypotheses can account for the high angular momentum of the Earth–Moon system. The prevailing theory is that the Earth–Moon system formed after a giant impact of"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_5",
    "chunk": "a Mars-sized body (named Theia) with the proto-Earth. The oblique impact blasted material into orbit about the Earth and the material accreted and formed the Moon just beyond the Earth's Roche limit of ~2.56 R🜨. Giant impacts are thought to have been common in the early Solar System. Computer simulations of giant impacts have produced results that are consistent with the mass of the lunar core and the angular momentum of the Earth–Moon system. These simulations show that most of the Moon derived from the impactor, rather than the proto-Earth. However, models from 2007 and later suggest a larger fraction of the Moon derived from the proto-Earth. Other bodies of the inner Solar System such as Mars and Vesta have, according to meteorites from them, very different oxygen and tungsten isotopic compositions compared to Earth. However, Earth and the Moon have nearly identical isotopic compositions. The isotopic equalization of the Earth–Moon system might be explained by the post-impact mixing of the vaporized material that formed the two, although this is debated. The impact would have released enough energy to liquefy both the ejecta and the Earth's crust, forming a magma ocean. The liquefied ejecta could have then re-accreted into the"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_6",
    "chunk": "Earth–Moon system. The newly formed Moon would have had its own magma ocean; its depth is estimated from about 500 km (300 miles) to 1,737 km (1,079 miles). While the giant-impact theory explains many lines of evidence, some questions are still unresolved, most of which involve the Moon's composition. Models that have the Moon acquiring a significant amount of the proto-earth are more difficult to reconcile with geochemical data for the isotopes of zirconium, oxygen, silicon, and other elements. A study published in 2022, using high-resolution simulations (up to 10 particles), found that giant impacts can immediately place a satellite with similar mass and iron content to the Moon into orbit far outside Earth's Roche limit. Even satellites that initially pass within the Roche limit can reliably and predictably survive, by being partially stripped and then torqued onto wider, stable orbits. On November 1, 2023, scientists reported that, according to computer simulations, remnants of Theia could still be present inside the Earth. The newly formed Moon settled into a much closer Earth orbit than it has today. Each body therefore appeared much larger in the sky of the other, eclipses were more frequent, and tidal effects were stronger. Due to"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_7",
    "chunk": "tidal acceleration, the Moon's orbit around Earth has become significantly larger, with a longer period. Following formation, the Moon has cooled and most of its atmosphere has been stripped. The lunar surface has since been shaped by large impact events and many small ones, forming a landscape featuring craters of all ages. The Moon was volcanically active until 1.2 billion years ago, which laid down the prominent lunar maria. Most of the mare basalts erupted during the Imbrian period, 3.3–3.7 billion years ago, though some are as young as 1.2 billion years and some as old as 4.2 billion years. There are differing explanations for the eruption of mare basalts, particularly their uneven occurrence which mainly appear on the near-side. Causes of the distribution of the lunar highlands on the far side are also not well understood. Topological measurements show the near side crust is thinner than the far side. One possible scenario then is that large impacts on the near side may have made it easier for lava to flow onto the surface. The Moon is a very slightly scalene ellipsoid due to tidal stretching, with its long axis displaced 30° from facing the Earth, due to gravitational anomalies"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_8",
    "chunk": "from impact basins. Its shape is more elongated than current tidal forces can account for. This 'fossil bulge' indicates that the Moon solidified when it orbited at half its current distance to the Earth, and that it is now too cold for its shape to restore hydrostatic equilibrium at its current orbital distance. The Moon is by size and mass the fifth largest natural satellite of the Solar System, categorizable as one of its planetary-mass moons, making it a satellite planet under the geophysical definitions of the term. It is smaller than Mercury and considerably larger than the largest dwarf planet of the Solar System, Pluto. The Moon is the largest natural satellite in the Solar System relative to its primary planet. The Moon's diameter is about 3,500 km, more than one-quarter of Earth's, with the face of the Moon comparable to the width of either mainland Australia, Europe or the contiguous United States. The whole surface area of the Moon is about 38 million square kilometers, comparable to that of the Americas. The Moon's mass is 1⁄81 of Earth's, being the second densest among the planetary moons, and having the second highest surface gravity, after Io, at 0.1654 g"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_9",
    "chunk": "and an escape velocity of 2.38 km/s (8600 km/h; 5300 mph). The Moon is a differentiated body that was initially in hydrostatic equilibrium but has since departed from this condition. It has a geochemically distinct crust, mantle, and core. The Moon has a solid iron-rich inner core with a radius possibly as small as 240 kilometres (150 mi) and a fluid outer core primarily made of liquid iron with a radius of roughly 300 kilometres (190 mi). Around the core is a partially molten boundary layer with a radius of about 500 kilometres (310 mi). This structure is thought to have developed through the fractional crystallization of a global magma ocean shortly after the Moon's formation 4.5 billion years ago. Crystallization of this magma ocean would have created a mafic mantle from the precipitation and sinking of the minerals olivine, clinopyroxene, and orthopyroxene; after about three-quarters of the magma ocean had crystallized, lower-density plagioclase minerals could form and float into a crust atop. The final liquids to crystallize would have been initially sandwiched between the crust and mantle, with a high abundance of incompatible and heat-producing elements. Consistent with this perspective, geochemical mapping made from orbit suggests a crust of"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_10",
    "chunk": "mostly anorthosite. The Moon rock samples of the flood lavas that erupted onto the surface from partial melting in the mantle confirm the mafic mantle composition, which is more iron-rich than that of Earth. The crust is on average about 50 kilometres (31 mi) thick. The Moon is the second-densest satellite in the Solar System, after Io. However, the inner core of the Moon is small, with a radius of about 350 kilometres (220 mi) or less, around 20% of the radius of the Moon. Its composition is not well understood but is probably metallic iron alloyed with a small amount of sulfur and nickel analyses of the Moon's time-variable rotation suggest that it is at least partly molten. The pressure at the lunar core is estimated to be 5 GPa (49,000 atm). On average the Moon's surface gravity is 1.62 m/s (0.1654 g; 5.318 ft/s), about half of the surface gravity of Mars and about a sixth of Earth's. The Moon's gravitational field is not uniform. The details of the gravitational field have been measured through tracking the Doppler shift of radio signals emitted by orbiting spacecraft. The main lunar gravity features are mascons, large positive gravitational anomalies associated"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_11",
    "chunk": "with some of the giant impact basins, partly caused by the dense mare basaltic lava flows that fill those basins. The anomalies greatly influence the orbit of spacecraft about the Moon. There are some puzzles: lava flows by themselves cannot explain all of the gravitational signature, and some mascons exist that are not linked to mare volcanism. The Moon has an external magnetic field of less than 0.2 nanoteslas, or less than one hundred thousandth that of Earth. The Moon does not have a global dipolar magnetic field and only has crustal magnetization likely acquired early in its history when a dynamo was still operating. Early in its history, 4 billion years ago, its magnetic field strength was likely close to that of Earth today. This early dynamo field apparently expired by about one billion years ago, after the lunar core had crystallized. Theoretically, some of the remnant magnetization may originate from transient magnetic fields generated during large impacts through the expansion of plasma clouds. These clouds are generated during large impacts in an ambient magnetic field. This is supported by the location of the largest crustal magnetizations situated near the antipodes of the giant impact basins. The Moon has"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_12",
    "chunk": "an atmosphere consisting of only an exosphere, which is so tenuous as to be nearly vacuum, with a total mass of less than 10 tonnes (9.8 long tons; 11 short tons). The surface pressure of this small mass is around 3 × 10 atm (0.3 nPa); it varies with the lunar day. Its sources include outgassing and sputtering, a product of the bombardment of lunar soil by solar wind ions. Elements that have been detected include sodium and potassium, produced by sputtering (also found in the atmospheres of Mercury and Io); helium-4 and neon from the solar wind; and argon-40, radon-222, and polonium-210, outgassed after their creation by radioactive decay within the crust and mantle. The absence of such neutral species (atoms or molecules) as oxygen, nitrogen, carbon, hydrogen and magnesium, which are present in the regolith, is not understood. Water vapor has been detected by Chandrayaan-1 and found to vary with latitude, with a maximum at ~60–70 degrees; it is possibly generated from the sublimation of water ice in the regolith. These gases either return into the regolith because of the Moon's gravity or are lost to space, either through solar radiation pressure or, if they are ionized, by"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_13",
    "chunk": "being swept away by the solar wind's magnetic field. Studies of Moon magma samples retrieved by the Apollo missions demonstrate that the Moon had once possessed a relatively thick atmosphere for a period of 70 million years between 3 and 4 billion years ago. This atmosphere, sourced from gases ejected from lunar volcanic eruptions, was twice the thickness of that of present-day Mars. The ancient lunar atmosphere was eventually stripped away by solar winds and dissipated into space. A permanent Moon dust cloud exists around the Moon, generated by small particles from comets. Estimates are 5 tons of comet particles strike the Moon's surface every 24 hours, resulting in the ejection of dust particles. The dust stays above the Moon approximately 10 minutes, taking 5 minutes to rise, and 5 minutes to fall. On average, 120 kilograms of dust are present above the Moon, rising up to 100 kilometers above the surface. Dust counts made by LADEE's Lunar Dust EXperiment (LDEX) found particle counts peaked during the Geminid, Quadrantid, Northern Taurid, and Omicron Centaurid meteor showers, when the Earth, and Moon pass through comet debris. The lunar dust cloud is asymmetric, being denser near the boundary between the Moon's dayside"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_14",
    "chunk": "and nightside. Ionizing radiation from cosmic rays, their resulting neutron radiation, and the Sun results in an average radiation level of 1.369 millisieverts per day during lunar daytime, which is about 2.6 times more than the level on the International Space Station, 5–10 times more than the level during a trans-Atlantic flight, and 200 times more than the level on Earth's surface. For further comparison, radiation levels average about 1.84 millisieverts per day on a flight to Mars and about 0.64 millisieverts per day on Mars itself, with some locations on Mars possibly having levels as low as 0.342 millisieverts per day. Solar radiation also electrically charges the highly abrasive lunar dust and makes it levitate. This effect contributes to the easy spread of the sticky, lung- and gear-damaging lunar dust. The Moon's axial tilt with respect to the ecliptic is only 1.5427°, much less than the 23.44° of Earth. This small axial tilt means that the Moon's solar illumination varies much less with season than Earth's, and it also allows for the existence of some peaks of eternal light at the Moon's north pole, at the rim of the crater Peary. The lunar surface is exposed to drastic temperature"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_15",
    "chunk": "differences ranging from 120 °C to −171 °C depending on the solar irradiance. Because of the lack of atmosphere, temperatures of different areas vary particularly upon whether they are in sunlight or shadow, making topographical details play a decisive role on local surface temperatures. Parts of many craters, particularly the bottoms of many polar craters, are permanently shadowed. These craters of eternal darkness have extremely low temperatures. The Lunar Reconnaissance Orbiter measured the lowest summer temperatures in craters at the southern pole at 35 K (−238 °C; −397 °F) and just 26 K (−247 °C; −413 °F) close to the winter solstice in the north polar crater Hermite. This is the coldest temperature in the Solar System ever measured by a spacecraft, colder even than the surface of Pluto. Blanketed on top of the Moon's crust is a highly comminuted (broken into ever smaller particles) and impact gardened mostly gray surface layer called regolith, formed by impact processes. The finer regolith, the lunar soil of silicon dioxide glass, has a texture resembling snow and a scent resembling spent gunpowder. The regolith of older surfaces is generally thicker than for younger surfaces: it varies in thickness from 10–15 m (33–49 ft)"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_16",
    "chunk": "in the highlands and 4–5 m (13–16 ft) in the maria. Beneath the finely comminuted regolith layer is the megaregolith, a layer of highly fractured bedrock many kilometers thick. These extreme conditions are considered to make it unlikely for spacecraft to harbor bacterial spores at the Moon for longer than just one lunar orbit. The topography of the Moon has been measured with laser altimetry and stereo image analysis. Its most extensive topographic feature is the giant far-side South Pole–Aitken basin, some 2,240 km (1,390 mi) in diameter, the largest crater on the Moon and the second-largest confirmed impact crater in the Solar System. At 13 km (8.1 mi) deep, its floor is the lowest point on the surface of the Moon, reaching −9.178 kilometres (−5.703 mi) at 70°22′05″S 172°24′47″W﻿ / ﻿70.368°S 172.413°W﻿ / -70.368; -172.413 in a crater within Antoniadi crater. The highest elevations of the Moon's surface, with the so-called Selenean summit at 10.629 kilometres (6.605 mi) , are located directly to the northeast (5°26′28″N 158°39′22″W﻿ / ﻿5.441°N 158.656°W﻿ / 5.441; -158.656), which might have been thickened by the oblique formation impact of the South Pole–Aitken basin. Other large impact basins such as Imbrium, Serenitatis, Crisium, Smythii, and"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_17",
    "chunk": "Orientale possess regionally low elevations and elevated rims. The far side of the lunar surface is on average about 1.9 km (1.2 mi) higher than that of the near side. The discovery of fault scarp cliffs suggest that the Moon has shrunk by about 90 metres (300 ft) within the past billion years. Similar shrinkage features exist on Mercury. Mare Frigoris, a basin near the north pole long assumed to be geologically dead, has cracked and shifted. Since the Moon does not have tectonic plates, its tectonic activity is slow, and cracks develop as it loses heat. Scientists have confirmed the presence of a cave on the Moon near the Sea of Tranquillity, not far from the 1969 Apollo 11 landing site. The cave, identified as an entry point to a collapsed lava tube, is roughly 45 meters wide and up to 80 m long. This discovery marks the first confirmed entry point to a lunar cave. The analysis was based on photos taken in 2010 by NASA's Lunar Reconnaissance Orbiter. The cave's stable temperature of around 17 °C could provide a hospitable environment for future astronauts, protecting them from extreme temperatures, solar radiation, and micrometeorites. However, challenges include accessibility"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_18",
    "chunk": "and risks of avalanches and cave-ins. This discovery offers potential for future lunar bases or emergency shelters. The main features visible from Earth by the naked eye are dark and relatively featureless lunar plains called maria (singular mare; Latin for \"seas\", as they were once believed to be filled with water) are vast solidified pools of ancient basaltic lava. Although similar to terrestrial basalts, lunar basalts have more iron and no minerals altered by water. The majority of these lava deposits erupted or flowed into the depressions associated with impact basins, though the Moon's largest expanse of basalt flooding, Oceanus Procellarum, does not correspond to an obvious impact basin. Different episodes of lava flow in maria can often be recognized by variations in surface albedo and distinct flow margins. As the maria formed, cooling and contraction of the basaltic lava created wrinkle ridges in some areas. These low, sinuous ridges can extend for hundreds of kilometers and often outline buried structures within the mare. Another result of maria formation is the creation of concentric depressions along the edges, known as arcuate rilles. These features occur as the mare basalts sink inward under their own weight, causing the edges to fracture"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_19",
    "chunk": "and separate. In addition to the visible maria, the Moon has mare deposits covered by ejecta from impacts. Called cryptomares, these hidden mares are likely older than the exposed ones. Conversely, mare lava has obscured many impact melt sheets and pools. Impact melts are formed when intense shock pressures from collisions vaporize and melt zones around the impact site. Where still exposed, impact melt can be distinguished from mare lava by its distribution, albedo, and texture. Sinuous rilles, found in and around maria, are likely extinct lava channels or collapsed lava tubes. They typically originate from volcanic vents, meandering and sometimes branching as they progress. The largest examples, such as Schroter's Valley and Rima Hadley, are significantly longer, wider, and deeper than terrestrial lava channels, sometimes featuring bends and sharp turns that again, are uncommon on Earth. Mare volcanism has altered impact craters in various ways, including filling them to varying degrees, and raising and fracturing their floors from uplift of mare material beneath their interiors. Examples of such craters include Taruntius and Gassendi. Some craters, such as Hyginus, are of wholly volcanic origin, forming as calderas or collapse pits. Such craters are relatively rare and tend to be smaller"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_20",
    "chunk": "(typically a few kilometers wide), shallower, and more irregularly shaped than impact craters. They also lack the upturned rims characteristic of impact craters. Several geologic provinces containing shield volcanoes and volcanic domes are found within the near side maria. There are also some regions of pyroclastic deposits, scoria cones and non-basaltic domes made of particularly high viscosity lava. Almost all maria are on the near side of the Moon, and cover 31% of the surface of the near side compared with 2% of the far side. This is likely due to a concentration of heat-producing elements under the crust on the near side, which would have caused the underlying mantle to heat up, partially melt, rise to the surface and erupt. Most of the Moon's mare basalts erupted during the Imbrian period, 3.3–3.7 billion years ago, though some being as young as 1.2 billion years and as old as 4.2 billion years. In 2006, a study of Ina, a tiny depression in Lacus Felicitatis, found jagged, relatively dust-free features that, because of the lack of erosion by infalling debris, appeared to be only 2 million years old. Moonquakes and releases of gas indicate continued lunar activity. Evidence of recent lunar"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_21",
    "chunk": "volcanism has been identified at 70 irregular mare patches, some less than 50 million years old. This raises the possibility of a much warmer lunar mantle than previously believed, at least on the near side where the deep crust is substantially warmer because of the greater concentration of radioactive elements. Evidence has been found for 2–10 million years old basaltic volcanism within the crater Lowell, inside the Orientale basin. Some combination of an initially hotter mantle and local enrichment of heat-producing elements in the mantle could be responsible for prolonged activities on the far side in the Orientale basin. The lighter-colored regions of the Moon are called terrae, or more commonly highlands, because they are higher than most maria. They have been radiometrically dated to having formed 4.4 billion years ago and may represent plagioclase cumulates of the lunar magma ocean. In contrast to Earth, no major lunar mountains are believed to have formed as a result of tectonic events. The concentration of maria on the near side likely reflects the substantially thicker crust of the highlands of the Far Side, which may have formed in a slow-velocity impact of a second moon of Earth a few tens of millions"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_22",
    "chunk": "of years after the Moon's formation. Alternatively, it may be a consequence of asymmetrical tidal heating when the Moon was much closer to the Earth. A major geologic process that has affected the Moon's surface is impact cratering, with craters formed when asteroids and comets collide with the lunar surface. There are estimated to be roughly 300,000 craters wider than 1 km (0.6 mi) on the Moon's near side. Lunar craters exhibit a variety of forms, depending on their size. In order of increasing diameter, the basic types are simple craters with smooth bowl shaped interiors and upturned rims, complex craters with flat floors, terraced walls and central peaks, peak ring basins, and multi-ring basins with two or more concentric rings of peaks. The vast majority of impact craters are circular, but some, like Cantor and Janssen, have more polygonal outlines, possibly guided by underlying faults and joints. Others, such as the Messier pair, Schiller, and Daniell, are elongated. Such elongation can result from highly oblique impacts, binary asteroid impacts, fragmentation of impactors before surface strike, or closely spaced secondary impacts. The lunar geologic timescale is based on the most prominent impact events, such as multi-ring formations like Nectaris, Imbrium,"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_23",
    "chunk": "and Orientale that are between hundreds and thousands of kilometers in diameter and associated with a broad apron of ejecta deposits that form a regional stratigraphic horizon. The lack of an atmosphere, weather, and recent geological processes mean that many of these craters are well-preserved. Although only a few multi-ring basins have been definitively dated, they are useful for assigning relative ages. Because impact craters accumulate at a nearly constant rate, counting the number of craters per unit area can be used to estimate the age of the surface. However care needs to be exercised with the crater counting technique due to the potential presence of secondary craters. Ejecta from impacts can create secondary craters that often appear in clusters or chains but can also occur as isolated formations at a considerable distance from the impact. These can resemble primary craters, and may even dominate small crater populations, so their unidentified presence can distort age estimates. The radiometric ages of impact-melted rocks collected during the Apollo missions cluster between 3.8 and 4.1 billion years old: this has been used to propose a Late Heavy Bombardment period of increased impacts. High-resolution images from the Lunar Reconnaissance Orbiter in the 2010s show"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_24",
    "chunk": "a contemporary crater-production rate significantly higher than was previously estimated. A secondary cratering process caused by distal ejecta is thought to churn the top two centimeters of regolith on a timescale of 81,000 years. This rate is 100 times faster than the rate computed from models based solely on direct micrometeorite impacts. Lunar swirls are enigmatic features found across the Moon's surface. They are characterized by a high albedo, appear optically immature (i.e. the optical characteristics of a relatively young regolith), and often have a sinuous shape. Their shape is often accentuated by low albedo regions that wind between the bright swirls. They are located in places with enhanced surface magnetic fields and many are located at the antipodal point of major impacts. Well known swirls include the Reiner Gamma feature and Mare Ingenii. They are hypothesized to be areas that have been partially shielded from the solar wind, resulting in slower space weathering. Liquid water cannot persist on the lunar surface. When exposed to solar radiation, water quickly decomposes through a process known as photodissociation and is lost to space. However, since the 1960s, scientists have hypothesized that water ice may be deposited by impacting comets or possibly produced"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_25",
    "chunk": "by the reaction of oxygen-rich lunar rocks, and hydrogen from solar wind, leaving traces of water which could possibly persist in cold, permanently shadowed craters at either pole on the Moon. Computer simulations suggest that up to 14,000 km (5,400 sq mi) of the surface may be in permanent shadow. The presence of usable quantities of water on the Moon is an important factor in rendering lunar habitation as a cost-effective plan; the alternative of transporting water from Earth would be prohibitively expensive. In years since, signatures of water have been found to exist on the lunar surface. In 1994, the bistatic radar experiment located on the Clementine spacecraft, indicated the existence of small, frozen pockets of water close to the surface. However, later radar observations by Arecibo, suggest these findings may rather be rocks ejected from young impact craters. In 1998, the neutron spectrometer on the Lunar Prospector spacecraft showed that high concentrations of hydrogen are present in the first meter of depth in the regolith near the polar regions. Volcanic lava beads, brought back to Earth aboard Apollo 15, showed small amounts of water in their interior. The 2008 Chandrayaan-1 spacecraft has since confirmed the existence of surface"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_26",
    "chunk": "water ice, using the on-board Moon Mineralogy Mapper. The spectrometer observed absorption lines common to hydroxyl, in reflected sunlight, providing evidence of large quantities of water ice, on the lunar surface. The spacecraft showed that concentrations may possibly be as high as 1,000 ppm. Using the mapper's reflectance spectra, indirect lighting of areas in shadow confirmed water ice within 20° latitude of both poles in 2018. In 2009, LCROSS sent a 2,300 kg (5,100 lb) impactor into a permanently shadowed polar crater, and detected at least 100 kg (220 lb) of water in a plume of ejected material. Another examination of the LCROSS data showed the amount of detected water to be closer to 155 ± 12 kg (342 ± 26 lb). In May 2011, 615–1410 ppm water in melt inclusions in lunar sample 74220 was reported, the famous high-titanium \"orange glass soil\" of volcanic origin collected during the Apollo 17 mission in 1972. The inclusions were formed during explosive eruptions on the Moon approximately 3.7 billion years ago. This concentration is comparable with that of magma in Earth's upper mantle. Although of considerable selenological interest, this insight does not mean that water is easily available since the sample originated"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_27",
    "chunk": "many kilometers below the surface, and the inclusions are so difficult to access that it took 39 years to find them with a state-of-the-art ion microprobe instrument. Analysis of the findings of the Moon Mineralogy Mapper (M3) revealed in August 2018 for the first time \"definitive evidence\" for water-ice on the lunar surface. The data revealed the distinct reflective signatures of water-ice, as opposed to dust and other reflective substances. The ice deposits were found on the North and South poles, although it is more abundant in the South, where water is trapped in permanently shadowed craters and crevices, allowing it to persist as ice on the surface since they are shielded from the sun. In October 2020, astronomers reported detecting molecular water on the sunlit surface of the Moon by several independent spacecraft, including the Stratospheric Observatory for Infrared Astronomy (SOFIA). The Earth and the Moon form the Earth–Moon satellite system with a shared center of mass, or barycenter. This barycenter is 1,700 km (1,100 mi) (about a quarter of Earth's radius) beneath the Earth's surface. The Moon's orbit is slightly elliptical, with an orbital eccentricity of 0.055. The semi-major axis of the geocentric lunar orbit, called the lunar"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_28",
    "chunk": "distance, is approximately 400,000 km (250,000 miles or 1.28 light-seconds), comparable to going around Earth 9.5 times. The Moon makes a complete orbit around Earth with respect to the fixed stars, its sidereal period, about once every 27.3 days. However, because the Earth–Moon system moves at the same time in its orbit around the Sun, it takes slightly longer, 29.5 days, to return to the same lunar phase, completing a full cycle, as seen from Earth. This synodic period or synodic month is commonly known as the lunar month and is equal to the length of the solar day on the Moon. Due to tidal locking, the Moon has a 1:1 spin–orbit resonance. This rotation–orbit ratio makes the Moon's orbital periods around Earth equal to its corresponding rotation periods. This is the reason for only one side of the Moon, its so-called near side, being visible from Earth. That said, while the movement of the Moon is in resonance, it still is not without nuances such as libration, resulting in slightly changing perspectives, making over time and location on Earth about 59% of the Moon's surface visible from Earth. Unlike most satellites of other planets, the Moon's orbital plane is"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_29",
    "chunk": "closer to the ecliptic plane than to the planet's equatorial plane. The Moon's orbit is subtly perturbed by the Sun and Earth in many small, complex and interacting ways. For example, the plane of the Moon's orbit gradually rotates once every 18.61 years, which affects other aspects of lunar motion. These follow-on effects are mathematically described by Cassini's laws. The gravitational attraction that Earth and the Moon (as well as the Sun) exert on each other manifests in a slightly greater attraction on the sides closest to each other, resulting in tidal forces. Ocean tides are the most widely experienced result of this, but tidal forces also considerably affect other mechanics of Earth, as well as the Moon and their system. The lunar solid crust experiences tides of around 10 cm (4 in) amplitude over 27 days, with three components: a fixed one due to Earth, because they are in synchronous rotation, a variable tide due to orbital eccentricity and inclination, and a small varying component from the Sun. The Earth-induced variable component arises from changing distance and libration, a result of the Moon's orbital eccentricity and inclination (if the Moon's orbit were perfectly circular and un-inclined, there would only"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_30",
    "chunk": "be solar tides). According to recent research, scientists suggest that the Moon's influence on the Earth may contribute to maintaining Earth's magnetic field. The cumulative effects of stress built up by these tidal forces produces moonquakes. Moonquakes are much less common and weaker than are earthquakes, although moonquakes can last for up to an hour – significantly longer than terrestrial quakes – because of scattering of the seismic vibrations in the dry fragmented upper crust. The existence of moonquakes was an unexpected discovery from seismometers placed on the Moon by Apollo astronauts from 1969 through 1972. The most commonly known effect of tidal forces is elevated sea levels called ocean tides. While the Moon exerts most of the tidal forces, the Sun also exerts tidal forces and therefore contributes to the tides as much as 40% of the Moon's tidal force; producing in interplay the spring and neap tides. The tides are two bulges in the Earth's oceans, one on the side facing the Moon and the other on the side opposite. As the Earth rotates on its axis, one of the ocean bulges (high tide) is held in place \"under\" the Moon, while another such tide is opposite. The"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_31",
    "chunk": "tide under the Moon is explained by the Moon's gravity being stronger on the water close to it. The tide on the opposite side can be explained either by the centrifugal force as the Earth orbits the barycenter or by the water's inertia as the Moon's gravity is stronger on the solid Earth close to it and it is pull away from the farther water. Thus, there are two high tides, and two low tides in about 24 hours. Since the Moon is orbiting the Earth in the same direction of the Earth's rotation, the high tides occur about every 12 hours and 25 minutes; the 25 minutes is due to the Moon's time to orbit the Earth. If the Earth were a water world (one with no continents) it would produce a tide of only one meter, and that tide would be very predictable, but the ocean tides are greatly modified by other effects: As a result, the timing of the tides at most points on the Earth is a product of observations that are explained, incidentally, by theory. Delays in the tidal peaks of both ocean and solid-body tides cause torque in opposition to the Earth's rotation. This"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_32",
    "chunk": "\"drains\" angular momentum and rotational kinetic energy from Earth's rotation, slowing the Earth's rotation. That angular momentum, lost from the Earth, is transferred to the Moon in a process known as tidal acceleration, which lifts the Moon into a higher orbit while lowering orbital speed around the Earth. Thus the distance between Earth and Moon is increasing, and the Earth's rotation is slowing in reaction. Measurements from laser reflectors left during the Apollo missions (lunar ranging experiments) have found that the Moon's distance increases by 38 mm (1.5 in) per year (roughly the rate at which human fingernails grow). Atomic clocks show that Earth's Day lengthens by about 17 microseconds every year, slowly increasing the rate at which UTC is adjusted by leap seconds. This tidal drag makes the rotation of the Earth, and the orbital period of the Moon very slowly match. This matching first results in tidally locking the lighter body of the orbital system, as is already the case with the Moon. Theoretically, in 50 billion years, the Earth's rotation will have slowed to the point of matching the Moon's orbital period, causing the Earth to always present the same side to the Moon. However, the Sun"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_33",
    "chunk": "will become a red giant, most likely engulfing the Earth–Moon system long before then. If the Earth–Moon system isn't engulfed by the enlarged Sun, the drag from the solar atmosphere can cause the orbit of the Moon to decay. Once the orbit of the Moon closes to a distance of 18,470 km (11,480 mi), it will cross Earth's Roche limit, meaning that tidal interaction with Earth would break apart the Moon, turning it into a ring system. Most of the orbiting rings will begin to decay, and the debris will impact Earth. Hence, even if the Sun does not swallow up Earth, the planet may be left moonless. The Moon's highest altitude at culmination varies by its lunar phase, or more correctly its orbital position, and time of the year, or more correctly the position of the Earth's axis. The full moon is highest in the sky during winter and lowest during summer (for each hemisphere respectively), with its altitude changing towards dark moon to the opposite. At the North and South Poles the Moon is 24 hours above the horizon for two weeks every tropical month (about 27.3 days), comparable to the polar day of the tropical year. Zooplankton"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_34",
    "chunk": "in the Arctic use moonlight when the Sun is below the horizon for months on end. The apparent orientation of the Moon depends on its position in the sky and the hemisphere of the Earth from which it is being viewed. In the northern hemisphere it appears upside down compared to the view from the southern hemisphere. Sometimes the \"horns\" of a crescent moon appear to be pointing more upwards than sideways. This phenomenon is called a wet moon and occurs more frequently in the tropics. The distance between the Moon and Earth varies from around 356,400 km (221,500 mi) (perigee) to 406,700 km (252,700 mi) (apogee), making the Moon's distance and apparent size fluctuate up to 14%. On average the Moon's angular diameter is about 0.52°, roughly the same apparent size as the Sun (see § Eclipses). In addition, a purely psychological effect, known as the Moon illusion, makes the Moon appear larger when close to the horizon. The tidally locked synchronous rotation of the Moon as it orbits the Earth results in it always keeping nearly the same face turned towards the planet. The side of the Moon that faces Earth is called the near side, and the"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_35",
    "chunk": "opposite the far side. The far side is often inaccurately called the \"dark side\", but it is in fact illuminated as often as the near side: once every 29.5 Earth days. During dark moon to new moon, the near side is dark. The Moon originally rotated at a faster rate, but early in its history its rotation slowed and became tidally locked in this orientation as a result of frictional effects associated with tidal deformations caused by Earth. With time, the energy of rotation of the Moon on its axis was dissipated as heat, until there was no rotation of the Moon relative to Earth. In 2016, planetary scientists using data collected on the 1998–99 NASA Lunar Prospector mission found two hydrogen-rich areas (most likely former water ice) on opposite sides of the Moon. It is speculated that these patches were the poles of the Moon billions of years ago before it was tidally locked to Earth. The Moon rotates, as it orbits Earth, changing orientation toward the Sun, experiencing a lunar day. A lunar day is equal to one lunar month (one synodic orbit around Earth) due to it being tidally locked to Earth. Since the Moon is not"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_36",
    "chunk": "tidally locked to the Sun, lunar daylight and night times both occur around the Moon. The changing position of the illumination of the Moon by the Sun during a lunar day is observable from Earth as the changing lunar phases, waxing crescent being the sunrise and the waning crescent the sunset phase of a day observed from afar. Lunar night is the darkest on the far side and during lunar eclipses on the near side (and darker than a moonless night on Earth). The near side is during its night illuminated by Earthlight, making the near side illuminated enough by the Earthlight to see lunar surface features from Earth where it is dark during its night phase due to Earthlight being reflected back to Earth. Earthshine makes the night on the near side about 43 times brighter, and sometimes even 55 times brighter than a night on Earth illuminated by the light of the full moon. In Earth's sky brightness and apparent size of the Moon changes also due to its elliptic orbit around Earth. At perigee (closest), since the Moon is up to 14% closer to Earth than at apogee (most distant), it subtends a solid angle which is"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_37",
    "chunk": "up to 30% larger. Consequently, given the same phase, the Moon's brightness also varies by up to 30% between apogee and perigee. A full (or new) moon at such a position is called a supermoon. There has been historical controversy over whether observed features on the Moon's surface change over time. Today, many of these claims are thought to be illusory, resulting from observation under different lighting conditions, poor astronomical seeing, or inadequate drawings. However, outgassing does occasionally occur and could be responsible for a minor percentage of the reported lunar transient phenomena. Recently, it has been suggested that a roughly 3 km (1.9 mi) diameter region of the lunar surface was modified by a gas release event about a million years ago. The Moon has an exceptionally low albedo, giving it a reflectance that is slightly brighter than that of worn asphalt. Despite this, it is the brightest object in the sky after the Sun. This is due partly to the brightness enhancement of the opposition surge; the Moon at quarter phase is only one-tenth as bright, rather than half as bright, as at full moon. Additionally, color constancy in the visual system recalibrates the relations between the colors"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_38",
    "chunk": "of an object and its surroundings, and because the surrounding sky is comparatively dark, the sunlit Moon is perceived as a bright object. The edges of the full moon seem as bright as the center, without limb darkening, because of the reflective properties of lunar soil, which retroreflects light more towards the Sun than in other directions. The Moon's color depends on the light the Moon reflects, which in turn depends on the Moon's surface and its features, having for example large darker regions. In general, the lunar surface reflects a brown-tinged gray light. At times, the Moon can appear red or blue. It may appear red during a lunar eclipse, because of the red spectrum of the Sun's light being refracted onto the Moon by Earth's atmosphere. Because of this red color, lunar eclipses are also sometimes called blood moons. The Moon can also seem red when it appears at low angles and through a thick atmosphere. The Moon may appear blue depending on the presence of certain particles in the air, such as volcanic particles, in which case it can be called a blue moon. Because the words \"red moon\" and \"blue moon\" can also be used to"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_39",
    "chunk": "refer to specific full moons of the year, they do not always refer to the presence of red or blue moonlight. Eclipses only occur when the Sun, Earth, and Moon are all in a straight line (termed \"syzygy\"). Solar eclipses occur at new moon, when the Moon is between the Sun and Earth. In contrast, lunar eclipses occur at full moon, when Earth is between the Sun and Moon. The apparent size of the Moon is roughly the same as that of the Sun, with both being viewed at close to one-half a degree wide. The Sun is much larger than the Moon, but it is the vastly greater distance that gives it the same apparent size as the much closer and much smaller Moon from the perspective of Earth. The variations in apparent size, due to the non-circular orbits, are nearly the same as well, though occurring in different cycles. This makes possible both total (with the Moon appearing larger than the Sun) and annular (with the Moon appearing smaller than the Sun) solar eclipses. In a total eclipse, the Moon completely covers the disc of the Sun and the solar corona becomes visible to the naked eye. Because"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_40",
    "chunk": "the distance between the Moon and Earth is very slowly increasing over time, the angular diameter of the Moon is decreasing. As it evolves toward becoming a red giant, the size of the Sun, and its apparent diameter in the sky, are slowly increasing. The combination of these two changes means that hundreds of millions of years ago, the Moon would always completely cover the Sun on solar eclipses, and no annular eclipses were possible. Likewise, hundreds of millions of years in the future, the Moon will no longer cover the Sun completely, and total solar eclipses will not occur. As the Moon's orbit around Earth is inclined by about 5.145° (5° 9') to the orbit of Earth around the Sun, eclipses do not occur at every full and new moon. For an eclipse to occur, the Moon must be near the intersection of the two orbital planes. The periodicity and recurrence of eclipses of the Sun by the Moon, and of the Moon by Earth, is described by the saros, which has a period of approximately 18 years. Because the Moon continuously blocks the view of a half-degree-wide circular area of the sky, the related phenomenon of occultation occurs"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_41",
    "chunk": "when a bright star or planet passes behind the Moon and is occulted: hidden from view. In this way, a solar eclipse is an occultation of the Sun. Because the Moon is comparatively close to Earth, occultations of individual stars are not visible everywhere on the planet, nor at the same time. Because of the precession of the lunar orbit, each year different stars are occulted. It is believed by some that the oldest cave paintings from up to 40,000 BP of bulls and geometric shapes, or 20–30,000 year old tally sticks were used to observe the phases of the Moon, keeping time using the waxing and waning of the Moon's phases. Aspects of the Moon were identified and aggregated in lunar deities from prehistoric times and were eventually documented and put into symbols from the very first instances of writing in the 4th millennium BC. One of the earliest-discovered possible depictions of the Moon is a 3,000 BCE rock carving Orthostat 47 at Knowth, Ireland. The crescent depicting the Moon as with the lunar deity Nanna/Sin have been found from the 3rd millennium BCE. The oldest named astronomer and poet Enheduanna, Akkadian high priestess to the lunar deity Nanna/Sin"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_42",
    "chunk": "and pricess, daughter of Sargon the Great (c. 2334 – c. 2279 BCE), had the Moon tracked in her chambers. The oldest found and identified depiction of the Moon in an astronomical relation to other astronomical features is the Nebra sky disc from c. 1800–1600 BCE, depicting features like the Pleiades next to the Moon. The ancient Greek philosopher Anaxagoras (d. 428 BC) reasoned that the Sun and Moon were both giant spherical rocks, and that the latter reflected the light of the former. Elsewhere in the 5th century BC to 4th century BC, Babylonian astronomers had recorded the 18-year Saros cycle of lunar eclipses, and Indian astronomers had described the Moon's monthly elongation. The Chinese astronomer Shi Shen (fl. 4th century BC) gave instructions for predicting solar and lunar eclipses. In Aristotle's (384–322 BC) description of the universe, the Moon marked the boundary between the spheres of the mutable elements (earth, water, air and fire), and the imperishable stars of aether, an influential philosophy that would dominate for centuries. Archimedes (287–212 BC) designed a planetarium that could calculate the motions of the Moon and other objects in the Solar System. In the 2nd century BC, Seleucus of Seleucia correctly"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_43",
    "chunk": "thought that tides were due to the attraction of the Moon, and that their height depends on the Moon's position relative to the Sun. In the same century, Aristarchus computed the size and distance of the Moon from Earth, obtaining a value of about twenty times the radius of Earth for the distance. The Chinese of the Han dynasty believed the Moon to be energy equated to qi and their 'radiating influence' theory recognized that the light of the Moon was merely a reflection of the Sun; Jing Fang (78–37 BC) noted the sphericity of the Moon. Ptolemy (90–168 AD) greatly improved on the numbers of Aristarchus, calculating a mean distance of 59 times Earth's radius and a diameter of 0.292 Earth diameters, close to the correct values of about 60 and 0.273 respectively. In the 2nd century AD, Lucian wrote the novel A True Story, in which the heroes travel to the Moon and meet its inhabitants. In 510 AD, the Indian astronomer Aryabhata mentioned in his Aryabhatiya that reflected sunlight is the cause of the shining of the Moon. The astronomer and physicist Ibn al-Haytham (965–1039) found that sunlight was not reflected from the Moon like a mirror,"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_44",
    "chunk": "but that light was emitted from every part of the Moon's sunlit surface in all directions. Shen Kuo (1031–1095) of the Song dynasty created an allegory equating the waxing and waning of the Moon to a round ball of reflective silver that, when doused with white powder and viewed from the side, would appear to be a crescent. During the Middle Ages, before the invention of the telescope, the Moon was increasingly recognized as a sphere, though many believed that it was \"perfectly smooth\". The telescope was developed and reported on in 1608. The first record of telescopic astronomy and rough mapping of the Moon's features is from early summer 1609 by Thomas Harriot, but which he did not publish. At the same time Galileo Galilei too started to use telescopes to observe the sky and the Moon, recording later that year more detailed observations and crucial conclusions, such as that the Moon was not smooth, featuring mountains and craters, which he published in 1610 in his ground-breaking and soon widely discussed book Sidereus Nuncius. Later in the 17th century, the efforts of Giovanni Battista Riccioli and Francesco Maria Grimaldi led to the system of naming of lunar features in"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_45",
    "chunk": "use today. The more exact 1834–1836 Mappa Selenographica of Wilhelm Beer and Johann Heinrich von Mädler, and their associated 1837 book Der Mond, the first trigonometrically accurate study of lunar features, included the heights of more than a thousand mountains, and introduced the study of the Moon at accuracies possible in earthly geography. Lunar craters, first noted by Galileo, were thought to be volcanic until the 1870s proposal of Richard Proctor that they were formed by collisions. This view gained support in 1892 from the experimentation of geologist Grove Karl Gilbert, and from comparative studies from 1920 to the 1940s, leading to the development of lunar stratigraphy, which by the 1950s was becoming a new and growing branch of astrogeology. After World War II the first launch systems were developed and by the end of the 1950s they reached capabilities that allowed the Soviet Union and the United States to launch spacecraft into space. The Cold War fueled a closely followed development of launch systems by the two states, resulting in the so-called Space Race and its later phase the Moon Race, accelerating efforts and interest in exploration of the Moon. After the first spaceflight of Sputnik 1 in 1957"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_46",
    "chunk": "during International Geophysical Year the spacecraft of the Soviet Union's Luna program were the first to accomplish a number of goals. Following three unnamed failed missions in 1958, the first human-made object Luna 1 escaped Earth's gravity and passed near the Moon in 1959. Later that year the first human-made object Luna 2 reached the Moon's surface by intentionally impacting. By the end of the year Luna 3 reached as the first human-made object the normally occluded far side of the Moon, taking the first photographs of it. The first spacecraft to perform a successful lunar soft landing was Luna 9 and the first vehicle to orbit the Moon was Luna 10, both in 1966. Following President John F. Kennedy's 1961 commitment to a crewed Moon landing before the end of the decade, the United States, under NASA leadership, launched a series of uncrewed probes to develop an understanding of the lunar surface in preparation for human missions: the Jet Propulsion Laboratory's Ranger program, the Lunar Orbiter program and the Surveyor program. The crewed Apollo program was developed in parallel; after a series of uncrewed and crewed tests of the Apollo spacecraft in Earth orbit, and spurred on by a"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_47",
    "chunk": "potential Soviet lunar human landing, in 1968 Apollo 8 made the first human mission to lunar orbit (the first Earthlings, two tortoises, had circled the Moon three months earlier on the Soviet Union's Zond 5, followed by turtles on Zond 6). The first time a person landed on the Moon and any extraterrestrial body was when Neil Armstrong, the commander of the American mission Apollo 11, set foot on the Moon at 02:56 UTC on July 21, 1969. Considered the culmination of the Space Race, an estimated 500 million people worldwide watched the transmission by the Apollo TV camera, the largest television audience for a live broadcast at that time. While at the same time another mission, the robotic sample return mission Luna 15 by the Soviet Union had been in orbit around the Moon, becoming together with Apollo 11 the first ever case of two extraterrestrial missions being conducted at the same time. The Apollo missions 11 to 17 (except Apollo 13, which aborted its planned lunar landing) removed 380.05 kilograms (837.87 lb) of lunar rock and soil in 2,196 separate samples. Scientific instrument packages were installed on the lunar surface during all the Apollo landings. Long-lived instrument stations,"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_48",
    "chunk": "including heat flow probes, seismometers, and magnetometers, were installed at the Apollo 12, 14, 15, 16, and 17 landing sites. Direct transmission of data to Earth concluded in late 1977 because of budgetary considerations, but as the stations' lunar laser ranging corner-cube retroreflector arrays are passive instruments, they are still being used. Apollo 17 in 1972 remains the last crewed mission to the Moon. Explorer 49 in 1973 was the last dedicated U.S. probe to the Moon until the 1990s. The Soviet Union continued sending robotic missions to the Moon until 1976, deploying in 1970 with Luna 17 the first remote controlled rover Lunokhod 1 on an extraterrestrial surface, and collecting and returning 0.3 kg of rock and soil samples with three Luna sample return missions (Luna 16 in 1970, Luna 20 in 1972, and Luna 24 in 1976). Following the last Soviet mission to the Moon of 1976, there was little further lunar exploration for fourteen years. Astronautics had shifted its focus towards the exploration of the inner (e.g. Venera program) and outer (e.g. Pioneer 10, 1972) Solar System planets, but also towards Earth orbit, developing and continuously operating, beside communication satellites, Earth observation satellites (e.g. Landsat program, 1972),"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_49",
    "chunk": "space telescopes and particularly space stations (e.g. Salyut program, 1971). Negotiation in 1979 of Moon treaty, and its subsequent ratification in 1984 was the only major activity regarding the Moon until 1990. In 1990 Hiten – Hagoromo, the first dedicated lunar mission since 1976, reached the Moon. Sent by Japan, it became the first mission that was not a Soviet Union or U.S. mission to the Moon. In 1994, the U.S. dedicated a mission to fly a spacecraft (Clementine) to the Moon again for the first time since 1973. This mission obtained the first near-global topographic map of the Moon, and the first global multispectral images of the lunar surface. In 1998, this was followed by the Lunar Prospector mission, whose instruments indicated the presence of excess hydrogen at the lunar poles, which is likely to have been caused by the presence of water ice in the upper few meters of the regolith within permanently shadowed craters. The next years saw a row of first missions to the Moon by a new group of states actively exploring the Moon. Between 2004 and 2006 the first spacecraft by the European Space Agency (ESA) (SMART-1) reached the Moon, recording the first detailed"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_50",
    "chunk": "survey of chemical elements on the lunar surface. The Chinese Lunar Exploration Program reached the Moon for the first time with the orbiter Chang'e 1 (2007–2009), obtaining a full image map of the Moon. India reached, orbited and impacted the Moon in 2008 for the first time with its Chandrayaan-1 and Moon Impact Probe, becoming the fifth and sixth state to do so, creating a high-resolution chemical, mineralogical and photo-geological map of the lunar surface, and confirming the presence of water molecules in lunar soil. The U.S. launched the Lunar Reconnaissance Orbiter (LRO) and the LCROSS impactor on June 18, 2009. LCROSS completed its mission by making a planned and widely observed impact in the crater Cabeus on October 9, 2009, whereas LRO is currently in operation, obtaining precise lunar altimetry and high-resolution imagery. China continued its lunar program in 2010 with Chang'e 2, mapping the surface at a higher resolution over an eight-month period, and in 2013 with Chang'e 3, a lunar lander along with a lunar rover named Yutu (Chinese: 玉兔; lit. 'Jade Rabbit'). This was the first lunar rover mission since Lunokhod 2 in 1973 and the first lunar soft landing since Luna 24 in 1976, making"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_51",
    "chunk": "China the third country to achieve this. In 2014 the first privately funded probe, the Manfred Memorial Moon Mission, reached the Moon. Another Chinese rover mission, Chang'e 4, achieved the first landing on the Moon's far side in early 2019. Also in 2019, India successfully sent its second probe, Chandrayaan-2 to the Moon. In 2020, China carried out its first robotic sample return mission (Chang'e 5), bringing back 1,731 grams of lunar material to Earth. The U.S. developed plans for returning to the Moon beginning in 2004, and with the signing of the U.S.-led Artemis Accords in 2020, the Artemis program aims to return the astronauts to the Moon in the 2020s. The Accords have been joined by a growing number of countries. The introduction of the Artemis Accords has fueled a renewed discussion about the international framework and cooperation of lunar activity, building on the Moon Treaty and the ESA-led Moon Village concept. 2022 South Korea launched Danuri successfully, its first mission to the Moon, launched from the US. 2023 and 2024 India and Japan became the fourth and fifth country to soft land a spacecraft on the Moon, following the Soviet Union and United States in the 1960s,"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_52",
    "chunk": "and China in the 2010s. Notably, Japan's spacecraft, the Smart Lander for Investigating Moon, survived 3 lunar nights. The IM-1 lander became the first commercially built lander to land on the Moon in 2024. China launched the Chang'e 6 on May 3, 2024, which conducted another lunar sample return from the far side of the Moon. It also carried a Chinese rover to conduct infrared spectroscopy of lunar surface. Pakistan sent a lunar orbiter called ICUBE-Q along with Chang'e 6. Beside the progressing Artemis program and supporting Commercial Lunar Payload Services, leading an international and commercial crewed opening up of the Moon and sending the first woman, person of color and non-US citizen to the Moon in the 2020s, China is continuing its ambitious Chang'e program, having announced with Russia's struggling Luna-Glob program joint missions. Both the Chinese and US lunar programs have the goal to establish in the 2030s a lunar base with their international partners, though the US and its partners will first establish an orbital Lunar Gateway station in the 2020s, from which Artemis missions will land the Human Landing System to set up temporary surface camps. While the Apollo missions were explorational in nature, the Artemis"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_53",
    "chunk": "program plans to establish a more permanent presence. To this end, NASA is partnering with industry leaders to establish key elements such as modern communication infrastructure. A 4G connectivity demonstration is to be launched aboard an Intuitive Machines Nova-C lander in 2024. Another focus is on in situ resource utilization, which is a key part of the DARPA lunar programs. DARPA has requested that industry partners develop a 10–year lunar architecture plan to enable the beginning of a lunar economy. In 1959 the first extraterrestrial probes reached the Moon (Luna program), just a year into the space age, after the first ever orbital flight. Since then, humans have sent a range of probes and people to the Moon. The first stay of people on the Moon was conducted in 1969, in a series of crewed exploration missions (the Apollo Program), the last having taken place in 1972. Uninterrupted presence has been the case through the remains of impactors, landings and lunar orbiters. Some landings and orbiters have maintained a small lunar infrastructure, providing continuous observation and communication at the Moon. Increasing human activity in cislunar space as well as on the Moon's surface, particularly missions at the far side of"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_54",
    "chunk": "the Moon or the lunar north and south polar regions, are in need for a lunar infrastructure. For that purpose, orbiters in orbits around the Moon or the Earth–Moon Lagrange points, have since 2006 been operated. With highly eccentric orbits providing continuous communication, as with the orbit of Queqiao and Queqiao-2 relay satellite or the planned first extraterrestrial space station, the Lunar Gateway. While the Moon has the lowest planetary protection target-categorization, its degradation as a pristine body and scientific place has been discussed. If there is astronomy performed from the Moon, it will need to be free from any physical and radio pollution. While the Moon has no significant atmosphere, traffic and impacts on the Moon causes clouds of dust that can spread far and possibly contaminate the original state of the Moon and its special scientific content. Scholar Alice Gorman asserts that, although the Moon is inhospitable, it is not dead, and that sustainable human activity would require treating the Moon's ecology as a co-participant. The so-called \"Tardigrade affair\" of the 2019 crashed Beresheet lander and its carrying of tardigrades has been discussed as an example for lacking measures and lacking international regulation for planetary protection. Space debris"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_55",
    "chunk": "beyond Earth around the Moon has been considered as a future challenge with increasing numbers of missions to the Moon, particularly as a danger for such missions. As such lunar waste management has been raised as an issue which future lunar missions, particularly on the surface, need to tackle. Human remains have been transported to the Moon, including by private companies such as Celestis and Elysium Space. Because the Moon has been sacred or significant to many cultures, the practice of space burials have attracted criticism from indigenous peoples leaders. For example, then–Navajo Nation president Albert Hale criticized NASA for sending the cremated ashes of scientist Eugene Shoemaker to the Moon in 1998. Beside the remains of human activity on the Moon, there have been some intended permanent installations like the Moon Museum art piece, Apollo 11 goodwill messages, six lunar plaques, the Fallen Astronaut memorial, and other artifacts. Longterm missions continuing to be active are some orbiters such as the 2009-launched Lunar Reconnaissance Orbiter surveilling the Moon for future missions, as well as some Landers such as the 2013-launched Chang'e 3 with its Lunar Ultraviolet Telescope still operational. Five retroreflectors have been installed on the Moon since the 1970s"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_56",
    "chunk": "and since used for accurate measurements of the physical librations through laser ranging to the Moon. There are several missions by different agencies and companies planned to establish a long-term human presence on the Moon, with the Lunar Gateway as the currently most advanced project as part of the Artemis program. The Moon has been used as a site for astronomical and Earth observations. The Earth appears in the Moon's sky with an apparent size of 1° 48′ to 2°, three to four times the size of the Moon or Sun in Earth's sky, or about the apparent width of two little fingers at an arm's length away. Observations from the Moon started as early as 1966 with the first images of Earth from the Moon, taken by Lunar Orbiter 1. Of particular cultural significance is the 1968 photograph called Earthrise, taken by Bill Anders of Apollo 8 in 1968. In April 1972 the Apollo 16 mission set up the first dedicated telescope, the Far Ultraviolet Camera/Spectrograph, recording various astronomical photos and spectra. The Moon is recognized as an excellent site for telescopes. It is relatively nearby; certain craters near the poles are permanently dark and cold and especially useful"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_57",
    "chunk": "for infrared telescopes; and radio telescopes on the far side would be shielded from the radio chatter of Earth. The lunar soil, although it poses a problem for any moving parts of telescopes, can be mixed with carbon nanotubes and epoxies and employed in the construction of mirrors up to 50 meters in diameter. A lunar zenith telescope can be made cheaply with an ionic liquid. The only instances of humans living on the Moon have taken place in an Apollo Lunar Module for several days at a time (for example, during the Apollo 17 mission). One challenge to astronauts during their stay on the surface is that lunar dust sticks to their suits and is carried into their quarters. Astronauts could taste and smell the dust, which smells like gunpowder and was called the \"Apollo aroma\". This fine lunar dust can cause health issues. In 2019, at least one plant seed sprouted in an experiment on the Chang'e 4 lander. It was carried from Earth along with other small life in its Lunar Micro Ecosystem. Although Luna landers scattered pennants of the Soviet Union on the Moon, and U.S. flags were symbolically planted at their landing sites by the"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_58",
    "chunk": "Apollo astronauts, no nation claims ownership of any part of the Moon's surface. Likewise no private ownership of parts of the Moon, or as a whole, is considered credible. The 1967 Outer Space Treaty defines the Moon and all outer space as the \"province of all mankind\". It restricts the use of the Moon to peaceful purposes, explicitly banning military installations and weapons of mass destruction. A majority of countries are parties of this treaty. The 1979 Moon Agreement was created to elaborate, and restrict the exploitation of the Moon's resources by any single nation, leaving it to a yet unspecified international regulatory regime. As of January 2020, it has been signed and ratified by 18 nations, none of which have human spaceflight capabilities. Since 2020, countries have joined the U.S. in their Artemis Accords, which are challenging the treaty. The U.S. has furthermore emphasized in a presidential executive order (\"Encouraging International Support for the Recovery and Use of Space Resources.\") that \"the United States does not view outer space as a 'global commons'\" and calls the Moon Agreement \"a failed attempt at constraining free enterprise.\" With Australia signing and ratifying both the Moon Treaty in 1986 as well as"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_59",
    "chunk": "the Artemis Accords in 2020, there has been a discussion if they can be harmonized. In this light an Implementation Agreement for the Moon Treaty has been advocated for, as a way to compensate for the shortcomings of the Moon Treaty and to harmonize it with other laws and agreements such as the Artemis Accords, allowing it to be more widely accepted. In the face of such increasing commercial and national interest, particularly prospecting territories, U.S. lawmakers have introduced in late 2020 specific regulation for the conservation of historic landing sites and interest groups have argued for making such sites World Heritage Sites and zones of scientific value protected zones, all of which add to the legal availability and territorialization of the Moon. In 2021, the Declaration of the Rights of the Moon was created by a group of \"lawyers, space archaeologists and concerned citizens\", drawing on precedents in the Rights of Nature movement and the concept of legal personality for non-human entities in space. Increasing human activity at the Moon has raised the need for coordination to safeguard international and commercial lunar activity. Issues from cooperation to mere coordination, through for example the development of a shared Lunar time,"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_60",
    "chunk": "have been raised. In particular the establishment of an international or United Nations regulatory regime for lunar human activity has been called for by the Moon Treaty and suggested through an Implementation Agreement, but remains contentious. Current lunar programs are multilateral, with the US-led Artemis program and the China-led International Lunar Research Station. For broader international cooperation and coordination, the International Lunar Exploration Working Group (ILEWG), the Moon Village Association (MVA) and more generally the International Space Exploration Coordination Group (ISECG) has been established. Since pre-historic times people have taken note of the Moon's phases and its waxing and waning cycle and used it to keep record of time. Tally sticks, notched bones dating as far back as 20–30,000 years ago, are believed by some to mark the phases of the Moon. The counting of the days between the Moon's phases eventually gave rise to generalized time periods of lunar cycles as months, and possibly of its phases as weeks. The words for the month in a range of different languages carry this relation between the period of the month and the Moon etymologically. The English month as well as moon, and its cognates in other Indo-European languages (e.g. the"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_61",
    "chunk": "Latin mensis and Ancient Greek μείς (meis) or μήν (mēn), meaning \"month\") stem from the Proto-Indo-European (PIE) root of moon, *méh1nōt, derived from the PIE verbal root *meh1-, \"to measure\", \"indicat[ing] a functional conception of the Moon, i.e. marker of the month\" (cf. the English words measure and menstrual). To give another example from a different language family, the Chinese language uses the same word (月) for moon as for month, which furthermore can be found in the symbols for the word week (星期). This lunar timekeeping gave rise to the historically dominant, but varied, lunisolar calendars. The 7th-century Islamic calendar is an example of a purely lunar calendar, where months are traditionally determined by the visual sighting of the hilal, or earliest crescent moon, over the horizon. Of particular significance has been the occasion of full moon, highlighted and celebrated in a range of calendars and cultures, an example being the Buddhist Vesak. The full moon around the southern or northern autumnal equinox is often called the harvest moon and is celebrated with festivities such as the Harvest Moon Festival of the Chinese lunar calendar, its second most important celebration after the Chinese lunisolar Lunar New Year. Furthermore, association"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_62",
    "chunk": "of time with the Moon can also be found in religion, such as the ancient Egyptian temporal and lunar deity Khonsu. Humans have not only observed the Moon since prehistoric times, but have also developed intricate perceptions of the Moon. Over time the Moon has been characterized and associated in many different ways, from having a spirit or being a deity, and an aspect thereof or an aspect in astrology, being made an important part of many cosmologies. This rich history of humans viewing the Moon has been evidenced starting with depictions from 40,000 BP and in written form from the 4th millennium BCE in the earliest cases of writing. The oldest named astronomer and poet Enheduanna, Akkadian high priestess to the lunar deity Nanna/Sin and pricess, daughter of Sargon the Great (c. 2334 – c. 2279 BCE), tracked the Moon and wrote poems about her divine Moon. For the representation of the Moon, especially its lunar phases, the crescent (🌙) has been a recurring symbol in a range of cultures since at least 3,000 BCE or possibly earlier with bull horns dating to the earliest cave paintings at 40,000 BP. In writing systems such as Chinese the crescent has"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_63",
    "chunk": "developed into the symbol 月, the word for Moon, and in ancient Egyptian it was the symbol 𓇹, meaning Moon and spelled like the ancient Egyptian lunar deity Iah, which the other ancient Egyptian lunar deities Khonsu and Thoth were associated with. Iconographically the crescent was used in Mesopotamia as the primary symbol of Nanna/Sîn, the ancient Sumerian lunar deity, who was the father of Inanna/Ishtar, the goddess of the planet Venus (symbolized as the eight pointed Star of Ishtar), and Utu/Shamash, the god of the Sun (symbolized as a disc, optionally with eight rays), all three often depicted next to each other. Nanna/Sîn is, like some other lunar deities, for example Iah and Khonsu of ancient Egypt, Mene/Selene of ancient Greece and Luna of ancient Rome, depicted as a horned deity, featuring crescent shaped headgears or crowns. The particular arrangement of the crescent with a star known as the star and crescent (☪️) goes back to the Bronze Age, representing either the Sun and Moon, or the Moon and the planet Venus, in combination. It came to represent the selene goddess Artemis, and via the patronage of Hecate, which as triple deity under the epithet trimorphos/trivia included aspects of"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_64",
    "chunk": "Artemis/Diana, came to be used as a symbol of Byzantium, with Virgin Mary (Queen of Heaven) later taking her place, becoming depicted in Marian veneration on a crescent and adorned with stars. Since then the heraldric use of the star and crescent proliferated, Byzantium's symbolism possibly influencing the development of the Ottoman flag, specifically the combination of the Turkish crescent with a star, and becoming a popular symbol for Islam (as the hilal of the Islamic calendar) and for a range of nations. The features of the Moon, the contrasting brighter highlands and darker maria, have been seen by different cultures forming abstract shapes. Such shapes are among others the Man in the Moon (e.g. Coyolxāuhqui) or the Moon Rabbit (e.g. the Chinese Tu'er Ye or in Indigenous American mythologies the aspect of the Mayan Moon goddess, from which possibly Awilix is derived, or of Metztli/Tēcciztēcatl). Occasionally some lunar deities have been also depicted driving a chariot across the sky, such as the Hindu Chandra/Soma, the Greek Artemis, which is associated with Selene, or Luna, Selene's ancient Roman equivalent. Color and material wise the Moon has been associated in Western alchemy with silver, while gold is associated with the Sun."
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_65",
    "chunk": "Through a miracle, the so-called splitting of the Moon (Arabic: انشقاق القمر) in Islam, association with the Moon applies also to Muhammad. The perception of the Moon in the modern era has been informed by telescope-enabled modern astronomy and later by spaceflight which enabled actual human activity at the Moon, particularly the culturally impactful lunar landings. These new insights inspired cultural references, connecting romantic reflections about the Moon and speculative fiction such as science-fiction dealing with the Moon. Contemporarily the Moon has been seen as a place for economic expansion into space, with missions prospecting for lunar resources. This has been accompanied with renewed public and critical reflection on humanity's cultural and legal relation to the celestial body, especially regarding colonialism, as in the 1970 poem \"Whitey on the Moon\". In this light the Moon's nature has been invoked, particularly for lunar conservation and as a common. In 2021 20 July, the date of the first crewed Moon landing, became the annual International Moon Day. The lunar effect is a purported unproven correlation between specific stages of the roughly 29.5-day lunar cycle and behavior and physiological changes in living beings on Earth, including humans. The Moon has long been associated"
  },
  {
    "source": "Moon.txt",
    "chunk_id": "Moon.txt_66",
    "chunk": "with insanity and irrationality; the words lunacy and lunatic are derived from the Latin name for the Moon, Luna. Philosophers Aristotle and Pliny the Elder argued that the full moon induced insanity in susceptible individuals, believing that the brain, which is mostly water, must be affected by the Moon and its power over the tides, but the Moon's gravity is too slight to affect any single person. Even today, people who believe in a lunar effect claim that admissions to psychiatric hospitals, traffic accidents, homicides or suicides increase during a full moon, but dozens of studies invalidate these claims."
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_0",
    "chunk": "# Netherlands The Netherlands, informally Holland, is a country in Northwestern Europe, with overseas territories in the Caribbean. It is the largest of the four constituent countries of the Kingdom of the Netherlands. The Netherlands consists of twelve provinces; it borders Germany to the east and Belgium to the south, with a North Sea coastline to the north and west. It shares maritime borders with the United Kingdom, Germany, and Belgium. The official language is Dutch, with West Frisian as a secondary official language in the province of Friesland. Dutch, English, and Papiamento are official in the Caribbean territories. Netherlands literally means \"lower countries\" in reference to its low elevation and flat topography, with 26% below sea level. Most of the areas below sea level, known as polders, are the result of land reclamation that began in the 14th century. In the Republican period, which began in 1588, the Netherlands entered a unique era of political, economic, and cultural greatness, ranked among the most powerful and influential in Europe and the world; this period is known as the Dutch Golden Age. During this time, its trading companies, the Dutch East India Company and the Dutch West India Company, established colonies"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_1",
    "chunk": "and trading posts all over the world. With a population of over 18 million people, all living within a total area of 41,850 km (16,160 sq mi)—of which the land area is 33,500 km (12,900 sq mi)—the Netherlands is the 33rd most densely populated country, with a density of 535 people per square kilometre (1,390 people/sq mi). Nevertheless, it is the world's second-largest exporter of food and agricultural products by value, owing to its fertile soil, mild climate, intensive agriculture, and inventiveness. The four largest cities in the Netherlands are Amsterdam, Rotterdam, The Hague and Utrecht. Amsterdam is the country's most populous city and the nominal capital, though the primary national political institutions are located in the Hague. The Netherlands has been a parliamentary constitutional monarchy with a unitary structure since 1848. The country has a tradition of pillarisation (separation of citizens into groups by religion and political beliefs) and a long record of social tolerance, having legalised prostitution and euthanasia, along with maintaining a liberal drug policy. The Netherlands allowed women's suffrage in 1919 and was the first country to legalise same-sex marriage in 2001. Its mixed-market advanced economy has the eleventh-highest per capita income globally. The Hague holds"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_2",
    "chunk": "the seat of the States General, cabinet, and Supreme Court. The Port of Rotterdam is the busiest in Europe. Schiphol is the busiest airport in the Netherlands, and the fourth busiest in Europe. Being a developed country, the Netherlands is a founding member of the European Union, eurozone, G10, NATO, OECD, and WTO, as well as a part of the Schengen Area and the trilateral Benelux Union. It hosts intergovernmental organisations and international courts, many of which are in The Hague. The countries that comprise the region called the Low Countries (Netherlands, Belgium, and Luxembourg) all have comparatively the same toponymy. Place names with Neder, Nieder, Nedre, Nether, Lage(r) or Low(er) (in Germanic languages) and Bas or Inferior (in Romance languages) are in use in low-lying places all over Europe. The Romans made a distinction between the Roman provinces of downstream Germania Inferior (nowadays part of Belgium and the Netherlands) and upstream Germania Superior. Thus, in the case of the Low Countries and the Netherlands, the geographical location of this lower region is more or less downstream and near the sea, compared to that of the upper region of Germania Superior. The designation 'Low' returned in the 10th-century Duchy of"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_3",
    "chunk": "Lower Lorraine, which covered much of the Low Countries. The Dukes of Burgundy used the term les pays de par deçà (\"the lands over here\") for the Low Countries. Under Habsburg rule, this became pays d'embas (\"lands down-here\"). This was translated as Neder-landen in contemporary Dutch official documents. From a regional point of view, Niderlant was also the area between the Meuse and the lower Rhine in the late Middle Ages. From the mid-sixteenth century, the \"Low Countries\" and the \"Netherlands\" lost their original deictic meaning. In most Romance languages, the term \"Low Countries\" is officially used as the name for the Netherlands. The term Holland has frequently been used informally to refer to the whole of the modern country of the Netherlands in various languages, including Dutch and English. In some languages, Holland is used as the formal name for the Netherlands. However, Holland is a region within the Netherlands that consists of the two provinces of North and South Holland. Formerly these were a single province, and earlier the County of Holland, which included parts of present-day Utrecht. The emphasis on Holland during the formation of the Dutch Republic, the Eighty Years' War, and the Anglo-Dutch Wars in"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_4",
    "chunk": "the 16th, 17th, and 18th centuries, made Holland a pars pro toto for the entire country. Many Dutch people object to the country being referred to as Holland instead of the Netherlands, on much the same grounds as many Welsh or Scottish people object to the United Kingdom being referred to as England. In particular, those from regions other than Holland find it undesirable or misrepresentative to use the term Holland for the whole country, as the Holland region only comprises two of the twelve provinces, and 38% of Dutch citizens. As of 2019, the Dutch government officially has preferred the Netherlands instead of Holland when talking about the country. Often Holland or Hollanders is used by the Flemish to refer to the Dutch in the Netherlands, and by the Southern Dutch (Dutch living \"below the great rivers\", a natural cultural, social and religious boundary formed by the rivers Rhine and Meuse) to refer to the Northern Dutch (Dutch living North of these rivers). In the Southern province of Limburg, the term is used for the Dutch from the other 11 provinces. The use of the term in this context by the Southern Dutch is in a derogatory fashion. Dutch"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_5",
    "chunk": "is used as the adjective for the Netherlands, as well as the demonym. The origins of the word go back to Proto-Germanic *þiudiskaz, Latinised into Theodiscus, meaning \"popular\" or \"of the people\", akin to Old Dutch Dietsch or Old English þeodisc, meaning \"(of) the common (Germanic) people\". At first, the English language used Dutch to refer to any or all speakers of West Germanic languages. Gradually its meaning shifted to the West Germanic people they had the most contact with. The oldest human (Neanderthal) traces in the Netherlands, believed to be about 250,000 years old, were found near Maastricht. At the end of the Ice Age, the nomadic late Upper Palaeolithic Hamburg culture (13,000–10,000 BC) hunted reindeer in the area, using spears. The later Ahrensburg culture (11,200–9,500 BC) used bow and arrow. From Mesolithic Maglemosian-like tribes (c. 8000 BC), the world's oldest canoe was found in Drenthe. Indigenous late Mesolithic hunter-gatherers from the Swifterbant culture (c. 5600 BC), related to the southern Scandinavian Ertebølle culture, were strongly linked to rivers and open water. Between 4800 and 4500 BC, the Swifterbant people started to adopt from the neighbouring Linear Pottery culture the practice of animal husbandry, and between 4300 and 4000"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_6",
    "chunk": "BC agriculture. The Funnelbeaker culture (4300–2800 BC) erected the dolmens, large stone grave monuments found in Drenthe. There was a quick transition from the Funnelbeaker farming culture to the pan-European Corded Ware pastoralist culture (c. 2950 BC). In the southwest, the Seine-Oise-Marne culture—related to the Vlaardingen culture (c. 2600 BC)—survived well into the Neolithic period, until it too was succeeded by the Corded Ware culture. The subsequent Bell Beaker culture (2700–2100 BC) introduced metalwork in copper, gold and later bronze and opened new international trade routes, reflected in copper artefacts. Finds of rare bronze objects suggest that Drenthe was a trading centre in the Bronze Age (2000–800 BC). The Bell Beaker culture developed locally into the Barbed-Wire Beaker culture (2100–1800 BC) and later the Elp culture (1800–800 BC), a Middle Bronze Age culture marked by earthenware pottery. The southern region became dominated by the related Hilversum culture (1800–800 BC). From 800 BC onwards, the Iron Age Celtic Hallstatt culture became influential, replacing the Hilversum culture. Iron ore brought a measure of prosperity and was available throughout the country. Smiths travelled from settlement to settlement with bronze and iron, fabricating tools on demand. The King's grave of Oss (700 BC) was"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_7",
    "chunk": "found in a burial mound, the largest of its kind in Western Europe. The deteriorating climate in Scandinavia from 850 BC and 650 BC might have triggered the migration of Germanic tribes from the North. By the time this migration was complete, around 250 BC, a few general cultural and linguistic groups had emerged. The North Sea Germanic Ingaevones inhabited the northern part of the Low Countries. They would later develop into the Frisii and the early Saxons. The Weser–Rhine Germanic (or Istvaeones) extended along the middle Rhine and Weser and inhabited the Low Countries south of the great rivers. These tribes would eventually develop into the Salian Franks. The Celtic La Tène culture (c. 450 BC to the Roman conquest) expanded over a wide range, including the southern area of the Low Countries. Some scholars have speculated that even a third ethnic identity and language, neither Germanic nor Celtic, survived in the Netherlands until the Roman period, the Nordwestblock culture. The first author to describe the coast of Holland and Flanders was the geographer Pytheas, who noted in c. 325 BC that in these regions, \"more people died in the struggle against water than in the struggle against men.\""
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_8",
    "chunk": "During the Gallic Wars, the area south and west of the Rhine was conquered by Roman forces under Julius Caesar from 57 BC to 53 BC. Caesar describes two main Celtic tribes living in what is now the southern Netherlands: the Menapii and the Eburones. Under Augustus, the Roman Empire would conquer the entirety of the modern day Netherlands, incorporating it into the province of Germania Antiqua in 7 BC, but would be repelled back across the Rhine after the Battle of Teutoburg Forest in 9 AD, with the Rhine becoming fixed as Rome's permanent northern frontier around 12 AD. Notable towns would arise along the Limes Germanicus: Nijmegen and Voorburg. In the first part of Gallia Belgica, the area south of the Limes became part of the Roman province of Germania Inferior. The area to the north of the Rhine, inhabited by the Frisii, remained outside Roman rule, while the Germanic border tribes of the Batavi and Cananefates served in the Roman cavalry. The Batavi rose against the Romans in the Batavian rebellion of 69 AD but were eventually defeated. The Batavi later merged with other tribes into the confederation of the Salian Franks, whose identity emerged in the"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_9",
    "chunk": "first half of the third century. Salian Franks appear in Roman texts as both allies and enemies. They were forced by the confederation of the Saxons from the east to move over the Rhine into Roman territory in the fourth century. From their new base in West Flanders and the Southwest Netherlands, they were raiding the English Channel. Roman forces pacified the region but did not expel the Franks, who continued to be feared at least until the time of Julian the Apostate (358) when Salian Franks were allowed to settle as foederati in Texandria. After the Roman government in the area collapsed in roughly the year 406, the Franks expanded their territories into numerous kingdoms. By the 490s, Clovis I had conquered and united all these territories in the southern Netherlands in one Frankish kingdom, and from there continued his conquests into Gaul. During this expansion, Franks migrating to the south (modern territory of France and Walloon part of Belgium) eventually adopted the Vulgar Latin of the local population. A widening cultural divide grew with the Franks remaining in their original homeland in the north (i.e. the southern Netherlands and Flanders), who kept on speaking Old Frankish, which by"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_10",
    "chunk": "the ninth century had evolved into Old Low Franconian or Old Dutch. A Dutch-French language boundary hence came into existence. To the north of the Franks, climatic conditions improved, and during the Migration Period Saxons, the closely related Angles, Jutes, and Frisii settled the coast. Many moved on to England and came to be known as Anglo-Saxons, but those who stayed would be referred to as Frisians and their language as Frisian. Frisian was spoken along the entire southern North Sea coast. By the seventh century, a Frisian Kingdom (650–734) under King Aldegisel and King Redbad emerged with Traiectum (Utrecht) as its centre of power, while Dorestad was a flourishing trading place. Between 600 and around 719 the cities were often fought over between the Frisians and the Franks. In 734, at the Battle of the Boarn, the Frisians were defeated after a series of wars. With the approval of the Franks, the Anglo-Saxon missionary Willibrord converted the Frisian people to Christianity and established the Archdiocese of Utrecht. However, his successor Boniface was murdered by the Frisians in 754. The Frankish Carolingian empire controlled much of Western Europe. In 843, it was divided into three parts—East, Middle, and West Francia."
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_11",
    "chunk": "Most of present-day Netherlands became part of Middle Francia, which was a weak kingdom and subject to numerous partitions and annexation attempts by its stronger neighbours. It comprised territories from Frisia in the north to the Kingdom of Italy in the south. Around 850, Lothair I of Middle Francia acknowledged the Viking Rorik of Dorestad as ruler of most of Frisia. When the kingdom of Middle Francia was partitioned in 855, the lands north of the Alps passed to Lothair II and subsequently were named Lotharingia. After he died in 869, Lotharingia was partitioned, into Upper and Lower Lotharingia, the latter comprising the Low Countries that became part of East Francia in 870. Around 879, another Viking expedition led by Godfrid, Duke of Frisia, raided the Frisian lands. Resistance to the Vikings, if any, came from local nobles, who gained in stature as a result, and that laid the basis for the disintegration of Lower Lotharingia into semi-independent states. One of these local nobles was Gerolf of Holland, who assumed lordship in Frisia, and Viking rule came to an end. The Holy Roman Empire ruled much of the Low Countries in the 10th and 11th century but was not able"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_12",
    "chunk": "to maintain political unity. Powerful local nobles turned their cities, counties and duchies into private kingdoms that felt little sense of obligation to the emperor. Holland, Hainaut, Flanders, Gelre, Brabant, and Utrecht were in a state of almost continual war or paradoxically formed personal unions. As Frankish settlement progressed from Flanders and Brabant, the area quickly became Old Low Franconian (or Old Dutch). Around 1000 AD, agrarian conditions started to improve, which lead to increase in population, reclamation of wasteland by farmers, and steady growth of trade and industry. Towns grew around monasteries and castles, and a mercantile middle class began to develop in these urban areas, especially in Flanders, and later Brabant. Wealthy cities started to buy certain privileges for themselves from the sovereign. Around 1100 AD, farmers from Flanders and Utrecht began draining and cultivating uninhabited swampy land in the western Netherlands, making the emergence of the County of Holland as the centre of power possible. The title of Count of Holland was fought over in the Hook and Cod Wars between 1350 and 1490. The Cod faction consisted of the more progressive cities, while the Hook faction consisted of the conservative noblemen. These noblemen invited Duke Philip"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_13",
    "chunk": "the Good of Burgundy to conquer Holland. Most of the Imperial and French fiefs in what is now the Netherlands and Belgium were united in a personal union by Philip the Good in 1433. The House of Valois-Burgundy and their Habsburg heirs would rule the Low Countries from 1384 to 1581. The new rulers defended Dutch trading interests. The fleets of the County of Holland defeated the fleets of the Hanseatic League several times. Amsterdam grew and in the 15th century became the primary trading port in Europe for grain from the Baltic region. Amsterdam distributed grain to the major cities of Belgium, Northern France and England. This trade was vital because Holland could no longer produce enough grain to feed itself. Land drainage had caused the peat of the former wetlands to reduce to a level that was too low for drainage to be maintained. Under Habsburg Charles V, all fiefs in the current Netherlands region were united into the Seventeen Provinces, which included most of present-day Belgium, Luxembourg, and parts of France and Germany. In 1568, under Phillip II, the Eighty Years' War between the Provinces and their Spanish ruler began. The level of ferocity exhibited by both"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_14",
    "chunk": "sides can be gleaned from a Dutch chronicler's report: On more than one occasion men were seen hanging their own brothers, who had been taken prisoners in the enemy's ranks... A Spaniard had ceased to be human in their eyes. On one occasion, a surgeon at Veer cut the heart from a Spanish prisoner, nailed it on a vessel's prow, and invited the townsmen to come and fasten their teeth in it, which many did with savage satisfaction. The Duke of Alba attempted to suppress the Protestant movement in the Netherlands. Netherlanders were \"burned, strangled, beheaded, or buried alive\" by his \"Blood Council\" and Spanish soldiers. Bodies were displayed along roads to terrorise the population into submission. Alba boasted of having executed 18,600; this figure does not include those who perished by war and famine. The first great siege was Alba's effort to capture Haarlem and thereby cut Holland in half. It dragged on from December 1572 to the next summer, when Haarlemers finally surrendered on 13 July upon the promise that the city would be spared from being sacked. It was a stipulation Don Fadrique was unable to honour, when his soldiers mutinied, angered over pay owed and the"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_15",
    "chunk": "miserable conditions of the campaign. On 4 November 1576, Spanish tercios seized Antwerp and subjected it to the worst pillage in the Netherlands' history. The citizens resisted but were overcome; seven thousand were killed and a thousand buildings were torched. Following the sack of Antwerp, delegates from Catholic Brabant, Protestant Holland and Zeeland agreed to join Utrecht and William the Silent in driving out Spanish troops and forming a new government for the Netherlands. Don Juan of Austria, the new Spanish governor, was forced to concede initially, but within months returned to active hostilities. The Dutch looked for help from the Protestant Elizabeth I of England, but she initially stood by her commitments to the Spanish in the Treaty of Bristol of 1574. When the next large-scale battle occurred at Gembloux in 1578, the Spanish forces won easily. In light of the defeat at Gembloux, the southern states of the Seventeen Provinces distanced themselves from the rebels in the north with the 1579 Union of Arras. Opposing them, the northern half of the Seventeen Provinces forged the Union of Utrecht in which they committed to support each other against the Spanish. The Union of Utrecht is seen as the foundation"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_16",
    "chunk": "of the modern Netherlands. Spanish troops sacked Maastricht in 1579, killing over 10,000 civilians. In 1581, the northern provinces adopted the Act of Abjuration, the declaration of independence in which the provinces officially deposed Philip II. Against the rebels Philip could draw on the resources of the Spanish Empire. Elizabeth I sympathised with the Dutch struggle and sent an army of 7,600 soldiers to aid them. English forces faced the Spanish in the Netherlands under the Duke of Parma in a series of largely indecisive actions that tied down significant numbers of Spanish troops and bought time for the Dutch to reorganise their defences. The war continued until 1648, when Spain under King Philip IV recognised the independence of the seven north-western provinces in the Peace of Münster. Parts of the southern provinces became de facto colonies of the new republican-mercantile empire. Following the declaration of independence, the provinces of Holland, Zeeland, Groningen, Friesland, Utrecht, Overijssel, and Gelderland entered into a confederation. All these duchies, lordships and counties enjoyed a significant degree of autonomy and was governed by its own administrative body known as the States-Provincial. The confederal government, known as the States General, was headquartered in The Hague and"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_17",
    "chunk": "comprised representatives from each of the seven provinces. The sparsely populated region of Drenthe was part of the republic, albeit not considered a province in its own right. Moreover, during the Eighty Years' War, the Republic came to occupy a number of Generality Lands located in Flanders, Brabant and Limburg. These areas were primarily inhabited by Roman Catholics and lacked a distinct governmental structure of their own. They were utilized as a buffer zone between the Republic and the Spanish-controlled Southern Netherlands. In the Dutch Golden Age, spanning much of the 17th century, the Dutch Empire grew to become one of the major seafaring and economic powers. Science, military and art (especially painting) were among the most acclaimed in the world. By 1650, the Dutch owned 16,000 merchant ships. The Dutch East India Company and the Dutch West India Company established colonies and trading posts all over the world. The Dutch settlement in North America began with the founding of New Amsterdam in 1614. In South Africa, the Dutch settled the Cape Colony in 1652. Dutch colonies in South America were established along the many rivers in the fertile Guyana plains, among them Colony of Surinam (now Suriname). In Asia,"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_18",
    "chunk": "the Dutch established a presence in India, the Dutch East Indies (now Indonesia), Formosa (now Taiwan), and the only western trading post in Japan, Dejima. During the period of Proto-industrialisation, the empire received 50% of textiles and 80% of silks import from the India's Mughal Empire. Many economic historians regard the Netherlands as the first thoroughly capitalist country. In early modern Europe, it had the wealthiest trading city in Amsterdam, and the first full-time stock exchange. The inventiveness of the traders led to insurance and retirement funds as well as phenomena such as the boom-bust cycle, the world's first asset-inflation bubble, the tulip mania of 1636–1637, and the world's first bear raider, Isaac le Maire. In 1672 – known in Dutch history as the Rampjaar (Disaster Year) – the Dutch Republic was attacked by France, England and three German Bishoprics simultaneously, in what would become known as the Franco-Dutch War. At sea, it could successfully prevent the English and French navies from blockading the western shores. On land, however, it was almost taken over by the advancing French and German armies from the east. It managed to turn the tide by inundating parts of Holland. From 1672 to 1712, the"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_19",
    "chunk": "Republic, led by William III of Orange and Anthonie Heinsius would regularly clash with France in what some historians have come to call the Forty Years' War. In the Nine Years' War and the War of the Spanish Succession, the Republic was at the centre of anti-French coalitions. The Dutch ultimately successfully defended the Spanish Netherlands, established a barrier there, and their troops proved central to the alliance which halted French territorial expansion in Europe until a new cycle began in 1792 with the French Revolutionary Wars. However, the wars left them effectively bankrupt, and inflicted permanent damage on the Dutch merchant navy; while they remained the dominant economic power in the Far East, Britain took over as the pre-eminent global commercial and maritime power. Between 1590 and 1713, the United Provinces consistently possessed one of Europe's largest and most capable armies. However, following the conclusion of the War of the Spanish Succession, other major powers such as Prussia, Austria, Britain, and Russia significantly expanded their military forces. The Republic struggled to match these developments, and gradually assumed the status of a mid-tier power. However, historians have sometimes overstated the extent of this decline, especially when considering the period up"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_20",
    "chunk": "to the 1750s. In the 18th century the Dutch Republic had seen a state of a general decline, with economic competition from England and long-standing rivalries between the two main factions in Dutch society, the republican Staatsgezinden and the supporters of the stadtholder the Prinsgezinden as main political factions. With the armed support of revolutionary France, Dutch republicans proclaimed the Batavian Republic, modelled after the French Republic and rendering the Netherlands a unitary state on 19 January 1795. The stadtholder William V of Orange had fled to England. From 1806 to 1810, the Kingdom of Holland was set up by Napoleon Bonaparte as a puppet kingdom governed by his brother Louis Bonaparte. However, King Louis Bonaparte tried to serve Dutch interests instead of his brother's, and he was forced to abdicate on 1 July 1810. The Emperor sent in an army and the Netherlands became part of the French Empire until November 1813, when Napoleon was defeated in the Battle of Leipzig. William Frederick, son of the last stadtholder, returned to the Netherlands in 1813 and proclaimed himself Sovereign Prince. Two years later, the Congress of Vienna added the southern Netherlands to the north to create a strong country on"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_21",
    "chunk": "the northern border of France. William Frederick raised this United Netherlands to the status of a kingdom and proclaimed himself as King William I in 1815. William became hereditary Grand Duke of Luxembourg in exchange for his German possessions. However, the Southern Netherlands had been culturally separate from the north since 1581, and rebelled. The south gained independence in 1830 as Belgium (recognised by the Northern Netherlands in 1839 as the Kingdom of the Netherlands was created by decree), while the personal union between Luxembourg and the Netherlands was severed in 1890, when William III died with no surviving male heirs. Ascendancy laws prevented his daughter Queen Wilhelmina from becoming the next Grand Duchess. The Belgian Revolution and the Java War in the Dutch East Indies brought the Netherlands to the brink of bankruptcy. However, the Cultivation System was introduced in 1830; in the Dutch East Indies, 20% of village land had to be devoted to government crops for export. The policy brought the Dutch enormous wealth and made the colony self-sufficient. The Netherlands abolished slavery in its colonies in 1863. Enslaved people in Suriname would be fully free only in 1873. The Netherlands remained neutral during World War I,"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_22",
    "chunk": "in part because the import of goods through the Netherlands proved essential to German survival until the blockade by the British Royal Navy in 1916. That changed in World War II, when Germany invaded the Netherlands on 10 May 1940. The Rotterdam Blitz forced most of the Dutch army to surrender. During the occupation, over 100,000 Dutch Jews were transported to Nazi extermination camps; only a few survived. Dutch workers were conscripted for forced labour in Germany, civilians who resisted were killed in reprisal for attacks on German soldiers, and the countryside was plundered for food. Although there were thousands of Dutch who risked their lives by hiding Jews from the Germans, over 20,000 Dutch fascists joined the Waffen SS. Political collaborators were members of the fascist NSB, the only legal political party in the occupied Netherlands. On 8 December 1941, the Dutch government-in-exile in London declared war on Japan, but could not prevent the Japanese occupation of the Dutch East Indies. In 1944–45, the First Canadian Army liberated much of the Netherlands. Soon after VE Day, the Dutch fought a colonial war against the new Republic of Indonesia. In 1954, the Charter for the Kingdom of the Netherlands reformed"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_23",
    "chunk": "the political structure as a result of international pressure to carry out decolonisation. The Dutch colonies of Surinam and Curaçao and Dependencies and the European country all became countries within the Kingdom, on a basis of equality. Indonesia had declared its independence in August 1945. Suriname followed in 1975. The Netherlands was one of the founding members of Benelux and NATO. In the 1950s, the Netherlands became one of the six founding countries of the European Communities, following the 1952 establishment of the European Coal and Steel Community, and subsequent 1958 creations of the European Economic Community and European Atomic Energy Community. In 1993, the former two were incorporated into the European Union. Government-encouraged emigration efforts to reduce population density prompted some 500,000 Dutch to leave the country after the war. The 1960s and 1970s were a time of great social and cultural change, such as rapid de-pillarisation. Students and other youth rejected traditional mores and pushed for change in matters such as women's rights, sexuality, disarmament and environmental issues. In 2002 the euro was introduced as fiat money, and in 2010 the Netherlands Antilles was dissolved. Referendums were held on each island. As a result, Bonaire, Sint Eustatius and"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_24",
    "chunk": "Saba (the BES islands) were incorporated as special municipalities upon the dissolution of the Netherlands Antilles. The special municipalities are collectively known as the Caribbean Netherlands. The European Netherlands has a total area of 41,543 km (16,040 sq mi), including water bodies, and a land area of 33,481 km (12,927 sq mi). The Caribbean Netherlands has a total area of 328 km (127 sq mi) It lies between latitudes 50° and 54° N, and longitudes 3° and 8° E. The Netherlands is geographically very low relative to sea level and is considered a flat country, with about 26% of its area and 21% of its population below sea level. The European part of the country is for the most part flat, with the exception of foothills in the far southeast, up to a height of no more than 322 m (1,056 ft) at the Vaalserberg, and some low hill ranges in the central parts. Most of the areas below sea level are caused by peat extraction or achieved through land reclamation. Since the late 16th century, large polder areas are preserved through elaborate drainage systems that include dikes, canals and pumping stations. Much of the country was originally formed by"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_25",
    "chunk": "the estuaries of three large European rivers: the Rhine (Rijn), the Meuse (Maas) and the Scheldt (Schelde), as well as their tributaries. The south-western part of the Netherlands is a river delta of these rivers, the Rhine–Meuse–Scheldt delta. The European Netherlands is divided into north and south parts by the Rhine, the Waal, its main tributary branch, and the Meuse. These rivers functioned as a natural barrier between fiefdoms and hence historically created a cultural divide, as is evident in some phonetic traits that are recognisable on either side of what the Dutch call their \"Great Rivers\" (de Grote Rivieren). Another significant branch of the Rhine, the IJssel river, discharges into Lake IJssel, the former Zuiderzee ('southern sea'). Just like the previous, this river forms a linguistic divide: people to the northeast of this river speak Dutch Low Saxon dialects (except for the province of Friesland, which has its own language). The Netherlands is mostly composed of deltaic, coastal and aeolian derived sediments during the Pleistocene glacial and interglacial periods. Almost the entire west Netherlands is composed of the Rhine-Meuse river estuary. In the east of the Netherlands, remains are found of the last ice age, which ended approximately ten"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_26",
    "chunk": "thousand years ago. As the continental ice sheet moved in from the north, it pushed moraine forward. The ice sheet halted as it covered the eastern half of the Netherlands. After the ice age ended, the moraine remained in the form of a long hill-line. The cities of Arnhem and Nijmegen are built on these hills. Over the centuries, the Dutch coastline has changed considerably as a result of natural disasters and human intervention. On 14 December 1287, St. Lucia's flood affected the Netherlands and Germany, killing more than 50,000 people in one of the most destructive floods in recorded history. The St. Elizabeth flood of 1421 and the mismanagement in its aftermath destroyed a newly reclaimed polder, replacing it with the 72 km (28 sq mi) Biesbosch tidal floodplains. The huge North Sea flood of February 1953 caused the collapse of several dikes in the southwest Netherlands; more than 1,800 people drowned. The Dutch government subsequently instituted a large-scale programme, the \"Delta Works\", to protect the country against future flooding, which was completed over a period of more than 40 years. The impact of disasters was, to an extent, increased through human activity. Relatively high-lying swampland was drained to"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_27",
    "chunk": "be used as farmland. The drainage caused the fertile peat to contract and ground levels to drop; groundwater levels were lowered to compensate, causing the underlying peat to contract further. Additionally, until the 19th century peat was mined, dried, and used for fuel, further exacerbating the problem. Even in flooded areas, peat extraction continued through turf dredging. To guard against floods, a series of defences against the water were contrived. In the first millennium AD, villages and farmhouses were built on hills called terps. Later, these terps were connected by dikes. In the 12th century, local government agencies called \"waterschappen\" (\"water boards\") or \"hoogheemraadschappen\" (\"high home councils\") started to appear, whose job it was to maintain the water level and to protect a region from floods; these agencies continue to exist. As the ground level dropped, the dikes by necessity grew and merged into an integrated system. By the 13th century windmills had come into use to pump water. The windmills were later used to drain lakes, creating the famous polders. In 1932 the Afsluitdijk (\"Closure Dike\") was completed, blocking the former Zuiderzee (Southern Sea) from the North Sea and thus creating the IJsselmeer (IJssel Lake). It became part of"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_28",
    "chunk": "the larger Zuiderzee Works in which four polders totalling 2,500 square kilometres (965 sq mi) were reclaimed from the sea. The Netherlands is one of the countries that may suffer most from climate change. Not only is the rising sea a problem, but erratic weather patterns may cause the rivers to overflow. After the 1953 disaster, the Delta Works was constructed, which is a comprehensive set of civil works throughout the Dutch coast. The project started in 1958 and was largely completed in 1997 with the completion of the Maeslantkering. Since then, new projects have been periodically started to renovate and renew the Delta Works. The main goal of the Delta project was to reduce the risk of flooding in South Holland and Zeeland. This was achieved by raising 3,000 km (1,900 mi) of outer sea-dikes and 10,000 km (6,200 mi) of the inner, canal, and river dikes, and by closing off the sea estuaries of Zeeland. New risk assessments occasionally show problems requiring additional Delta project dike reinforcements. The Delta project is considered by the American Society of Civil Engineers as one of the seven wonders of the modern world. It is anticipated that global warming will result in"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_29",
    "chunk": "a rise in sea level. The Netherlands is actively preparing for a sea-level rise. A politically neutral Delta Commission has formulated an action plan to cope with a sea-level rise of 1.10 m (4 ft) and a simultaneous land height decline of 10 cm (4 in). The plan encompasses the reinforcement of existing coastal defences like dikes and dunes with 1.30 m (4.3 ft) of additional flood protection. Climate change will not only threaten the Netherlands from the coast, but could also alter rainfall patterns and river run-off. To protect the country from river flooding, another programme is already being executed. The Room for the River plan grants more flow space to rivers, protects the major populated areas and allows for periodic flooding of indefensible lands. The few residents who lived in these so-called \"overflow areas\" have been moved to higher ground, with some of that ground having been raised above anticipated flood levels. The Netherlands is already affected by climate change. The average temperature in the Netherlands rose by more than 2 °C from 1901 to 2020. Climate change has resulted in increased frequency of droughts and heatwaves. Because significant portions of the Netherlands have been reclaimed from the"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_30",
    "chunk": "sea or otherwise are very near sea level, the Netherlands is very vulnerable to sea level rise. The Netherlands has the fourth largest greenhouse gas emissions per capita of the European Union, in part due to the large number of cows. The Dutch government has set goals to lower emissions in the next few decades. The Dutch response to climate change is driven by a number of unique factors, including larger green recovery plans by the European Union in the face of the COVID-19 and a climate change litigation case, State of the Netherlands v. Urgenda Foundation, which created mandatory climate change mitigation through emissions reductions 25% below 1990 levels. In 2021 CO2 emissions were down 14% compared to 1990 levels. The goal of the Dutch government is to reduce emissions in 2030 by 49%. The Netherlands has 21 national parks and hundreds of other nature reserves. Most of these are owned by Staatsbosbeheer, the national department for forestry and nature conservation and Natuurmonumenten, a private organisation that buys, protects and manages nature reserves. The Wadden Sea in the north, with its tidal flats and wetlands, is rich in biological diversity, and is a UNESCO World Heritage Nature Site. The"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_31",
    "chunk": "Eastern Scheldt, formerly the northeast estuary of the river Scheldt was designated a national park in 2002, making it the largest national park in the Netherlands at an area of 370 km (140 sq mi). Phytogeographically, the European Netherlands is shared between the Atlantic European and Central European provinces of the Circumboreal Region within the Boreal Kingdom. According to the World Wide Fund for Nature, the European territory of the Netherlands belongs to the ecoregion of Atlantic mixed forests. In 1871, the last old original natural woods were cut down. These woods were planted on anthropogenic heaths and sand-drifts (overgrazed heaths) (Veluwe). The Netherlands had a 2019 Forest Landscape Integrity Index mean score of 0.6/10, ranking it 169th globally out of 172 countries. Nitrogen pollution is a problem. The number of flying insects in the Netherlands has dropped by 75% since the 1990s. In the Lesser Antilles islands of the Caribbean, the territories of Curaçao, Aruba and Sint Maarten have a constituent country status within the wider Kingdom of the Netherlands. Another three territories which make up the Caribbean Netherlands are designated as special municipalities. The Caribbean Netherlands have maritime borders with Anguilla, Curaçao, France (Saint Barthélemy), Saint Kitts and"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_32",
    "chunk": "Nevis, the U.S. Virgin Islands and Venezuela. The islands of the Caribbean Netherlands enjoy a tropical climate with warm weather all year round. The Netherlands has been a constitutional monarchy since 1815 and a parliamentary democracy since 1848. The Netherlands is described as a consociational state. Dutch politics and governance are characterised by an effort to achieve broad consensus on important issues. The Netherlands was ranked as the 17th best electoral democracy in the world by V-Dem Democracy indices in 2023 and 9th most democratic country in the world by the Democracy Index (The Economist) in 2022. The monarch is the head of state, at present King Willem-Alexander of the Netherlands. Constitutionally, the position is equipped with limited powers due to ministerial responsibility. The executive power is formed by the government that includes the monarch and the Council of Ministers, the deliberative organ of the Dutch cabinet. The cabinet usually consists of 13 to 16 ministers and a varying number of state secretaries. One to three ministers are ministers without portfolio. The council of ministers is presided over by the Prime Minister of the Netherlands, who often is the leader of the largest party of the coalition. The Prime Minister"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_33",
    "chunk": "is a primus inter pares, with no explicit powers beyond those of the other ministers. Dick Schoof has been Prime Minister since July 2024, succeeding the longest-serving Prime Minister Mark Rutte. The cabinet is responsible to the bicameral parliament, the States General, which also has legislative powers. The 150 members of the House of Representatives, the lower house, are elected in direct elections on the basis of party-list proportional representation. These are held every four years, or sooner in case the cabinet falls. The provincial assemblies, the States Provincial, are directly elected every four years as well. The members of the provincial assemblies elect the 75 members of the Senate, the upper house, which has the power to reject laws, but not amend them. Both trade unions and employers organisations are consulted in policymaking in the financial, economic and social areas. They meet regularly with the government in the Social-Economic Council. The Netherlands has a tradition of social tolerance. In the late 19th century this Dutch tradition of religious tolerance transformed into a system of pillarisation, in which religious groups coexisted separately and only interacted at the level of government. Protection for LGBT and abortion rights are enshrined within the"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_34",
    "chunk": "Netherlands' foreign aid policy. No single party has held a majority in parliament since the 19th century, and as a result, coalition cabinets had to be formed. Since male suffrage became universal in 1917, the Dutch political system has been dominated by three families of political parties: Christian Democrats (currently the CDA), Social Democrats (currently the PvdA), and Liberals (currently the VVD). In November 2023, the right-wing populist Party for Freedom of Geert Wilders was the winner of a general election, securing 37 out of 150 seats. A cabinet was inaugurated in July 2024, and Dick Schoof succeeded Mark Rutte as prime minister. The Netherlands is divided into twelve provinces, each under a King's Commissioner. All provinces are divided into municipalities (gemeenten), of which there are 342 (2023). The country is subdivided into 21 water districts, governed by a water board (waterschap or hoogheemraadschap), each having authority in matters concerning water management. The creation of water boards pre-dates that of the nation itself, the first appearing in 1196. The Dutch water boards are among the oldest democratic entities in the world still in existence. Direct elections of the water boards take place every four years. Within the Dutch town of"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_35",
    "chunk": "Baarle-Nassau, are 22 Belgian exclaves and within those are 8 Dutch enclaves. The administrative structure on the three BES islands, collectively known as the Caribbean Netherlands, is outside the twelve provinces. These islands have the status of openbare lichamen (public bodies). In the Netherlands these administrative units are often referred to as special municipalities. The history of Dutch foreign policy has been characterized by its neutrality. According to the 2024 Global Peace Index, Netherlands is the 18th most peaceful country in the world. Since World War II, the Netherlands has become a member of a large number of international organisations, most prominently the UN, NATO and the EU. The foreign policy of the Netherlands is based on four basic commitments: to Atlantic co-operation, to European integration, to international development and to international law. One of the more controversial international issues surrounding the Netherlands is its liberal policy towards soft drugs. The historical ties inherited from its colonial past in Indonesia and Suriname still influence the foreign relations of the Netherlands. Many with heritage from these countries now live permanently in the Netherlands. The Netherlands has one of the oldest standing armies in Europe; it was first established in the late"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_36",
    "chunk": "1500s. After the defeat of Napoleon, the Dutch army was transformed into a conscription army. The Netherlands abandoned its neutrality in 1948 when it signed the Treaty of Brussels, and became a founding member of NATO in 1949. The Dutch military was therefore part of the NATO strength in Cold War Europe. In 1983 the (ceremonial) function of commander of chief of the monarch was transferred to the government, which means the monarch (nominal head of state) has no formal military function. In 1996 conscription was suspended, and the Dutch army was once again transformed into a professional army. Since the 1990s the Dutch army has been involved in the Bosnian War and the Kosovo War, it held a province in Iraq after the defeat of Saddam Hussein, and it was engaged in Afghanistan. The Netherlands has ratified many international conventions concerning war law. The Netherlands decided not to sign the UN treaty on the Prohibition of Nuclear Weapons. The military is composed of four branches, all of which carry the prefix Koninklijke (Royal): The submarine service opened to women on 1 January 2017. The Korps Commandotroepen, the Special Operations Force of the Netherlands Army, is open to women, but"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_37",
    "chunk": "because of the extremely high physical demands for initial training, it is almost impossible for a woman to become a commando. The Dutch Ministry of Defence employs more than 70,000 personnel, including over 20,000 civilians and over 50,000 military personnel. Since the 16th century, shipping, fishing, agriculture, trade, and banking have been leading sectors of the Dutch economy. The Netherlands has a high level of economic freedom. The Netherlands is one of the top countries in the Global Enabling Trade Report (2nd in 2016), and was ranked the fifth most competitive economy in the world by the Swiss International Institute for Management Development in 2017. The country was ranked the 8th most innovative nation in the world in the 2024 Global Innovation Index down from 2nd in 2018. As of 2020, the key trading partners of the Netherlands were Germany, Belgium, the United Kingdom, the United States, France, Italy, China and Russia. The Netherlands is one of the world's 10 leading exporting countries. Foodstuffs form the largest industrial sector. Other major industries include chemicals, metallurgy, machinery, electrical goods, trade, services and tourism. Examples of international Dutch companies operating in the Netherlands include Randstad NV, Heineken, KLM, financial services (ING, ABN"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_38",
    "chunk": "AMRO, Rabobank), chemicals (DSM, AKZO), petroleum refining (Shell plc), electronic machinery (Philips, ASML), and satellite navigation (TomTom). The Netherlands has the 17th-largest economy in the world, and ranks 11th in GDP (nominal) per capita. The Netherlands has low income inequality, but wealth inequality is relatively high. Despite ranking 11th in GDP per capita, UNICEF ranked the Netherlands 1st in child well-being in rich countries, both in 2007 and in 2013. Amsterdam is the financial and business capital of the Netherlands. The Amsterdam Stock Exchange (AEX), part of Euronext, is the world's oldest stock exchange and is one of Europe's largest bourses. As a founding member of the euro, the Netherlands replaced (for accounting purposes) its former currency, the \"gulden\" (guilder), on 1 January 1999. Actual euro coins and banknotes followed on 1 January 2002. One euro was equivalent to 2.20371 Dutch guilders. In the Caribbean Netherlands, the United States dollar is used instead. The Netherlands is a \"conduit country\" that helps to funnel profits from high-tax countries to tax havens. It has been ranked as the 4th largest tax haven in the world. The Dutch location gives it prime access to markets in the United Kingdom and Germany, with the"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_39",
    "chunk": "Port of Rotterdam being the largest port in Europe. Other important parts of the economy are international trade, banking and transport. The Netherlands successfully addressed the issue of public finances and stagnating job growth long before its European partners. Amsterdam is the 5th-busiest tourist destination in Europe, with more than 4.2 million international visitors. Since the enlargement of the EU, large numbers of migrant workers have arrived in the Netherlands from Central and Eastern Europe. The Netherlands continues to be one of the leading European nations for attracting foreign direct investment and is one of the five largest investors in the United States. The economy experienced a slowdown in 2005, but in 2006 recovered to the fastest pace in six years on the back of increased exports and strong investment. The pace of job growth reached 10-year highs in 2007. The Netherlands is the fourth-most competitive economy in the world, according to the World Economic Forum's Global Competitiveness Report. Beginning in the 1950s, the Netherlands discovered huge natural gas resources. The sale of natural gas generated enormous revenues for the Netherlands for decades, adding, over sixty years, hundreds of billions of euros to the government's budget, about 1.5% of GNP."
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_40",
    "chunk": "However, the unforeseen consequences of the country's huge energy wealth impacted the competitiveness of other sectors of the economy, leading to the theory of Dutch disease. The field is operated by government-owned Gasunie and output is jointly exploited by the government, Royal Dutch Shell, and ExxonMobil. Gas production caused earthquakes which damaged housing. After a large public backlash, the government decided to phase out gas production from the field. The Netherlands has made notable progress in its transition to a carbon-neutral economy. Thanks to increasing energy efficiency, energy demand shows signs of decoupling from economic growth. The share of energy from renewable sources doubled from 2008 to 2019, with especially strong growth in offshore wind and rooftop solar. However, the Netherlands remains heavily reliant on fossil fuels and has a concentration of energy- and emission-intensive industries that will not be easy to decarbonise. Its 2019 Climate Agreement defines policies and measures to support the achievement of Dutch climate targets and was developed through a collaborative process involving parties from across Dutch society. As of 2018, the Netherlands had one of the highest rates of carbon dioxide emissions per person in the European Union. The Netherlands' biocapacity totals only 0.8 global"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_41",
    "chunk": "hectares per person in 2016, 0.2 of which are dedicated to agriculture. The Dutch biocapacity per person is just about half of the 1.6 global hectares of biocapacity per person available worldwide. In contrast, in 2016, the Dutch used on average 4.8 global hectares of biocapacity – their ecological footprint of consumption. As a result, the Netherlands was running a biocapacity deficit of 4.0 global hectares per person in 2016. The Dutch waste more food than any other EU citizen, at over three times the EU average. The Dutch agricultural sector is highly mechanised, and has a strong focus on international exports. It employs about 4% of the Dutch labour force but produces large surpluses in the food-processing industry and accounts for 21% of the Dutch total export value. The Dutch rank first in the European Union and second worldwide in value of agricultural exports, behind only the United States, with agricultural exports earning €80.7 billion in 2014, up from €75.4 billion in 2012. In 2019 agricultural exports were worth €94.5 billion. In an effort to reduce agricultural pollution, the Dutch government is imposing strict limits on the productivity of the farming sector, triggering Dutch farmers' protests. One-third of the"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_42",
    "chunk": "world's exports of chilis, tomatoes, and cucumbers go through the country. The Netherlands exports one-fifteenth of the world's apples. A significant portion of Dutch agricultural exports consists of fresh-cut plants, flowers, and flower bulbs, with the Netherlands exporting two-thirds of the world's total. The Netherlands had an estimated population of 17,947,406 as of 31 November 2023. It is the 6th most densely populated country in Europe and the 33rd most densely populated country in the world with a density of 424 per square kilometre (1,100/sq mi). Between 1900 and 1950, the country's population almost doubled from 5.1 to 10 million. From 1950 to 2000, the population further increased, to 15.9 million. The fertility rate in the Netherlands is 1.78 children per woman (2018 estimate), which is high compared with many other European countries, but below the rate of 2.1 children per woman required for natural population replacement. The Netherlands has one of the oldest populations in the world, with the average age of 42.7 years. Life expectancy is high in the Netherlands: 84.3 years for newborn girls and 79.7 for boys (2020 estimate). The Dutch are the tallest people in the world, by nationality, with an average height of 1.81"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_43",
    "chunk": "metres (5 ft 11.3 in) for men and 1.67 metres (5 ft 5.7 in) for women in 2009. The average height of young men in the Netherlands increased from 5 feet, 4 inches to approximately 6 feet between the 1850s until the early 2000s. The country has a migration rate of 1.9 migrants per 1,000 inhabitants per year. The majority of the population of the Netherlands is ethnically Dutch. In 2022, the population was 74.8% ethnically Dutch, 8.3% other European, 2.4% Turkish, 2.4% Moroccan, 2.0% Indonesian, 2.0% Surinamese, and 8.1% others. Some 150,000 to 200,000 people living in the Netherlands are expatriates, mostly concentrated in and around Amsterdam and The Hague, now constituting almost 10% of the population of these cities. Significant minorities in the country include Frisians 700,000, Jews 41,000-45,000 and the Roma and the Sinti 40,000. According to Eurostat, in 2010 there were 1.8 million foreign-born residents in the Netherlands, corresponding to 11.1% of the total population. Of these, 1.4 million (8.5%) were born outside the EU and 0.43 million (2.6%) were born in another EU Member State. In 2022, there were 4.4 million residents in the Netherlands with at least one foreign-born parent. Over half the young"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_44",
    "chunk": "people in Amsterdam and Rotterdam have a non-western background. Dutch people, or descendants of Dutch people, are found in migrant communities worldwide, notably in South Africa and the United States. The Randstad is the country's largest conurbation located in the west of the country and contains the four largest cities: Amsterdam in the province North Holland, Rotterdam and The Hague in the province South Holland, and Utrecht in the province Utrecht. The Randstad has a population of about 8.2 million inhabitants and is the 5th largest metropolitan area in Europe. According to Dutch Central Statistics Bureau, in 2015, 28 per cent of the Dutch population had a spendable income above 45,000 euros (which does not include spending on health care or education). The official language of the Netherlands is Dutch, which is spoken by the vast majority of inhabitants. The dialects most spoken in the Netherlands are the Brabantian-Hollandic dialects. Besides Dutch, West Frisian is recognised as a second official language in the northern province of Friesland (Fryslân in West Frisian). West Frisian has a formal status for government correspondence in that province. Four other languages are protected under the European Charter for Regional or Minority Languages. The first of"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_45",
    "chunk": "these recognised regional languages is Low Saxon (Nedersaksisch in Dutch). Low Saxon consists of several dialects of the Low German language spoken in the north and east of the Netherlands, like Tweants in the region of Twente, and Drents in the province of Drenthe. Limburgish is recognised as a regional language. It consists of Dutch varieties of Meuse-Rhenish and is spoken in the south-eastern province of Limburg. Yiddish and the Romani language were recognised in 1996 as non-territorial languages. English has a formal status in the special municipalities of Saba and Sint Eustatius. It is widely spoken on these islands. Papiamento has a formal status in the special municipality of Bonaire. The Netherlands has a long tradition of learning foreign languages, formalised in Dutch education laws. Some 90% of the total population are able to converse in English, 70% in German, and 29% in French. English is a mandatory course in all secondary schools. In most lower level secondary school educations (vmbo), one additional modern foreign language is mandatory during the first two years. In higher level secondary schools (havo and vwo), the acquisition of two additional modern foreign language skills is mandatory. Besides English, the standard modern languages are"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_46",
    "chunk": "French and German, although schools can replace one of these with Chinese, Spanish, Russian, Italian, Turkish or Arabic. Additionally, schools in Friesland teach and have exams in West Frisian. Forms of Christianity have dominated religious life in what is now the Netherlands for more than 1,200 years, and by the middle of the sixteenth century the country was strongly Protestant (Calvinist). The population was predominantly Christian until the late 20th century. Although significant religious diversity remains, there has been a decline of religious adherence. In 2020, Statistics Netherlands found that 55% of the total population declared itself non-religious. Groups that represent the non-religious in the Netherlands include Humanistisch Verbond. Catholics comprised 19.8% of the total population, Protestants (14.4%). Muslims comprised 5.2% of the total population and followers of other Christian denominations and other religions (like Judaism, Buddhism and Hinduism) comprised the remaining 5.1%. A 2015 survey from another source found that Protestants outnumbered Catholics. The southern provinces of North Brabant and Limburg have historically been strongly Catholic, and some residents consider the Catholic Church as a base for their cultural identity. Protestantism in the Netherlands consists of a number of churches within various traditions. The largest of these is the"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_47",
    "chunk": "Protestant Church in the Netherlands (PKN), a united church which is Calvinist and Lutheran in orientation. It was formed in 2004 as a merger of the Dutch Reformed Church, the Reformed Churches in the Netherlands and a smaller Lutheran Church. Several orthodox Calvinist and liberal churches did not merge into the PKN. Although Christianity has become a minority in the Netherlands, it contains a Bible Belt from Zeeland to the northern parts of the province Overijssel, in which Protestant beliefs remain strong. Several Christian religious holidays are national holidays (Christmas, Easter, Pentecost, and the Ascension of Jesus). Islam is the second largest religion in the state. The Muslim population increased from the 1960 as a result of large numbers of migrant workers. This included migrant workers from Turkey and Morocco, as well as migrants from former Dutch colonies, such as Surinam and Indonesia. During the 1990s, Muslim refugees arrived from countries like Bosnia and Herzegovina, Iran, Iraq, Somalia, and Afghanistan. Since 2000 there has been raised awareness of religion, mainly due to Islamic extremism. Another religion practised is Hinduism, with around 215,000 adherents (slightly over 1% of the population). Most of these are Indo-Surinamese. There are sizeable populations of Hindu"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_48",
    "chunk": "immigrants from India and Sri Lanka, and Western adherents of Hinduism-orientated new religious movements such as Hare Krishnas. The Netherlands has an estimated 250,000 Buddhists or people strongly attracted to this religion, mainly ethnic Dutch people. There are about 30,000 Jews in the Netherlands, though the Institute for Jewish Policy Research estimates range from 30,000 to 63,000, depending on how the number is calculated. The Constitution of the Netherlands guarantees freedom of education, which means that all schools that adhere to general quality criteria receive the same government funding. This includes schools based on religious principles by religious groups (especially Catholic and Protestant). Three political parties in the Dutch parliament, (CDA, and two small parties, ChristianUnion and SGP) are based upon the Christian belief. Several Christian religious holidays are national holidays (Christmas, Easter, Pentecost and the Ascension of Jesus). A survey in December 2014 concluded that for the first time there were more atheists (25%) than theists (17%) in the Netherlands, while the remainder of the population was agnostic (31%) or ietsistic (27%). In 2015, a vast majority of the inhabitants of the Netherlands (82%) said they had never or almost never visited a church, and 59% stated that they"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_49",
    "chunk": "had never been to a church. Of all the people questioned, 24% saw themselves as atheist, an increase of 11% compared to the previous study done in 2006. The expected rise of spirituality has come to a halt according to research in 2015. In 2006, 40% of respondents considered themselves spiritual; in 2015 this has dropped to 31%. The number who believed in the existence of a higher power fell from 36% to 28% over the same period. Education in the Netherlands is compulsory between the ages of 5 and 16. If a child does not have a \"starting qualification\" (HAVO, VWO or MBO 2+ degree) they are still forced to attend classes until they achieve such a qualification or reach the age of 18. Children in the Netherlands attend elementary school from (on average) ages 4 to 12. It has eight grades and the first is facultative. Based on an aptitude test, the eighth grade teacher's recommendation and the opinion of the pupil's parents or caretakers, a choice is made for one of the three main streams of secondary education. The VMBO has four grades and is subdivided over several levels. Successfully completing the VMBO results in a low-level"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_50",
    "chunk": "vocational degree that grants access to the MBO. The MBO (middle-level applied education) is a form of education that primarily focuses on teaching a practical trade or a vocational degree. With the MBO certification, a student can apply for the HBO. The HAVO has 5 grades and allows for admission to the HBO. The HBO (higher professional education) are universities of professional education (applied sciences) that award professional bachelor's degrees; similar to polytechnic degrees. An HBO degree gives access to the university system. The VWO (comprising atheneum and gymnasium) has 6 grades and prepares for studying at a research university. Universities offer a three-year bachelor's degree, followed by a one or two-year master's degree, which in turn can be followed by a doctoral degree programme. Doctoral candidates in the Netherlands are generally non-tenured employees of a university. All Dutch schools and universities are publicly funded and managed with the exception of religious schools. Dutch universities have a tuition fee of about 2,000 euros a year for students from the Netherlands and the EU, and 15,000 euros for non-EU students. In 2016, the Netherlands maintained its position at the top of the annual Euro Health Consumer Index (EHCI), which compares healthcare"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_51",
    "chunk": "systems in Europe, scoring 916 of a maximum 1,000 points. The Netherlands has been among the top three countries in each report published since 2005. On 48 indicators such as patient rights and information, accessibility, prevention and outcomes, the Netherlands secured its top position among 37 European countries for six years in a row. The Netherlands was ranked first in a study in 2009 comparing the health care systems of the United States, Australia, Canada, Germany and New Zealand. According to the Health Consumer Powerhouse (HCP), patients have a great degree of freedom from where to buy their health insurance, to where they get their healthcare. Healthcare decisions are made in dialogue between patients and healthcare professionals. Healthcare in the Netherlands is split 3 ways: in somatic and mental health care and in 'cure' (short term) and 'care' (long term). Home doctors (huisartsen, comparable to general practitioners) form the largest part of the first level. Being referred by a member of the first level is mandatory for access to the second and third level. The health care system is, in comparison to other Western countries, effective but not the most cost-effective. Healthcare is financed by a dual system that came"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_52",
    "chunk": "into effect in January 2006. Long-term treatments, especially those that involve semi-permanent hospitalisation, and disability costs such as wheelchairs, are covered by a state-controlled mandatory insurance. In 2009 this insurance covered 27% of all health care expenses. Other sources of health care payment are taxes (14%), out of pocket payments (9%), additional optional health insurance packages (4%) and a range of other sources (4%). Health insurance in the Netherlands is mandatory. Healthcare in the Netherlands is covered by two statutory forms of insurance: While Dutch residents are automatically insured by the government for AWBZ, everyone has to buy their own basic healthcare insurance, except those under 18 who are automatically covered under their parents. Insurance companies are obliged to provide a package with a defined set of insured treatments. This insurance covers 41% of all health care expenses. Insurers have to offer a universal package for everyone over 18, regardless of age or state of health – it is illegal to refuse an application or impose special conditions. The funding burden for all short-term health care coverage is carried 50% by employers, 45% by the insured person and 5% by the government. Those on low incomes receive compensation to help"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_53",
    "chunk": "them pay their insurance. Premiums paid by the insured are about €135 per month. Mobility on Dutch roads has grown continuously since the 1950s and now exceeds 200 billion km travelled per year, three quarters of which are done by car. Around half of all trips in the Netherlands are made by car, 25% by bicycle, 20% walking, and 5% by public transport. The Netherlands has one of the densest road networks in the world. The Netherlands has a relatively high uptake of electric vehicles, as the government implemented ambitious policy on both charging infrastructure and tax benefits. As of 2019, the Netherlands hosts approximately 30% of all recharging stations in the European Union. Moreover, newly sold cars in the Netherlands have on average the lowest CO2 emissions in the EU. About 13% of all distance is travelled by public transport, the majority of which is by train. The Dutch rail network of 3,013 km route is also rather dense. The network is mostly focused on passenger rail services and connects all major cities, with over 400 stations. Trains are frequent, with two trains per hour on lesser lines, two to four trains per hour on average, and up to"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_54",
    "chunk": "eight trains an hour on the busiest lines. The Dutch national train network includes the HSL-Zuid, a high-speed line between the Amsterdam metropolitan area and the Belgian border for trains running from Paris and London, to the Netherlands. Cycling is a ubiquitous mode of transport. Almost as many kilometres are covered by bicycle as by train. The Dutch are estimated to have at least 18 million bicycles, which makes more than one per capita, and twice as many as the circa 9 million motor vehicles on the road. In 2013, the European Cyclists' Federation ranked the Netherlands and Denmark as the most bike-friendly countries in Europe. Cycling infrastructure is extensive. Busy roads have received some 35,000 km of dedicated cycle tracks, physically segregated from motorised traffic. Busy junctions are often equipped with bicycle-specific traffic lights. There are large bicycle parking facilities, particularly in city centres and train stations. The Port of Rotterdam is the largest port in Europe and the largest port outside East Asia, with the rivers Meuse and Rhine providing excellent access to the hinterland upstream. As of 2022, Rotterdam was the world's tenth largest container port. The port's main activities are petrochemical industries and general cargo handling"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_55",
    "chunk": "and transshipment. The harbour functions as an important transit point for bulk materials and between the European continent and overseas. The Volkeraksluizen between Rotterdam and Antwerp are the biggest sluices for inland navigation in terms of tonnage. In 2007, the Betuweroute, a new fast freight railway from Rotterdam to Germany, was completed. Amsterdam is Europe's 4th largest port. The inland shipping fleet of the Netherlands is the largest in Europe. Passenger boats in the Netherlands includes a ferry network in Amsterdam, and waterbusses and taxis in Rotterdam. Schiphol Airport, just southwest of Amsterdam, is the main international airport in the Netherlands, and the third busiest airport in Europe by number of passengers. Schiphol is the main hub for KLM, the nation's flag carrier and the world's oldest airline. In 2016, the Royal Schiphol Group airports handled 70 million passengers. All air traffic is international and Schiphol Airport is connected to over 300 destinations worldwide, more than any other European airport. The airport is a major freight hub as well, processing 1.44 million tonnes of cargo in 2020. Smaller international airports are located in or near Eindhoven, Rotterdam, Maastricht and Groningen. Air transport is of vital significance for the Caribbean part"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_56",
    "chunk": "of the Netherlands, with all islands having their own airport. This includes the shortest runway in the world on Saba. The Netherlands has had many well-known painters. In the Middle Ages Hieronymus Bosch and Pieter Bruegel the Elder were leading Dutch pioneers. During the Dutch Golden Age, the Dutch Republic was prosperous and witnessed a flourishing artistic movement. The \"Dutch Masters\", spanning this 17th century era, included Rembrandt, Johannes Vermeer, Jan Steen, and Jacob van Ruisdael. Famous Dutch painters of the 19th and 20th century included Vincent van Gogh and Piet Mondrian. Literature flourished during the Dutch Golden Age, with Joost van den Vondel and P. C. Hooft as the most famous writers. In the 19th century, Multatuli wrote about the poor treatment of the natives in the Dutch colony. Diary of a Young Girl by Anne Frank is the most translated book from Dutch. Other important 20th century authors include Harry Mulisch, Jan Wolkers, Hella Haasse, Willem Frederik Hermans, Cees Nooteboom and Gerard Reve. Various architectural styles can be distinguished in the Netherlands. The Romanesque architecture was built between 950 and 1250. Gothic architecture was used from about 1230. Gothic buildings had large windows, pointed arches and were richly"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_57",
    "chunk": "decorated. Brabantine Gothic originated with the rise of the Duchy of Brabant and spread throughout the Burgundian provinces. Dutch Baroque architecture (1525–1630) and classicism (1630–1700) is especially evident in the west. Other architectural styles are Art Nouveau, Expressionism, De Stijl, Traditionalism and Brutalism. Erasmus and Spinoza were famous Dutch philosophers. The Dutch scientist Christiaan Huygens (1629–95) discovered Saturn's moon Titan, argued that light travelled as waves, invented the pendulum clock, and was the first physicist to use mathematical formulae. Antonie van Leeuwenhoek was the first to observe and describe single-celled organisms with a microscope. Windmills, tulips, clogs, cheese, and cannabis have grown to symbolize the Netherlands, especially among tourists. The Dutch are proud of their cultural heritage, rich history in art, and involvement in international affairs. A predominant attitude in the Netherlands is to think of the nation as being \"both tolerant and cosmopolitan.\" A Dutch saying indicating their sense of national pride in their reclamation of land from the sea and marshes is \"God created the world, but the Dutch created the Netherlands.\" Dutch manners are open and direct with a no-nonsense attitude—informality combined with adherence to basic behaviour. \"Dealing with the Dutch\" by Jacob Vossestein states: \"Dutch egalitarianism"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_58",
    "chunk": "is the idea that people are equal, especially from a moral point of view, and accordingly, causes the somewhat ambiguous stance the Dutch have towards hierarchy and status.\" The Netherlands is one of the most secular countries of Europe. Religion in the Netherlands is generally considered as a personal matter which is not supposed to be propagated in public, although it often remains a discussion subject. The Netherlands has multiple music traditions. Traditional Dutch music is a genre known as \"Levenslied\", meaning Song of life. These songs typically have a simple melody and rhythm, and a straightforward structure of verses and choruses. Themes can be light, but are often sentimental and include love, death and loneliness. Traditional musical instruments such as the accordion and the barrel organ are a staple of levenslied music, though in recent years many artists use synthesisers and guitars. Contemporary Dutch rock and pop music (Nederpop) originated in the 1960s, heavily influenced by popular music from the United States and Britain. Bands such as Shocking Blue, Golden Earring, Tee Set, George Baker Selection and Focus enjoyed international success. From the 1980s, more and more pop musicians started working in the Dutch language, partly inspired by the"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_59",
    "chunk": "huge success of the band Doe Maar. Current symphonic metal bands Epica, Delain, ReVamp, The Gathering, Asrai, Autumn, Ayreon and Within Temptation as well as jazz and pop singer Caro Emerald are having international success. Metal bands like Hail of Bullets, God Dethroned, Izegrim, Asphyx, Textures, Heidevolk, and Slechtvalk are popular guests at the biggest metal festivals in Europe. Contemporary local stars include pop singer Anouk, country pop singer Ilse DeLange, Limburgish dialect singing folk band Rowwen Hèze, rock band BLØF and duo Nick & Simon. Early 1990s Dutch and Belgian house music came together in Eurodance project 2 Unlimited. Selling 18 million records, the two singers in the band are the most successful Dutch music artists to this day. Tracks like \"Get Ready for This\" are still popular themes of U.S. sports events. In the mid-1990s Dutch language rap and hip hop (Nederhop) came to fruition and has become popular in the Netherlands and Belgium. Since the 1990s, Dutch electronic dance music (EDM) gained widespread popularity in the world in many forms. Some of the world's best known dance music DJs hail from the Netherlands, including Armin van Buuren, Tiësto, Hardwell, Martin Garrix, Dash Berlin, Julian Jordan, Nicky Romero,"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_60",
    "chunk": "W&W, Don Diablo, Ummet Ozcan, Headhunterz, Sander van Doorn, and Afrojack; the first four of which have been ranked as best in the world by DJ Mag Top 100 DJs. The Amsterdam Dance Event (ADE) is the world's leading electronic music conference and the biggest club festival for the many electronic subgenres on the planet. The Netherlands has participated in the Eurovision Song Contest since its first edition in 1956, and has won five times. In classical music, Jan Sweelinck is a famous Dutch composer, with Louis Andriessen among the best known contemporary Dutch classical composers. Ton Koopman is a Dutch conductor, organist and harpsichordist. Notable violinists are Janine Jansen and André Rieu. Some Dutch films – mainly by director Paul Verhoeven – have received international distribution and recognition, such as Turkish Delight (\"Turks Fruit\", 1973), Soldier of Orange (\"Soldaat van Oranje\", 1977), Spetters (1980), and The Fourth Man (\"De Vierde Man\", 1983). Verhoeven then went on to direct big Hollywood movies like RoboCop (1987), Total Recall (1990), and Basic Instinct (1992), and returned with Dutch film Black Book (\"Zwartboek\", 2006). Other well-known Dutch film directors are Jan de Bont, Anton Corbijn, Dick Maas, Fons Rademakers, and documentary makers Bert"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_61",
    "chunk": "Haanstra and Joris Ivens. Film director Theo van Gogh achieved international notoriety in 2004 when he was murdered by Mohammed Bouyeri in the streets of Amsterdam after directing the short film Submission. Directors of photography from the Netherlands include Hoyte van Hoytema and Theo van de Sande. Internationally successful Dutch actors include Famke Janssen, Carice van Houten, Rutger Hauer, and Jeroen Krabbé. The Netherlands has a well developed television market, with both multiple commercial and public broadcasters. Imported TV programmes, as well as interviews with responses in a foreign language, are virtually always shown with the original sound and subtitled. Only foreign shows for children are dubbed. TV exports from the Netherlands mostly take the form of specific formats and franchises, most notably was the internationally active TV production conglomerate Endemol, founded by Dutch media tycoons John de Mol and Joop van den Ende. Endemol and its subsidiaries created and ran reality, talent, and game show franchises worldwide, including Big Brother and Deal or No Deal. Endemol merged with Shine Group in 2015, and again with Banijay in 2020. Approximately 4.5 million of the 16.8 million people in the Netherlands are registered in one of the 35,000 sports clubs in"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_62",
    "chunk": "the country. About two-thirds of the population between 15 and 75 participate in sports weekly. Football is the most popular team sport, followed by field hockey and volleyball. Tennis, gymnastics and golf are the three most widely engaged in individual sports. Organisation of sports began at the end of the 19th century and beginning of the 20th century. Federations for sports were established, rules were unified and sports clubs came into existence. A Dutch National Olympic Committee was established in 1912. The national football team was runner-up in the World Cup of 1974, 1978, and 2010, and won the European Championship of 1988. Of SI's 50 greatest footballers of all time, Johan Cruyff (#5), Marco van Basten (#19), Ruud Gullit (#25), and Johan Neeskens (#36) are Dutch. The women's national team was runner-up in 2019 World Cup and won the European Championship of 2017. The Netherlands women's field hockey team won 9 out of 15 World Cups. The Netherlands baseball team have won the European championship 24 times out of 33 events. The volleyball national women's team won the European Championship in 1995 and the World Grand Prix in 2007. The Netherlands has won 266 medals at the Summer Olympic"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_63",
    "chunk": "Games and 110 at the Winter Olympic Games. Joop Zoetemelk won the 1979 Vuelta a Espana, the 1980 Tour de France, and the 1985 UCI World Championship. Jan Janssen won the 1968 Tour de France, Tom Dumoulin the 2017 Giro d'Italia. Max Verstappen, the youngest Formula 1 driver to make his debut and to win a race, was the first Dutchman to win a Grand Prix and a Formula One World Drivers Championship. Dutch K-1 kickboxers have won the K-1 World Grand Prix 15 times out of 19 tournaments. Dutch cuisine is simple and straightforward, and contains many dairy products. Breakfast and lunch are typically bread with toppings, with cereal for breakfast as an alternative. Traditionally, dinner consists of potatoes, meat, and vegetables. The Dutch diet was high in carbohydrates and fat, reflecting the dietary needs of the labourers whose culture moulded the country. During the twentieth century this diet changed and became more cosmopolitan, with most global cuisines being represented in the major cities. In early 2014, Oxfam ranked the Netherlands as the country with the most nutritious, plentiful and healthy food. Modern culinary writers distinguish between three regional forms of Dutch cuisine: northeast, west and south: The regions"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_64",
    "chunk": "in the northeast are the least populated areas of the Netherlands. The late 18th century introduction of large scale agriculture means the cuisine is known for its meats. The relative lack of farms allowed for an abundance of game and husbandry, though dishes near the coastal regions include a large amount of fish. The various dried sausages, belonging to the metworst-family of Dutch sausages are found throughout this region. Smoked sausages are common, of which (Gelderse) rookworst is the most renowned. Larger sausages are eaten alongside stamppot, hutspot, or zuurkool (sauerkraut); whereas smaller ones are eaten as a street food. The provinces are home to hard textured rye bread, pastries and cookies. As a coastal region, Friesland is home to low-lying grasslands, and thus has a cheese production in common with the Western cuisine. Cookies are produced in great number and contain a lot of butter and sugar. The traditional alcoholic beverages are beer (strong pale lager) and Jenever, a high proof juniper-flavoured spirit, that came to be known in England as gin. An exception within the traditional Dutch alcoholic landscape, Advocaat, a rich and creamy liqueur made from eggs, sugar and brandy, is native to this region. In the"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_65",
    "chunk": "west, the abundance of water and flat grasslands, mean the area is known for its dairy products, which include prominent cheeses such as Gouda, Leyden (spiced cheese with cumin), and Edam (traditionally in small spheres) as well as Leerdammer and Beemster, while the adjacent Zaanstreek in North Holland has since the 16th century been known for its mayonnaise and typical whole-grain mustards. A by-product of the butter-making process, karnemelk (buttermilk), is considered typical for this region. Seafood such as soused herring, mussels, eels, oysters and shrimps are widely available and typical for the region. The southern Dutch cuisine consists of the cuisines of the Dutch provinces of North Brabant and Limburg and the Flemish Region in Belgium. It is renowned for its rich pastries, soups, stews and vegetable dishes. It is the only Dutch culinary region that developed an haute cuisine. Pastries are abundant, often with rich fillings of cream, custard or fruits. Cakes, such as the Vlaai from Limburg and the Moorkop and Bossche bol from Brabant, are typical pastries. Savoury pastries abound, with the worstenbroodje (a roll with a sausage of ground beef, literally translates into sausage bread) being the most popular. The alcoholic beverage of the region"
  },
  {
    "source": "Netherlands.txt",
    "chunk_id": "Netherlands.txt_66",
    "chunk": "is beer, there are many local brands, ranging from Trappist to Kriek."
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_0",
    "chunk": "# Neutrino astronomy Neutrino astronomy is the branch of astronomy that gathers information about astronomical objects by observing and studying neutrinos emitted by them with the help of neutrino detectors in special Earth observatories. It is an emerging field in astroparticle physics providing insights into the high-energy and non-thermal processes in the universe. Neutrinos are nearly massless and electrically neutral or chargeless elementary particles. They are created as a result of certain types of radioactive decay, nuclear reactions such as those that take place in the Sun or high energy astrophysical phenomena, in nuclear reactors, or when cosmic rays hit atoms in the atmosphere. Neutrinos rarely interact with matter (only via the weak nuclear force), travel at nearly the speed of light in straight lines, pass through large amounts of matter without any notable absorption or without being deflected by magnetic fields. Unlike photons, neutrinos rarely scatter along their trajectory. But like photons, neutrinos are some of the most common particles in the universe. Because of this, neutrinos offer a unique opportunity to observe processes that are inaccessible to optical telescopes, such as reactions in the Sun's core. Neutrinos that are created in the Sun’s core are barely absorbed, so"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_1",
    "chunk": "a large quantity of them escape from the Sun and reach the Earth. Neutrinos can also offer a very strong pointing direction compared to charged particle cosmic rays. Neutrinos are very hard to detect due to their non-interactive nature. In order to detect neutrinos, scientists have to shield the detectors from cosmic rays, which can penetrate hundreds of meters of rock. Neutrinos, on the other hand, can go through the entire planet without being absorbed, like \"ghost particles\". That's why neutrino detectors are placed many hundreds of meter underground, usually at the bottom of mines. There a neutrino detection liquid such as a Chlorine-rich solution is placed; the neutrinos react with a Chlorine isotope and can create radioactive Argon. Gallium to Germanium conversion has also been used. The IceCube Neutrino Observatory built in 2010 in the south pole is the biggest neutrino detector, consisting of thousands of optical sensors buried 500 meters underneath a cubic kilometer of deep, ultra-transparent ice, detects light emitted by charged particles that are produced when a single neutrino collides with a proton or neutron inside an atom. The resulting nuclear reaction produces secondary particles traveling at high speeds that give off a blue light called"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_2",
    "chunk": "Cherenkov radiation. Super-Kamiokande in Japan and ANTARES and KM3NeT in the Mediterranean are some other important neutrino detectors. Since neutrinos interact weakly, neutrino detectors must have large target masses (often thousands of tons). The detectors also must use shielding and effective software to remove background signal. Since neutrinos are very difficult to detect, the only bodies that have been studied in this way are the sun and the supernova SN1987A, which exploded in 1987. Scientist predicted that supernova explosions would produce bursts of neutrinos, and a similar burst was actually detected from Supernova 1987A. In the future neutrino astronomy promises to discover other aspects of the universe, including coincidental gravitational waves, gamma ray bursts, the cosmic neutrino background, origins of ultra-high-energy neutrinos, neutrino properties (such as neutrino mass hierarchy), dark matter properties, etc. It will become an integral part of multi-messenger astronomy, complementing gravitational astronomy and traditional telescopic astronomy. Neutrinos were first recorded in 1956 by Clyde Cowan and Frederick Reines in an experiment employing a nearby nuclear reactor as a neutrino source. Their discovery was acknowledged with a Nobel Prize in Physics in 1995. This was followed by the first atmospheric neutrino detection in 1965 by two groups almost"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_3",
    "chunk": "simultaneously. One was led by Frederick Reines who operated a liquid scintillator - the Case-Witwatersrand-Irvine or CWI detector - in the East Rand gold mine in South Africa at an 8.8 km water depth equivalent. The other was a Bombay-Osaka-Durham collaboration that operated in the Indian Kolar Gold Field mine at an equivalent water depth of 7.5 km. Although the KGF group detected neutrino candidates two months later than Reines CWI, they were given formal priority due to publishing their findings two weeks earlier. In 1968, Raymond Davis, Jr. and John N. Bahcall successfully detected the first solar neutrinos in the Homestake experiment. Davis, along with Japanese physicist Masatoshi Koshiba were jointly awarded half of the 2002 Nobel Prize in Physics \"for pioneering contributions to astrophysics, in particular for the detection of cosmic neutrinos (the other half went to Riccardo Giacconi for corresponding pioneering contributions which have led to the discovery of cosmic X-ray sources).\" The first generation of undersea neutrino telescope projects began with the proposal by Moisey Markov in 1960 \"...to install detectors deep in a lake or a sea and to determine the location of charged particles with the help of Cherenkov radiation.\" The first underwater neutrino"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_4",
    "chunk": "telescope began as the DUMAND project. DUMAND stands for Deep Underwater Muon and Neutrino Detector. The project began in 1976 and although it was eventually cancelled in 1995, it acted as a precursor to many of the following telescopes in the following decades. The Baikal Neutrino Telescope is installed in the southern part of Lake Baikal in Russia. The detector is located at a depth of 1.1 km and began surveys in 1980. In 1993, it was the first to deploy three strings to reconstruct the muon trajectories as well as the first to record atmospheric neutrinos underwater. AMANDA (Antarctic Muon And Neutrino Detector Array) used the 3 km thick ice layer at the South Pole and was located several hundred meters from the Amundsen-Scott station. Holes 60 cm in diameter were drilled with pressurized hot water in which strings with optical modules were deployed before the water refroze. The depth proved to be insufficient to be able to reconstruct the trajectory due to the scattering of light on air bubbles. A second group of 4 strings were added in 1995/96 to a depth of about 2000 m that was sufficient for track reconstruction. The AMANDA array was subsequently upgraded"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_5",
    "chunk": "until January 2000 when it consisted of 19 strings with a total of 667 optical modules at a depth range between 1500 m and 2000 m. AMANDA would eventually be the predecessor to IceCube in 2005. An example of an early neutrino detector is the Artyomovsk Scintillation Detector [ru] (ASD), located in the Soledar Salt Mine in Ukraine at a depth of more than 100 m. It was created in the Department of High Energy Leptons and Neutrino Astrophysics of the Institute of Nuclear Research of the USSR Academy of Sciences in 1969 to study antineutrino fluxes from collapsing stars in the Galaxy, as well as the spectrum and interactions of muons of cosmic rays with energies up to 10 ^ 13 eV. A feature of the detector is a 100-ton scintillation tank with dimensions on the order of the length of an electromagnetic shower with an initial energy of 100 GeV. After the decline of DUMAND the participating groups split into three branches to explore deep sea options in the Mediterranean Sea. ANTARES was anchored to the sea floor in the region off Toulon at the French Mediterranean coast. It consists of 12 strings, each carrying 25 \"storeys\" equipped"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_6",
    "chunk": "with three optical modules, an electronic container, and calibration devices down to a maximum depth of 2475 m. NEMO (NEutrino Mediterranean Observatory) was pursued by Italian groups to investigate the feasibility of a cubic-kilometer scale deep-sea detector. A suitable site at a depth of 3.5 km about 100 km off Capo Passero at the South-Eastern coast of Sicily has been identified. From 2007 to 2011 the first prototyping phase tested a \"mini-tower\" with 4 bars deployed for several weeks near Catania at a depth of 2 km. The second phase as well as plans to deploy the full-size prototype tower will be pursued in the KM3NeT framework. The NESTOR Project was installed in 2004 to a depth of 4 km and operated for one month until a failure of the cable to shore forced it to be terminated. The data taken still successfully demonstrated the detector's functionality and provided a measurement of the atmospheric muon flux. The proof of concept will be implemented in the KM3Net framework. The second generation of deep-sea neutrino telescope projects reach or even exceed the size originally conceived by the DUMAND pioneers. IceCube, located at the South Pole and incorporating its predecessor AMANDA, was completed"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_7",
    "chunk": "in December 2010. It currently consists of 5160 digital optical modules installed on 86 strings at depths of 1450 to 2550 m in the Antarctic ice. The KM3NeT in the Mediterranean Sea and the GVD are in their preparatory/prototyping phase. IceCube instruments 1 km of ice. GVD is also planned to cover 1 km but at a much higher energy threshold. KM3NeT is planned to cover several km and have two components; ARCA (Astroparticle Research with Cosmics in the Abyss) and ORCA (Oscillations Research with Cosmics in the Abyss). Both KM3NeT and GVD have completed at least part of their construction and it is expected that these two along with IceCube will form a global neutrino observatory. In July 2018, the IceCube Neutrino Observatory announced that they have traced an extremely-high-energy neutrino that hit their Antarctica-based research station in September 2017 back to its point of origin in the blazar TXS 0506+056 located 3.7 billion light-years away in the direction of the constellation Orion. This is the first time that a neutrino detector has been used to locate an object in space and that a source of cosmic rays has been identified. In November 2022, the IceCube collaboration made another"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_8",
    "chunk": "significant progress towards identifying the origin of cosmic rays, reporting the observation of 79 neutrinos with an energy over 1 TeV originated from the nearby galaxy M77. These findings in a well-known object are expected to help study the active nucleus of this galaxy, as well as serving as a baseline for future observations. In June 2023, astronomers reported using a new technique to detect, for the first time, the release of neutrinos from the galactic plane of the Milky Way galaxy. Neutrinos interact incredibly rarely with matter, so the vast majority of neutrinos will pass through a detector without interacting. If a neutrino does interact, it will only do so once. Therefore, to perform neutrino astronomy, large detectors must be used to obtain enough statistics. The method of neutrino detection depends on the energy and type of the neutrino. A famous example is that anti-electron neutrinos can interact with a nucleus in the detector by inverse beta decay and produce a positron and a neutron. The positron immediately will annihilate with an electron, producing two 511keV photons. The neutron will attach to another nucleus and give off a gamma with an energy of a few MeV. In general, neutrinos"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_9",
    "chunk": "can interact through neutral-current and charged-current interactions. In neutral-current interactions, the neutrino interacts with a nucleus or electron and the neutrino retains its original flavor. In charged-current interactions, the neutrino is absorbed by the nucleus and produces a lepton corresponding to the neutrino's flavor ( ν e ⟶ e − {\\displaystyle {\\ce {\\nu_{e}-> e^-}}} , ν μ ⟶ μ − {\\displaystyle {\\ce {\\nu_{\\mu}-> \\mu^{-}}}} , etc.). If the charged resultants are moving fast enough, they can create Cherenkov light. To observe neutrino interactions, detectors use photomultiplier tubes (PMTs) to detect individual photons. From the timing of the photons, it is possible to determine the time and place of the neutrino interaction. If the neutrino creates a muon during its interaction, then the muon will travel in a line, creating a \"track\" of Cherenkov photons. The data from this track can be used to reconstruct the directionality of the muon. For high-energy interactions, the neutrino and muon directions are the same, so it's possible to tell where the neutrino came from. This is pointing direction is important in extra-solar system neutrino astronomy. Along with time, position, and possibly direction, it's possible to infer the energy of the neutrino from the interactions."
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_10",
    "chunk": "The number of photons emitted is related to the neutrino energy, and neutrino energy is important for measuring the fluxes from solar and geo-neutrinos. Due to the rareness of neutrino interactions, it is important to maintain a low background signal. For this reason, most neutrino detectors are constructed under a rock or water overburden. This overburden shields against most cosmic rays in the atmosphere; only some of the highest-energy muons are able to penetrate to the depths of our detectors. Detectors must include ways of dealing with data from muons so as to not confuse them with neutrinos. Along with more complicated measures, if a muon track is first detected outside of the desired \"fiducial\" volume, the event is treated as a muon and not considered. Ignoring events outside the fiducial volume also decreases the signal from radiation outside the detector. Despite shielding efforts, it is inevitable that some background will make it into the detector, many times in the form of radioactive impurities within the detector itself. At this point, if it is impossible to differentiate between the background and true signal, a Monte Carlo simulation must be used to model the background. While it may be unknown if"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_11",
    "chunk": "an individual event is background or signal, it is possible to detect an excess about the background, signifying existence of the desired signal. When astronomical bodies, such as the Sun, are studied using light, only the surface of the object can be directly observed. Any light produced in the core of a star will interact with gas particles in the outer layers of the star, taking hundreds of thousands of years to make it to the surface, making it impossible to observe the core directly. Since neutrinos are also created in the cores of stars (as a result of stellar fusion), the core can be observed using neutrino astronomy. Other sources of neutrinos- such as neutrinos released by supernovae- have been detected. Several neutrino experiments have formed the Supernova Early Warning System (SNEWS), where they search for an increase of neutrino flux that could signal a supernova event. There are currently goals to detect neutrinos from other sources, such as active galactic nuclei (AGN), as well as gamma-ray bursts and starburst galaxies. Neutrino astronomy may also indirectly detect dark matter. Seven neutrino experiments (Super-K, LVD, IceCube, KamLAND, Borexino, Daya Bay, and HALO) work together as the Supernova Early Warning System"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_12",
    "chunk": "(SNEWS). In a core collapse supernova, ninety-nine percent of the energy released will be in neutrinos. While photons can be trapped in the dense supernova for hours, neutrinos are able to escape on the order of seconds. Since neutrinos travel at roughly the speed of light, they can reach Earth before photons do. If two or more of SNEWS detectors observe a coincidence of an increased flux of neutrinos, an alert is sent to professional and amateur astronomers to be on the lookout for supernova light. By using the distance between detectors and the time difference between detections, the alert can also include directionality as to the supernova's location in the sky. The Sun, like other stars, is powered by nuclear fusion in its core. The core is incredibly large, meaning that photons produced in the core will take a long time to diffuse outward. Therefore, neutrinos are the only way that we can obtain real-time data about the nuclear processes in the Sun. There are two main processes for stellar nuclear fusion. The first is the Proton-Proton (PP) chain, in which protons are fused together into helium, sometimes temporarily creating the heavier elements of lithium, beryllium, and boron along"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_13",
    "chunk": "the way. The second is the CNO cycle, in which carbon, nitrogen, and oxygen are fused with protons, and then undergo alpha decay (helium nucleus emission) to begin the cycle again. The PP chain is the primary process in the Sun, while the CNO cycle is more dominant in stars more massive than the Sun. Each step in the process has an allowed spectra of energy for the neutrino (or a discrete energy for electron capture processes). The relative rates of the Sun's nuclear processes can be determined by observations in its flux at different energies. This would shed insight into the Sun's properties, such as metallicity, which is the composition of heavier elements. Borexino is one of the detectors studying solar neutrinos. In 2018, they found 5σ significance for the existence of neutrinos from the fusing of two protons with an electron (pep neutrinos). In 2020, they found for the first time evidence of CNO neutrinos in the Sun. Improvements on the CNO measurement will be especially helpful in determining the Sun's metallicity. The interior of Earth contains radioactive elements such as K 40 {\\displaystyle {\\ce {^{40}K}}} and the decay chains of U 238 {\\displaystyle {\\ce {^{238}U}}} and Th"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_14",
    "chunk": "232 {\\displaystyle {\\ce {^{232}Th}}} . These elements decay via Beta decay, which emits an anti-neutrino. The energies of these anti-neutrinos are dependent on the parent nucleus. Therefore, by detecting the anti-neutrino flux as a function of energy, we can obtain the relative compositions of these elements and set a limit on the total power output of Earth's geo-reactor. Most of our current data about the core and mantle of Earth comes from seismic data, which does not provide any information as to the nuclear composition of these layers. Borexino has detected these geo-neutrinos through the process ν ¯ + p + ⟶ e + + n {\\displaystyle {\\ce {{\\bar {\\nu }}+p^{+}\\longrightarrow e^{+}{+n}}}} . The resulting positron will immediately annihilate with an electron and produce two gamma-rays each with an energy of 511keV (the rest mass of an electron). The neutron will later be captured by another nucleus, which will lead to a 2.22MeV gamma-ray as the nucleus de-excites. This process on average takes on the order of 256 microseconds. By searching for time and spatial coincidence of these gamma rays, the experimenters can be certain there was an event. Using over 3,200 days of data, Borexino used geoneutrinos to place"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_15",
    "chunk": "constraints on the composition and power output of the mantle. They found that the ratio of U 238 {\\displaystyle {\\ce {^{238}U}}} to Th 232 {\\displaystyle {\\ce {^{232}Th}}} is the same as chondritic meteorites. The power output from uranium and thorium in Earth's mantle was found to be 14.2-35.7 TW with a 68% confidence interval. Neutrino tomography also provides insight into the interior of Earth. For neutrinos with energies of a few TeV, the interaction probability becomes non-negligible when passing through Earth. The interaction probability will depend on the number of nucleons the neutrino passed along its path, which is directly related to density. If the initial flux is known (as it is in the case of atmospheric neutrinos), then detecting the final flux provides information about the interactions that occurred. The density can then be extrapolated from knowledge of these interactions. This can provide an independent check on the information obtained from seismic data. In 2018, one year worth of IceCube data was evaluated to perform neutrino tomography. The analysis studied upward going muons, which provide both the energy and directionality of the neutrinos after passing through the Earth. A model of Earth with five layers of constant density was"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_16",
    "chunk": "fit to the data, and the resulting density agreed with seismic data. The values determined for the total mass of Earth, the mass of the core, and the moment of inertia all agree with the data obtained from seismic and gravitational data. With the current data, the uncertainties on these values are still large, but future data from IceCube and KM3NeT will place tighter restrictions on this data. Neutrinos can either be primary cosmic rays (astrophysical neutrinos), or be produced from cosmic ray interactions. In the latter case, the primary cosmic ray will produce pions and kaons in the atmosphere. As these hadrons decay, they produce neutrinos (called atmospheric neutrinos). At low energies, the flux of atmospheric neutrinos is many times greater than astrophysical neutrinos. At high energies, the pions and kaons have a longer lifetime (due to relativistic time dilation). The hadrons are now more likely to interact before they decay. Because of this, the astrophysical neutrino flux will dominate at high energies (~100TeV). To perform neutrino astronomy of high-energy objects, experiments rely on the highest energy neutrinos. To perform astronomy of distant objects, a strong angular resolution is required. Neutrinos are electrically neutral and interact weakly, so they"
  },
  {
    "source": "Neutrino astronomy.txt",
    "chunk_id": "Neutrino astronomy.txt_17",
    "chunk": "travel mostly unperturbed in straight lines. If the neutrino interacts within a detector and produces a muon, the muon will produce an observable track. At high energies, the neutrino direction and muon direction are closely correlated, so it is possible to trace back the direction of the incoming neutrino. These high-energy neutrinos are either the primary or secondary cosmic rays produced by energetic astrophysical processes. Observing neutrinos could provide insights into these processes beyond what is observable with electromagnetic radiation. In the case of the neutrino detected from a distant blazar, multi-wavelength astronomy was used to show spatial coincidence, confirming the blazar as the source. In the future, neutrinos could be used to supplement electromagnetic and gravitational observations, leading to multi-messenger astronomy."
  },
  {
    "source": "Nuclear reaction.txt",
    "chunk_id": "Nuclear reaction.txt_0",
    "chunk": "# Nuclear reaction In nuclear physics and nuclear chemistry, a nuclear reaction is a process in which two nuclei, or a nucleus and an external subatomic particle, collide to produce one or more new nuclides. Thus, a nuclear reaction must cause a transformation of at least one nuclide to another. If a nucleus interacts with another nucleus or particle, they then separate without changing the nature of any nuclide, the process is simply referred to as a type of nuclear scattering, rather than a nuclear reaction. In principle, a reaction can involve more than two particles colliding, but because the probability of three or more nuclei to meet at the same time at the same place is much less than for two nuclei, such an event is exceptionally rare (see triple alpha process for an example very close to a three-body nuclear reaction). The term \"nuclear reaction\" may refer either to a change in a nuclide induced by collision with another particle or to a spontaneous change of a nuclide without collision. Natural nuclear reactions occur in the interaction between cosmic rays and matter, and nuclear reactions can be employed artificially to obtain nuclear energy, at an adjustable rate, on-demand."
  },
  {
    "source": "Nuclear reaction.txt",
    "chunk_id": "Nuclear reaction.txt_1",
    "chunk": "Nuclear chain reactions in fissionable materials produce induced nuclear fission. Various nuclear fusion reactions of light elements power the energy production of the Sun and stars. Most nuclear reactions (fusion and fission) results in transmutation of nuclei (called also nuclear transmutation). In 1919, Ernest Rutherford was able to accomplish transmutation of nitrogen into oxygen at the University of Manchester, using alpha particles directed at nitrogen N + α → O + p. This was the first observation of an induced nuclear reaction, that is, a reaction in which particles from one decay are used to transform another atomic nucleus. Eventually, in 1932 at Cambridge University, a fully artificial nuclear reaction and nuclear transmutation was achieved by Rutherford's colleagues John Cockcroft and Ernest Walton, who used artificially accelerated protons against lithium-7, to split the nucleus into two alpha particles. The feat was popularly known as \"splitting the atom\", although it was not the modern nuclear fission reaction later (in 1938) discovered in heavy elements by the German scientists Otto Hahn, Lise Meitner, and Fritz Strassmann. Nuclear reactions may be shown in a form similar to chemical equations, for which invariant mass must balance for each side of the equation, and in"
  },
  {
    "source": "Nuclear reaction.txt",
    "chunk_id": "Nuclear reaction.txt_2",
    "chunk": "which transformations of particles must follow certain conservation laws, such as conservation of charge and baryon number (total atomic mass number). An example of this notation follows: To balance the equation above for mass, charge and mass number, the second nucleus to the right must have atomic number 2 and mass number 4; it is therefore also helium-4. The complete equation therefore reads: Instead of using the full equations in the style above, in many situations a compact notation is used to describe nuclear reactions. This style of the form A(b,c)D is equivalent to A + b producing c + D. Common light particles are often abbreviated in this shorthand, typically p for proton, n for neutron, d for deuteron, α representing an alpha particle or helium-4, β for beta particle or electron, γ for gamma photon, etc. The reaction above would be written as Li(d,α)α. Kinetic energy may be released during the course of a reaction (exothermic reaction) or kinetic energy may have to be supplied for the reaction to take place (endothermic reaction). This can be calculated by reference to a table of very accurate particle rest masses, as follows: according to the reference tables, the 3Li nucleus"
  },
  {
    "source": "Nuclear reaction.txt",
    "chunk_id": "Nuclear reaction.txt_3",
    "chunk": "has a standard atomic weight of 6.015 atomic mass units (abbreviated u), the deuterium has 2.014 u, and the helium-4 nucleus has 4.0026 u. Thus: In a nuclear reaction, the total (relativistic) energy is conserved. The \"missing\" rest mass must therefore reappear as kinetic energy released in the reaction; its source is the nuclear binding energy. Using Einstein's mass-energy equivalence formula E = mc, the amount of energy released can be determined. We first need the energy equivalent of one atomic mass unit: Expressed differently: the mass is reduced by 0.3%, corresponding to 0.3% of 90 PJ/kg is 270 TJ/kg. This is a large amount of energy for a nuclear reaction; the amount is so high because the binding energy per nucleon of the helium-4 nucleus is unusually high because the He-4 nucleus is \"doubly magic\". (The He-4 nucleus is unusually stable and tightly bound for the same reason that the helium atom is inert: each pair of protons and neutrons in He-4 occupies a filled 1s nuclear orbital in the same way that the pair of electrons in the helium atom occupy a filled 1s electron orbital). Consequently, alpha particles appear frequently on the right-hand side of nuclear reactions."
  },
  {
    "source": "Nuclear reaction.txt",
    "chunk_id": "Nuclear reaction.txt_4",
    "chunk": "The energy released in a nuclear reaction can appear mainly in one of three ways: When the product nucleus is metastable, this is indicated by placing an asterisk (\"*\") next to its atomic number. This energy is eventually released through nuclear decay. A small amount of energy may also emerge in the form of X-rays. Generally, the product nucleus has a different atomic number, and thus the configuration of its electron shells is wrong. As the electrons rearrange themselves and drop to lower energy levels, internal transition X-rays (X-rays with precisely defined emission lines) may be emitted. In writing down the reaction equation, in a way analogous to a chemical equation, one may, in addition, give the reaction energy on the right side: For the particular case discussed above, the reaction energy has already been calculated as Q = 22.2 MeV. Hence: The reaction energy (the \"Q-value\") is positive for exothermal reactions and negative for endothermal reactions, opposite to the similar expression in chemistry. On the one hand, it is the difference between the sums of kinetic energies on the final side and on the initial side. But on the other hand, it is also the difference between the nuclear"
  },
  {
    "source": "Nuclear reaction.txt",
    "chunk_id": "Nuclear reaction.txt_5",
    "chunk": "rest masses on the initial side and on the final side (in this way, we have calculated the Q-value above). If the reaction equation is balanced, that does not mean that the reaction really occurs. The rate at which reactions occur depends on the energy and the flux of the incident particles, and the reaction cross section. An example of a large repository of reaction rates is the REACLIB database, as maintained by the Joint Institute for Nuclear Astrophysics. In the initial collision which begins the reaction, the particles must approach closely enough so that the short-range strong force can affect them. As most common nuclear particles are positively charged, this means they must overcome considerable electrostatic repulsion before the reaction can begin. Even if the target nucleus is part of a neutral atom, the other particle must penetrate well beyond the electron cloud and closely approach the nucleus, which is positively charged. Thus, such particles must be first accelerated to high energy, for example by: Also, since the force of repulsion is proportional to the product of the two charges, reactions between heavy nuclei are rarer, and require higher initiating energy, than those between a heavy and light nucleus;"
  },
  {
    "source": "Nuclear reaction.txt",
    "chunk_id": "Nuclear reaction.txt_6",
    "chunk": "while reactions between two light nuclei are the most common ones. Neutrons, on the other hand, have no electric charge to cause repulsion, and are able to initiate a nuclear reaction at very low energies. In fact, at extremely low particle energies (corresponding, say, to thermal equilibrium at room temperature), the neutron's de Broglie wavelength is greatly increased, possibly greatly increasing its capture cross-section, at energies close to resonances of the nuclei involved. Thus low-energy neutrons may be even more reactive than high-energy neutrons. While the number of possible nuclear reactions is immense, there are several types that are more common, or otherwise notable. Some examples include: An intermediate energy projectile transfers energy or picks up or loses nucleons to the nucleus in a single quick (10 second) event. Energy and momentum transfer are relatively small. These are particularly useful in experimental nuclear physics, because the reaction mechanisms are often simple enough to calculate with sufficient accuracy to probe the structure of the target nucleus. Energy and charge are transferred between projectile and target. Some examples of this kind of reactions are: Usually at moderately low energy, one or more nucleons are transferred between the projectile and target. These are"
  },
  {
    "source": "Nuclear reaction.txt",
    "chunk_id": "Nuclear reaction.txt_7",
    "chunk": "useful in studying outer shell structure of nuclei. Transfer reactions can occur: Reactions with neutrons are important in nuclear reactors and nuclear weapons. While the best-known neutron reactions are neutron scattering, neutron capture, and nuclear fission, for some light nuclei (especially odd-odd nuclei) the most probable reaction with a thermal neutron is a transfer reaction: Either a low-energy projectile is absorbed or a higher energy particle transfers energy to the nucleus, leaving it with too much energy to be fully bound together. On a time scale of about 10 seconds, particles, usually neutrons, are \"boiled\" off. That is, it remains together until enough energy happens to be concentrated in one neutron to escape the mutual attraction. The excited quasi-bound nucleus is called a compound nucleus."
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_0",
    "chunk": "# Observable universe The observable universe is a spherical region of the universe consisting of all matter that can be observed from Earth; the electromagnetic radiation from these objects has had time to reach the Solar System and Earth since the beginning of the cosmological expansion. Assuming the universe is isotropic, the distance to the edge of the observable universe is the same in every direction. That is, the observable universe is a spherical region centered on the observer. Every location in the universe has its own observable universe, which may or may not overlap with the one centered on Earth. The word observable in this sense does not refer to the capability of modern technology to detect light or other information from an object, or whether there is anything to be detected. It refers to the physical limit created by the speed of light itself. No signal can travel faster than light, hence there is a maximum distance, called the particle horizon, beyond which nothing can be detected, as the signals could not have reached the observer yet. Sometimes astrophysicists distinguish between the observable universe and the visible universe. The former includes signals since the end of the inflationary"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_1",
    "chunk": "epoch, while the latter includes only signals emitted since recombination. According to calculations, the current comoving distance to particles from which the cosmic microwave background radiation (CMBR) was emitted, which represents the radius of the visible universe, is about 14.0 billion parsecs (about 45.7 billion light-years). The comoving distance to the edge of the observable universe is about 14.3 billion parsecs (about 46.6 billion light-years), about 2% larger. The radius of the observable universe is therefore estimated to be about 46.5 billion light-years. Using the critical density and the diameter of the observable universe, the total mass of ordinary matter in the universe can be calculated to be about 1.5×10 kg. In November 2018, astronomers reported that extragalactic background light (EBL) amounted to 4×10 photons. As the universe's expansion is accelerating, all currently observable objects, outside the local supercluster, will eventually appear to freeze in time, while emitting progressively redder and fainter light. For instance, objects with the current redshift z from 5 to 10 will only be observable up to an age of 4–6 billion years. In addition, light emitted by objects currently situated beyond a certain comoving distance (currently about 19 gigaparsecs (62 Gly)) will never reach Earth."
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_2",
    "chunk": "The universe's size is unknown, and it may be infinite in extent. Some parts of the universe are too far away for the light emitted since the Big Bang to have had enough time to reach Earth or space-based instruments, and therefore lie outside the observable universe. In the future, light from distant galaxies will have had more time to travel, so one might expect that additional regions will become observable. Regions distant from observers (such as us) are expanding away faster than the speed of light, at rates estimated by Hubble's law. The expansion rate appears to be accelerating, which dark energy was proposed to explain. Assuming dark energy remains constant (an unchanging cosmological constant) so that the expansion rate of the universe continues to accelerate, there is a \"future visibility limit\" beyond which objects will never enter the observable universe at any time in the future because light emitted by objects outside that limit could never reach the Earth. Note that, because the Hubble parameter is decreasing with time, there can be cases where a galaxy that is receding from Earth only slightly faster than light emits a signal that eventually reaches Earth. This future visibility limit is"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_3",
    "chunk": "calculated at a comoving distance of 19 billion parsecs (62 billion light-years), assuming the universe will keep expanding forever, which implies the number of galaxies that can ever be theoretically observed in the infinite future is only larger than the number currently observable by a factor of 2.36 (ignoring redshift effects). In principle, more galaxies will become observable in the future; in practice, an increasing number of galaxies will become extremely redshifted due to ongoing expansion, so much so that they will seem to disappear from view and become invisible. A galaxy at a given comoving distance is defined to lie within the \"observable universe\" if we can receive signals emitted by the galaxy at any age in its history, say, a signal sent from the galaxy only 500 million years after the Big Bang. Because of the universe's expansion, there may be some later age at which a signal sent from the same galaxy can never reach the Earth at any point in the infinite future, so, for example, we might never see what the galaxy looked like 10 billion years after the Big Bang, even though it remains at the same comoving distance less than that of the"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_4",
    "chunk": "observable universe. This can be used to define a type of cosmic event horizon whose distance from the Earth changes over time. For example, the current distance to this horizon is about 16 billion light-years, meaning that a signal from an event happening at present can eventually reach the Earth if the event is less than 16 billion light-years away, but the signal will never reach the Earth if the event is further away. The space before this cosmic event horizon can be called \"reachable universe\", that is all galaxies closer than that could be reached if we left for them today, at the speed of light; all galaxies beyond that are unreachable. Simple observation will show the future visibility limit (62 billion light-years) is exactly equal to the reachable limit (16 billion light-years) added to the current visibility limit (46 billion light-years). Both popular and professional research articles in cosmology often use the term \"universe\" to mean \"observable universe\". This can be justified on the grounds that we can never know anything by direct observation about any part of the universe that is causally disconnected from the Earth, although many credible theories require a total universe much larger than"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_5",
    "chunk": "the observable universe. No evidence exists to suggest that the boundary of the observable universe constitutes a boundary on the universe as a whole, nor do any of the mainstream cosmological models propose that the universe has any physical boundary in the first place. However, some models propose it could be finite but unbounded, like a higher-dimensional analogue of the 2D surface of a sphere that is finite in area but has no edge. It is plausible that the galaxies within the observable universe represent only a minuscule fraction of the galaxies in the universe. According to the theory of cosmic inflation initially introduced by Alan Guth and D. Kazanas, if it is assumed that inflation began about 10 seconds after the Big Bang and that the pre-inflation size of the universe was approximately equal to the speed of light times its age, that would suggest that at present the entire universe's size is at least 1.5×10 light-years — this is at least 3×10 times the radius of the observable universe. If the universe is finite but unbounded, it is also possible that the universe is smaller than the observable universe. In this case, what we take to be very"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_6",
    "chunk": "distant galaxies may actually be duplicate images of nearby galaxies, formed by light that has circumnavigated the universe. It is difficult to test this hypothesis experimentally because different images of a galaxy would show different eras in its history, and consequently might appear quite different. Bielewicz et al. claim to establish a lower bound of 27.9 gigaparsecs (91 billion light-years) on the diameter of the last scattering surface. This value is based on matching-circle analysis of the WMAP 7-year data. This approach has been disputed. The comoving distance from Earth to the edge of the observable universe is about 14.26 gigaparsecs (46.5 billion light-years or 4.40×10 m) in any direction. The observable universe is thus a sphere with a diameter of about 28.5 gigaparsecs (93 billion light-years or 8.8×10 m). Assuming that space is roughly flat (in the sense of being a Euclidean space), this size corresponds to a comoving volume of about 1.22×10 Gpc (4.22×10 Gly or 3.57×10 m). These are distances now (in cosmological time), not distances at the time the light was emitted. For example, the cosmic microwave background radiation that we see right now was emitted at the time of photon decoupling, estimated to have occurred"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_7",
    "chunk": "about 380,000 years after the Big Bang, which occurred around 13.8 billion years ago. This radiation was emitted by matter that has, in the intervening time, mostly condensed into galaxies, and those galaxies are now calculated to be about 46 billion light-years from Earth. To estimate the distance to that matter at the time the light was emitted, we may first note that according to the Friedmann–Lemaître–Robertson–Walker metric, which is used to model the expanding universe, if we receive light with a redshift of z, then the scale factor at the time the light was originally emitted is given by WMAP nine-year results combined with other measurements give the redshift of photon decoupling as z = 1091.64±0.47, which implies that the scale factor at the time of photon decoupling would be 1⁄1092.64. So if the matter that originally emitted the oldest CMBR photons has a present distance of 46 billion light-years, then the distance would have been only about 42 million light-years at the time of decoupling. The light-travel distance to the edge of the observable universe is the age of the universe times the speed of light, 13.8 billion light years. This is the distance that a photon emitted"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_8",
    "chunk": "shortly after the Big Bang, such as one from the cosmic microwave background, has traveled to reach observers on Earth. Because spacetime is curved, corresponding to the expansion of space, this distance does not correspond to the true distance at any moment in time. The observable universe contains as many as an estimated 2 trillion galaxies and, overall, as many as an estimated 10 stars – more stars (and, potentially, Earth-like planets) than all the grains of beach sand on planet Earth. Other estimates are in the hundreds of billions rather than trillions. The estimated total number of stars in an inflationary universe (observed and unobserved) is 10. Assuming the mass of ordinary matter is about 1.45×10 kg as discussed above, and assuming all atoms are hydrogen atoms (which are about 74% of all atoms in the Milky Way by mass), the estimated total number of atoms in the observable universe is obtained by dividing the mass of ordinary matter by the mass of a hydrogen atom. The result is approximately 10 hydrogen atoms, also known as the Eddington number. The mass of the observable universe is often quoted as 10 kg. In this context, mass refers to ordinary (baryonic)"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_9",
    "chunk": "matter and includes the interstellar medium (ISM) and the intergalactic medium (IGM). However, it excludes dark matter and dark energy. This quoted value for the mass of ordinary matter in the universe can be estimated based on critical density. The calculations are for the observable universe only as the volume of the whole is unknown and may be infinite. Critical density is the energy density for which the universe is flat. If there is no dark energy, it is also the density for which the expansion of the universe is poised between continued expansion and collapse. From the Friedmann equations, the value for ρ c {\\displaystyle \\rho _{\\text{c}}} critical density, is: where G is the gravitational constant and H = H0 is the present value of the Hubble constant. The value for H0, as given by the European Space Agency's Planck Telescope, is H0 = 67.15 kilometres per second per megaparsec. This gives a critical density of 0.85×10 kg/m, or about 5 hydrogen atoms per cubic metre. This density includes four significant types of energy/mass: ordinary matter (4.8%), neutrinos (0.1%), cold dark matter (26.8%), and dark energy (68.3%). Although neutrinos are Standard Model particles, they are listed separately because they"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_10",
    "chunk": "are ultra-relativistic and hence behave like radiation rather than like matter. The density of ordinary matter, as measured by Planck, is 4.8% of the total critical density or 4.08×10 kg/m. To convert this density to mass we must multiply by volume, a value based on the radius of the \"observable universe\". Since the universe has been expanding for 13.8 billion years, the comoving distance (radius) is now about 46.6 billion light-years. Thus, volume (⁠4/3⁠πr) equals 3.58×10 m and the mass of ordinary matter equals density (4.08×10 kg/m) times volume (3.58×10 m) or 1.46×10 kg. Sky surveys and mappings of the various wavelength bands of electromagnetic radiation (in particular 21-cm emission) have yielded much information on the content and character of the universe's structure. The organization of structure appears to follow a hierarchical model with organization up to the scale of superclusters and filaments. Larger than this (at scales between 30 and 200 megaparsecs), there seems to be no continued structure, a phenomenon that has been referred to as the End of Greatness. The shape of the large scale structure can be summarized by the matter power spectrum. The organization of structure arguably begins at the stellar level, though most cosmologists"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_11",
    "chunk": "rarely address astrophysics on that scale. Stars are organized into galaxies, which in turn form galaxy groups, galaxy clusters, superclusters, sheets, walls and filaments, which are separated by immense voids, creating a vast foam-like structure sometimes called the \"cosmic web\". Prior to 1989, it was commonly assumed that virialized galaxy clusters were the largest structures in existence, and that they were distributed more or less uniformly throughout the universe in every direction. However, since the early 1980s, more and more structures have been discovered. In 1983, Adrian Webster identified the Webster LQG, a large quasar group consisting of 5 quasars. The discovery was the first identification of a large-scale structure, and has expanded the information about the known grouping of matter in the universe. In 1987, Robert Brent Tully identified the Pisces–Cetus Supercluster Complex, the galaxy filament in which the Milky Way resides. It is about 1 billion light-years across. That same year, an unusually large region with a much lower than average distribution of galaxies was discovered, the Giant Void, which measures 1.3 billion light-years across. Based on redshift survey data, in 1989 Margaret Geller and John Huchra discovered the \"Great Wall\", a sheet of galaxies more than 500"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_12",
    "chunk": "million light-years long and 200 million light-years wide, but only 15 million light-years thick. The existence of this structure escaped notice for so long because it requires locating the position of galaxies in three dimensions, which involves combining location information about the galaxies with distance information from redshifts. Two years later, astronomers Roger G. Clowes and Luis E. Campusano discovered the Clowes–Campusano LQG, a large quasar group measuring two billion light-years at its widest point, which was the largest known structure in the universe at the time of its announcement. In April 2003, another large-scale structure was discovered, the Sloan Great Wall. In August 2007, a possible supervoid was detected in the constellation Eridanus. It coincides with the 'CMB cold spot', a cold region in the microwave sky that is highly improbable under the currently favored cosmological model. This supervoid could cause the cold spot, but to do so it would have to be improbably big, possibly a billion light-years across, almost as big as the Giant Void mentioned above. Another large-scale structure is the SSA22 Protocluster, a collection of galaxies and enormous gas bubbles that measures about 200 million light-years across. In 2011, a large quasar group was discovered,"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_13",
    "chunk": "U1.11, measuring about 2.5 billion light-years across. On January 11, 2013, another large quasar group, the Huge-LQG, was discovered, which was measured to be four billion light-years across, the largest known structure in the universe at that time. In November 2013, astronomers discovered the Hercules–Corona Borealis Great Wall, an even bigger structure twice as large as the former. It was defined by the mapping of gamma-ray bursts. In 2021, the American Astronomical Society announced the detection of the Giant Arc; a crescent-shaped string of galaxies that span 3.3 billion light years in length, located 9.2 billion light years from Earth in the constellation Boötes from observations captured by the Sloan Digital Sky Survey. The End of Greatness is an observational scale discovered at roughly 100 Mpc (roughly 300 million light-years) where the lumpiness seen in the large-scale structure of the universe is homogenized and isotropized in accordance with the cosmological principle. At this scale, no pseudo-random fractalness is apparent. The superclusters and filaments seen in smaller surveys are randomized to the extent that the smooth distribution of the universe is visually apparent. It was not until the redshift surveys of the 1990s were completed that this scale could accurately be"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_14",
    "chunk": "observed. Another indicator of large-scale structure is the 'Lyman-alpha forest'. This is a collection of absorption lines that appear in the spectra of light from quasars, which are interpreted as indicating the existence of huge thin sheets of intergalactic (mostly hydrogen) gas. These sheets appear to collapse into filaments, which can feed galaxies as they grow where filaments either cross or are dense. An early direct evidence for this cosmic web of gas was the 2019 detection, by astronomers from the RIKEN Cluster for Pioneering Research in Japan and Durham University in the U.K., of light from the brightest part of this web, surrounding and illuminated by a cluster of forming galaxies, acting as cosmic flashlights for intercluster medium hydrogen fluorescence via Lyman-alpha emissions. In 2021, an international team, headed by Roland Bacon from the Centre de Recherche Astrophysique de Lyon (France), reported the first observation of diffuse extended Lyman-alpha emission from redshift 3.1 to 4.5 that traced several cosmic web filaments on scales of 2.5−4 cMpc (comoving mega-parsecs), in filamentary environments outside massive structures typical of web nodes. Some caution is required in describing structures on a cosmic scale because they are often different from how they appear. Gravitational"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_15",
    "chunk": "lensing can make an image appear to originate in a different direction from its real source, when foreground objects curve surrounding spacetime (as predicted by general relativity) and deflect passing light rays. Rather usefully, strong gravitational lensing can sometimes magnify distant galaxies, making them easier to detect. Weak lensing by the intervening universe in general also subtly changes the observed large-scale structure. The large-scale structure of the universe also looks different if only redshift is used to measure distances to galaxies. For example, galaxies behind a galaxy cluster are attracted to it and fall towards it, and so are blueshifted (compared to how they would be if there were no cluster). On the near side, objects are redshifted. Thus, the environment of the cluster looks somewhat pinched if using redshifts to measure distance. The opposite effect is observed on galaxies already within a cluster: the galaxies have some random motion around the cluster center, and when these random motions are converted to redshifts, the cluster appears elongated. This creates a \"finger of God\"—the illusion of a long chain of galaxies pointed at Earth. At the centre of the Hydra–Centaurus Supercluster, a gravitational anomaly called the Great Attractor affects the motion"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_16",
    "chunk": "of galaxies over a region hundreds of millions of light-years across. These galaxies are all redshifted, in accordance with Hubble's law. This indicates that they are receding from us and from each other, but the variations in their redshift are sufficient to reveal the existence of a concentration of mass equivalent to tens of thousands of galaxies. The Great Attractor, discovered in 1986, lies at a distance of between 150 million and 250 million light-years in the direction of the Hydra and Centaurus constellations. In its vicinity there is a preponderance of large old galaxies, many of which are colliding with their neighbours, or radiating large amounts of radio waves. In 1987, astronomer R. Brent Tully of the University of Hawaii's Institute of Astronomy identified what he called the Pisces–Cetus Supercluster Complex, a structure one billion light-years long and 150 million light-years across in which, he claimed, the Local Supercluster is embedded. The most distant astronomical object identified (as of August of 2024) is a galaxy classified as JADES-GS-z14-0. In 2009, a gamma ray burst, GRB 090423, was found to have a redshift of 8.2, which indicates that the collapsing star that caused it exploded when the universe was only"
  },
  {
    "source": "Observable universe.txt",
    "chunk_id": "Observable universe.txt_17",
    "chunk": "630 million years old. The burst happened approximately 13 billion years ago, so a distance of about 13 billion light-years was widely quoted in the media, or sometimes a more precise figure of 13.035 billion light-years. This would be the \"light travel distance\" (see Distance measures (cosmology)) rather than the \"proper distance\" used in both Hubble's law and in defining the size of the observable universe. Cosmologist Ned Wright argues against using this measure. The proper distance for a redshift of 8.2 would be about 9.2 Gpc, or about 30 billion light-years. The limit of observability in the universe is set by cosmological horizons which limit—based on various physical constraints—the extent to which information can be obtained about various events in the universe. The most famous horizon is the particle horizon which sets a limit on the precise distance that can be seen due to the finite age of the universe. Additional horizons are associated with the possible future extent of observations, larger than the particle horizon owing to the expansion of space, an \"optical horizon\" at the surface of last scattering, and associated horizons with the surface of last scattering for neutrinos and gravitational waves."
  },
  {
    "source": "Occultation.txt",
    "chunk_id": "Occultation.txt_0",
    "chunk": "# Occultation An occultation is an event that occurs when one object is hidden from the observer by another object that passes between them. The term is often used in astronomy, but can also refer to any situation in which an object in the foreground blocks from view (occults) an object in the background. In this general sense, occultation applies to the visual scene observed from low-flying aircraft (or computer-generated imagery) when foreground objects obscure distant objects dynamically, as the scene changes over time. If the closer body does not entirely conceal the farther one, the event is called a transit. Both transit and occultation may be referred to generally as occlusion; and if a shadow is cast onto the observer, it is called an eclipse. The term occultation is most frequently used to describe lunar occultations, those relatively frequent occasions when the Moon passes in front of a star during the course of its orbital motion around the Earth. Since the Moon, with an angular speed with respect to the stars of 0.55 arcsec/s or 2.7 μrad/s, has a very thin atmosphere and stars have an angular diameter of at most 0.057 arcseconds or 0.28 μrad, a star that"
  },
  {
    "source": "Occultation.txt",
    "chunk_id": "Occultation.txt_1",
    "chunk": "is occulted by the Moon will disappear or reappear in 0.1 seconds or less on the Moon's edge, or limb. Events that take place on the Moon's dark limb are of particular interest to observers, because the lack of glare allows easier observation and timing. The Moon's orbit is inclined slightly with respect to the ecliptic (see orbit of the Moon) meaning any star with an ecliptic latitude between –6.6 and +6.6 degrees may be occulted by it. Three first magnitude stars appear well within that band – Regulus, Spica, and Antares – meaning they may be occulted by the Moon or by planets. Occultations of Aldebaran are in this epoch only possible by the Moon, because the planets pass Aldebaran to the north. Neither planetary nor lunar occultations of Pollux are currently possible, however several thousand years ago lunar occultations were possible. Some notably close deep-sky objects, such as the Pleiades, can be occulted by the Moon. Within a few kilometres of the edge of an occultation's predicted path, referred to as its northern or southern limit, an observer may see the star intermittently disappearing and reappearing as the irregular limb of the Moon moves past the star, creating"
  },
  {
    "source": "Occultation.txt",
    "chunk_id": "Occultation.txt_2",
    "chunk": "what is known as a grazing lunar occultation. From an observational and scientific standpoint, these \"grazes\" are the most dynamic and interesting of lunar occultations. The accurate timing of lunar occultations is performed regularly by (primarily amateur) astronomers. Lunar occultations timed to an accuracy of a few tenths of a second have various scientific uses, particularly in refining our knowledge of lunar topography. Photoelectric analysis of lunar occultations have also discovered some stars to be very close visual or spectroscopic binaries. Some angular diameters of stars have been measured by timing of lunar occultations, which is useful for determining effective temperatures of those stars. Early radio astronomers found occultations of radio sources by the Moon valuable for determining their exact positions, because the long wavelength of radio waves limited the resolution available through direct observation. This was crucial for the unambiguous identification of the radio source 3C 273 with the optical quasar and its jet, and a fundamental prerequisite for Maarten Schmidt's discovery of the cosmological nature of quasars. Several times during the year the Moon can be seen occulting a planet. Since planets, unlike stars, have significant angular sizes, lunar occultations of planets will create a narrow zone on"
  },
  {
    "source": "Occultation.txt",
    "chunk_id": "Occultation.txt_3",
    "chunk": "Earth from which a partial occultation of the planet will occur. An observer located within that narrow zone could observe the planet's disk partly blocked by the slowly moving Moon. The same mechanism can be seen with the Sun, where observers on Earth will view it as a solar eclipse. Therefore, a total solar eclipse is essentially the Moon occulting the Sun. Stars may also be occulted by planets. Occultations of bright stars are rare. In 1959, Venus occulted Regulus, and the next occultation of a bright star (also Regulus by Venus) will be in 2044. Uranus's rings were first discovered when that planet occulted a star in 1977. On 3 July 1989, Saturn passed in front of the 5th magnitude star 28 Sagittarii. Pluto occulted stars in 1988, 2002, and 2006, allowing its tenuous atmosphere to be studied via atmospheric limb sounding. In rare cases, one planet can pass in front of another. If the nearer planet appears larger than the more distant one, the event is called a mutual planetary occultation. The last occultation or transit occurred on 3 January 1818 and the next will occur on 22 November 2065, in both cases involving the same two planets—Venus"
  },
  {
    "source": "Occultation.txt",
    "chunk_id": "Occultation.txt_4",
    "chunk": "and Jupiter. Jupiter rarely occults Saturn. This is one of the rarest events known, with the next occurrence on February 10, 7541. This event is visible worldwide since the duo would be positioned almost in opposition to the sun, in the border line between the constellations of Orion and Taurus. In some areas this occultation cannot be seen, but when viewed through even small telescopes, both gas giants appear to be in the same part of view through the eyepiece. The last one occurred in 6857 B.C.E. A further set of occultations are those when a small Solar System body or dwarf planet passes in front of a star, temporarily blocking its light as seen from Earth. These occultations are useful for measuring the size and position of body much more precisely than can be done by other means. A cross-sectional profile of the shape of a body can even be determined if a number of observers at different, nearby, locations observe the occultation. Occultations have been used to calculate the diameter of trans-Neptunian objects such as 2002 TX300, Ixion and Varuna. Software for coordinating observations is available for download at http://www.occultwatcher.net/ In addition, mutual occultation and eclipsing events can"
  },
  {
    "source": "Occultation.txt",
    "chunk_id": "Occultation.txt_5",
    "chunk": "occur between a primary and its satellite. A large number of moons have been discovered analyzing the photometric light curves of small bodies and detecting a second, superimposed brightness variation, from which an orbital period for the satellite (secondary), and a secondary-to-primary diameter-ratio (for the binary system) can often be derived. The Moon or another celestial body can occult multiple celestial bodies at the same time. Because of its relatively large angular diameter the Moon, at any given time, occults an indeterminate number of stars and galaxies. However the Moon occulting (obscuring) two bright objects (e.g. two planets or a bright star and a planet) simultaneously is extremely rare and can be seen only from a small part of the world: the last such event was on 23 April 1998 when it occulted Venus and Jupiter for observers on Ascension Island. The Big Occulting Steerable Satellite (BOSS) was a proposed satellite that would work in conjunction with a telescope to detect planets around distant stars. The satellite consists of a large, very lightweight sheet, and a set of maneuvering thrusters and navigation systems. It would maneuver to a position along the line of sight between the telescope and a nearby"
  },
  {
    "source": "Occultation.txt",
    "chunk_id": "Occultation.txt_6",
    "chunk": "star. The satellite would thereby block the radiation from the star, permitting the orbiting planets to be observed. The proposed satellite would have a dimension of 70 by 70 metres (230 ft × 230 ft), a mass of about 600 kg, and maneuver by means of an ion drive engine in combination with using the sheet as a light sail. Positioned at a distance of 100,000 km from the telescope, it would block more than 99.998% of the starlight. There are two possible configurations of this satellite. The first would work with a space telescope, most likely positioned near the Earth's L2 Lagrangian point. The second would place the satellite in a highly elliptical orbit about the Earth, and work in conjunction with a ground telescope. At the apogee of the orbit, the satellite would remain relatively stationary with respect to the ground, allowing longer exposure times. An updated version of this design is called the Starshade, which uses a sunflower-shaped coronagraph disc. A comparable proposal was also made for a satellite to occult bright X-ray sources, called an X-ray Occulting Steerable Satellite or XOSS."
  },
  {
    "source": "OCLC.txt",
    "chunk_id": "OCLC.txt_0",
    "chunk": "# OCLC OCLC, Inc. is an American nonprofit cooperative organization \"that provides shared technology services, original research, and community programs for its membership and the library community at large\". It was founded in 1967 as the Ohio College Library Center, then became the Online Computer Library Center as it expanded. In 2017, the name was formally changed to OCLC, Inc. OCLC and thousands of its member libraries cooperatively produce and maintain WorldCat, the largest online public access catalog in the world. OCLC is funded mainly by the fees that libraries pay (around $217.8 million annually in total as of 2021) for the many different services it offers. OCLC also maintains the Dewey Decimal Classification system. OCLC began in 1967, as the Ohio College Library Center, through a collaboration of university presidents, vice presidents, and library directors who wanted to create a cooperative, computerized network for libraries in the state of Ohio. The group first met on July 5, 1967, on the campus of Ohio State University to sign the articles of incorporation for the nonprofit organization and hired Frederick G. Kilgour, a former Yale University medical school librarian, as first executive director. Kilgour and Ralph H. Parker, who was the"
  },
  {
    "source": "OCLC.txt",
    "chunk_id": "OCLC.txt_1",
    "chunk": "head of libraries at the University of Missouri, had proposed the shared cataloging system in a 1965 report as consultants to the Committee of Librarians of the Ohio College Association. Kilgour and Parker wished to merge the latest information storage and retrieval system of the time, the computer, with the oldest, the library. They were inspired in part by the earlier Columbia–Harvard–Yale Medical Libraries Computerization Project, an attempt at shared automated printing of catalog cards. The plan was to merge the catalogs of Ohio libraries electronically through a computer network and database to streamline operations, control costs, and increase efficiency in library management, bringing libraries together cooperatively to best serve researchers and scholars. The first library to do online cataloging through OCLC was the Alden Library at Ohio University on August 26, 1971. This was the first online cataloging by any library worldwide. Between 1967 and 1977, OCLC membership was limited to institutions in Ohio, but in 1978, a new governance structure was established that allowed institutions from other states to join. With this expansion, the name changed to the Online Computer Library Center in 1977. In 2002, the governance structure was again modified to accommodate participation from outside the"
  },
  {
    "source": "OCLC.txt",
    "chunk_id": "OCLC.txt_2",
    "chunk": "United States. As OCLC expanded services in the United States outside Ohio, it relied on establishing strategic partnerships with \"networks\", organizations that provided training, support and marketing services. By 2008, there were 15 independent United States regional service providers. OCLC networks played a key role in OCLC governance, with networks electing delegates to serve on the OCLC Members Council. During 2008, OCLC commissioned two studies to look at distribution channels; at the same time, the council approved governance changes that had been recommended by the Board of Trustees severing the tie between the networks and governance. In early 2009, OCLC negotiated new contracts with the former networks and opened a centralized support center. In July 2010, the company was sued by SkyRiver, a rival startup, in an antitrust suit. Library automation company Innovative Interfaces joined SkyRiver in the suit. The suit was dropped in March 2013, however, following the acquisition of SkyRiver by Innovative Interfaces. Innovative Interfaces was bought by ExLibris in 2020, therefore passing OCLC as the dominant supplier of ILS services in the US (over 70% market share for academic libraries and over 50% for public libraries for ExLibris, versus OCLC's 10% market share of both types of"
  },
  {
    "source": "OCLC.txt",
    "chunk_id": "OCLC.txt_3",
    "chunk": "libraries in 2019). In 2022, membership and governance expanded to include any institution with a subscription to one of many qualifying OCLC products (previously institutions qualified for membership by \"contributing intellectual content or participating in global resource or reference sharing\"), with the exception of for-profit organizations that are part of OCLC's partner program. This change reflected OCLC's expanding number of services due to its corporate acquisitions. OCLC and its member libraries cooperatively produce and maintain WorldCat—the OCLC Online Union Catalog, the largest online public access catalog (OPAC) in the world. WorldCat has holding records from public and private libraries worldwide. The Online Computer Library Center acquired the trademark and copyrights associated with the Dewey Decimal Classification System when it bought Forest Press in 1988. A browser for books with their Dewey Decimal Classifications was available until July 2013; it was replaced by the Classify Service. Until August 2009, when it was sold to Backstage Library Works, OCLC owned a preservation microfilm and digitization operation called the OCLC Preservation Service Center, with its principal office in Bethlehem, Pennsylvania. Starting in 1971, OCLC produced catalog cards for members alongside its shared online catalog; the company printed its last catalog cards on October"
  },
  {
    "source": "OCLC.txt",
    "chunk_id": "OCLC.txt_4",
    "chunk": "1, 2015. QuestionPoint, an around-the-clock reference service provided to users by a cooperative of participating global libraries, was acquired by Springshare from OCLC in 2019 and migrated to Springshare's LibAnswers platform. OCLC has been conducting research for the library community for more than 30 years. In accordance with its mission, OCLC makes its research outcomes known through various publications. These publications, including journal articles, reports, newsletters, and presentations, are available through the organization's website. During the COVID-19 pandemic, OCLC participated in the REopening Archives, Libraries, and Museums (REALM) project funded by the IMLS to study the surface transmission risks of SARS-CoV-2 on common library and museum materials and surfaces, and published a series of reports. Advocacy has been a part of OCLC's mission since its founding in 1967. OCLC staff members meet and work regularly with library leaders, information professionals, researchers, entrepreneurs, political leaders, trustees, students and patrons to advocate \"advancing research, scholarship, education, community development, information access, and global cooperation\". WebJunction, which provides training services to librarians, is a division of OCLC funded by grants from the Bill & Melinda Gates Foundation beginning in 2003. OCLC partnered with search engine providers in 2003 to advocate for libraries and share"
  },
  {
    "source": "OCLC.txt",
    "chunk_id": "OCLC.txt_5",
    "chunk": "information across the Internet landscape. Google, Yahoo!, and Ask.com all collaborated with OCLC to make WorldCat records searchable through those search engines. OCLC's advocacy campaign \"Geek the Library\", started in 2009, highlights the role of public libraries. The campaign, funded by a grant from the Bill & Melinda Gates Foundation, uses a strategy based on the findings of the 2008 OCLC report, \"From Awareness to Funding: A study of library support in America\". Other past advocacy campaigns have focused on sharing the knowledge gained from library and information research. Such projects have included communities such as the Society of American Archivists, the Open Archives Initiative, the Institute for Museum and Library Services, the International Organization for Standardization, the National Information Standards Organization, the World Wide Web Consortium, the Internet Engineering Task Force, and Internet2. One of the most successful contributions to this effort was the Dublin Core Metadata Initiative, \"an open forum of libraries, archives, museums, technology organizations, and software companies who work together to develop interoperable online metadata standards that support a broad range of purposes and business models.\" OCLC has collaborated with the Wikimedia Foundation and the Wikimedia volunteer community, through integrating library metadata with Wikimedia projects, hosting"
  },
  {
    "source": "OCLC.txt",
    "chunk_id": "OCLC.txt_6",
    "chunk": "a Wikipedian in residence, and doing a national training program through WebJunction called \"Wikipedia + Libraries: Better Together\". OCLC's WorldCat database is used by the general public and by librarians for cataloging and research. WorldCat is available to the public for searching via a subscription web-based service called FirstSearch, to which many libraries subscribe, as well as through the publicly available WorldCat.org. OCLC assigns a unique control number (referred to as an \"OCN\" for \"OCLC Control Number\") to each new bibliographic record in WorldCat. Numbers are assigned serially, and in mid-2013 over a billion OCNs had been created. In September 2013, the OCLC declared these numbers to be in the public domain, removing a perceived barrier to widespread use of OCNs outside OCLC itself. The control numbers link WorldCat's records to local library system records by providing a common reference key for a record across libraries. OCNs are particularly useful as identifiers for books and other bibliographic materials that do not have ISBNs (e.g., books published before 1970). OCNs are often used as identifiers for Wikipedia and Wikidata. In October 2013, it was reported that out of 29,673 instances of book infoboxes in Wikipedia, \"there were 23,304 ISBNs and 15,226"
  },
  {
    "source": "OCLC.txt",
    "chunk_id": "OCLC.txt_7",
    "chunk": "OCNs\", and regarding Wikidata: \"of around 14 million Wikidata items, 28,741 were books. 5403 Wikidata items have an ISBN associated with them, and 12,262 have OCNs.\" OCLC also runs the Virtual International Authority File (VIAF), an international name authority file, with oversight from the VIAF Council composed of representatives of institutions that contribute data to VIAF. VIAF numbers are broadly used as standard identifiers, including in Wikipedia. In 2024, OCLC launched a new linked data management tool called OCLC Meridian. This was released alongside a suite of APIs for WorldCat Entities to allow greater control, connection, and integration of linked data for user institutions. This suite of APIs “enables the creation of linked data entities and descriptive relationships, forming connections to the existing value in MARC records and other datasets across the global information ecosystem.” The use of these APIs and WorldCat Entities is designed to improve discoverability and relevance for users, integrate data management into your existing workflows, and discover, emphasize and analyze important relationships. The WorldCat Entities APIs enable users to connect identifiers from different information sources including ORCID, ISNI, and VIAF to allow your records and systems to learn of changes to WorldCat Entities data and related"
  },
  {
    "source": "OCLC.txt",
    "chunk_id": "OCLC.txt_8",
    "chunk": "information for local use. OCLC acquired NetLibrary, a provider of electronic books and textbooks, in 2002 and sold it in 2010 to EBSCO Industries. OCLC owns 100% of the shares of OCLC PICA, a library automation systems and services company which has its headquarters in Leiden in the Netherlands and which was renamed \"OCLC\" at the end of 2007. In July 2006, the Research Libraries Group (RLG) merged with OCLC. On January 11, 2008, OCLC announced that it had purchased EZproxy. It has also acquired OAIster. The process started in January 2009 and from October 31, 2009, OAIster records are freely available via WorldCat.org. In 2013, OCLC acquired the Dutch library automation company HKA and its integrated library system Wise, which OCLC calls a \"community engagement system\" that \"combines the power of customer relationship management, marketing, and analytics with ILS functions\". OCLC began offering Wise to libraries in the United States in 2019. In January 2015, OCLC acquired Sustainable Collection Services (SCS). SCS offered consulting services based on analyzing library print collection data to help libraries manage and share materials. In 2017, OCLC acquired Relais International, a library interlibrary loan service provider based in Ottawa, Canada. A more complete list"
  },
  {
    "source": "OCLC.txt",
    "chunk_id": "OCLC.txt_9",
    "chunk": "of mergers and acquisitions is available on the OCLC website. In May 2008, OCLC was criticized by Jeffrey Beall for monopolistic practices, among other faults. Library blogger Rick Mason responded that although he thought Beall had some \"valid criticisms\" of OCLC, he demurred from some of Beall's statements and warned readers to \"beware the hyperbole and the personal nature of his criticism, for they strongly overshadow that which is worth stating\". In November 2008, the Board of Directors of OCLC unilaterally issued a new Policy for Use and Transfer of WorldCat Records that would have required member libraries to include an OCLC policy note on their bibliographic records; the policy caused an uproar among librarian bloggers. Among those who protested the policy was the non-librarian activist Aaron Swartz, who believed the policy would threaten projects such as the Open Library, Zotero, and Wikipedia, and who started a petition to \"Stop the OCLC powergrab\". Swartz's petition garnered 858 signatures, but the details of his proposed actions went largely unheeded. Within a few months, the library community had forced OCLC to retract its policy and to create a Review Board to consult with member libraries more transparently. In August 2012, OCLC recommended"
  },
  {
    "source": "OCLC.txt",
    "chunk_id": "OCLC.txt_10",
    "chunk": "that member libraries adopt the Open Data Commons Attribution (ODC-BY) license when sharing library catalog data, although some member libraries have explicit agreements with OCLC that they can publish catalog data using the CC0 Public Domain Dedication."
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_0",
    "chunk": "# Orion (constellation) Orion is a prominent set of stars visible during winter in the northern celestial hemisphere. It is one of the 88 modern constellations; it was among the 48 constellations listed by the 2nd-century astronomer Ptolemy. It is named after a hunter in Greek mythology. Orion is most prominent during winter evenings in the Northern Hemisphere, as are five other constellations that have stars in the Winter Hexagon asterism. Orion's two brightest stars, Rigel (β) and Betelgeuse (α), are both among the brightest stars in the night sky; both are supergiants and slightly variable. There are a further six stars brighter than magnitude 3.0, including three making the short straight line of the Orion's Belt asterism. Orion also hosts the radiant of the annual Orionids, the strongest meteor shower associated with Halley's Comet, and the Orion Nebula, one of the brightest nebulae in the sky. Orion is bordered by Taurus to the northwest, Eridanus to the southwest, Lepus to the south, Monoceros to the east, and Gemini to the northeast. Covering 594 square degrees, Orion ranks twenty-sixth of the 88 constellations in size. The constellation boundaries, as set by Belgian astronomer Eugène Delporte in 1930, are defined by"
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_1",
    "chunk": "a polygon of 26 sides. In the equatorial coordinate system, the right ascension coordinates of these borders lie between 04 43.3 and 06 25.5 , while the declination coordinates are between 22.87° and −10.97°. The constellation's three-letter abbreviation, as adopted by the International Astronomical Union in 1922, is \"Ori\". Orion is most visible in the evening sky from January to April, winter in the Northern Hemisphere, and summer in the Southern Hemisphere. In the tropics (less than about 8° from the equator), the constellation transits at the zenith. In the period May–July (summer in the Northern Hemisphere, winter in the Southern Hemisphere), Orion is in the daytime sky and thus invisible at most latitudes. However, for much of Antarctica in the Southern Hemisphere's winter months, the Sun is below the horizon even at midday. Stars (and thus Orion, but only the brightest stars) are then visible at twilight for a few hours around local noon, just in the brightest section of the sky low in the North where the Sun is just below the horizon. At the same time of day at the South Pole itself (Amundsen–Scott South Pole Station), Rigel is only 8° above the horizon, and the Belt"
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_2",
    "chunk": "sweeps just along it. In the Southern Hemisphere's summer months, when Orion is normally visible in the night sky, the constellation is actually not visible in Antarctica because the sun does not set at that time of year south of the Antarctic Circle. In countries close to the equator (e.g., Kenya, Indonesia, Colombia, Ecuador), Orion appears overhead in December around midnight and in the February evening sky. Orion is very useful as an aid to locating other stars. By extending the line of the Belt southeastward, Sirius (α CMa) can be found; northwestward, Aldebaran (α Tau). A line eastward across the two shoulders indicates the direction of Procyon (α CMi). A line from Rigel through Betelgeuse points to Castor and Pollux (α Gem and β Gem). Additionally, Rigel is part of the Winter Circle asterism. Sirius and Procyon, which may be located from Orion by following imaginary lines (see map), also are points in both the Winter Triangle and the Circle. Orion's seven brightest stars form a distinctive hourglass-shaped asterism, or pattern, in the night sky. Four stars—Rigel, Betelgeuse, Bellatrix, and Saiph—form a large roughly rectangular shape, at the center of which lies the three stars of Orion's Belt—Alnitak, Alnilam,"
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_3",
    "chunk": "and Mintaka. His head is marked by an additional 8th star called Meissa, which is fairly bright to the observer. Descending from the \"belt\" is a smaller line of three stars, Orion's Sword (the middle of which is in fact not a star but the Orion Nebula), also known as the hunter's sword. Many of the stars are luminous hot blue supergiants, with the stars of the belt and sword forming the Orion OB1 association. Standing out by its red hue, Betelgeuse may nevertheless be a runaway member of the same group. Orion's Belt or The Belt of Orion is an asterism within the constellation. It consists of the three bright stars Zeta (Alnitak), Epsilon (Alnilam), and Delta (Mintaka). Alnitak is around 800 light years away from Earth and is 100,000 times more luminous than the Sun and shines with magnitude 1.8; much of its radiation is in the ultraviolet range, which the human eye cannot see. Alnilam is approximately 2,000 light years away from Earth, shines with magnitude 1.70, and with ultraviolet light is 375,000 times more luminous than the Sun. Mintaka is 915 light years away and shines with magnitude 2.21. It is 90,000 times more luminous than"
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_4",
    "chunk": "the Sun and is a double star: the two orbit each other every 5.73 days. In the Northern Hemisphere, Orion's Belt is best visible in the night sky during the month of January around 9:00 pm, when it is approximately around the local meridian. Just southwest of Alnitak lies Sigma Orionis, a multiple star system composed of five stars that have a combined apparent magnitude of 3.7 and lying 1150 light years distant. Southwest of Mintaka lies the quadruple star Eta Orionis. Orion's Sword contains the Orion Nebula, the Messier 43 nebula, the Running Man Nebula, and the stars Theta Orionis, Iota Orionis, and 42 Orionis. Three stars comprise a small triangle that marks the head. The apex is marked by Meissa (Lambda Orionis), a hot blue giant of spectral type O8 III and apparent magnitude 3.54, which lies some 1100 light years distant. Phi-1 and Phi-2 Orionis make up the base. Also nearby is the very young star FU Orionis. Stretching north from Betelgeuse are the stars that make up Orion's club. Mu Orionis marks the elbow, Nu and Xi mark the handle of the club, and Chi and Chi mark the end of the club. Just east of"
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_5",
    "chunk": "Chi is the Mira-type variable red giant U Orionis. West from Bellatrix lie six stars all designated Pi Orionis (π Ori, π Ori, π Ori, π Ori, π Ori and π Ori) which make up Orion's shield. Around 20 October each year the Orionid meteor shower (Orionids) reaches its peak. Coming from the border with the constellation Gemini as many as 20 meteors per hour can be seen. The shower's parent body is Halley's Comet. Hanging from Orion's belt is his sword, consisting of the multiple stars θ1 and θ2 Orionis, called the Trapezium and the Orion Nebula (M42). This is a spectacular object that can be clearly identified with the naked eye as something other than a star. Using binoculars, its clouds of nascent stars, luminous gas, and dust can be observed. The Trapezium cluster has many newborn stars, including several brown dwarfs, all of which are at an approximate distance of 1,500 light-years. Named for the four bright stars that form a trapezoid, it is largely illuminated by the brightest stars, which are only a few hundred thousand years old. Observations by the Chandra X-ray Observatory show both the extreme temperatures of the main stars—up to 60,000 kelvins—and"
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_6",
    "chunk": "the star forming regions still extant in the surrounding nebula. M78 (NGC 2068) is a nebula in Orion. With an overall magnitude of 8.0, it is significantly dimmer than the Great Orion Nebula that lies to its south; however, it is at approximately the same distance, at 1600 light-years from Earth. It can easily be mistaken for a comet in the eyepiece of a telescope. M78 is associated with the variable star V351 Orionis, whose magnitude changes are visible in very short periods of time. Another fairly bright nebula in Orion is NGC 1999, also close to the Great Orion Nebula. It has an integrated magnitude of 10.5 and is 1500 light-years from Earth. The variable star V380 Orionis is embedded in NGC 1999. Another famous nebula is IC 434, the Horsehead Nebula, near ζ Orionis. It contains a dark dust cloud whose shape gives the nebula its name. Besides these nebulae, surveying Orion with a small telescope will reveal a wealth of interesting deep-sky objects, including M43, M78, as well as multiple stars including Iota Orionis and Sigma Orionis. A larger telescope may reveal objects such as the Flame Nebula (NGC 2024), as well as fainter and tighter multiple"
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_7",
    "chunk": "stars and nebulae. Barnard's Loop can be seen on very dark nights or using long-exposure photography. All of these nebulae are part of the larger Orion molecular cloud complex, which is located approximately 1,500 light-years away and is hundreds of light-years across. It is one of the most intense regions of stellar formation visible within the Milky Way Galaxy. The distinctive pattern of Orion is recognized in numerous cultures around the world, and many myths are associated with it. Orion is used as a symbol in the modern world. The Babylonian star catalogues of the Late Bronze Age name Orion SIPA.ZI.AN.NA, \"The Heavenly Shepherd\" or \"True Shepherd of Anu\" – Anu being the chief god of the heavenly realms. The Babylonian constellation is sacred to Papshukal and Ninshubur, both minor gods fulfilling the role of 'messenger to the gods'. Papshukal is closely associated with the figure of a walking bird on Babylonian boundary stones, and on the star map the figure of the Rooster is located below and behind the figure of the True Shepherd—both constellations represent the herald of the gods, in his bird and human forms respectively. In ancient Egypt, the stars of Orion were regarded as a"
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_8",
    "chunk": "god, called Sah. Because Orion rises before Sirius, the star whose heliacal rising was the basis for the Solar Egyptian calendar, Sah was closely linked with Sopdet, the goddess who personified Sirius. The god Sopdu is said to be the son of Sah and Sopdet. Sah is syncretized with Osiris, while Sopdet is syncretized with Osiris' mythological wife, Isis. In the Pyramid Texts, from the 24th and 23rd centuries BC, Sah is one of many gods whose form the dead pharaoh is said to take in the afterlife. The Armenians identified their legendary patriarch and founder Hayk with Orion. Hayk is also the name of the Orion constellation in the Armenian translation of the Bible. The Bible mentions Orion three times, naming it \"Kesil\" (כסיל, literally – fool). Though, this name perhaps is etymologically connected with \"Kislev\", the name for the ninth month of the Hebrew calendar (i.e. November–December), which, in turn, may derive from the Hebrew root K-S-L as in the words \"kesel, kisla\" (כֵּסֶל, כִּסְלָה, hope, positiveness), i.e. hope for winter rains.: Job 9:9 (\"He is the maker of the Bear and Orion\"), Job 38:31 (\"Can you loosen Orion's belt?\"), and Amos 5:8 (\"He who made the Pleiades"
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_9",
    "chunk": "and Orion\"). In ancient Aram, the constellation was known as Nphîlā′, the Nephilim are said to be Orion's descendants. In Greek mythology, Orion was a gigantic, supernaturally strong hunter, born to Euryale, a Gorgon, and Poseidon (Neptune), god of the sea. One myth recounts Gaia's rage at Orion, who dared to say that he would kill every animal on Earth. The angry goddess tried to dispatch Orion with a scorpion. This is given as the reason that the constellations of Scorpius and Orion are never in the sky at the same time. However, Ophiuchus, the Serpent Bearer, revived Orion with an antidote. This is said to be the reason that the constellation of Ophiuchus stands midway between the Scorpion and the Hunter in the sky. The constellation is mentioned in Horace's Odes (Ode 3.27.18), Homer's Odyssey (Book 5, line 283) and Iliad, and Virgil's Aeneid (Book 1, line 535). In medieval Muslim astronomy, Orion was known as al-jabbar, \"the giant\". Orion's sixth brightest star, Saiph, is named from the Arabic, saif al-jabbar, meaning \"sword of the giant\". In China, Orion was one of the 28 lunar mansions Sieu (Xiù) (宿). It is known as Shen (參), literally meaning \"three\", for"
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_10",
    "chunk": "the stars of Orion's Belt. (See Chinese constellations) The Chinese character 參 (pinyin shēn) originally meant the constellation Orion (Chinese: 參宿; pinyin: shēnxiù); its Shang dynasty version, over three millennia old, contains at the top a representation of the three stars of Orion's belt atop a man's head (the bottom portion representing the sound of the word was added later). Nataraja, 'the cosmic dancer', is often interpreted as the representation of Orion. Rudra, the Rigvedic form of Shiva, is the presiding deity of Ardra nakshatra (Betelgeuse) of Hindu astrology. The Jain Symbol carved in Udayagiri and Khandagiri Caves, India in 1st century BCE has striking resemblance with Orion. Bugis sailors identified the three stars in Orion's Belt as tanra tellué, meaning \"sign of three\". In old Hungarian tradition, Orion is known as \"Archer\" (Íjász), or \"Reaper\" (Kaszás). In recently rediscovered myths, he is called Nimrod (Hungarian: Nimród), the greatest hunter, father of the twins Hunor and Magor. The π and o stars (on upper right) form together the reflex bow or the lifted scythe. In other Hungarian traditions, Orion's belt is known as \"Judge's stick\" (Bírópálca). In Scandinavian tradition, Orion's belt was known as \"Frigg's Distaff\" (friggerock) or \"Freyja's distaff\"."
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_11",
    "chunk": "The Finns call Orion's belt and the stars below it \"Väinämöinen's scythe\" (Väinämöisen viikate). Another name for the asterism of Alnilam, Alnitak and Mintaka is \"Väinämöinen's Belt\" (Väinämöisen vyö) and the stars \"hanging\" from the belt as \"Kaleva's sword\" (Kalevanmiekka). In Siberia, the Chukchi people see Orion as a hunter; an arrow he has shot is represented by Aldebaran (Alpha Tauri), with the same figure as other Western depictions. There are claims in popular media that the Adorant from the Geißenklösterle cave, an ivory carving estimated to be 35,000 to 40,000 years old, is the first known depiction of the constellation. Scholars dismiss such interpretations, saying that perceived details such as a belt and sword derive from preexisting features in the grain structure of the ivory. The Seri people of northwestern Mexico call the three stars in the belt of Orion Hapj (a name denoting a hunter) which consists of three stars: Hap (mule deer), Haamoja (pronghorn), and Mojet (bighorn sheep). Hap is in the middle and has been shot by the hunter; its blood has dripped onto Tiburón Island. The same three stars are known in Spain and most of Latin America as \"Las tres Marías\" (Spanish for \"The"
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_12",
    "chunk": "Three Marys\"). In Puerto Rico, the three stars are known as the \"Los Tres Reyes Magos\" (Spanish for The three Wise Men). The Ojibwa/Chippewa Native Americans call this constellation Mesabi for Big Man. To the Lakota Native Americans, Tayamnicankhu (Orion's Belt) is the spine of a bison. The great rectangle of Orion is the bison's ribs; the Pleiades star cluster in nearby Taurus is the bison's head; and Sirius in Canis Major, known as Tayamnisinte, is its tail. Another Lakota myth mentions that the bottom half of Orion, the Constellation of the Hand, represented the arm of a chief that was ripped off by the Thunder People as a punishment from the gods for his selfishness. His daughter offered to marry the person who can retrieve his arm from the sky, so the young warrior Fallen Star (whose father was a star and whose mother was human) returned his arm and married his daughter, symbolizing harmony between the gods and humanity with the help of the younger generation. The index finger is represented by Rigel; the Orion Nebula is the thumb; the Belt of Orion is the wrist; and the star Beta Eridani is the pinky finger. The seven primary"
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_13",
    "chunk": "stars of Orion make up the Polynesian constellation Heiheionakeiki which represents a child's string figure similar to a cat's cradle. Several precolonial Filipinos referred to the belt region in particular as \"balatik\" (ballista) as it resembles a trap of the same name which fires arrows by itself and is usually used for catching pigs from the bush. Spanish colonization later led to some ethnic groups referring to Orion's belt as \"Tres Marias\" or \"Tatlong Maria.\" In Māori tradition, the star Rigel (known as Puanga or Puaka) is closely connected with the celebration of Matariki. The rising of Matariki (the Pleiades) and Rigel before sunrise in midwinter marks the start of the Māori year. In Javanese culture, the constellation is often called Lintang Waluku or Bintang Bajak, referring to the shape of a paddy field plow. The imagery of the belt and sword has found its way into popular western culture, for example in the form of the shoulder insignia of the 27th Infantry Division of the United States Army during both World Wars, probably owing to a pun on the name of the division's first commander, Major General John F. O'Ryan. In artistic renderings, the surrounding constellations are sometimes related"
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_14",
    "chunk": "to Orion: he is depicted standing next to the river Eridanus with his two hunting dogs Canis Major and Canis Minor, fighting Taurus. He is sometimes depicted hunting Lepus the hare. He sometimes is depicted to have a lion's hide in his hand. There are alternative ways to visualise Orion. From the Southern Hemisphere, Orion is oriented south-upward, and the belt and sword are sometimes called the saucepan or pot in Australia and New Zealand. Orion's Belt is called Drie Konings (Three Kings) or the Drie Susters (Three Sisters) by Afrikaans speakers in South Africa and are referred to as les Trois Rois (the Three Kings) in Daudet's Lettres de Mon Moulin (1866). The appellation Driekoningen (the Three Kings) is also often found in 17th- and 18th-century Dutch star charts and seaman's guides. The same three stars are known in Spain, Latin America, and the Philippines as \"Las Tres Marías\" (The Three Marys), and as \"Los Tres Reyes Magos\" (The three Wise Men) in Puerto Rico. Even traditional depictions of Orion have varied greatly. Cicero drew Orion in a similar fashion to the modern depiction. The Hunter held an unidentified animal skin aloft in his right hand; his hand was"
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_15",
    "chunk": "represented by Omicron Orionis and the skin was represented by the 5 stars designated Pi Orionis. Kappa and Beta Orionis represented his left and right knees, while Eta and Lambda Leporis were his left and right feet, respectively. As in the modern depiction, Delta, Epsilon, and Zeta represented his belt. His left shoulder was represented by Alpha Orionis, and Mu Orionis made up his left arm. Lambda Orionis was his head and Gamma, his right shoulder. The depiction of Hyginus was similar to that of Cicero, though the two differed in a few important areas. Cicero's animal skin became Hyginus's shield (Omicron and Pi Orionis), and instead of an arm marked out by Mu Orionis, he holds a club (Chi Orionis). His right leg is represented by Theta Orionis and his left leg is represented by Lambda, Mu, and Epsilon Leporis. Further Western European and Arabic depictions have followed these two models. Orion is located on the celestial equator, but it will not always be so located due to the effects of precession of the Earth's axis. Orion lies well south of the ecliptic, and it only happens to lie on the celestial equator because the point on the ecliptic"
  },
  {
    "source": "Orion (constellation).txt",
    "chunk_id": "Orion (constellation).txt_16",
    "chunk": "that corresponds to the June solstice is close to the border of Gemini and Taurus, to the north of Orion. Precession will eventually carry Orion further south, and by AD 14000, Orion will be far enough south that it will no longer be visible from the latitude of Great Britain. Further in the future, Orion's stars will gradually move away from the constellation due to proper motion. However, Orion's brightest stars all lie at a large distance from the Earth on an astronomical scale—much farther away than Sirius, for example. Orion will still be recognizable long after most of the other constellations—composed of relatively nearby stars—have distorted into new configurations, with the exception of a few of its stars eventually exploding as supernovae, for example Betelgeuse, which is predicted to explode sometime in the next million years."
  },
  {
    "source": "Orrery.txt",
    "chunk_id": "Orrery.txt_0",
    "chunk": "# Orrery An orrery is a mechanical model of the Solar System that illustrates or predicts the relative positions and motions of the planets and moons, usually according to the heliocentric model. It may also represent the relative sizes of these bodies; however, since accurate scaling is often not practical due to the actual large ratio differences, it may use a scaled-down approximation. The Greeks had working planetaria, but the first modern example was produced c. 1712 by John Rowley. He named it \"orrery\" for his patron Charles Boyle, 4th Earl of Orrery (in County Cork, Ireland). The plaque on it reads \"Orrery invented by Graham 1700 improved by Rowley and presented by him to John [sic] Earl of Orrery after whom it was named at the suggestion of Richard Steele.\" Orreries are typically driven by a clockwork mechanism with a globe representing the Sun at the centre, and with a planet at the end of each of a series of arms. The Antikythera mechanism, discovered in 1901 in a wreck off the Greek island of Antikythera in the Mediterranean Sea, exhibited the diurnal motions of the Sun, Moon, and the five planets known to the ancient Greeks. It has"
  },
  {
    "source": "Orrery.txt",
    "chunk_id": "Orrery.txt_1",
    "chunk": "been dated between 205 to 87 BC. The mechanism is considered one of the first orreries. It was geocentric and used as a mechanical calculator to calculate astronomical positions. Cicero, the Roman philosopher and politician writing in the first century BC, has references describing planetary mechanical models. According to him, the Greek polymaths Thales and Posidonius both constructed a device modeling celestial motion. In 1348, Giovanni Dondi built the first known clock driven mechanism of the system. It displays the ecliptic position of the Moon, Sun, Mercury, Venus, Mars, Jupiter and Saturn according to the complicated geocentric Ptolemaic planetary theories. The clock itself is lost, but Dondi left a complete description of its astronomic gear trains. As late as 1650, P. Schirleus built a geocentric planetarium with the Sun as a planet, and with Mercury and Venus revolving around the Sun as its moons. At the court of William IV, Landgrave of Hesse-Kassel two complicated astronomic clocks were built in 1561 and 1563–1568. These use four sides to show the ecliptical positions of the Sun, Mercury, Venus, Mars, Jupiter, Saturn, the Moon, Sun and Dragon (Nodes of the Moon) according to Ptolemy, a calendar, the sunrise and sunset, and an"
  },
  {
    "source": "Orrery.txt",
    "chunk_id": "Orrery.txt_2",
    "chunk": "automated celestial sphere with an animated Sun symbol which, for the first time on a celestial globe, shows the real position of the Sun, including the equation of time. The clocks are now on display in Kassel at the Astronomisch-Physikalisches Kabinett and in Dresden at the Mathematisch-Physikalischer Salon. In De revolutionibus orbium coelestium, published in Nuremberg in 1543, Nicolaus Copernicus challenged the Western teaching of a geocentric universe in which the Sun revolved daily around the Earth. He observed that some Greek philosophers such as Aristarchus of Samos had proposed a heliocentric universe. This simplified the apparent epicyclic motions of the planets, making it feasible to represent the planets' paths as simple circles. This could be modeled by the use of gears. Tycho Brahe's improved instruments made precise observations of the skies (1576–1601), and from these Johannes Kepler (1621) deduced that planets orbited the Sun in ellipses. In 1687 Isaac Newton explained the cause of elliptic motion in his theory of gravitation. There is an orrery built by clock makers George Graham and Thomas Tompion dated c. 1710 in the History of Science Museum, Oxford. Graham gave the first model, or its design, to the celebrated instrument maker John Rowley"
  },
  {
    "source": "Orrery.txt",
    "chunk_id": "Orrery.txt_3",
    "chunk": "of London to make a copy for Prince Eugene of Savoy. Rowley was commissioned to make another copy for his patron Charles Boyle, 4th Earl of Orrery, from which the device took its name in English. This model was presented to Charles' son John, later the 5th Earl of Cork and 5th Earl of Orrery. Independently, Christiaan Huygens published in 1703 details of a heliocentric planetary machine which he had built while living in Paris between 1665 and 1681. He calculated the gear trains needed to represent a year of 365.242 days, and used that to produce the cycles of the principal planets. Joseph Wright's painting A Philosopher giving a Lecture on the Orrery (c. 1766), which hangs in the Derby Museum and Art Gallery, depicts a group listening to a lecture by a natural philosopher. The Sun in a brass orrery provides the only light in the room. The orrery depicted in the painting has rings, which give it an appearance similar to that of an armillary sphere. The demonstration was thereby able to depict eclipses. To put this in chronological context, in 1762 John Harrison's marine chronometer first enabled accurate measurement of longitude. In 1766, astronomer Johann Daniel"
  },
  {
    "source": "Orrery.txt",
    "chunk_id": "Orrery.txt_4",
    "chunk": "Titius first demonstrated that the mean distance of each planet from the Sun could be represented by the following progression: 4 + 0 10 , 4 + 3 10 , 4 + 6 10 , 4 + 12 10 , 4 + 24 10 , . . . {\\displaystyle {\\frac {4+0}{10}},{\\frac {4+3}{10}},{\\frac {4+6}{10}},{\\frac {4+12}{10}},{\\frac {4+24}{10}},...} That is, 0.4, 0.7, 1.0, 1.6, 2.8, ... The numbers refer to astronomical units, the mean distance between Sun and Earth, which is 1.496 × 10 km (93 × 10 miles). The Derby Orrery does not show mean distance, but demonstrated the relative planetary movements. The Eisinga Planetarium was built from 1774 to 1781 by Eise Eisinga in his home in Franeker, in the Netherlands. It displays the planets across the width of a room's ceiling, and has been in operation almost continually since it was created. This orrery is a planetarium in both senses of the word: a complex machine showing planetary orbits, and a theatre for depicting the planets' movement. Eisinga house was bought by the Dutch Royal family who gave him a pension. In 1764, Benjamin Martin devised a new type of planetary model, in which the planets were carried on brass"
  },
  {
    "source": "Orrery.txt",
    "chunk_id": "Orrery.txt_5",
    "chunk": "arms leading from a series of concentric or coaxial tubes. With this construction it was difficult to make the planets revolve, and to get the moons to turn around the planets. Martin suggested that the conventional orrery should consist of three parts: the planetarium where the planets revolved around the Sun, the tellurion (also tellurian or tellurium) which showed the inclined axis of the Earth and how it revolved around the Sun, and the lunarium which showed the eccentric rotations of the Moon around the Earth. In one orrery, these three motions could be mounted on a common table, separately using the central spindle as a prime mover. All orreries are planetariums. The term orrery has only existed since 1714. A grand orrery is one that includes the outer planets known at the time of its construction. The word planetarium has shifted meaning, and now usually refers to hemispherical theatres in which images of the night sky are projected onto an overhead surface. Orreries can range widely in size from hand-held to room-sized. An orrery is used to demonstrate the motion of the planets, while a mechanical device used to predict eclipses and transits is called an astrarium. An orrery"
  },
  {
    "source": "Orrery.txt",
    "chunk_id": "Orrery.txt_6",
    "chunk": "should properly include the Sun, the Earth and the Moon (plus optionally other planets). A model that only includes the Earth, the Moon, and the Sun is called a tellurion or tellurium, and one which only includes the Earth and the Moon is a lunarium. A jovilabe is a model of Jupiter and its moons. A planetarium will show the orbital period of each planet and the rotation rate, as shown in the table above. A tellurion will show the Earth with the Moon revolving around the Sun. It will use the angle of inclination of the equator from the table above to show how it rotates around its own axis. It will show the Earth's Moon, rotating around the Earth. A lunarium is designed to show the complex motions of the Moon as it revolves around the Earth. Orreries are usually not built to scale. Human orreries, where humans move about as the planets, have also been constructed, but most are temporary. There is a permanent human orrery at Armagh Observatory in Northern Ireland, which has the six ancient planets, Ceres, and comets Halley and Encke. Uranus and beyond are also shown, but in a fairly limited way. Another"
  },
  {
    "source": "Orrery.txt",
    "chunk_id": "Orrery.txt_7",
    "chunk": "is at Sky's the Limit Observatory and Nature Center in Twentynine Palms, California; it is a true to scale (20 billion to one), true to position (accurate to within four days) human orrery. The first four planets are relatively close to one another, but the next four require a certain amount of hiking in order to visit them. A census of all permanent human orreries has been initiated by the French group F-HOU with a new effort to study their impact for education in schools. A map of known human orreries is available. A normal mechanical clock could be used to produce an extremely simple orrery to demonstrate the principle, with the Sun in the centre, Earth on the minute hand and Jupiter on the hour hand; Earth would make 12 revolutions around the Sun for every 1 revolution of Jupiter. As Jupiter's actual year is 11.86 Earth years long, the model would lose accuracy rapidly. Many planetariums have a projection orrery, which projects onto the dome of the planetarium a Sun with either dots or small images of the planets. These usually are limited to the planets from Mercury to Saturn, although some include Uranus. The light sources for"
  },
  {
    "source": "Orrery.txt",
    "chunk_id": "Orrery.txt_8",
    "chunk": "the planets are projected onto mirrors which are geared to a motor which drives the images on the dome. Typically the Earth will circle the Sun in one minute, while the other planets will complete an orbit in time periods proportional to their actual motion. Thus Venus, which takes 224.7 days to orbit the Sun, will take 37 seconds to complete an orbit on an orrery, and Jupiter will take 11 minutes, 52 seconds. Some planetariums have taken advantage of this to use orreries to simulate planets and their moons. Thus Mercury orbits the Sun in 0.24 of an Earth year, while Phobos and Deimos orbit Mars in a similar 4:1 time ratio. Planetarium operators wishing to show this have placed a red cap on the Sun (to make it resemble Mars) and turned off all the planets but Mercury and Earth. Similar approximations can be used to show Pluto and its five moons. Shoemaker John Fulton of Fenwick, Ayrshire, built three between 1823 and 1833. The last is in Glasgow's Kelvingrove Art Gallery and Museum. The Eisinga Planetarium built by a wool carder named Eise Eisinga in his own living room, in the small city of Franeker in Friesland,"
  },
  {
    "source": "Orrery.txt",
    "chunk_id": "Orrery.txt_9",
    "chunk": "is in fact an orrery. It was constructed between 1774 and 1781. The base of the model faces down from the ceiling of the room, with most of the mechanical works in the space above the ceiling. It is driven by a pendulum clock, which has 9 weights or ponds. The planets move around the model in real time. An innovative concept is to have people play the role of the moving planets and other Solar System objects. Such a model, called a human orrery, has been laid out at the Armagh Observatory. In 2024, the LEGO Group commercially produced an orrery of the Sun, Earth, and Moon. The model is assembled exclusively from LEGO elements and reproduces solar and lunar orbits, as well Earth's rotation about a tilted axis."
  },
  {
    "source": "Pallas family.txt",
    "chunk_id": "Pallas family.txt_0",
    "chunk": "# Pallas family The Pallas family (adj. Palladian; FIN: 801) is a small asteroid family of B-type asteroids at very high inclinations in the intermediate asteroid belt. The family was identified by Kiyotsugu Hirayama in 1928. The namesake of the family is 2 Pallas, an extremely large asteroid with a mean diameter of about 512 km. The remaining Palladian asteroids are far smaller; the largest is 5222 Ioffe with an estimated diameter of 22 km. This, along with the preponderance of the otherwise rare B spectral type among its members, indicates that this is likely a cratering family composed of ejecta from impacts on Pallas. Another suspected Palladian is 3200 Phaethon, the parent body of the Geminid meteor shower. At the present epoch, the range of osculating orbital elements of the members (by comparison to the MPCORB database ) is about"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_0",
    "chunk": "# Parmenides Parmenides of Elea (/pɑːrˈmɛnɪdiːz ... ˈɛliə/; Ancient Greek: Παρμενίδης ὁ Ἐλεάτης; fl. late sixth or early fifth century BC) was a pre-Socratic Greek philosopher from Elea in Magna Graecia (Southern Italy). Parmenides was born in the Greek colony of Elea to a wealthy and illustrious family. The exact date of his birth is not known with certainty; on the one hand, according to the doxographer Diogenes Laërtius, Parmenides flourished in the period immediately preceding 500 BC, which would place his year of birth around 540 BC; on the other hand, in the dialogue Parmenides Plato portrays him as visiting Athens at the age of 65, when Socrates was a young man, c. 450 BC, which, if true, suggests a potential year of birth of c. 515 BC. Parmenides is thought to have been in his prime (or \"floruit\") around 475 BC. The single known work by Parmenides is a poem whose original title is unknown but which is often referred to as On Nature. Only fragments of it survive. In his poem, Parmenides prescribes two views of reality. The first, the Way of \"Aletheia\" or truth, describes how all reality is one, change is impossible, and existence is"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_1",
    "chunk": "timeless and uniform. The second view, the way of \"Doxa\" or opinion, describes the world of appearances, in which one's sensory faculties lead to conceptions which are false and deceitful. Parmenides has been considered the founder of ontology and has, through his influence on Plato, influenced the whole history of Western philosophy. He is also considered to be the founder of the Eleatic school of philosophy, which also included Zeno of Elea and Melissus of Samos. Zeno's paradoxes of motion were developed to defend Parmenides's views. In contemporary philosophy, Parmenides's work has remained relevant in debates about the philosophy of time. Parmenides was born in Elea to an aristocratic family. Diogenes Laertius says that his father was Pires. Laertius transmits two divergent sources regarding the teacher of the philosopher. One, dependent on Sotion, indicates that he was first a student of Xenophanes, but did not follow him, and later became associated with a Pythagorean, Aminias, whom he preferred as his teacher. Another tradition, dependent on Theophrastus, indicates that he was a disciple of Anaximander. Parmenides was one of the pre-Socratic philosophers. As with all pre-Socratic philosophers, the little known about his life and work comes from writings and quotations by"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_2",
    "chunk": "later philosophers. Parmenides founded his school of thought in Elea. His ideas were followed by Melissus of Samos and Zeno of Elea, with the latter being a close friend of Parmenides. All conjectures regarding Parmenides's date of birth are based on two ancient sources. One comes from Apollodorus and is transmitted to us by Diogenes Laertius: this source marks the Olympiad 69th (between 504 BC and 500 BC) as the moment of maturity, placing his birth 40 years earlier (544 BC – 540 BC). The other is Plato, in his dialogue Parmenides. There Plato composes a situation in which Parmenides, 65, and Zeno, 40, travel to Athens to attend the Panathenaic Games. On that occasion they meet Socrates, who was still very young according to the Platonic text. The inaccuracy of the dating from Apollodorus is well known, who chooses the date of a historical event to make it coincide with the maturity (the floruit) of a philosopher, a maturity that he invariably reached at forty years of age. He tries to always match the maturity of a philosopher with the birth of his alleged disciple. In this case Apollodorus, according to Burnet, based his date of the foundation of"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_3",
    "chunk": "Elea (540 BC) to chronologically locate the maturity of Xenophanes and thus the birth of his supposed disciple, Parmenides. Knowing this, Burnet and later classicists like Cornford, Raven, Guthrie, and Schofield preferred to base the calculations on the Platonic dialogue. According to the latter, the fact that Plato adds so much detail regarding ages in his text is a sign that he writes with chronological precision. Plato says that Socrates was very young, and this is interpreted to mean that he was less than twenty years old. We know the year of Socrates's death (399 BC) and his age—he was about seventy years old—making the date of his birth 469 BC. The Panathenaic games were held every four years, and of those held during Socrates's youth (454, 450, 446), the most likely is that of 450 BC, when Socrates was nineteen years old. Thus, if at this meeting Parmenides was about sixty-five years old, his birth occurred around 515 BC. However, neither Raven nor Schofield, who follows the former, finds a dating based on a late Platonic dialogue entirely satisfactory. Other scholars directly prefer not to use the Platonic testimony and propose other dates. According to a scholar of the"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_4",
    "chunk": "Platonic dialogues, R. Hirzel, Conrado Eggers Lan indicates that the historical has no value for Plato. The fact that the meeting between Socrates and Parmenides is mentioned in the dialogues Theaetetus (183e) and Sophist (217c) only indicates that it is referring to the same fictional event, and this is possible because both the Theaetetus and the Sophist are considered after the Parmenides. In Soph. 217c the dialectic procedure of Socrates is attributed to Parmenides, which would confirm that this is nothing more than a reference to the fictitious dramatic situation of the dialogue. Eggers Lan proposes a correction of the traditional date of the foundation of Elea. Based on Herodotus I, 163–167, which indicates that the Phocians, after defeating the Carthaginians in naval battle, founded Elea, and adding the reference to Thucydides I, 13, where it is indicated that such a battle occurred in the time of Cambyses II, the foundation of Elea can be placed between 530 BC and 522 BC So Parmenides could not have been born before 530 BC or after 520 BC, given that it predates Empedocles. This last dating procedure is not infallible either, because it has been questioned that the fact that links the"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_5",
    "chunk": "passages of Herodotus and Thucydides is the same. Nestor Luis Cordero also rejects the chronology based on the Platonic text, and the historical reality of the encounter, in favor of the traditional date of Apollodorus. He follows the traditional datum of the founding of Elea in 545 BC, pointing to it not only as terminus post quem, but as a possible date of Parmenides's birth, from which he concludes that his parents were part of the founding contingent of the city and that he was a contemporary of Heraclitus. The evidence suggests that Parmenides could not have written much after the death of Heraclitus. Parmenides would have been familiar with previous philosophers, such as the Milesians, as well as writers such as Homer and Hesiod. He is understood to have known a Pythagorean philosopher, Ameinias, who introduced him to philosophy. Parmenides is sometimes described as beginning his study in the Milesian school under the tutelage of Anaximenes. He may alternatively have been a student of Xenophanes. Plato said that Parmenides traveled to Athens in 450 BCE, where he interacted with Socrates in the latter's youth. Beyond the speculations and inaccuracies about his date of birth, some specialists have turned their"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_6",
    "chunk": "attention to certain passages of his work to specify the relationship of Parmenides with other thinkers. It was thought to find in his poem certain controversial allusions to the doctrine of Anaximenes and the Pythagoreans (fragment B 8, verse 24, and frag. B 4), and also against Heraclitus (frag .B 6, vv.8–9), while Empedocles and Anaxagoras frequently refer to Parmenides. The reference to Heraclitus has been debated. Bernays's thesis that Parmenides attacks Heraclitus, to which Diels, Kranz, Gomperz, Burnet and others adhered, was discussed by Reinhardt, whom Jaeger followed. Guthrie finds it surprising that Heraclitus would not have censured Parmenides if he had known him, as he did with Xenophanes and Pythagoras. His conclusion, however, does not arise from this consideration, but points out that, due to the importance of his thought, Parmenides splits the history of pre-Socratic philosophy in two; therefore his position with respect to other thinkers is easy to determine. From this point of view, the philosophy of Heraclitus seems to him pre-Parmenidean, while those of Empedocles, Anaxagoras and Democritus are post-Parmenidean. Parmenides worked in government, as was common for pre-Socratic philosophers. Plutarch, Strabo and Diogenes—following the testimony of Speusippus—agree that Parmenides participated in the government of"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_7",
    "chunk": "his city, organizing it and giving it a code of admirable laws. In 1969, the plinth of a statue dated to the 1st century AD was excavated in Velia. On the plinth were four words: ΠΑ[Ρ]ΜΕΝΕΙΔΗΣ ΠΥΡΗΤΟΣ ΟΥΛΙΑΔΗΣ ΦΥΣΙΚΟΣ. The first two clearly read \"Parmenides, son of Pires.\" The fourth word φυσικός (fysikós, \"physicist\") was commonly used to designate philosophers who devoted themselves to the observation of nature. On the other hand, there is no agreement on the meaning of the third (οὐλιάδης, ouliadēs): it can simply mean \"a native of Elea\" (the name \"Velia\" is in Greek Οὐέλια), or \"belonging to the Οὐλιος\" (Ulios), that is, to a medical school (the patron of which was Apollo Ulius). If this last hypothesis were true, then Parmenides would be, in addition to being a legislator, a doctor. The hypothesis is reinforced by the ideas contained in fragment 18 of his poem, which contains anatomical and physiological observations. However, other specialists believe that the only certainty we can extract from the discovery is that of the social importance of Parmenides in the life of his city, already indicated by the testimonies that indicate his activity as a legislator. Plato, in his dialogue"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_8",
    "chunk": "Parmenides, relates that, accompanied by his disciple Zeno of Elea, Parmenides visited Athens when he was approximately sixty-five years old and that, on that occasion, Socrates, then a young man, conversed with him. Athenaeus of Naucratis had noted that, although the ages make a dialogue between Parmenides and Socrates hardly possible, the fact that Parmenides has sustained arguments similar to those sustained in the Platonic dialogue is something that seems impossible. Most modern classicists consider the visit to Athens and the meeting and conversation with Socrates to be fictitious. Allusions to this visit in other Platonic works are only references to the same fictitious dialogue and not to a historical fact. Parmenides's sole work, which has only survived in fragments, is a poem in dactylic hexameter, later titled On Nature. Approximately 160 verses remain today from an original total that was probably near 800. The poem was originally divided into three parts: an introductory proem that contains an allegorical narrative which explains the purpose of the work, a former section known as \"The Way of Truth\" (aletheia, ἀλήθεια), and a latter section known as \"The Way of Appearance/Opinion\" (doxa, δόξα). Despite the poem's fragmentary nature, the general plan of both"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_9",
    "chunk": "the proem and the first part, \"The Way of Truth\", have been ascertained by modern scholars, thanks to large excerpts made by Sextus Empiricus and Simplicius of Cilicia. Unfortunately, the second part, \"The Way of Opinion\", which is supposed to have been much longer than the first, only survives in small fragments and prose paraphrases. The introductory poem describes the narrator's journey to receive a revelation from an unnamed goddess on the nature of reality. The remainder of the work is then presented as the spoken revelation of the goddess without any accompanying narrative. The narrative of the poet's journey includes a variety of allegorical symbols, such as a speeding chariot with glowing axles, horses, the House of Night, Gates of the paths of Night and Day, and maidens who are \"the daughters of the Sun\" who escort the poet from the ordinary daytime world to a strange destination, outside our human paths. The allegorical themes in the poem have attracted a variety of different interpretations, including comparisons to Homer and Hesiod, and attempts to relate the journey towards either illumination or darkness, but there is little scholarly consensus about any interpretation, and the surviving evidence from the poem itself,"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_10",
    "chunk": "as well as any other literary use of allegory from the same time period, may be too sparse to ever determine any of the intended symbolism with certainty. In the Way of Truth, an estimated 90% of which has survived, Parmenides distinguishes between the unity of nature and its variety, insisting in the Way of Truth upon the reality of its unity, which is therefore the object of knowledge, and upon the unreality of its variety, which is therefore the object, not of knowledge, but of opinion. This contrasts with the argument in the section called \"the way of opinion\", which discusses that which is illusory. In the significantly longer, but far worse preserved latter section of the poem, Way of Opinion, Parmenides propounds a theory of the world of seeming and its development, pointing out, however, that, in accordance with the principles already laid down, these cosmological speculations do not pretend to anything more than mere appearance. The structure of the cosmos is a fundamental binary principle that governs the manifestations of all the particulars: \"the Aether fire of flame\" (B 8.56), which is gentle, mild, soft, thin and clear, and self-identical, and the other is \"ignorant night\", body"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_11",
    "chunk": "thick and heavy. Cosmology originally comprised the greater part of his poem, explaining the world's origins and operations. Some idea of the sphericity of the Earth also seems to have been known to Parmenides. Parmenides' concept of Being and that which is was the first introduction of such ideas in the Western world. As the first of the Eleatics, Parmenides is generally credited with being the philosopher who first defined ontology as a separate discipline distinct from theology. He also introduced the contemplation of thought. Parmenides was a contemporary of Heraclitus. The two broached many of the same ideas, such as a world made of two competing opposites, reliability of the senses, and the nature of plurality—it is unclear if either influenced the other. Parmenides' philosophy formed the basis of the Eleatic school, which was continued by Melissus of Samos and Zeno of Elea. Melissus built on Parmenides' ideas, proposing a continuous eternal Being instead of a constant existence of the present. Zeno created reductio ad absurdum arguments to defend Parmenides' ideas against their critics. Parmenides influenced both the Sophists and the students of Socrates. The Sophists rejected Parmenides' concept of discourse, arguing that discourse should consist of opinions and"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_12",
    "chunk": "known entities. The school of atomism was created by Leucippus and Democritus as a direct response to the Eleatics, and Leucippus may have initially studied as an Eleatic. Plato portrayed Parmenides and his philosophy positively in the Platonic dialogues, though he considered it to be particularly challenging to interpret. In the dialogue Parmenides, Plato portrays Parmenides teaching Socrates about the unity of Being and the physical world. To demonstrate the complexity of ideas existing in unity as Being, he has the two characters create metaphors: Socrates compares ideas to daylight as one entity that shines in many places, and Parmenides compares them to a single veil draped over several people. Plato contemplated Parmenides' concept of Being in the Sophist. His dialogue Timaeus is understood to be influenced by Parmenides' cosmology. Plato referred to the Eleatics as \"the partisans of the universe\". He agreed with Parmenides' conception of Being and adapted Parmenides' different levels of truth to distinguish science (episteme) and opinion (doxa). In Plato's philosophy, Being exists as idea or form (eidos). He considered Nonbeing to be something that exists, albeit separately from Being. Aristotle considered the relationship between Being and thought. He rejected Eleatic ideas as unscientific. Instead, Aristotle"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_13",
    "chunk": "believed there are different qualities that described the nature of a given subject's Being and truth, and that there is a state of potentially being that exists between Being and Nonbeing. Subsequent philosophers separated thought from Being, seeing it as a way to find truth rather than existing as the essence of truth. Eusebius of Caesarea, quoting Aristocles of Messene, says that Parmenides was part of a line of skeptical philosophy that culminated in Pyrrhonism for he, by the root, rejects the validity of perception through the senses whilst, at any rate, it is first through our five forms of senses that we become aware of things and then by faculty of reasoning. Though his ideas preceded and influenced metaphysics, modern philosophers do not consider Parmenides to have studied metaphysics himself, as Being is not a distinct entity from the physical world. Some exceptions exist, such as Pierre Aubenque, who described Parmenides' work as \"the birth certificate of Western metaphysics\". The philosophy of Martin Heidegger revisited the concept of a Parmenidean eternal Being. He described Parmenides as the first to create a unified concept of Being and Nonbeing. In On Nature, Parmenides describes the truth as the path of that"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_14",
    "chunk": "is. The philologist Hermann Fränkel identified this use of an impersonal verb as an atypical grammatical construction in Ancient Greek. It is understood to refer to the concept of Being, described through postposition. The goddess's description of Being is effectively the affirmation of its existence. Aubenque commented that this is \"the thesis of Being itself\". Ernst Hoffman proposed that Being, discourse, and thought were all the same thing. Friedrich Nietzsche suggested that Parmenides believed his cosmology, even though he understood that it was only part of the seeming world. Karl Popper said that it was necessary for Parmenides to disprove his own cosmology to prove that the seeming world was not true. He likened this to modern physics, in which theories of physics do not necessarily correspond to what appears to be true. The question of whether time and space are continuous or discrete is prominent in modern physics, where several mathematicians and physicists propose Parmenidean models. Albert Einstein developed a single model of spacetime to explain the universe. Hermann Weyl argued that time exists only subjectively. Karl Popper further described the ideas of Ludwig Boltzmann, Kurt Gödel, Hermann Minkowski, and Erwin Schrödinger as reminiscent of Parmenides. The Wheeler–DeWitt equation"
  },
  {
    "source": "Parmenides.txt",
    "chunk_id": "Parmenides.txt_15",
    "chunk": "suggests that time does not exist as its own distinct entity. In the Diels–Kranz numbering for testimony and fragments of Pre-Socratic philosophy, Parmenides is catalogued as number 28. The most recent edition of this catalogue is: Diels, Hermann; Kranz, Walther (1957). Plamböck, Gert (ed.). Die Fragmente der Vorsokratiker (in Ancient Greek and German). Rowohlt. ISBN 5875607416. Retrieved 11 April 2022. {{cite book}}: ISBN / Date incompatibility (help)"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_0",
    "chunk": "# Particle accelerator A particle accelerator is a machine that uses electromagnetic fields to propel charged particles to very high speeds and energies to contain them in well-defined beams. Small accelerators are used for fundamental research in particle physics. Accelerators are also used as synchrotron light sources for the study of condensed matter physics. Smaller particle accelerators are used in a wide variety of applications, including particle therapy for oncological purposes, radioisotope production for medical diagnostics, ion implanters for the manufacturing of semiconductors, and accelerator mass spectrometers for measurements of rare isotopes such as radiocarbon. Large accelerators include the Relativistic Heavy Ion Collider at Brookhaven National Laboratory in New York, and the largest accelerator, the Large Hadron Collider near Geneva, Switzerland, operated by CERN. It is a collider accelerator, which can accelerate two beams of protons to an energy of 6.5 TeV and cause them to collide head-on, creating center-of-mass energies of 13 TeV. There are more than 30,000 accelerators in operation around the world. There are two basic classes of accelerators: electrostatic and electrodynamic (or electromagnetic) accelerators. Electrostatic particle accelerators use static electric fields to accelerate particles. The most common types are the Cockcroft–Walton generator and the Van de"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_1",
    "chunk": "Graaff generator. A small-scale example of this class is the cathode-ray tube in an ordinary old television set. The achievable kinetic energy for particles in these devices is determined by the accelerating voltage, which is limited by electrical breakdown. Electrodynamic or electromagnetic accelerators, on the other hand, use changing electromagnetic fields (either magnetic induction or oscillating radio frequency fields) to accelerate particles. Since in these types the particles can pass through the same accelerating field multiple times, the output energy is not limited by the strength of the accelerating field. This class, which was first developed in the 1920s, is the basis for most modern large-scale accelerators. Rolf Widerøe, Gustaf Ising, Leo Szilard, Max Steenbeck, and Ernest Lawrence are considered pioneers of this field, having conceived and built the first operational linear particle accelerator, the betatron, as well as the cyclotron. Because the target of the particle beams of early accelerators was usually the atoms of a piece of matter, with the goal being to create collisions with their nuclei in order to investigate nuclear structure, accelerators were commonly referred to as atom smashers in the 20th century. The term persists despite the fact that many modern accelerators create collisions"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_2",
    "chunk": "between two subatomic particles, rather than a particle and an atomic nucleus. Beams of high-energy particles are useful for fundamental and applied research in the sciences and also in many technical and industrial fields unrelated to fundamental research. There are approximately 30,000 accelerators worldwide; of these, only about 1% are research machines with energies above 1 GeV, while about 44% are for radiotherapy, 41% for ion implantation, 9% for industrial processing and research, and 4% for biomedical and other low-energy research. For the most basic inquiries into the dynamics and structure of matter, space, and time, physicists seek the simplest kinds of interactions at the highest possible energies. These typically entail particle energies of many GeV, and interactions of the simplest kinds of particles: leptons (e.g. electrons and positrons) and quarks for the matter, or photons and gluons for the field quanta. Since isolated quarks are experimentally unavailable due to color confinement, the simplest available experiments involve the interactions of, first, leptons with each other, and second, of leptons with nucleons, which are composed of quarks and gluons. To study the collisions of quarks with each other, scientists resort to collisions of nucleons, which at high energy may be usefully"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_3",
    "chunk": "considered as essentially 2-body interactions of the quarks and gluons of which they are composed. This elementary particle physicists tend to use machines creating beams of electrons, positrons, protons, and antiprotons, interacting with each other or with the simplest nuclei (e.g., hydrogen or deuterium) at the highest possible energies, generally hundreds of GeV or more. The largest and highest-energy particle accelerator used for elementary particle physics is the Large Hadron Collider (LHC) at CERN, operating since 2009. Nuclear physicists and cosmologists may use beams of bare atomic nuclei, stripped of electrons, to investigate the structure, interactions, and properties of the nuclei themselves, and of condensed matter at extremely high temperatures and densities, such as might have occurred in the first moments of the Big Bang. These investigations often involve collisions of heavy nuclei – of atoms like iron or gold – at energies of several GeV per nucleon. The largest such particle accelerator is the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National Laboratory. Particle accelerators can also produce proton beams, which can produce proton-rich medical or research isotopes as opposed to the neutron-rich ones made in fission reactors; however, recent work has shown how to make Mo, usually made"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_4",
    "chunk": "in reactors, by accelerating isotopes of hydrogen, although this method still requires a reactor to produce tritium. An example of this type of machine is LANSCE at Los Alamos National Laboratory. Electrons propagating through a magnetic field emit very bright and coherent photon beams via synchrotron radiation. It has numerous uses in the study of atomic structure, chemistry, condensed matter physics, biology, and technology. A large number of synchrotron light sources exist worldwide. Examples in the U.S. are SSRL at SLAC National Accelerator Laboratory, APS at Argonne National Laboratory, ALS at Lawrence Berkeley National Laboratory, and NSLS-II at Brookhaven National Laboratory. In Europe, there are MAX IV in Lund, Sweden, BESSY in Berlin, Germany, Diamond in Oxfordshire, UK, ESRF in Grenoble, France, the latter has been used to extract detailed 3-dimensional images of insects trapped in amber. Free-electron lasers (FELs) are a special class of light sources based on synchrotron radiation that provides shorter pulses with higher temporal coherence. A specially designed FEL is the most brilliant source of x-rays in the observable universe. The most prominent examples are the LCLS in the U.S. and European XFEL in Germany. More attention is being drawn towards soft x-ray lasers, which together"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_5",
    "chunk": "with pulse shortening opens up new methods for attosecond science. Apart from x-rays, FELs are used to emit terahertz light, e.g. FELIX in Nijmegen, Netherlands, TELBE in Dresden, Germany and NovoFEL in Novosibirsk, Russia. Thus there is a great demand for electron accelerators of moderate (GeV) energy, high intensity and high beam quality to drive light sources. Everyday examples of particle accelerators are cathode ray tubes found in television sets and X-ray generators. These low-energy accelerators use a single pair of electrodes with a DC voltage of a few thousand volts between them. In an X-ray generator, the target itself is one of the electrodes. A low-energy particle accelerator called an ion implanter is used in the manufacture of integrated circuits. At lower energies, beams of accelerated nuclei are also used in medicine as particle therapy, for the treatment of cancer. DC accelerator types capable of accelerating particles to speeds sufficient to cause nuclear reactions are Cockcroft–Walton generators or voltage multipliers, which convert AC to high voltage DC, or Van de Graaff generators that use static electricity carried by belts. Electron beam processing is commonly used for sterilization. Electron beams are an on-off technology that provide a much higher dose"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_6",
    "chunk": "rate than gamma or X-rays emitted by radioisotopes like cobalt-60 (Co) or caesium-137 (Cs). Due to the higher dose rate, less exposure time is required and polymer degradation is reduced. Because electrons carry a charge, electron beams are less penetrating than both gamma and X-rays. Historically, the first accelerators used simple technology of a single static high voltage to accelerate charged particles. The charged particle was accelerated through an evacuated tube with an electrode at either end, with the static potential across it. Since the particle passed only once through the potential difference, the output energy was limited to the accelerating voltage of the machine. While this method is still extremely popular today, with the electrostatic accelerators greatly out-numbering any other type, they are more suited to lower energy studies owing to the practical voltage limit of about 1 MV for air insulated machines, or 30 MV when the accelerator is operated in a tank of pressurized gas with high dielectric strength, such as sulfur hexafluoride. In a tandem accelerator the potential is used twice to accelerate the particles, by reversing the charge of the particles while they are inside the terminal. This is possible with the acceleration of atomic"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_7",
    "chunk": "nuclei by using anions (negatively charged ions), and then passing the beam through a thin foil to strip electrons off the anions inside the high voltage terminal, converting them to cations (positively charged ions), which are accelerated again as they leave the terminal. The two main types of electrostatic accelerator are the Cockcroft–Walton accelerator, which uses a diode-capacitor voltage multiplier to produce high voltage, and the Van de Graaff accelerator, which uses a moving fabric belt to carry charge to the high voltage electrode. Although electrostatic accelerators accelerate particles along a straight line, the term linear accelerator is more often used for accelerators that employ oscillating rather than static electric fields. Due to the high voltage ceiling imposed by electrical discharge, in order to accelerate particles to higher energies, techniques involving dynamic fields rather than static fields are used. Electrodynamic acceleration can arise from either of two mechanisms: non-resonant magnetic induction, or resonant circuits or cavities excited by oscillating radio frequency (RF) fields. Electrodynamic accelerators can be linear, with particles accelerating in a straight line, or circular, using magnetic fields to bend particles in a roughly circular orbit. Magnetic induction accelerators accelerate particles by induction from an increasing magnetic field,"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_8",
    "chunk": "as if the particles were the secondary winding in a transformer. The increasing magnetic field creates a circulating electric field which can be configured to accelerate the particles. Induction accelerators can be either linear or circular. Linear induction accelerators utilize ferrite-loaded, non-resonant induction cavities. Each cavity can be thought of as two large washer-shaped disks connected by an outer cylindrical tube. Between the disks is a ferrite toroid. A voltage pulse applied between the two disks causes an increasing magnetic field which inductively couples power into the charged particle beam. The linear induction accelerator was invented by Christofilos in the 1960s. Linear induction accelerators are capable of accelerating very high beam currents (>1000 A) in a single short pulse. They have been used to generate X-rays for flash radiography (e.g. DARHT at LANL), and have been considered as particle injectors for magnetic confinement fusion and as drivers for free electron lasers. The Betatron is a circular magnetic induction accelerator, invented by Donald Kerst in 1940 for accelerating electrons. The concept originates ultimately from Norwegian-German scientist Rolf Widerøe. These machines, like synchrotrons, use a donut-shaped ring magnet (see below) with a cyclically increasing B field, but accelerate the particles by induction"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_9",
    "chunk": "from the increasing magnetic field, as if they were the secondary winding in a transformer, due to the changing magnetic flux through the orbit. Achieving constant orbital radius while supplying the proper accelerating electric field requires that the magnetic flux linking the orbit be somewhat independent of the magnetic field on the orbit, bending the particles into a constant radius curve. These machines have in practice been limited by the large radiative losses suffered by the electrons moving at nearly the speed of light in a relatively small radius orbit. In a linear particle accelerator (linac), particles are accelerated in a straight line with a target of interest at one end. They are often used to provide an initial low-energy kick to particles before they are injected into circular accelerators. The longest linac in the world is the Stanford Linear Accelerator, SLAC, which is 3 km (1.9 mi) long. SLAC was originally an electron–positron collider but is now a X-ray Free-electron laser. Linear high-energy accelerators use a linear array of plates (or drift tubes) to which an alternating high-energy field is applied. As the particles approach a plate they are accelerated towards it by an opposite polarity charge applied to"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_10",
    "chunk": "the plate. As they pass through a hole in the plate, the polarity is switched so that the plate now repels them and they are now accelerated by it towards the next plate. Normally a stream of \"bunches\" of particles are accelerated, so a carefully controlled AC voltage is applied to each plate to continuously repeat this process for each bunch. As the particles approach the speed of light the switching rate of the electric fields becomes so high that they operate at radio frequencies, and so microwave cavities are used in higher energy machines instead of simple plates. Linear accelerators are also widely used in medicine, for radiotherapy and radiosurgery. Medical grade linacs accelerate electrons using a klystron and a complex bending magnet arrangement which produces a beam of energy 6–30 MeV. The electrons can be used directly or they can be collided with a target to produce a beam of X-rays. The reliability, flexibility and accuracy of the radiation beam produced has largely supplanted the older use of cobalt-60 therapy as a treatment tool. In the circular accelerator, particles move in a circle until they reach enough energy. The particle track is typically bent into a circle using"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_11",
    "chunk": "electromagnets. The advantage of circular accelerators over linear accelerators (linacs) is that the ring topology allows continuous acceleration, as the particle can transit indefinitely. Another advantage is that a circular accelerator is smaller than a linear accelerator of comparable power (i.e. a linac would have to be extremely long to have the equivalent power of a circular accelerator). Depending on the energy and the particle being accelerated, circular accelerators suffer a disadvantage in that the particles emit synchrotron radiation. When any charged particle is accelerated, it emits electromagnetic radiation and secondary emissions. As a particle traveling in a circle is always accelerating towards the center of the circle, it continuously radiates towards the tangent of the circle. This radiation is called synchrotron light and depends highly on the mass of the accelerating particle. For this reason, many high energy electron accelerators are linacs. Certain accelerators (synchrotrons) are however built specially for producing synchrotron light (X-rays). Since the special theory of relativity requires that matter always travels slower than the speed of light in vacuum, in high-energy accelerators, as the energy increases the particle speed approaches the speed of light as a limit, but never attains it. Therefore, particle physicists do"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_12",
    "chunk": "not generally think in terms of speed, but rather in terms of a particle's energy or momentum, usually measured in electron volts (eV). An important principle for circular accelerators, and particle beams in general, is that the curvature of the particle trajectory is proportional to the particle charge and to the magnetic field, but inversely proportional to the (typically relativistic) momentum. The earliest operational circular accelerators were cyclotrons, invented in 1929 by Ernest Lawrence at the University of California, Berkeley. Cyclotrons have a single pair of hollow D-shaped plates to accelerate the particles and a single large dipole magnet to bend their path into a circular orbit. It is a characteristic property of charged particles in a uniform and constant magnetic field B that they orbit with a constant period, at a frequency called the cyclotron frequency, so long as their speed is small compared to the speed of light c. This means that the accelerating D's of a cyclotron can be driven at a constant frequency by a RF accelerating power source, as the beam spirals outwards continuously. The particles are injected in the center of the magnet and are extracted at the outer edge at their maximum energy."
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_13",
    "chunk": "Cyclotrons reach an energy limit because of relativistic effects whereby the particles effectively become more massive, so that their cyclotron frequency drops out of sync with the accelerating RF. Therefore, simple cyclotrons can accelerate protons only to an energy of around 15 million electron volts (15 MeV, corresponding to a speed of roughly 10% of c), because the protons get out of phase with the driving electric field. If accelerated further, the beam would continue to spiral outward to a larger radius but the particles would no longer gain enough speed to complete the larger circle in step with the accelerating RF. To accommodate relativistic effects the magnetic field needs to be increased to higher radii as is done in isochronous cyclotrons. An example of an isochronous cyclotron is the PSI Ring cyclotron in Switzerland, which provides protons at the energy of 590 MeV which corresponds to roughly 80% of the speed of light. The advantage of such a cyclotron is the maximum achievable extracted proton current which is currently 2.2 mA. The energy and current correspond to 1.3 MW beam power which is the highest of any accelerator currently existing. A classic cyclotron can be modified to increase its"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_14",
    "chunk": "energy limit. The historically first approach was the synchrocyclotron, which accelerates the particles in bunches. It uses a constant magnetic field B {\\displaystyle B} , but reduces the accelerating field's frequency so as to keep the particles in step as they spiral outward, matching their mass-dependent cyclotron resonance frequency. This approach suffers from low average beam intensity due to the bunching, and again from the need for a huge magnet of large radius and constant field over the larger orbit demanded by high energy. The second approach to the problem of accelerating relativistic particles is the isochronous cyclotron. In such a structure, the accelerating field's frequency (and the cyclotron resonance frequency) is kept constant for all energies by shaping the magnet poles so to increase magnetic field with radius. Thus, all particles get accelerated in isochronous time intervals. Higher energy particles travel a shorter distance in each orbit than they would in a classical cyclotron, thus remaining in phase with the accelerating field. The advantage of the isochronous cyclotron is that it can deliver continuous beams of higher average intensity, which is useful for some applications. The main disadvantages are the size and cost of the large magnet needed, and"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_15",
    "chunk": "the difficulty in achieving the high magnetic field values required at the outer edge of the structure. Synchrocyclotrons have not been built since the isochronous cyclotron was developed. To reach still higher energies, with relativistic mass approaching or exceeding the rest mass of the particles (for protons, billions of electron volts or GeV), it is necessary to use a synchrotron. This is an accelerator in which the particles are accelerated in a ring of constant radius. An immediate advantage over cyclotrons is that the magnetic field need only be present over the actual region of the particle orbits, which is much narrower than that of the ring. (The largest cyclotron built in the US had a 184-inch-diameter (4.7 m) magnet pole, whereas the diameter of synchrotrons such as the LEP and LHC is nearly 10 km. The aperture of the two beams of the LHC is of the order of a centimeter.) The LHC contains 16 RF cavities, 1232 superconducting dipole magnets for beam steering, and 24 quadrupoles for beam focusing. Even at this size, the LHC is limited by its ability to steer the particles without them going adrift. This limit is theorized to occur at 14 TeV. However,"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_16",
    "chunk": "since the particle momentum increases during acceleration, it is necessary to turn up the magnetic field B in proportion to maintain constant curvature of the orbit. In consequence, synchrotrons cannot accelerate particles continuously, as cyclotrons can, but must operate cyclically, supplying particles in bunches, which are delivered to a target or an external beam in beam \"spills\" typically every few seconds. Since high energy synchrotrons do most of their work on particles that are already traveling at nearly the speed of light c, the time to complete one orbit of the ring is nearly constant, as is the frequency of the RF cavity resonators used to drive the acceleration. In modern synchrotrons, the beam aperture is small and the magnetic field does not cover the entire area of the particle orbit as it does for a cyclotron, so several necessary functions can be separated. Instead of one huge magnet, one has a line of hundreds of bending magnets, enclosing (or enclosed by) vacuum connecting pipes. The design of synchrotrons was revolutionized in the early 1950s with the discovery of the strong focusing concept. The focusing of the beam is handled independently by specialized quadrupole magnets, while the acceleration itself is"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_17",
    "chunk": "accomplished in separate RF sections, rather similar to short linear accelerators. Also, there is no necessity that cyclic machines be circular, but rather the beam pipe may have straight sections between magnets where beams may collide, be cooled, etc. This has developed into an entire separate subject, called \"beam physics\" or \"beam optics\". More complex modern synchrotrons such as the Tevatron, LEP, and LHC may deliver the particle bunches into storage rings of magnets with a constant magnetic field, where they can continue to orbit for long periods for experimentation or further acceleration. The highest-energy machines such as the Tevatron and LHC are actually accelerator complexes, with a cascade of specialized elements in series, including linear accelerators for initial beam creation, one or more low energy synchrotrons to reach intermediate energy, storage rings where beams can be accumulated or \"cooled\" (reducing the magnet aperture required and permitting tighter focusing; see beam cooling), and a last large ring for final acceleration and experimentation. Circular electron accelerators fell somewhat out of favor for particle physics around the time that SLAC's linear particle accelerator was constructed, because their synchrotron losses were considered economically prohibitive and because their beam intensity was lower than for"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_18",
    "chunk": "the unpulsed linear machines. The Cornell Electron Synchrotron, built at low cost in the late 1970s, was the first in a series of high-energy circular electron accelerators built for fundamental particle physics, the last being LEP, built at CERN, which was used from 1989 until 2000. A large number of electron synchrotrons have been built in the past two decades, as part of synchrotron light sources that emit ultraviolet light and X rays; see below. Some circular accelerators have been built to deliberately generate radiation (called synchrotron light) as X-rays also called synchrotron radiation, for example the Diamond Light Source which has been built at the Rutherford Appleton Laboratory in England or the Advanced Photon Source at Argonne National Laboratory in Illinois, USA. High-energy X-rays are useful for X-ray spectroscopy of proteins or X-ray absorption fine structure (XAFS), for example. Synchrotron radiation is more powerfully emitted by lighter particles, so these accelerators are invariably electron accelerators. Synchrotron radiation allows for better imaging as researched and developed at SLAC's SPEAR. Fixed-Field Alternating Gradient accelerators (FFA)s, in which a magnetic field which is fixed in time, but with a radial variation to achieve strong focusing, allows the beam to be accelerated with"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_19",
    "chunk": "a high repetition rate but in a much smaller radial spread than in the cyclotron case. Isochronous FFAs, like isochronous cyclotrons, achieve continuous beam operation, but without the need for a huge dipole bending magnet covering the entire radius of the orbits. Some new developments in FFAs are covered in. A Rhodotron is an industrial electron accelerator first proposed in 1987 by J. Pottier of the French Atomic Energy Agency (CEA), manufactured by Belgian company Ion Beam Applications. It accelerates electrons by recirculating them across the diameter of a cylinder-shaped radiofrequency cavity. A Rhodotron has an electron gun, which emits an electron beam that is attracted to a pillar in the center of the cavity. The pillar has holes the electrons can pass through. The electron beam passes through the pillar via one of these holes and then travels through a hole in the wall of the cavity, and meets a bending magnet, the beam is then bent and sent back into the cavity, to another hole in the pillar, the electrons then again go across the pillar and pass though another part of the wall of the cavity and into another bending magnet, and so on, gradually increasing the"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_20",
    "chunk": "energy of the beam until it is allowed to exit the cavity for use. The cylinder and pillar may be lined with copper on the inside. Ernest Lawrence's first cyclotron was a mere 4 inches (100 mm) in diameter. Later, in 1939, he built a machine with a 60-inch diameter pole face, and planned one with a 184-inch diameter in 1942, which was, however, taken over for World War II-related work connected with uranium isotope separation; after the war it continued in service for research and medicine over many years. The first large proton synchrotron was the Cosmotron at Brookhaven National Laboratory, which accelerated protons to about 3 GeV (1953–1968). The Bevatron at Berkeley, completed in 1954, was specifically designed to accelerate protons to enough energy to create antiprotons, and verify the particle–antiparticle symmetry of nature, then only theorized. The Alternating Gradient Synchrotron (AGS) at Brookhaven (1960–) was the first large synchrotron with alternating gradient, \"strong focusing\" magnets, which greatly reduced the required aperture of the beam, and correspondingly the size and cost of the bending magnets. The Proton Synchrotron, built at CERN (1959–), was the first major European particle accelerator and generally similar to the AGS. The Stanford Linear"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_21",
    "chunk": "Accelerator, SLAC, became operational in 1966, accelerating electrons to 30 GeV in a 3 km long waveguide, buried in a tunnel and powered by hundreds of large klystrons. It is still the largest linear accelerator in existence, and has been upgraded with the addition of storage rings and an electron-positron collider facility. It is also an X-ray and UV synchrotron photon source. The Fermilab Tevatron has a ring with a beam path of 4 miles (6.4 km). It has received several upgrades, and has functioned as a proton-antiproton collider until it was shut down due to budget cuts on September 30, 2011. The largest circular accelerator ever built was the LEP synchrotron at CERN with a circumference 26.6 kilometers, which was an electron/positron collider. It achieved an energy of 209 GeV before it was dismantled in 2000 so that the tunnel could be used for the Large Hadron Collider (LHC). The LHC is a proton collider, and currently the world's largest and highest-energy accelerator, achieving 6.5 TeV energy per beam (13 TeV in total). The aborted Superconducting Super Collider (SSC) in Texas would have had a circumference of 87 km. Construction was started in 1991, but abandoned in 1993. Very"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_22",
    "chunk": "large circular accelerators are invariably built in tunnels a few metres wide to minimize the disruption and cost of building such a structure on the surface, and to provide shielding against intense secondary radiations that occur, which are extremely penetrating at high energies. Current accelerators such as the Spallation Neutron Source, incorporate superconducting cryomodules. The Relativistic Heavy Ion Collider, and Large Hadron Collider also make use of superconducting magnets and RF cavity resonators to accelerate particles. The output of a particle accelerator can generally be directed towards multiple lines of experiments, one at a given time, by means of a deviating electromagnet. This makes it possible to operate multiple experiments without needing to move things around or shutting down the entire accelerator beam. Except for synchrotron radiation sources, the purpose of an accelerator is to generate high-energy particles for interaction with matter. This is usually a fixed target, such as the phosphor coating on the back of the screen in the case of a television tube; a piece of uranium in an accelerator designed as a neutron source; or a tungsten target for an X-ray generator. In a linac, the target is simply fitted to the end of the accelerator."
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_23",
    "chunk": "The particle track in a cyclotron is a spiral outwards from the centre of the circular machine, so the accelerated particles emerge from a fixed point as for a linear accelerator. For synchrotrons, the situation is more complex. Particles are accelerated to the desired energy. Then, a fast acting dipole magnet is used to switch the particles out of the circular synchrotron tube and towards the target. A variation commonly used for particle physics research is a collider, also called a storage ring collider. Two circular synchrotrons are built in close proximity – usually on top of each other and using the same magnets (which are then of more complicated design to accommodate both beam tubes). Bunches of particles travel in opposite directions around the two accelerators and collide at intersections between them. This can increase the energy enormously; whereas in a fixed-target experiment the energy available to produce new particles is proportional to the square root of the beam energy, in a collider the available energy is linear. The detectors gather clues about the particles including their speed and charge. Using these, the scientists can actually work on the particle. The process of detection is very complex it requires"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_24",
    "chunk": "strong electromagnets and accelerators to generate enough usable information. At present the highest energy accelerators are all circular colliders, but both hadron accelerators and electron accelerators are running into limits. Higher energy hadron and ion cyclic accelerators will require accelerator tunnels of larger physical size due to the increased beam rigidity. For cyclic electron accelerators, a limit on practical bend radius is placed by synchrotron radiation losses and the next generation will probably be linear accelerators 10 times the current length. An example of such a next generation electron accelerator is the proposed 40 km long International Linear Collider. It is believed that plasma wakefield acceleration in the form of electron-beam \"afterburners\" and standalone laser pulsers might be able to provide dramatic increases in efficiency over RF accelerators within two to three decades. In plasma wakefield accelerators, the beam cavity is filled with a plasma (rather than vacuum). A short pulse of electrons or laser light either constitutes or immediately precedes the particles that are being accelerated. The pulse disrupts the plasma, causing the charged particles in the plasma to integrate into and move toward the rear of the bunch of particles that are being accelerated. This process transfers energy"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_25",
    "chunk": "to the particle bunch, accelerating it further, and continues as long as the pulse is coherent. Energy gradients as steep as 200 GeV/m have been achieved over millimeter-scale distances using laser pulsers and gradients approaching 1 GeV/m are being produced on the multi-centimeter-scale with electron-beam systems, in contrast to a limit of about 0.1 GeV/m for radio-frequency acceleration alone. Existing electron accelerators such as SLAC could use electron-beam afterburners to greatly increase the energy of their particle beams, at the cost of beam intensity. Electron systems in general can provide tightly collimated, reliable beams; laser systems may offer more power and compactness. Thus, plasma wakefield accelerators could be used – if technical issues can be resolved – to both increase the maximum energy of the largest accelerators and to bring high energies into university laboratories and medical centres. Higher than 0.25 GeV/m gradients have been achieved by a dielectric laser accelerator, which may present another viable approach to building compact high-energy accelerators. Using femtosecond duration laser pulses, an electron accelerating gradient 0.69 GeV/m was recorded for dielectric laser accelerators. Higher gradients of the order of 1 to 6 GeV/m are anticipated after further optimizations. Advanced Accelerator Concepts encompasses methods of"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_26",
    "chunk": "beam acceleration with gradients beyond state of the art in operational facilities. This includes diagnostics methods, timing technology, special needs for injectors, beam matching, beam dynamics and development of adequate simulations. Workshops dedicated to this subject are being held in the US (alternating locations) and in Europe, mostly on Isola d'Elba. The series of Advanced Accelerator Concepts Workshops, held in the US, started as an international series in 1982. The European Advanced Accelerator Concepts Workshop series started in 2019. Topics related to Advanced Accelerator Concepts: According to the Inverse scattering problem, any mechanism by which a particle produces radiation (where kinetic energy of the particle is transferred to the electromagnetic field), can be inverted such that the same radiation mechanism leads to the acceleration of the particle (energy of the radiation field is transferred to kinetic energy of the particle). The opposite is also true, any acceleration mechanism can be inverted to deposit the energy of the particle into a decelerating field, like in a kinetic energy recovery system. This is the idea enabling an energy recovery linac. This principle, which is also behind the plasma or dielectric wakefield accelerrators, led to a few other interesting developments in advanced accelerator"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_27",
    "chunk": "concepts: In the future, the possibility of a black hole production at the highest energy accelerators may arise if certain predictions of superstring theory are accurate. This and other possibilities have led to public safety concerns that have been widely reported in connection with the LHC, which began operation in 2008. The various possible dangerous scenarios have been assessed as presenting \"no conceivable danger\" in the latest risk assessment produced by the LHC Safety Assessment Group. If black holes are produced, it is theoretically predicted that such small black holes should evaporate extremely quickly via Bekenstein–Hawking radiation, but which is as yet experimentally unconfirmed. If colliders can produce black holes, cosmic rays (and particularly ultra-high-energy cosmic rays, UHECRs) must have been producing them for eons, but they have yet to harm anybody. It has been argued that to conserve energy and momentum, any black holes created in a collision between an UHECR and local matter would necessarily be produced moving at relativistic speed with respect to the Earth, and should escape into space, as their accretion and growth rate should be very slow, while black holes produced in colliders (with components of equal mass) would have some chance of having"
  },
  {
    "source": "Particle accelerator.txt",
    "chunk_id": "Particle accelerator.txt_28",
    "chunk": "a velocity less than Earth escape velocity, 11.2 km per sec, and would be liable to capture and subsequent growth. Yet even on such scenarios the collisions of UHECRs with white dwarfs and neutron stars would lead to their rapid destruction, but these bodies are observed to be common astronomical objects. Thus if stable micro black holes should be produced, they must grow far too slowly to cause any noticeable macroscopic effects within the natural lifetime of the solar system. The use of advanced technologies such as superconductivity, cryogenics, and high powered radiofrequency amplifiers, as well as the presence of ionizing radiation, pose challenges for the safe operation of accelerator facilities. An accelerator operator controls the operation of a particle accelerator, adjusts operating parameters such as aspect ratio, current intensity, and position on target. They communicate with and assist accelerator maintenance personnel to ensure readiness of support systems, such as vacuum, magnets, magnetic and radiofrequency power supplies and controls, and cooling systems. Additionally, the accelerator operator maintains a record of accelerator related events."
  },
  {
    "source": "Philosophical Transactions of the Royal Society A.txt",
    "chunk_id": "Philosophical Transactions of the Royal Society A.txt_0",
    "chunk": "# Philosophical Transactions of the Royal Society A Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences is a fortnightly peer-reviewed scientific journal published by the Royal Society. It publishes original research and review content in a wide range of physical scientific disciplines. Articles can be accessed online a few months prior to the printed journal. All articles become freely accessible two years after their publication date. The current editor-in-chief is John Dainton. Philosophical Transactions of the Royal Society A publishes themed journal issues on topics of current scientific importance and general interest within the physical, mathematical and engineering sciences, edited by leading authorities and comprising original research, reviews and opinions from prominent researchers. Past issue titles include \"Supercritical fluids - green solvents for green chemistry?\", \"Tsunamis: Bridging science, engineering and society\", \"Spatial transformations: from fundamentals to applications\", and \"Before, behind and beyond the discovery of the Higgs boson\". Philosophical Transactions of the Royal Society was established in 1665 by the Royal Society and is the oldest scientific journal in the English-speaking world. Henry Oldenburg was appointed as the first (joint) secretary to the society and he was also the first editor of the society's journal. In"
  },
  {
    "source": "Philosophical Transactions of the Royal Society A.txt",
    "chunk_id": "Philosophical Transactions of the Royal Society A.txt_1",
    "chunk": "1887 the journal expanded to become two separate publications, one serving the physical sciences, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, and the other focusing on the life sciences, Philosophical Transactions of the Royal Society B: Biological Sciences. Nowadays, both journals publish themed issues and discussion meeting issues, while individual research articles are published in the sister journal Proceedings of the Royal Society. The journal celebrated its 350th anniversary in 2015. To commemorate this event it published a special collection of commentaries on landmark papers from the archive by scientists such as Isaac Newton, Humphry Davy and Michael Faraday. According to the 2022 Journal Citation Reports, the journal has an impact factor of 5.0."
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_0",
    "chunk": "# Phobos (moon) Phobos (/ˈfoʊbəs/; systematic designation: Mars I) is the innermost and larger of the two natural satellites of Mars, the other being Deimos. The two moons were discovered in 1877 by American astronomer Asaph Hall. Phobos is named after the Greek god of fear and panic, who is the son of Ares (Mars) and twin brother of Deimos. Phobos is a small, irregularly shaped object with a mean radius of 11 km (7 mi). It orbits 6,000 km (3,700 mi) from the Martian surface, closer to its primary body than any other known natural satellite to a planet. It orbits Mars much faster than Mars rotates and completes an orbit in just 7 hours and 39 minutes. As a result, from the surface of Mars it appears to rise in the west, move across the sky in 4 hours and 15 minutes or less, and set in the east, twice each Martian day. Phobos is one of the least reflective bodies in the Solar System, with an albedo of 0.071. Surface temperatures range from about −4 °C (25 °F) on the sunlit side to −112 °C (−170 °F) on the shadowed side. The notable surface feature is the"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_1",
    "chunk": "large impact crater Stickney, which takes up a substantial proportion of the moon's surface. The surface is also marked by many grooves, and there are numerous theories as to how these grooves were formed. Images and models indicate that Phobos may be a rubble pile held together by a thin crust that is being torn apart by tidal interactions. Phobos gets closer to Mars by about 2 centimetres (0.79 in) per year. Phobos was discovered by the American astronomer Asaph Hall on 18 August 1877 at the United States Naval Observatory in Washington, D.C., at about 09:14 Greenwich Mean Time. (Contemporary sources, using the pre-1925 astronomical convention that began the day at noon, give the time of discovery as 17 August at 16:06 Washington Mean Time, meaning 18 August 04:06 in the modern convention.) Hall had discovered Deimos, Mars' other moon, a few days earlier. The discoveries were made using the world's largest refracting telescope, the 26-inch \"Great Equatorial\". The names, originally spelled Phobus and Deimus respectively, were suggested by the British academic Henry Madan, a science master at Eton College, who based them on Greek mythology, in which Phobos is a companion to the god, Ares. Planetary moons other"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_2",
    "chunk": "than Earth's were never given symbols in the astronomical literature. Denis Moskowitz, a software engineer who designed most of the dwarf planet symbols, proposed a Greek phi (the initial of Phobos) combined with Mars' spear as the symbol of Phobos (). This symbol is not widely used. Phobos has dimensions of 26 by 23 by 18 kilometres (16 mi × 14 mi × 11 mi), and retains too little mass to be rounded under its own gravity. Phobos does not have an atmosphere due to its low mass and low gravity. It is one of the least reflective bodies in the Solar System, with an albedo of about 0.071. Infrared spectra show that it has carbon-rich material found in carbonaceous chondrites, and its composition shows similarities to that of Mars' surface. Phobos's density is too low to be solid rock, and it is known to have significant porosity. These results led to the suggestion that Phobos might contain a substantial reservoir of ice. Spectral observations indicate that the surface regolith layer lacks hydration, but ice below the regolith is not ruled out. Surface temperatures range from about −4 °C (25 °F) on the sunlit side to −112 °C (−170 °F)"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_3",
    "chunk": "on the shadowed side. Unlike Deimos, Phobos is heavily cratered, with one of the craters near the equator having a central peak despite the moon's small size. The most prominent of these is the crater Stickney, an impact crater 9 km (5.6 mi) in diameter, which takes up a substantial proportion of the moon's surface area. As with the Saturnian moon Mimas's crater Herschel, the impact that created Stickney probably almost shattered Phobos. Many grooves and streaks cover the oddly shaped surface. The grooves are typically less than 30 meters (98 ft) deep, 100 to 200 meters (330 to 660 ft) wide, and up to 20 kilometers (12 mi) in length, and were originally assumed to have been the result of the same impact that created Stickney. Analysis of results from the Mars Express spacecraft revealed that the grooves are not radial to Stickney, but are centered on the leading apex of Phobos in its orbit (which is not far from Stickney). Researchers suspected that they had been excavated by material ejected into space by impacts on the surface of Mars. The grooves thus formed as crater chains, and all of them fade away as the trailing apex of Phobos"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_4",
    "chunk": "is approached. They have been grouped into 12 or more families of varying age, presumably representing at least 12 Martian impact events. In November 2018, based on computational probability analysis, astronomers concluded that the many grooves on Phobos were caused by boulders ejected from the asteroid impact that created Stickney crater. These boulders rolled in a predictable pattern on the surface of the moon. Faint dust rings produced by Phobos and Deimos have long been predicted but attempts to observe these rings have, to date, failed. Images from Mars Global Surveyor indicate that Phobos is covered with a layer of fine-grained regolith at least 100 meters thick; it is hypothesized to have been created by impacts from other bodies, but it is not known how the material stuck to an object with almost no gravity. The unique Kaidun meteorite that fell on a Soviet military base in Yemen in 1980 has been hypothesized to be a piece of Phobos, but this could not be verified because little is known about the exact composition of Phobos. In the late 1950s and 1960s, the unusual orbital characteristics of Phobos led to speculations that it might be hollow. Around 1958, Russian astrophysicist Iosif"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_5",
    "chunk": "Samuilovich Shklovsky, studying the secular acceleration of Phobos's orbital motion, suggested a \"thin sheet metal\" structure for Phobos, a suggestion which led to speculations that Phobos was of artificial origin. Shklovsky based his analysis on estimates of the upper Martian atmosphere's density, and deduced that for the weak braking effect to be able to account for the secular acceleration, Phobos had to be very light—one calculation yielded a hollow iron sphere 16 kilometers (9.9 mi) across but less than 6 centimetres (2.4 in) thick. In a February 1960 letter to the journal Astronautics, Fred Singer, then science advisor to U.S. President Dwight D. Eisenhower, said of Shklovsky's theory: If the satellite is indeed spiraling inward as deduced from astronomical observation, then there is little alternative to the hypothesis that it is hollow and therefore Martian made. The big 'if' lies in the astronomical observations; they may well be in error. Since they are based on several independent sets of measurements taken decades apart by different observers with different instruments, systematic errors may have influenced them. Subsequently, the systematic data errors that Singer predicted were found to exist, the claim was called into doubt, and accurate measurements of the orbit available"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_6",
    "chunk": "by 1969 showed that the discrepancy did not exist. Singer's critique was justified when earlier studies were discovered to have used an overestimated value of 5 centimetres (2.0 in) per year for the rate of altitude loss, which was later revised to 1.8 centimetres (0.71 in) per year. The secular acceleration is now attributed to tidal effects, which create drag on the moon and therefore cause it to spiral inward. The density of Phobos has now been directly measured by spacecraft to be 1.887 g/cm (0.0682 lb/cu in). Current observations are consistent with Phobos being a rubble pile. Images obtained by the Viking probes in the 1970s showed a natural object, not an artificial one. Nevertheless, mapping by the Mars Express probe and subsequent volume calculations do suggest the presence of voids and indicate that it is not a solid chunk of rock but a porous body. The porosity of Phobos was calculated to be 30% ± 5%, or a quarter to a third being empty. Geological features on Phobos are named after astronomers who studied Phobos and people and places from Jonathan Swift's Gulliver's Travels. There is one named regio, Laputa Regio, and one named planitia, Lagado Planitia; both"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_7",
    "chunk": "are named after places in Gulliver's Travels (the fictional Laputa, a flying island, and Lagado, imaginary capital of the fictional nation Balnibarbi). The only named ridge on Phobos is Kepler Dorsum, named after the astronomer Johannes Kepler. The orbital motion of Phobos has been intensively studied, making it \"the best studied natural satellite in the Solar System\" in terms of orbits completed. Its close orbit around Mars produces some distinct effects. With an altitude of 5,989 km (3,721 mi), Phobos orbits Mars below the synchronous orbit radius, meaning that it moves around Mars faster than Mars itself rotates. Therefore, from the point of view of an observer on the surface of Mars, it rises in the west, moves comparatively rapidly across the sky (in 4 h 15 min or less) and sets in the east, approximately twice each Martian day (every 11 h 6 min). Because it is close to the surface and in an equatorial orbit, it cannot be seen above the horizon from latitudes greater than 70.4°. Its orbit is so low that its angular diameter, as seen by an observer on Mars, varies visibly with its position in the sky. Seen at the horizon, Phobos is about"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_8",
    "chunk": "0.14° wide; at zenith, it is 0.20°, one-third as wide as the full Moon as seen from Earth. By comparison, the Sun has an apparent size of about 0.35° in the Martian sky. Phobos's phases, inasmuch as they can be observed from Mars, take 0.3191 days (Phobos's synodic period) to run their course, a mere 13 seconds longer than Phobos's sidereal period. An observer situated on the Martian surface, in a position to observe Phobos, would see regular transits of Phobos across the Sun. Several of these transits have been photographed by the Mars Rover Opportunity. During the transits, Phobos casts a shadow on the surface of Mars; this event has been photographed by several spacecraft. Phobos is not large enough to cover the Sun's disk, and so cannot cause a total eclipse. Tidal deceleration is gradually decreasing the orbital radius of Phobos by approximately 2 m (6 ft 7 in) every 100 years, and with decreasing orbital radius the likelihood of breakup due to tidal forces increases, estimated in approximately 30–50 million years, or about 43 million years in one study's estimate. Phobos's grooves were long thought to be fractures caused by the impact that formed the Stickney crater."
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_9",
    "chunk": "Other modelling suggested since the 1970s support the idea that the grooves are more like \"stretch marks\" that occur when Phobos gets deformed by tidal forces, but in 2015 when the tidal forces were calculated and used in a new model, the stresses were too weak to fracture a solid moon of that size, unless Phobos is a rubble pile surrounded by a layer of powdery regolith about 100 m (330 ft) thick. Stress fractures calculated for this model line up with the grooves on Phobos. The model is supported with the discovery that some of the grooves are younger than others, implying that the process that produces the grooves is ongoing. Given Phobos's irregular shape and assuming that it is a pile of rubble (specifically a Mohr–Coulomb body), it will eventually break up due to tidal forces when it reaches approximately 2.1 Mars radii. When Phobos is broken up, it will form a planetary ring around Mars. This predicted ring may last from 1 million to 100 million years. The fraction of the mass of Phobos that will form the ring depends on the unknown internal structure of Phobos. Loose, weakly bound material will form the ring. Components of"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_10",
    "chunk": "Phobos with strong cohesion will escape tidal breakup and will enter the Martian atmosphere. The origin of the Martian moons has been disputed. Phobos and Deimos both have much in common with carbonaceous C-type asteroids, with spectra, albedo, and density very similar to those of C- or D-type asteroids. Based on their similarity, one hypothesis is that both moons may be captured main-belt asteroids. Both moons have very circular orbits which lie almost exactly in Mars' equatorial plane, and hence a capture origin requires a mechanism for circularizing the initially highly eccentric orbit, and adjusting its inclination into the equatorial plane, most probably by a combination of atmospheric drag and tidal forces, although it is not clear that sufficient time is available for this to occur for Deimos. Capture also requires dissipation of energy. The current Martian atmosphere is too thin to capture a Phobos-sized object by atmospheric braking. Geoffrey A. Landis has pointed out that the capture could have occurred if the original body was a binary asteroid that separated under tidal forces. Phobos could be a second-generation Solar System object that coalesced in orbit after Mars formed, rather than forming concurrently out of the same birth cloud as"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_11",
    "chunk": "Mars. Another hypothesis is that Mars was once surrounded by many Phobos- and Deimos-sized bodies, perhaps ejected into orbit around it by a collision with a large planetesimal. The high porosity of the interior of Phobos (based on the density of 1.88 g/cm, voids are estimated to comprise 25 to 35 percent of Phobos's volume) is inconsistent with an asteroidal origin. Observations of Phobos in the thermal infrared suggest a composition containing mainly phyllosilicates, which are well known from the surface of Mars. The spectra are distinct from those of all classes of chondrite meteorites, again pointing away from an asteroidal origin. Both sets of findings support an origin of Phobos from material ejected by an impact on Mars that reaccreted in Martian orbit, similar to the prevailing theory for the origin of Earth's moon. Some areas of the surface are reddish in color, while others are bluish. The hypothesis is that gravity pull from Mars makes the reddish regolith move over the surface, exposing relatively fresh, unweathered and bluish material from the moon, while the regolith covering it over time has been weathered due to exposure of solar radiation. Because the blue rock differs from known Martian rock, it"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_12",
    "chunk": "could contradict the theory that the moon is formed from leftover planetary material after the impact of a large object. In February 2021, Amirhossein Bagheri (ETH Zurich), Amir Khan (ETH Zurich), Michael Efroimsky (US Naval Observatory) and their colleagues proposed a new hypothesis on the origin of the moons. By analyzing the seismic and orbital data from Mars InSight Mission and other missions, they proposed that the moons are born from disruption of a common parent body around 1 to 2.7 billion years ago. The common progenitor of Phobos and Deimos was most probably hit by another object and shattered to form both moons. Phobos has been photographed in close-up by several spacecraft whose primary mission has been to photograph Mars. The first was Mariner 7 in 1969, followed by Mariner 9 in 1971, Viking 1 in 1977, Phobos 2 in 1989 Mars Global Surveyor in 1998 and 2003, Mars Express in 2004, 2008, 2010 and 2019, and Mars Reconnaissance Orbiter in 2007 and 2008. On 25 August 2005, the Spirit rover, with an excess of energy due to wind blowing dust off of its solar panels, took several short-exposure photographs of the night sky from the surface of Mars,"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_13",
    "chunk": "and was able to successfully photograph both Phobos and Deimos. The Soviet Union undertook the Phobos program with two probes, both launched successfully in July 1988. Phobos 1 was shut down by an erroneous command from ground control issued in September 1988 and lost while still en route. Phobos 2 arrived at the Mars system in January 1989 and, after transmitting a small amount of data and imagery shortly before beginning its detailed examination of Phobos's surface, abruptly ceased transmission due either to failure of the onboard computer or of the radio transmitter, already operating on backup power. Other Mars missions collected more data, but no dedicated sample return mission has been successfully performed. The Russian Space Agency launched a sample return mission to Phobos in November 2011, called Fobos-Grunt. The return capsule also included a life science experiment of The Planetary Society, called Living Interplanetary Flight Experiment, or LIFE. A second contributor to this mission was the China National Space Administration, which supplied a surveying satellite called \"Yinghuo-1\", which would have been released in the orbit of Mars, and a soil-grinding and sieving system for the scientific payload of the Phobos lander. After achieving Earth orbit, the Fobos-Grunt probe"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_14",
    "chunk": "failed to initiate subsequent burns that would have sent it to Mars. Attempts to recover the probe were unsuccessful and it crashed back to Earth in January 2012. On 1 July 2020, the Mars orbiter of the Indian Space Research Organisation was able to capture photos of the body from 4,200 km away. In 1997 and 1998, the Aladdin mission was selected as a finalist in the NASA Discovery Program. The plan was to visit both Phobos and Deimos, and launch projectiles at the satellites. The probe would collect the ejecta as it performed a slow flyby (~1 km/s). These samples would be returned to Earth for study three years later. The Principal Investigator was Dr. Carle Pieters of Brown University. The total mission cost, including launch vehicle and operations was $247.7 million. Ultimately, the mission chosen to fly was MESSENGER, a probe to Mercury. In 2007, the European aerospace subsidiary EADS Astrium was reported to have been developing a mission to Phobos as a technology demonstrator. Astrium was involved in developing a European Space Agency plan for a sample return mission to Mars, as part of the ESA's Aurora programme, and sending a mission to Phobos with its low"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_15",
    "chunk": "gravity was seen as a good opportunity for testing and proving the technologies required for an eventual sample return mission to Mars. The mission was envisioned to start in 2016, was to last for three years. The company planned to use a \"mothership\", which would be propelled by an ion engine, releasing a lander to the surface of Phobos. The lander would perform some tests and experiments, gather samples in a capsule, then return to the mothership and head back to Earth where the samples would be jettisoned for recovery on the surface. In 2007, the Canadian Space Agency funded a study by Optech and the Mars Institute for an uncrewed mission to Phobos known as Phobos Reconnaissance and International Mars Exploration (PRIME). A proposed landing site for the PRIME spacecraft is at the \"Phobos monolith\", a prominent object near Stickney crater. The PRIME mission would be composed of an orbiter and lander, and each would carry 4 instruments designed to study various aspects of Phobos's geology. In 2008, NASA Glenn Research Center began studying a Phobos and Deimos sample return mission that would use solar electric propulsion. The study gave rise to the \"Hall\" mission concept, a New Frontiers-class"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_16",
    "chunk": "mission under further study as of 2010. Another concept of a sample return mission from Phobos and Deimos is OSIRIS-REx II, which would use heritage technology from the first OSIRIS-REx mission. As of January 2013, a new Phobos Surveyor mission is currently under development by a collaboration of Stanford University, NASA's Jet Propulsion Laboratory, and the Massachusetts Institute of Technology. The mission is currently in the testing phases, and the team at Stanford plans to launch the mission between 2023 and 2033. In March 2014, a Discovery class mission was proposed to place an orbiter in Mars orbit by 2021 to study Phobos and Deimos through a series of close flybys. The mission is called Phobos And Deimos & Mars Environment (PADME). Two other Phobos missions that were proposed for the Discovery 13 selection included a mission called Merlin, which would flyby Deimos but actually orbit and land on Phobos, and another one is Pandora which would orbit both Deimos and Phobos. The Japanese Aerospace Exploration Agency (JAXA) unveiled on 9 June 2015 the Martian Moons Exploration (MMX), a sample return mission targeting Phobos. MMX will land and collect samples from Phobos multiple times, along with conducting Deimos flyby observations"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_17",
    "chunk": "and monitoring Mars' climate. By using a corer sampling mechanism, the spacecraft aims to retrieve a minimum 10 g amount of samples. NASA, ESA, DLR, and CNES are also participating in the project, and will provide scientific instruments. The U.S. will contribute the Neutron and Gamma-Ray Spectrometer (NGRS), and France the Near IR Spectrometer (NIRS4/MacrOmega). Although the mission has been selected for implementation and is now beyond proposal stage, formal project approval by JAXA has been postponed following the Hitomi mishap. Development and testing of key components, including the sampler, is currently ongoing. As of 2017, MMX is scheduled to be launched in 2026, and will return to Earth five years later. Russia plans to repeat Fobos-Grunt mission in the late 2020s, and the European Space Agency is assessing a sample-return mission for 2024 called Phootprint. Phobos has been proposed as an early target for a human mission to Mars. The teleoperation of robotic scouts on Mars by humans on Phobos could be conducted without significant time delay, and planetary protection concerns in early Mars exploration might be addressed by such an approach. A landing on Phobos would be considerably less difficult and expensive than a landing on the surface"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_18",
    "chunk": "of Mars itself. A lander bound for Mars would need to be capable of atmospheric entry and subsequent return to orbit without any support facilities, or would require the creation of support facilities in-situ. A lander instead bound for Phobos could be based on equipment designed for lunar and asteroid landings. Furthermore, due to Phobos's very weak gravity, the delta-v required to land on Phobos and return is only 80% of that required for a trip to and from the surface of the Moon. It has been proposed that the sands of Phobos could serve as a valuable material for aerobraking during a Mars landing. A relatively small amount of chemical fuel brought from Earth could be used to lift a large amount of sand from the surface of Phobos to a transfer orbit. This sand could be released in front of a spacecraft during the descent maneuver causing a densification of the atmosphere just in front of the spacecraft. While human exploration of Phobos could serve as a catalyst for the human exploration of Mars, it could be scientifically valuable in its own right. First discussed in fiction in 1956 by Fontenay, Phobos has been proposed as a future"
  },
  {
    "source": "Phobos (moon).txt",
    "chunk_id": "Phobos (moon).txt_19",
    "chunk": "site for space elevator construction. This would involve a pair of space elevators: one extending 6,000 km from the Mars-facing side to the edge of Mars' atmosphere, the other extending 6,000 km (3,700 mi) from the other side and away from Mars. A spacecraft launching from Mars' surface to the lower space elevator would only need a delta-v of 0.52 km/s (0.32 mi/s), as opposed to the over 3.6 km/s (2.2 mi/s) needed to launch to low Mars orbit. The spacecraft could be lifted up using electrical power and then released from the upper space elevator with a hyperbolic velocity of 2.6 km/s (1.6 mi/s), enough to reach Earth and a significant fraction of the velocity needed to reach the asteroid belt. The space elevators could also work in reverse to help spacecraft enter the Martian system. The great mass of Phobos means that any forces from space elevator operation would have minimal effect on its orbit. Additionally, materials from Phobos could be used for space industry."
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_0",
    "chunk": "# Planet A planet is a large, rounded astronomical body that is generally required to be in orbit around a star, stellar remnant, or brown dwarf, and is not one itself. The Solar System has eight planets by the most restrictive definition of the term: the terrestrial planets Mercury, Venus, Earth, and Mars, and the giant planets Jupiter, Saturn, Uranus, and Neptune. The best available theory of planet formation is the nebular hypothesis, which posits that an interstellar cloud collapses out of a nebula to create a young protostar orbited by a protoplanetary disk. Planets grow in this disk by the gradual accumulation of material driven by gravity, a process called accretion. The word planet comes from the Greek πλανήται (planḗtai) 'wanderers'. In antiquity, this word referred to the Sun, Moon, and five points of light visible to the naked eye that moved across the background of the stars—namely, Mercury, Venus, Mars, Jupiter, and Saturn. Planets have historically had religious associations: multiple cultures identified celestial bodies with gods, and these connections with mythology and folklore persist in the schemes for naming newly discovered Solar System bodies. Earth itself was recognized as a planet when heliocentrism supplanted geocentrism during the 16th"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_1",
    "chunk": "and 17th centuries. With the development of the telescope, the meaning of planet broadened to include objects only visible with assistance: the moons of the planets beyond Earth; the ice giants Uranus and Neptune; Ceres and other bodies later recognized to be part of the asteroid belt; and Pluto, later found to be the largest member of the collection of icy bodies known as the Kuiper belt. The discovery of other large objects in the Kuiper belt, particularly Eris, spurred debate about how exactly to define a planet. In 2006, the International Astronomical Union (IAU) adopted a definition of a planet in the Solar System, placing the four terrestrial planets and the four giant planets in the planet category; Ceres, Pluto, and Eris are in the category of dwarf planet. Many planetary scientists have nonetheless continued to apply the term planet more broadly, including dwarf planets as well as rounded satellites like the Moon. Further advances in astronomy led to the discovery of over five thousand planets outside the Solar System, termed exoplanets. These often show unusual features that the Solar System planets do not show, such as hot Jupiters—giant planets that orbit close to their parent stars, like 51"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_2",
    "chunk": "Pegasi b—and extremely eccentric orbits, such as HD 20782 b. The discovery of brown dwarfs and planets larger than Jupiter also spurred debate on the definition, regarding where exactly to draw the line between a planet and a star. Multiple exoplanets have been found to orbit in the habitable zones of their stars (where liquid water can potentially exist on a planetary surface), but Earth remains the only planet known to support life. It is not known with certainty how planets are formed. The prevailing theory is that they coalesce during the collapse of a nebula into a thin disk of gas and dust. A protostar forms at the core, surrounded by a rotating protoplanetary disk. Through accretion (a process of sticky collision) dust particles in the disk steadily accumulate mass to form ever-larger bodies. Local concentrations of mass known as planetesimals form, and these accelerate the accretion process by drawing in additional material by their gravitational attraction. These concentrations become increasingly dense until they collapse inward under gravity to form protoplanets. After a planet reaches a mass somewhat larger than Mars's mass, it begins to accumulate an extended atmosphere, greatly increasing the capture rate of the planetesimals by means"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_3",
    "chunk": "of atmospheric drag. Depending on the accretion history of solids and gas, a giant planet, an ice giant, or a terrestrial planet may result. It is thought that the regular satellites of Jupiter, Saturn, and Uranus formed in a similar way; however, Triton was likely captured by Neptune, and Earth's Moon and Pluto's Charon might have formed in collisions. When the protostar has grown such that it ignites to form a star, the surviving disk is removed from the inside outward by photoevaporation, the solar wind, Poynting–Robertson drag and other effects. Thereafter there still may be many protoplanets orbiting the star or each other, but over time many will collide, either to form a larger, combined protoplanet or release material for other protoplanets to absorb. Those objects that have become massive enough will capture most matter in their orbital neighbourhoods to become planets. Protoplanets that have avoided collisions may become natural satellites of planets through a process of gravitational capture, or remain in belts of other objects to become either dwarf planets or small bodies. The energetic impacts of the smaller planetesimals (as well as radioactive decay) will heat up the growing planet, causing it to at least partially melt."
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_4",
    "chunk": "The interior of the planet begins to differentiate by density, with higher density materials sinking toward the core. Smaller terrestrial planets lose most of their atmospheres because of this accretion, but the lost gases can be replaced by outgassing from the mantle and from the subsequent impact of comets (smaller planets will lose any atmosphere they gain through various escape mechanisms). With the discovery and observation of planetary systems around stars other than the Sun, it is becoming possible to elaborate, revise or even replace this account. The level of metallicity—an astronomical term describing the abundance of chemical elements with an atomic number greater than 2 (helium)—appears to determine the likelihood that a star will have planets. Hence, a metal-rich population I star is more likely to have a substantial planetary system than a metal-poor, population II star. According to the IAU definition, there are eight planets in the Solar System, which are (in increasing distance from the Sun): Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Jupiter is the largest, at 318 Earth masses, whereas Mercury is the smallest, at 0.055 Earth masses. The planets of the Solar System can be divided into categories based on their composition."
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_5",
    "chunk": "Terrestrials are similar to Earth, with bodies largely composed of rock and metal: Mercury, Venus, Earth, and Mars. Earth is the largest terrestrial planet. Giant planets are significantly more massive than the terrestrials: Jupiter, Saturn, Uranus, and Neptune. They differ from the terrestrial planets in composition. The gas giants, Jupiter and Saturn, are primarily composed of hydrogen and helium and are the most massive planets in the Solar System. Saturn is one third as massive as Jupiter, at 95 Earth masses. The ice giants, Uranus and Neptune, are primarily composed of low-boiling-point materials such as water, methane, and ammonia, with thick atmospheres of hydrogen and helium. They have a significantly lower mass than the gas giants (only 14 and 17 Earth masses). Dwarf planets are gravitationally rounded, but have not cleared their orbits of other bodies. In increasing order of average distance from the Sun, the ones generally agreed among astronomers are Ceres, Orcus, Pluto, Haumea, Quaoar, Makemake, Gonggong, Eris, and Sedna. Ceres is the largest object in the asteroid belt, located between the orbits of Mars and Jupiter. The other eight all orbit beyond Neptune. Orcus, Pluto, Haumea, Quaoar, and Makemake orbit in the Kuiper belt, which is a"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_6",
    "chunk": "second belt of small Solar System bodies beyond the orbit of Neptune. Gonggong and Eris orbit in the scattered disc, which is somewhat further out and, unlike the Kuiper belt, is unstable towards interactions with Neptune. Sedna is the largest known detached object, a population that never comes close enough to the Sun to interact with any of the classical planets; the origins of their orbits are still being debated. All nine are similar to terrestrial planets in having a solid surface, but they are made of ice and rock rather than rock and metal. Moreover, all of them are smaller than Mercury, with Pluto being the largest known dwarf planet and Eris being the most massive. There are at least nineteen planetary-mass moons or satellite planets—moons large enough to take on ellipsoidal shapes: The Moon, Io, and Europa have compositions similar to the terrestrial planets; the others are made of ice and rock like the dwarf planets, with Tethys being made of almost pure ice. Europa is often considered an icy planet, though, because its surface ice layer makes it difficult to study its interior. Ganymede and Titan are larger than Mercury by radius, and Callisto almost equals it,"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_7",
    "chunk": "but all three are much less massive. Mimas is the smallest object generally agreed to be a geophysical planet, at about six millionths of Earth's mass, though there are many larger bodies that may not be geophysical planets (e.g. Salacia). An exoplanet is a planet outside the Solar System. As of 17 April 2025, there are 5,943 confirmed exoplanets in 4,461 planetary systems, with 976 systems having more than one planet. Known exoplanets range in size from gas giants about twice as large as Jupiter down to just over the size of the Moon. Analysis of gravitational microlensing data suggests a minimum average of 1.6 bound planets for every star in the Milky Way. In early 1992, radio astronomers Aleksander Wolszczan and Dale Frail announced the discovery of two planets orbiting the pulsar PSR 1257+12. This discovery was confirmed and is generally considered to be the first definitive detection of exoplanets. Researchers suspect they formed from a disk remnant left over from the supernova that produced the pulsar. The first confirmed discovery of an exoplanet orbiting an ordinary main-sequence star occurred on 6 October 1995, when Michel Mayor and Didier Queloz of the University of Geneva announced the detection of"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_8",
    "chunk": "51 Pegasi b, an exoplanet around 51 Pegasi. From then until the Kepler space telescope mission, most of the known exoplanets were gas giants comparable in mass to Jupiter or larger as they were more easily detected. The catalog of Kepler candidate planets consists mostly of planets the size of Neptune and smaller, down to smaller than Mercury. In 2011, the Kepler space telescope team reported the discovery of the first Earth-sized exoplanets orbiting a Sun-like star, Kepler-20e and Kepler-20f. Since that time, more than 100 planets have been identified that are approximately the same size as Earth, 20 of which orbit in the habitable zone of their star—the range of orbits where a terrestrial planet could sustain liquid water on its surface, given enough atmospheric pressure. One in five Sun-like stars is thought to have an Earth-sized planet in its habitable zone, which suggests that the nearest would be expected to be within 12 light-years distance from Earth. The frequency of occurrence of such terrestrial planets is one of the variables in the Drake equation, which estimates the number of intelligent, communicating civilizations that exist in the Milky Way. There are types of planets that do not exist in"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_9",
    "chunk": "the Solar System: super-Earths and mini-Neptunes, which have masses between that of Earth and Neptune. Objects less than about twice the mass of Earth are expected to be rocky like Earth; beyond that, they become a mixture of volatiles and gas like Neptune. The planet Gliese 581c, with a mass 5.5–10.4 times the mass of Earth, attracted attention upon its discovery for potentially being in the habitable zone, though later studies concluded that it is actually too close to its star to be habitable. Planets more massive than Jupiter are also known, extending seamlessly into the realm of brown dwarfs. Exoplanets have been found that are much closer to their parent star than any planet in the Solar System is to the Sun. Mercury, the closest planet to the Sun at 0.4 AU, takes 88 days for an orbit, but ultra-short period planets can orbit in less than a day. The Kepler-11 system has five of its planets in shorter orbits than Mercury's, all of them much more massive than Mercury. There are hot Jupiters, such as 51 Pegasi b, that orbit very close to their star and may evaporate to become chthonian planets, which are the leftover cores. There"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_10",
    "chunk": "are also exoplanets that are much farther from their star. Neptune is 30 AU from the Sun and takes 165 years to orbit, but there are exoplanets that are thousands of AU from their star and take more than a million years to orbit (e.g. COCONUTS-2b). Although each planet has unique physical characteristics, a number of broad commonalities do exist among them. Some of these characteristics, such as rings or natural satellites, have only as yet been observed in planets in the Solar System, whereas others are commonly observed in exoplanets. In the Solar System, all the planets orbit the Sun in the same direction as the Sun rotates: counter-clockwise as seen from above the Sun's north pole. At least one exoplanet, WASP-17b, has been found to orbit in the opposite direction to its star's rotation. The period of one revolution of a planet's orbit is known as its sidereal period or year. A planet's year depends on its distance from its star; the farther a planet is from its star, the longer the distance it must travel and the slower its speed, since it is less affected by its star's gravity. No planet's orbit is perfectly circular, and hence"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_11",
    "chunk": "the distance of each from the host star varies over the course of its year. The closest approach to its star is called its periastron, or perihelion in the Solar System, whereas its farthest separation from the star is called its apastron (aphelion). As a planet approaches periastron, its speed increases as it trades gravitational potential energy for kinetic energy, just as a falling object on Earth accelerates as it falls. As the planet nears apastron, its speed decreases, just as an object thrown upwards on Earth slows down as it reaches the apex of its trajectory. Planets have varying degrees of axial tilt; they spin at an angle to the plane of their stars' equators. This causes the amount of light received by each hemisphere to vary over the course of its year; when the Northern Hemisphere points away from its star, the Southern Hemisphere points towards it, and vice versa. Each planet therefore has seasons, resulting in changes to the climate over the course of its year. The time at which each hemisphere points farthest or nearest from its star is known as its solstice. Each planet has two in the course of its orbit; when one hemisphere"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_12",
    "chunk": "has its summer solstice with its day being the longest, the other has its winter solstice when its day is shortest. The varying amount of light and heat received by each hemisphere creates annual changes in weather patterns for each half of the planet. Jupiter's axial tilt is very small, so its seasonal variation is minimal; Uranus, on the other hand, has an axial tilt so extreme it is virtually on its side, which means that its hemispheres are either continually in sunlight or continually in darkness around the time of its solstices. In the Solar System, Mercury, Venus, Ceres, and Jupiter have very small tilts; Pallas, Uranus, and Pluto have extreme ones; and Earth, Mars, Vesta, Saturn, and Neptune have moderate ones. Among exoplanets, axial tilts are not known for certain, though most hot Jupiters are believed to have a negligible axial tilt as a result of their proximity to their stars. Similarly, the axial tilts of the planetary-mass moons are near zero, with Earth's Moon at 6.687° as the biggest exception; additionally, Callisto's axial tilt varies between 0 and about 2 degrees on timescales of thousands of years. The planets rotate around invisible axes through their centres. A"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_13",
    "chunk": "planet's rotation period is known as a stellar day. Most of the planets in the Solar System rotate in the same direction as they orbit the Sun, which is counter-clockwise as seen from above the Sun's north pole. The exceptions are Venus and Uranus, which rotate clockwise, though Uranus's extreme axial tilt means there are differing conventions on which of its poles is \"north\", and therefore whether it is rotating clockwise or anti-clockwise. Regardless of which convention is used, Uranus has a retrograde rotation relative to its orbit. The rotation of a planet can be induced by several factors during formation. A net angular momentum can be induced by the individual angular momentum contributions of accreted objects. The accretion of gas by the giant planets contributes to the angular momentum. Finally, during the last stages of planet building, a stochastic process of protoplanetary accretion can randomly alter the spin axis of the planet. There is great variation in the length of day between the planets, with Venus taking 243 days to rotate, and the giant planets only a few hours. The rotational periods of exoplanets are not known, but for hot Jupiters, their proximity to their stars means that they"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_14",
    "chunk": "are tidally locked (that is, their orbits are in sync with their rotations). This means, they always show one face to their stars, with one side in perpetual day, the other in perpetual night. Mercury and Venus, the closest planets to the Sun, similarly exhibit very slow rotation: Mercury is tidally locked into a 3:2 spin–orbit resonance (rotating three times for every two revolutions around the Sun), and Venus's rotation may be in equilibrium between tidal forces slowing it down and atmospheric tides created by solar heating speeding it up. All the large moons are tidally locked to their parent planets; Pluto and Charon are tidally locked to each other, as are Eris and Dysnomia, and probably Orcus and its moon Vanth. The other dwarf planets with known rotation periods rotate faster than Earth; Haumea rotates so fast that it has been distorted into a triaxial ellipsoid. The exoplanet Tau Boötis b and its parent star Tau Boötis appear to be mutually tidally locked. The defining dynamic characteristic of a planet, according to the IAU definition, is that it has cleared its neighborhood. A planet that has cleared its neighborhood has accumulated enough mass to gather up or sweep away"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_15",
    "chunk": "all the planetesimals in its orbit. In effect, it orbits its star in isolation, as opposed to sharing its orbit with a multitude of similar-sized objects. As described above, this characteristic was mandated as part of the IAU's official definition of a planet in August 2006. Although to date this criterion only applies to the Solar System, a number of young extrasolar systems have been found in which evidence suggests orbital clearing is taking place within their circumstellar discs. Gravity causes planets to be pulled into a roughly spherical shape, so a planet's size can be expressed roughly by an average radius (for example, Earth radius or Jupiter radius). However, planets are not perfectly spherical; for example, the Earth's rotation causes it to be slightly flattened at the poles with a bulge around the equator. Therefore, a better approximation of Earth's shape is an oblate spheroid, whose equatorial diameter is 43 kilometers (27 mi) larger than the pole-to-pole diameter. Generally, a planet's shape may be described by giving polar and equatorial radii of a spheroid or specifying a reference ellipsoid. From such a specification, the planet's flattening, surface area, and volume can be calculated; its normal gravity can be computed"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_16",
    "chunk": "knowing its size, shape, rotation rate, and mass. A planet's defining physical characteristic is that it is massive enough for the force of its own gravity to dominate over the electromagnetic forces binding its physical structure, leading to a state of hydrostatic equilibrium. This effectively means that all planets are spherical or spheroidal. Up to a certain mass, an object can be irregular in shape, but beyond that point, which varies depending on the chemical makeup of the object, gravity begins to pull an object towards its own centre of mass until the object collapses into a sphere. Mass is the prime attribute by which planets are distinguished from stars. No objects between the masses of the Sun and Jupiter exist in the Solar System, but there are exoplanets of this size. The lower stellar mass limit is estimated to be around 75 to 80 times that of Jupiter (MJ). Some authors advocate that this be used as the upper limit for planethood, on the grounds that the internal physics of objects does not change between approximately one Saturn mass (beginning of significant self-compression) and the onset of hydrogen burning and becoming a red dwarf star. Beyond roughly 13 MJ"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_17",
    "chunk": "(at least for objects with solar-type isotopic abundance), an object achieves conditions suitable for nuclear fusion of deuterium: this has sometimes been advocated as a boundary, even though deuterium burning does not last very long and most brown dwarfs have long since finished burning their deuterium. This is not universally agreed upon: the exoplanets Encyclopaedia includes objects up to 60 MJ, and the Exoplanet Data Explorer up to 24 MJ. The smallest known exoplanet with an accurately known mass is PSR B1257+12A, one of the first exoplanets discovered, which was found in 1992 in orbit around a pulsar. Its mass is roughly half that of the planet Mercury. Even smaller is WD 1145+017 b, orbiting a white dwarf; its mass is roughly that of the dwarf planet Haumea, and it is typically termed a minor planet. The smallest known planet orbiting a main-sequence star other than the Sun is Kepler-37b, with a mass (and radius) that is probably slightly higher than that of the Moon. The smallest object in the Solar System generally agreed to be a geophysical planet is Saturn's moon Mimas, with a radius about 3.1% of Earth's and a mass about 0.00063% of Earth's. Saturn's smaller moon"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_18",
    "chunk": "Phoebe, currently an irregular body of 1.7% Earth's radius and 0.00014% Earth's mass, is thought to have attained hydrostatic equilibrium and differentiation early in its history before being battered out of shape by impacts. Some asteroids may be fragments of protoplanets that began to accrete and differentiate, but suffered catastrophic collisions, leaving only a metallic or rocky core today, or a reaccumulation of the resulting debris. Every planet began its existence in an entirely fluid state; in early formation, the denser, heavier materials sank to the centre, leaving the lighter materials near the surface. Each therefore has a differentiated interior consisting of a dense planetary core surrounded by a mantle that either is or was a fluid. The terrestrial planets' mantles are sealed within hard crusts, but in the giant planets the mantle simply blends into the upper cloud layers. The terrestrial planets have cores of elements such as iron and nickel and mantles of silicates. Jupiter and Saturn are believed to have cores of rock and metal surrounded by mantles of metallic hydrogen. Uranus and Neptune, which are smaller, have rocky cores surrounded by mantles of water, ammonia, methane, and other ices. The fluid action within these planets' cores"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_19",
    "chunk": "creates a geodynamo that generates a magnetic field. Similar differentiation processes are believed to have occurred on some of the large moons and dwarf planets, though the process may not always have been completed: Ceres, Callisto, and Titan appear to be incompletely differentiated. The asteroid Vesta, though not a dwarf planet because it was battered by impacts out of roundness, has a differentiated interior similar to that of Venus, Earth, and Mars. All of the Solar System planets except Mercury have substantial atmospheres because their gravity is strong enough to keep gases close to the surface. Saturn's largest moon Titan also has a substantial atmosphere thicker than that of Earth; Neptune's largest moon Triton and the dwarf planet Pluto have more tenuous atmospheres. The larger giant planets are massive enough to keep large amounts of the light gases hydrogen and helium, whereas the smaller planets lose these gases into space. Analysis of exoplanets suggests that the threshold for being able to hold on to these light gases occurs at about 2.0+0.7−0.6 M🜨, so that Earth and Venus are near the maximum size for rocky planets. The composition of Earth's atmosphere is different from the other planets because the various life"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_20",
    "chunk": "processes that have transpired on the planet have introduced free molecular oxygen. The atmospheres of Mars and Venus are both dominated by carbon dioxide, but differ drastically in density: the average surface pressure of Mars's atmosphere is less than 1% that of Earth's (too low to allow liquid water to exist), while the average surface pressure of Venus's atmosphere is about 92 times that of Earth's. It is likely that Venus's atmosphere was the result of a runaway greenhouse effect in its history, which today makes it the hottest planet by surface temperature, hotter even than Mercury. Despite hostile surface conditions, temperature, and pressure at about 50–55 km altitude in Venus's atmosphere are close to Earthlike conditions (the only place in the Solar System beyond Earth where this is so), and this region has been suggested as a plausible base for future human exploration. Titan has the only nitrogen-rich planetary atmosphere in the Solar System other than Earth's. Just as Earth's conditions are close to the triple point of water, allowing it to exist in all three states on the planet's surface, so Titan's are to the triple point of methane. Planetary atmospheres are affected by the varying insolation or"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_21",
    "chunk": "internal energy, leading to the formation of dynamic weather systems such as hurricanes (on Earth), planet-wide dust storms (on Mars), a greater-than-Earth-sized anticyclone on Jupiter (called the Great Red Spot), and holes in the atmosphere (on Neptune). Weather patterns detected on exoplanets include a hot region on HD 189733 b twice the size of the Great Red Spot, as well as clouds on the hot Jupiter Kepler-7b, the super-Earth Gliese 1214 b, and others. Hot Jupiters, due to their extreme proximities to their host stars, have been shown to be losing their atmospheres into space due to stellar radiation, much like the tails of comets. These planets may have vast differences in temperature between their day and night sides that produce supersonic winds, although multiple factors are involved and the details of the atmospheric dynamics that affect the day-night temperature difference are complex. One important characteristic of the planets is their intrinsic magnetic moments, which in turn give rise to magnetospheres. The presence of a magnetic field indicates that the planet is still geologically alive. In other words, magnetized planets have flows of electrically conducting material in their interiors, which generate their magnetic fields. These fields significantly change the interaction"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_22",
    "chunk": "of the planet and solar wind. A magnetized planet creates a cavity in the solar wind around itself called the magnetosphere, which the wind cannot penetrate. The magnetosphere can be much larger than the planet itself. In contrast, non-magnetized planets have only small magnetospheres induced by interaction of the ionosphere with the solar wind, which cannot effectively protect the planet. Of the eight planets in the Solar System, only Venus and Mars lack such a magnetic field. Of the magnetized planets, the magnetic field of Mercury is the weakest and is barely able to deflect the solar wind. Jupiter's moon Ganymede has a magnetic field several times stronger, and Jupiter's is the strongest in the Solar System (so intense in fact that it poses a serious health risk to future crewed missions to all its moons inward of Callisto). The magnetic fields of the other giant planets, measured at their surfaces, are roughly similar in strength to that of Earth, but their magnetic moments are significantly larger. The magnetic fields of Uranus and Neptune are strongly tilted relative to the planets' rotational axes and displaced from the planets' centres. In 2003, a team of astronomers in Hawaii observing the star"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_23",
    "chunk": "HD 179949 detected a bright spot on its surface, apparently created by the magnetosphere of an orbiting hot Jupiter. Several planets or dwarf planets in the Solar System (such as Neptune and Pluto) have orbital periods that are in resonance with each other or with smaller bodies. This is common in satellite systems (e.g. the resonance between Io, Europa, and Ganymede around Jupiter, or between Enceladus and Dione around Saturn). All except Mercury and Venus have natural satellites, often called \"moons\". Earth has one, Mars has two, and the giant planets have numerous moons in complex planetary-type systems. Except for Ceres and Sedna, all the consensus dwarf planets are known to have at least one moon as well. Many moons of the giant planets have features similar to those on the terrestrial planets and dwarf planets, and some have been studied as possible abodes of life (especially Europa and Enceladus). The four giant planets are orbited by planetary rings of varying size and complexity. The rings are composed primarily of dust or particulate matter, but can host tiny 'moonlets' whose gravity shapes and maintains their structure. Although the origins of planetary rings are not precisely known, they are believed to"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_24",
    "chunk": "be the result of natural satellites that fell below their parent planets' Roche limits and were torn apart by tidal forces. The dwarf planets Haumea and Quaoar also have rings. No secondary characteristics have been observed around exoplanets. The sub-brown dwarf Cha 110913−773444, which has been described as a rogue planet, is believed to be orbited by a tiny protoplanetary disc, and the sub-brown dwarf OTS 44 was shown to be surrounded by a substantial protoplanetary disk of at least 10 Earth masses. The idea of planets has evolved over the history of astronomy, from the divine lights of antiquity to the earthly objects of the scientific age. The concept has expanded to include worlds not only in the Solar System, but in multitudes of other extrasolar systems. The consensus as to what counts as a planet, as opposed to other objects, has changed several times. It previously encompassed asteroids, moons, and dwarf planets like Pluto, and there continues to be some disagreement today. The five classical planets of the Solar System, being visible to the naked eye, have been known since ancient times and have had a significant impact on mythology, religious cosmology, and ancient astronomy. In ancient times,"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_25",
    "chunk": "astronomers noted how certain lights moved across the sky, as opposed to the \"fixed stars\", which maintained a constant relative position in the sky. Ancient Greeks called these lights πλάνητες ἀστέρες (planētes asteres) 'wandering stars' or simply πλανῆται (planētai) 'wanderers' from which today's word \"planet\" was derived. In ancient Greece, China, Babylon, and indeed all pre-modern civilizations, it was almost universally believed that Earth was the center of the Universe and that all the \"planets\" circled Earth. The reasons for this perception were that stars and planets appeared to revolve around Earth each day and the apparently common-sense perceptions that Earth was solid and stable and that it was not moving but at rest. The first civilization known to have a functional theory of the planets were the Babylonians, who lived in Mesopotamia in the first and second millennia BC. The oldest surviving planetary astronomical text is the Babylonian Venus tablet of Ammisaduqa, a 7th-century BC copy of a list of observations of the motions of the planet Venus, that probably dates as early as the second millennium BC. The MUL.APIN is a pair of cuneiform tablets dating from the 7th century BC that lays out the motions of the"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_26",
    "chunk": "Sun, Moon, and planets over the course of the year. Late Babylonian astronomy is the origin of Western astronomy and indeed all Western efforts in the exact sciences. The Enuma anu enlil, written during the Neo-Assyrian period in the 7th century BC, comprises a list of omens and their relationships with various celestial phenomena including the motions of the planets. The inferior planets Venus and Mercury and the superior planets Mars, Jupiter, and Saturn were all identified by Babylonian astronomers. These would remain the only known planets until the invention of the telescope in early modern times. The ancient Greeks initially did not attach as much significance to the planets as the Babylonians. In the 6th and 5th centuries BC, the Pythagoreans appear to have developed their own independent planetary theory, which consisted of the Earth, Sun, Moon, and planets revolving around a \"Central Fire\" at the center of the Universe. Pythagoras or Parmenides is said to have been the first to identify the evening star (Hesperos) and morning star (Phosphoros) as one and the same (Aphrodite, Greek corresponding to Latin Venus), though this had long been known in Mesopotamia. In the 3rd century BC, Aristarchus of Samos proposed a"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_27",
    "chunk": "heliocentric system, according to which Earth and the planets revolved around the Sun. The geocentric system remained dominant until the Scientific Revolution. By the 1st century BC, during the Hellenistic period, the Greeks had begun to develop their own mathematical schemes for predicting the positions of the planets. These schemes, which were based on geometry rather than the arithmetic of the Babylonians, would eventually eclipse the Babylonians' theories in complexity and comprehensiveness and account for most of the astronomical movements observed from Earth with the naked eye. These theories would reach their fullest expression in the Almagest written by Ptolemy in the 2nd century CE. So complete was the domination of Ptolemy's model that it superseded all previous works on astronomy and remained the definitive astronomical text in the Western world for 13 centuries. To the Greeks and Romans, there were seven known planets, each presumed to be circling Earth according to the complex laws laid out by Ptolemy. They were, in increasing order from Earth (in Ptolemy's order and using modern names): the Moon, Mercury, Venus, the Sun, Mars, Jupiter, and Saturn. After the fall of the Western Roman Empire, astronomy developed further in India and the medieval Islamic"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_28",
    "chunk": "world. In 499 CE, the Indian astronomer Aryabhata propounded a planetary model that explicitly incorporated Earth's rotation about its axis, which he explains as the cause of what appears to be an apparent westward motion of the stars. He also theorized that the orbits of planets were elliptical. Aryabhata's followers were particularly strong in South India, where his principles of the diurnal rotation of Earth, among others, were followed and a number of secondary works were based on them. The astronomy of the Islamic Golden Age mostly took place in the Middle East, Central Asia, Al-Andalus, and North Africa, and later in the Far East and India. These astronomers, like the polymath Ibn al-Haytham, generally accepted geocentrism, although they did dispute Ptolemy's system of epicycles and sought alternatives. The 10th-century astronomer Abu Sa'id al-Sijzi accepted that the Earth rotates around its axis. In the 11th century, the transit of Venus was observed by Avicenna. His contemporary Al-Biruni devised a method of determining the Earth's radius using trigonometry that, unlike the older method of Eratosthenes, only required observations at a single mountain. With the advent of the Scientific Revolution and the heliocentric model of Copernicus, Galileo, and Kepler, use of the"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_29",
    "chunk": "term \"planet\" changed from something that moved around the sky relative to the fixed star to a body that orbited the Sun, directly (a primary planet) or indirectly (a secondary or satellite planet). Thus the Earth was added to the roster of planets, and the Sun was removed. The Copernican count of primary planets stood until 1781, when William Herschel discovered Uranus. When four satellites of Jupiter (the Galilean moons) and five of Saturn were discovered in the 17th century, they joined Earth's Moon in the category of \"satellite planets\" or \"secondary planets\" orbiting the primary planets, though in the following decades they would come to be called simply \"satellites\" for short. Scientists generally considered planetary satellites to also be planets until about the 1920s, although this usage was not common among non-scientists. In the first decade of the 19th century, four new 'planets' were discovered: Ceres (in 1801), Pallas (in 1802), Juno (in 1804), and Vesta (in 1807). It soon became apparent that they were rather different from previously known planets: they shared the same general region of space, between Mars and Jupiter (the asteroid belt), with sometimes overlapping orbits. This was an area where only one planet had"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_30",
    "chunk": "been expected, and they were much smaller than all other planets; indeed, it was suspected that they might be shards of a larger planet that had broken up. Herschel called them asteroids (from the Greek for \"starlike\") because even in the largest telescopes they resembled stars, without a resolvable disk. The situation was stable for four decades, but in the 1840s several additional asteroids were discovered (Astraea in 1845; Hebe, Iris, and Flora in 1847; Metis in 1848; and Hygiea in 1849). New \"planets\" were discovered every year; as a result, astronomers began tabulating the asteroids (minor planets) separately from the major planets and assigning them numbers instead of abstract planetary symbols, although they continued to be considered as small planets. Neptune was discovered in 1846, its position having been predicted thanks to its gravitational influence upon Uranus. Because the orbit of Mercury appeared to be affected in a similar way, it was believed in the late 19th century that there might be another planet even closer to the Sun. However, the discrepancy between Mercury's orbit and the predictions of Newtonian gravity was instead explained by an improved theory of gravity, Einstein's general relativity. Pluto was discovered in 1930. After"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_31",
    "chunk": "initial observations led to the belief that it was larger than Earth, the object was immediately accepted as the ninth major planet. Further monitoring found the body was actually much smaller: in 1936, Ray Lyttleton suggested that Pluto may be an escaped satellite of Neptune, and Fred Whipple suggested in 1964 that Pluto may be a comet. The discovery of its large moon Charon in 1978 showed that Pluto was only 0.2% the mass of Earth. As this was still substantially more massive than any known asteroid, and because no other trans-Neptunian objects had been discovered at that time, Pluto kept its planetary status, only officially losing it in 2006. In the 1950s, Gerard Kuiper published papers on the origin of the asteroids. He recognized that asteroids were typically not spherical, as had previously been thought, and that the asteroid families were remnants of collisions. Thus he differentiated between the largest asteroids as \"true planets\" versus the smaller ones as collisional fragments. From the 1960s onwards, the term \"minor planet\" was mostly displaced by the term \"asteroid\", and references to the asteroids as planets in the literature became scarce, except for the geologically evolved largest three: Ceres, and less often"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_32",
    "chunk": "Pallas and Vesta. The beginning of Solar System exploration by space probes in the 1960s spurred a renewed interest in planetary science. A split in definitions regarding satellites occurred around then: planetary scientists began to reconsider the large moons as also being planets, but astronomers who were not planetary scientists generally did not. (This is not exactly the same as the definition used in the previous century, which classed all satellites as secondary planets, even non-round ones like Saturn's Hyperion or Mars's Phobos and Deimos.) All the eight major planets and their planetary-mass moons have since been explored by spacecraft, as have many asteroids and the dwarf planets Ceres and Pluto; however, so far the only planetary-mass body beyond Earth that has been explored by humans is the Moon. A growing number of astronomers argued for Pluto to be declassified as a planet, because many similar objects approaching its size had been found in the same region of the Solar System (the Kuiper belt) during the 1990s and early 2000s. Pluto was found to be just one \"small\" body in a population of thousands. They often referred to the demotion of the asteroids as a precedent, although that had been"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_33",
    "chunk": "done based on their geophysical differences from planets rather than their being in a belt. Some of the larger trans-Neptunian objects, such as Quaoar, Sedna, Eris, and Haumea, were heralded in the popular press as the tenth planet. The announcement of Eris in 2005, an object 27% more massive than Pluto, created the impetus for an official definition of a planet, as considering Pluto a planet would logically have demanded that Eris be considered a planet as well. Since different procedures were in place for naming planets versus non-planets, this created an urgent situation because under the rules Eris could not be named without defining what a planet was. At the time, it was also thought that the size required for a trans-Neptunian object to become round was about the same as that required for the moons of the giant planets (about 400 km diameter), a figure that would have suggested about 200 round objects in the Kuiper belt and thousands more beyond. Many astronomers argued that the public would not accept a definition creating a large number of planets. Source: \"IAU 2006 General Assembly: Resolutions 5 and 6\" (PDF). IAU. 24 August 2006. Retrieved 23 June 2009. To acknowledge"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_34",
    "chunk": "the problem, the International Astronomical Union (IAU) set about creating the definition of planet and produced one in August 2006. Under this definition, the Solar System is considered to have eight planets (Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune). Bodies that fulfill the first two conditions but not the third are classified as dwarf planets, provided they are not natural satellites of other planets. Originally an IAU committee had proposed a definition that would have included a larger number of planets as it did not include (c) as a criterion. After much discussion, it was decided via a vote that those bodies should instead be classified as dwarf planets. The IAU definition has not been universally used or accepted. In planetary geology, celestial objects are defined as planets by geophysical characteristics. A celestial body may acquire a dynamic (planetary) geology at approximately the mass required for its mantle to become plastic under its own weight. This leads to a state of hydrostatic equilibrium where the body acquires a stable, round shape, which is adopted as the hallmark of planethood by geophysical definitions. For example: a substellar-mass body that has never undergone nuclear fusion and has enough gravitation to"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_35",
    "chunk": "be round due to hydrostatic equilibrium, regardless of its orbital parameters. In the Solar System, this mass is generally less than the mass required for a body to clear its orbit; thus, some objects that are considered \"planets\" under geophysical definitions are not considered as such under the IAU definition, such as Ceres and Pluto. (In practice, the requirement for hydrostatic equilibrium is universally relaxed to a requirement for rounding and compaction under self-gravity; Mercury is not actually in hydrostatic equilibrium, but is universally included as a planet regardless.) Proponents of such definitions often argue that location should not matter and that planethood should be defined by the intrinsic properties of an object. Dwarf planets had been proposed as a category of small planet (as opposed to planetoids as sub-planetary objects) and planetary geologists continue to treat them as planets despite the IAU definition. The number of dwarf planets even among known objects is not certain. In 2019, Grundy et al. argued based on the low densities of some mid-sized trans-Neptunian objects that the limiting size required for a trans-Neptunian object to reach equilibrium was in fact much larger than it is for the icy moons of the giant planets,"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_36",
    "chunk": "being about 900–1000 km diameter. There is general consensus on Ceres in the asteroid belt and on the eight trans-Neptunians that probably cross this threshold—Orcus, Pluto, Haumea, Quaoar, Makemake, Gonggong, Eris, and Sedna. Planetary geologists may include the nineteen known planetary-mass moons as \"satellite planets\", including Earth's Moon and Pluto's Charon, like the early modern astronomers. Some go even further and include as planets relatively large, geologically evolved bodies that are nonetheless not very round today, such as Pallas and Vesta; rounded bodies that were completely disrupted by impacts and re-accreted like Hygiea; or even everything at least the diameter of Saturn's moon Mimas, the smallest planetary-mass moon. (This may even include objects that are not round but happen to be larger than Mimas, like Neptune's moon Proteus.) Astronomer Jean-Luc Margot proposed a mathematical criterion that determines whether an object can clear its orbit during the lifetime of its host star, based on the mass of the planet, its semimajor axis, and the mass of its host star. The formula produces a value called π that is greater than 1 for planets. The eight known planets and all known exoplanets have π values above 100, while Ceres, Pluto, and Eris"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_37",
    "chunk": "have π values of 0.1, or less. Objects with π values of 1 or more are expected to be approximately spherical, so that objects that fulfill the orbital-zone clearance requirement around Sun-like stars will also fulfill the roundness requirement – though this may not be the case around very low-mass stars. In 2024, Margot and collaborators proposed a revised version of the criterion with a uniform clearing timescale of 10 billion years (the approximate main-sequence lifetime of the Sun) or 13.8 billion years (the age of the Universe) to accommodate planets orbiting brown dwarfs. Even before the discovery of exoplanets, there were particular disagreements over whether an object should be considered a planet if it was part of a distinct population such as a belt, or if it was large enough to generate energy by the thermonuclear fusion of deuterium. Complicating the matter even further, bodies too small to generate energy by fusing deuterium can form by gas-cloud collapse just like stars and brown dwarfs, even down to the mass of Jupiter: there was thus disagreement about whether how a body formed should be taken into account. In 1992, astronomers Aleksander Wolszczan and Dale Frail announced the discovery of planets"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_38",
    "chunk": "around a pulsar, PSR B1257+12. This discovery is generally considered to be the first definitive detection of a planetary system around another star. Then, on 6 October 1995, Michel Mayor and Didier Queloz of the Geneva Observatory announced the first definitive detection of an exoplanet orbiting an ordinary main-sequence star (51 Pegasi). The discovery of exoplanets led to another ambiguity in defining a planet: the point at which a planet becomes a star. Many known exoplanets are many times the mass of Jupiter, approaching that of stellar objects known as brown dwarfs. Brown dwarfs are generally considered stars due to their theoretical ability to fuse deuterium, a heavier isotope of hydrogen. Although objects more massive than 75 times that of Jupiter fuse simple hydrogen, objects of 13 Jupiter masses can fuse deuterium. Deuterium is quite rare, constituting less than 0.0026% of the hydrogen in the galaxy, and most brown dwarfs would have ceased fusing deuterium long before their discovery, making them effectively indistinguishable from supermassive planets. The 2006 IAU definition presents some challenges for exoplanets because the language is specific to the Solar System and the criteria of roundness and orbital zone clearance are not presently observable for exoplanets. In"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_39",
    "chunk": "2018, this definition was reassessed and updated as knowledge of exoplanets increased. The current official working definition of an exoplanet is as follows: The IAU noted that this definition could be expected to evolve as knowledge improves. A 2022 review article discussing the history and rationale of this definition suggested that the words \"in young star clusters\" should be deleted in clause 3, as such objects have now been found elsewhere, and that the term \"sub-brown dwarfs\" should be replaced by the more current \"free-floating planetary mass objects\". The term \"planetary mass object\" has also been used to refer to ambiguous situations concerning exoplanets, such as objects with mass typical for a planet that are free-floating or orbit a brown dwarf instead of a star. Free-floating objects of planetary mass have sometimes been called planets anyway, specifically rogue planets. The limit of 13 Jupiter masses is not universally accepted. Objects below this mass limit can sometimes burn deuterium, and the amount of deuterium that is burned depends on an object's composition. Furthermore, deuterium is quite scarce, so the stage of deuterium burning does not actually last very long; unlike hydrogen burning in a star, deuterium burning does not significantly affect"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_40",
    "chunk": "the future evolution of an object. The relationship between mass and radius (or density) show no special feature at this limit, according to which brown dwarfs have the same physics and internal structure as lighter Jovian planets, and would more naturally be considered planets. Thus, many catalogues of exoplanets include objects heavier than 13 Jupiter masses, sometimes going up to 60 Jupiter masses. (The limit for hydrogen burning and becoming a red dwarf star is about 80 Jupiter masses.) The situation of main-sequence stars has been used to argue for such an inclusive definition of \"planet\" as well, as they also differ greatly along the two orders of magnitude that they cover, in their structure, atmospheres, temperature, spectral features, and probably formation mechanisms; yet they are all considered as one class, being all hydrostatic-equilibrium objects undergoing nuclear burning. The names for the planets of the Solar System (other than Earth) in the English language are derived from naming practices developed consecutively by the Babylonians, Greeks, and Romans of antiquity. The practice of grafting the names of gods onto the planets was almost certainly borrowed from the Babylonians by the ancient Greeks, and thereafter from the Greeks by the Romans. The"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_41",
    "chunk": "Babylonians named Venus after the Sumerian goddess of love with the Akkadian name Ishtar; Mars after their god of war, Nergal; Mercury after their god of wisdom Nabu; Jupiter after their chief god, Marduk; and Saturn after their god of farming, Ninurta. There are too many concordances between Greek and Babylonian naming conventions for them to have arisen separately. Given the differences in mythology, the correspondence was not perfect. For instance, the Babylonian Nergal was a god of war, and thus the Greeks identified him with Ares. Unlike Ares, Nergal was also a god of pestilence and ruler of the underworld. In ancient Greece, the two great luminaries, the Sun and the Moon, were called Helios and Selene, two ancient Titanic deities; the slowest planet, Saturn, was called Phainon, the shiner; followed by Phaethon, Jupiter, \"bright\"; the red planet, Mars was known as Pyroeis, the \"fiery\"; the brightest, Venus, was known as Phosphoros, the light bringer; and the fleeting final planet, Mercury, was called Stilbon, the gleamer. The Greeks assigned each planet to one among their pantheon of gods, the Olympians and the earlier Titans: Although modern Greeks still use their ancient names for the planets, other European languages, because"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_42",
    "chunk": "of the influence of the Roman Empire and, later, the Catholic Church, use the Roman (Latin) names rather than the Greek ones. The Romans inherited Proto-Indo-European mythology as the Greeks did and shared with them a common pantheon under different names, but the Romans lacked the rich narrative traditions that Greek poetic culture had given their gods. During the later period of the Roman Republic, Roman writers borrowed much of the Greek narratives and applied them to their own pantheon, to the point where they became virtually indistinguishable. When the Romans studied Greek astronomy, they gave the planets their own gods' names: Mercurius (for Hermes), Venus (Aphrodite), Mars (Ares), Iuppiter (Zeus), and Saturnus (Cronus). However, there was not much agreement on which god a particular planet was associated with; according to Pliny the Elder, while Phainon and Phaethon's associations with Saturn and Jupiter respectively were widely agreed upon, Pyroeis was also associated with the demi-god Hercules, Stilbon was also associated with Apollo, god of music, healing, and prophecy; Phosphoros was also associated with prominent goddesses Juno and Isis. Some Romans, following a belief possibly originating in Mesopotamia but developed in Hellenistic Egypt, believed that the seven gods after whom the"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_43",
    "chunk": "planets were named took hourly shifts in looking after affairs on Earth. The order of shifts went Saturn, Jupiter, Mars, Sun, Venus, Mercury, Moon (from the farthest to the closest planet). Therefore, the first day was started by Saturn (1st hour), second day by Sun (25th hour), followed by Moon (49th hour), Mars, Mercury, Jupiter, and Venus. Because each day was named by the god that started it, this became the order of the days of the week in the Roman calendar. In English, Saturday, Sunday, and Monday are straightforward translations of these Roman names. The other days were renamed after Tīw (Tuesday), Wōden (Wednesday), Þunor (Thursday), and Frīġ (Friday), the Anglo-Saxon gods considered similar or equivalent to Mars, Mercury, Jupiter, and Venus, respectively. Earth's name in English is not derived from Greco-Roman mythology. Because it was only generally accepted as a planet in the 17th century, there is no tradition of naming it after a god. (The same is true, in English at least, of the Sun and the Moon, though they are no longer generally considered planets.) The name originates from the Old English word eorþe, which was the word for \"ground\" and \"dirt\" as well as the"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_44",
    "chunk": "world itself. As with its equivalents in the other Germanic languages, it derives ultimately from the Proto-Germanic word erþō, as can be seen in the English earth, the German Erde, the Dutch aarde, and the Scandinavian jord. Many of the Romance languages retain the old Roman word terra (or some variation of it) that was used with the meaning of \"dry land\" as opposed to \"sea\". The non-Romance languages use their own native words. The Greeks retain their original name, Γή (Ge). Non-European cultures use other planetary-naming systems. India uses a system based on the Navagraha, which incorporates the seven traditional planets and the ascending and descending lunar nodes Rahu and Ketu. The planets are Surya 'Sun', Chandra 'Moon', Budha for Mercury, Shukra ('bright') for Venus, Mangala (the god of war) for Mars, Bṛhaspati (councilor of the gods) for Jupiter, and Shani (symbolic of time) for Saturn. The native Persian names of most of the planets are based on identifications of the Mesopotamian gods with Iranian gods, analogous to the Greek and Latin names. Mercury is Tir (Persian: تیر) for the western Iranian god Tīriya (patron of scribes), analogous to Nabu; Venus is Nāhid (ناهید) for Anahita; Mars is Bahrām"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_45",
    "chunk": "(بهرام) for Verethragna; and Jupiter is Hormoz (هرمز) for Ahura Mazda. The Persian name for Saturn, Keyvān (کیوان), is a borrowing from Akkadian kajamānu, meaning \"the permanent, steady\". China and the countries of eastern Asia historically subject to Chinese cultural influence (such as Japan, Korea, and Vietnam) use a naming system based on the five Chinese elements: water (Mercury 水星 \"water star\"), metal or gold (Venus 金星 \"gold star\"), fire (Mars 火星 \"fire star\"), wood (Jupiter 木星 \"wood star\"), and earth or soil (Saturn 土星 \"soil star\"). In traditional Hebrew astronomy, the seven traditional planets have (for the most part) descriptive names—the Sun is חמה Ḥammah or \"the hot one\", the Moon is לבנה Levanah or \"the white one\", Venus is כוכב נוגה Kokhav Nogah or \"the bright planet\", Mercury is כוכב Kokhav or \"the planet\" (given its lack of distinguishing features), Mars is מאדים Ma'adim or \"the red one\", and Saturn is שבתאי Shabbatai or \"the resting one\" (in reference to its slow movement compared to the other visible planets). The odd one out is Jupiter, called צדק Tzedeq or \"justice\". These names, first attested in the Babylonian Talmud, are not the original Hebrew names of the planets. In"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_46",
    "chunk": "377 Epiphanius of Salamis recorded another set of names that seem to have pagan or Canaanite associations: those names, since replaced for religious reasons, were probably the historical Semitic names, and may have much earlier roots going back to Babylonian astronomy. The etymologies for the Arabic names of the planets are less well understood. Mostly agreed among scholars are Venus (Arabic: الزهرة, az-Zuhara, \"the bright one\"), Earth (الأرض, al-ʾArḍ, from the same root as eretz), and Saturn (زُحَل, Zuḥal, \"withdrawer\"). Multiple suggested etymologies exist for Mercury (عُطَارِد, ʿUṭārid), Mars (اَلْمِرِّيخ, al-Mirrīkh), and Jupiter (المشتري, al-Muštarī), but there is no agreement among scholars. When subsequent planets were discovered in the 18th and 19th centuries, Uranus was named for a Greek deity and Neptune for a Roman one (the counterpart of Poseidon). The asteroids were initially named from mythology as well—Ceres, Juno, and Vesta are major Roman goddesses, and Pallas is an epithet of the major Greek goddess Athena—but as more and more were discovered, they first started being named after more minor goddesses, and the mythological restriction was dropped starting from the twentieth asteroid Massalia in 1852. Pluto (named after the Greek god of the underworld) was given a classical name,"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_47",
    "chunk": "as it was considered a major planet when it was discovered. The names of Uranus (天王星 \"sky king star\"), Neptune (海王星 \"sea king star\"), and Pluto (冥王星 \"underworld king star\") in Chinese, Korean, and Japanese are calques based on the roles of those gods in Roman and Greek mythology. In the 19th century, Alexander Wylie and Li Shanlan calqued the names of the first 117 asteroids into Chinese, and many of their names are still used today, e.g. Ceres (穀神星 \"grain goddess star\"), Pallas (智神星 \"wisdom goddess star\"), Juno (婚神星 \"marriage goddess star\"), Vesta (灶神星 \"hearth goddess star\"), and Hygiea (健神星 \"health goddess star\"). Such translations were extended to some later minor planets, including some of the dwarf planets discovered in the 21st century, e.g. Haumea (妊神星 \"pregnancy goddess star\"), Makemake (鳥神星 \"bird goddess star\"), and Eris (鬩神星 \"quarrel goddess star\"). However, except for the better-known asteroids and dwarf planets, many of them are rare outside Chinese astronomical dictionaries. Hebrew names were chosen for Uranus (אורון Oron, \"small light\") and Neptune (רהב Rahab, a Biblical sea monster) in 2009; prior to that the names \"Uranus\" and \"Neptune\" had simply been borrowed. After more objects were discovered beyond Neptune, naming conventions"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_48",
    "chunk": "depending on their orbits were put in place: those in the 2:3 resonance with Neptune (the plutinos) are given names from underworld myths, while others are given names from creation myths. Most of the trans-Neptunian planetoids are named after gods and goddesses from other cultures (e.g. Quaoar is named after a Tongva god). There are a few exceptions which continue the Roman and Greek scheme, notably including Eris as it had initially been considered a tenth planet. The moons (including the planetary-mass ones) are generally given names with some association with their parent planet. The planetary-mass moons of Jupiter are named after four of Zeus' lovers (or other sexual partners); those of Saturn are named after Cronus' brothers and sisters, the Titans; those of Uranus are named after characters from Shakespeare and Pope (originally specifically from fairy mythology, but that ended with the naming of Miranda). Neptune's planetary-mass moon Triton is named after the god's son; Pluto's planetary-mass moon Charon is named after the ferryman of the dead, who carries the souls of the newly deceased to the underworld (Pluto's domain). Exoplanets are commonly named after their parent star and their order of discovery within its planetary system, such as"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_49",
    "chunk": "Proxima Centauri b. (The lettering starts at b, with a considered to represent the parent star.) The written symbols for Mercury, Venus, Jupiter, Saturn, and possibly Mars have been traced to forms found in late Greek papyrus texts. The symbols for Jupiter and Saturn are identified as monograms of the corresponding Greek names, and the symbol for Mercury is a stylized caduceus. According to Annie Scott Dill Maunder, antecedents of the planetary symbols were used in art to represent the gods associated with the classical planets. Bianchini's planisphere, discovered by Francesco Bianchini in the 18th century but produced in the 2nd century, shows Greek personifications of planetary gods charged with early versions of the planetary symbols. Mercury has a caduceus; Venus has, attached to her necklace, a cord connected to another necklace; Mars, a spear; Jupiter, a staff; Saturn, a scythe; the Sun, a circlet with rays radiating from it; and the Moon, a headdress with a crescent attached. The modern shapes with the cross-marks first appeared around the 16th century. According to Maunder, the addition of crosses appears to be \"an attempt to give a savour of Christianity to the symbols of the old pagan gods.\" Earth itself was"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_50",
    "chunk": "not considered a classical planet; its symbol descends from a pre-heliocentric symbol for the four corners of the world. When further planets were discovered orbiting the Sun, symbols were invented for them. The most common astronomical symbol for Uranus, ⛢, was invented by Johann Gottfried Köhler, and was intended to represent the newly discovered metal platinum. An alternative symbol, ♅, was invented by Jérôme Lalande, and represents a globe with a H on top, for Uranus's discoverer Herschel. Today, ⛢ is mostly used by astronomers and ♅ by astrologers, though it is possible to find each symbol in the other context. The first few asteroids were considered to be planets when they were discovered, and were likewise given abstract symbols, e.g. Ceres' sickle (⚳), Pallas' spear (⚴), Juno's sceptre (⚵), and Vesta's hearth (⚶). However, as their number rose further and further, this practice stopped in favour of numbering them instead. (Massalia, the first asteroid not named from mythology, is also the first asteroid that was not assigned a symbol by its discoverer.) The symbols for the first four asteroids, Ceres through Vesta, remained in use for longer than the others, and even in the modern day NASA has used"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_51",
    "chunk": "the Ceres symbol—Ceres being the only asteroid that is also a dwarf planet. Neptune's symbol (♆) represents the god's trident. The astronomical symbol for Pluto is a P-L monogram (♇), though it has become less common since the IAU definition reclassified Pluto. Since Pluto's reclassification, NASA has used the traditional astrological symbol of Pluto (⯓), a planetary orb over Pluto's bident. The IAU discourages the use of planetary symbols in modern journal articles in favour of one-letter or (to disambiguate Mercury and Mars) two-letter abbreviations for the major planets. The symbols for the Sun and Earth are nonetheless common, as solar mass, Earth mass, and similar units are common in astronomy. Other planetary symbols today are mostly encountered in astrology. Astrologers have resurrected the old astronomical symbols for the first few asteroids and continue to invent symbols for other objects. This includes relatively standard astrological symbols for the dwarf planets discovered in the 21st century, which were not given symbols by astronomers because planetary symbols had mostly fallen out of use in astronomy by the time they were discovered. Many astrological symbols are included in Unicode, and a few of these new inventions (the symbols of Haumea, Makemake, and Eris)"
  },
  {
    "source": "Planet.txt",
    "chunk_id": "Planet.txt_52",
    "chunk": "have since been used by NASA in astronomy. The Eris symbol is a traditional one from Discordianism, a religion worshipping the goddess Eris. The other dwarf-planet symbols are mostly initialisms (except Haumea) in the native scripts of the cultures they come from; they also represent something associated with the corresponding deity or culture, e.g. Makemake's face or Gonggong's snake-tail. Moskowitz also devised symbols for the planetary-mass moons; most of them are initialisms combined with a feature of their parent planet. The exception is Charon, which combines the high orb of Pluto's bident symbol with a crescent, suggesting both Charon as a moon and the mythological Charon's boat crossing the river Styx."
  },
  {
    "source": "Planetary and Space Science.txt",
    "chunk_id": "Planetary and Space Science.txt_0",
    "chunk": "# Planetary and Space Science Planetary and Space Science (P&SS), published 15 times per year, is a peer-reviewed scientific journal established in 1959. It publishes original research articles along with short communications (letters). The main topic is Solar System processes which encompasses multiple areas of the natural sciences. Numerical simulations of solar system processes are also conducted at ground-based facilities or on-board space platforms. The editor-in-chief is Maria Cristina De Sanctis (National Institute of Astrophysics, Roma, Italy). It is published by Elsevier. Research that involves planetary and space sciences involves many disciplines, which is reflected by the scope of the journal. Celestial mechanics is part of these studies, as this science includes understanding the dynamic evolution of the Solar System, relativistic effects, among other areas of analysis and consideration. Cosmochemistry is also part of the published research in this journal. Cosmochemistry in this instance, includes all aspects of the initial physical and chemical formation along with the subsequent evolution of the solar system pertaining to these physical and chemical processes. The research expands to include the terrestrial planets, and their satellites. This involves the physics of the interior, the geology of the planet or satellite surface, the surface morphology, and"
  },
  {
    "source": "Planetary and Space Science.txt",
    "chunk_id": "Planetary and Space Science.txt_1",
    "chunk": "studying their tectonics, mineralogy and dating. Observing the outer planets and their satellites includes studying formation and evolution. This method of observation and study involves remote sensing at all wavelengths and in situ measurements. Planet formation and planet evolution is of interest when gathering and interpreting data for planetary atmospheres. Atmospheric circulation, meteorology, and boundary layers are also part of the original published research. Understanding is gained through remote sensing and laboratory simulation. The study of planets also includes magnetospheres and ionospheres. The origin of their respective magnetic fields, magnetospheric plasma and radiation belts is also of interest. Included in this area is the interaction of magnetospheres and ionospheres with the Sun, solar wind, and their natural satellites. Research that involves the small bodies of the Solar System is also published. Small bodies describes dust, objects of rings, asteroids, comets, zodiacal light. This research also describes their interaction with solar radiation and the solar wind. Beyond the Solar System, extrasolar system studies are also considered a field of interest for this journal. This includes detection of exoplanets, as well as determining whether or not given exoplanets or exosystems can be detected. Also the formation and evolution of these planets and"
  },
  {
    "source": "Planetary and Space Science.txt",
    "chunk_id": "Planetary and Space Science.txt_2",
    "chunk": "systems are of interest. According to the Journal Citation Reports, the journal has a 2020 impact factor of 2.03."
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_0",
    "chunk": "# Planetary nebula A planetary nebula is a type of emission nebula consisting of an expanding, glowing shell of ionized gas ejected from red giant stars late in their lives. The term \"planetary nebula\" is a misnomer because they are unrelated to planets. The term originates from the planet-like round shape of these nebulae observed by astronomers through early telescopes. The first usage may have occurred during the 1780s with the English astronomer William Herschel who described these nebulae as resembling planets; however, as early as January 1779, the French astronomer Antoine Darquier de Pellepoix described in his observations of the Ring Nebula, \"very dim but perfectly outlined; it is as large as Jupiter and resembles a fading planet\". Though the modern interpretation is different, the old term is still used. All planetary nebulae form at the end of the life of a star of intermediate mass, about 1-8 solar masses. It is expected that the Sun will form a planetary nebula at the end of its life cycle. They are relatively short-lived phenomena, lasting perhaps a few tens of millennia, compared to considerably longer phases of stellar evolution. Once all of the red giant's atmosphere has been dissipated, energetic"
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_1",
    "chunk": "ultraviolet radiation from the exposed hot luminous core, called a planetary nebula nucleus (P.N.N.), ionizes the ejected material. Absorbed ultraviolet light then energizes the shell of nebulous gas around the central star, causing it to appear as a brightly coloured planetary nebula. Planetary nebulae probably play a crucial role in the chemical evolution of the Milky Way by expelling elements into the interstellar medium from stars where those elements were created. Planetary nebulae are observed in more distant galaxies, yielding useful information about their chemical abundances. Starting from the 1990s, Hubble Space Telescope images revealed that many planetary nebulae have extremely complex and varied morphologies. About one-fifth are roughly spherical, but the majority are not spherically symmetric. The mechanisms that produce such a wide variety of shapes and features are not yet well understood, but binary central stars, stellar winds and magnetic fields may play a role. The first planetary nebula discovered (though not yet termed as such) was the Dumbbell Nebula in the constellation of Vulpecula. It was observed by Charles Messier on July 12, 1764 and listed as M27 in his catalogue of nebulous objects. To early observers with low-resolution telescopes, M27 and subsequently discovered planetary nebulae resembled"
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_2",
    "chunk": "the giant planets like Uranus. As early as January 1779, the French astronomer Antoine Darquier de Pellepoix described in his observations of the Ring Nebula, \"a very dull nebula, but perfectly outlined; as large as Jupiter and looks like a fading planet\". The nature of these objects remained unclear. In 1782, William Herschel, discoverer of Uranus, found the Saturn Nebula (NGC 7009) and described it as \"A curious nebula, or what else to call it I do not know\". He later described these objects as seeming to be planets \"of the starry kind\". As noted by Darquier before him, Herschel found that the disk resembled a planet but it was too faint to be one. In 1785, Herschel wrote to Jérôme Lalande: These are celestial bodies of which as yet we have no clear idea and which are perhaps of a type quite different from those that we are familiar with in the heavens. I have already found four that have a visible diameter of between 15 and 30 seconds. These bodies appear to have a disk that is rather like a planet, that is to say, of equal brightness all over, round or somewhat oval, and about as well"
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_3",
    "chunk": "defined in outline as the disk of the planets, of a light strong enough to be visible with an ordinary telescope of only one foot, yet they have only the appearance of a star of about ninth magnitude. He assigned these to Class IV of his catalogue of \"nebulae\", eventually listing 78 \"planetary nebulae\", most of which are in fact galaxies. Herschel used the term \"planetary nebulae\" for these objects. The origin of this term not known. The label \"planetary nebula\" became ingrained in the terminology used by astronomers to categorize these types of nebulae, and is still in use by astronomers today. The nature of planetary nebulae remained unknown until the first spectroscopic observations were made in the mid-19th century. Using a prism to disperse their light, William Huggins was one of the earliest astronomers to study the optical spectra of astronomical objects. On August 29, 1864, Huggins was the first to analyze the spectrum of a planetary nebula when he observed Cat's Eye Nebula. His observations of stars had shown that their spectra consisted of a continuum of radiation with many dark lines superimposed. He found that many nebulous objects such as the Andromeda Nebula (as it was"
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_4",
    "chunk": "then known) had spectra that were quite similar. However, when Huggins looked at the Cat's Eye Nebula, he found a very different spectrum. Rather than a strong continuum with absorption lines superimposed, the Cat's Eye Nebula and other similar objects showed a number of emission lines. Brightest of these was at a wavelength of 500.7 nanometres, which did not correspond with a line of any known element. At first, it was hypothesized that the line might be due to an unknown element, which was named nebulium. A similar idea had led to the discovery of helium through analysis of the Sun's spectrum in 1868. While helium was isolated on Earth soon after its discovery in the spectrum of the Sun, \"nebulium\" was not. In the early 20th century, Henry Norris Russell proposed that, rather than being a new element, the line at 500.7 nm was due to a familiar element in unfamiliar conditions. Physicists showed in the 1920s that in gas at extremely low densities, electrons can occupy excited metastable energy levels in atoms and ions that would otherwise be de-excited by collisions that would occur at higher densities. Electron transitions from these levels in nitrogen and oxygen ions (O,"
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_5",
    "chunk": "O (a.k.a. O iii), and N) give rise to the 500.7 nm emission line and others. These spectral lines, which can only be seen in very low-density gases, are called forbidden lines. Spectroscopic observations thus showed that nebulae were made of extremely rarefied gas. The central stars of planetary nebulae are very hot. Only when a star has exhausted most of its nuclear fuel can it collapse to a small size. Planetary nebulae are understood as a final stage of stellar evolution. Spectroscopic observations show that all planetary nebulae are expanding. This led to the idea that planetary nebulae were caused by a star's outer layers being thrown into space at the end of its life. Towards the end of the 20th century, technological improvements helped to further the study of planetary nebulae. Space telescopes allowed astronomers to study light wavelengths outside those that the Earth's atmosphere transmits. The first UV observations of PNe (IC 2149) were performed from space, with the Orion 2 Space Observatory (see Orion 1 and Orion 2 Space Observatories) on board the Soyuz 13 spacecraft in December 1973, two photon emission from nebulae was detected for the first time. Infrared and ultraviolet studies of planetary"
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_6",
    "chunk": "nebulae allowed much more accurate determinations of nebular temperatures, densities and elemental abundances. Charge-coupled device technology allowed much fainter spectral lines to be measured accurately than had previously been possible. The Hubble Space Telescope also showed that while many nebulae appear to have simple and regular structures when observed from the ground, the very high optical resolution achievable by telescopes above the Earth's atmosphere reveals extremely complex structures. Under the Morgan-Keenan spectral classification scheme, planetary nebulae are classified as Type-P, although this notation is seldom used in practice. Stars greater than 8 solar masses (M⊙) will probably end their lives in dramatic supernovae explosions, while planetary nebulae seemingly only occur at the end of the lives of intermediate and low mass stars between 0.8 M⊙ to 8.0 M⊙. Progenitor stars that form planetary nebulae will spend most of their lifetimes converting their hydrogen into helium in the star's core by nuclear fusion at about 15 million K. This generates energy in the core, which creates outward pressure that balances the crushing inward pressures of gravity. This state of equilibrium is known as the main sequence, which can last for tens of millions to billions of years, depending on the mass."
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_7",
    "chunk": "When the hydrogen in the core starts to run out, nuclear fusion generates less energy and gravity starts compressing the core, causing a rise in temperature to about 100 million K. Such high core temperatures then make the star's cooler outer layers expand to create much larger red giant stars. This end phase causes a dramatic rise in stellar luminosity, where the released energy is distributed over a much larger surface area, which in fact causes the average surface temperature to be lower. In stellar evolution terms, stars undergoing such increases in luminosity are known as asymptotic giant branch stars (AGB). During this phase, the star can lose 50–70% of its total mass from its stellar wind. For the more massive asymptotic giant branch stars that form planetary nebulae, whose progenitors exceed about 0.6M⊙, their cores will continue to contract. When temperatures reach about 100 million K, the available helium nuclei fuse into carbon and oxygen, so that the star again resumes radiating energy, temporarily stopping the core's contraction. This new helium burning phase (fusion of helium nuclei) forms a growing inner core of inert carbon and oxygen. Above it is a thin helium-burning shell, surrounded in turn by a"
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_8",
    "chunk": "hydrogen-burning shell. However, this new phase lasts only 20,000 years or so, a very short period compared to the entire lifetime of the star. The venting of atmosphere continues unabated into interstellar space, but when the outer surface of the exposed core reaches temperatures exceeding about 30,000 K, there are enough emitted ultraviolet photons to ionize the ejected atmosphere, causing the gas to shine as a planetary nebula. After a star passes through the asymptotic giant branch (AGB) phase, the short planetary nebula phase of stellar evolution begins as gases blow away from the central star at speeds of a few kilometers per second. The central star is the remnant of its AGB progenitor, an electron-degenerate carbon-oxygen core that has lost most of its hydrogen envelope due to mass loss on the AGB. As the gases expand, the central star undergoes a two-stage evolution, first growing hotter as it continues to contract and hydrogen fusion reactions occur in the shell around the core and then slowly cooling when the hydrogen shell is exhausted through fusion and mass loss. In the second phase, it radiates away its energy and fusion reactions cease, as the central star is not heavy enough to"
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_9",
    "chunk": "generate the core temperatures required for carbon and oxygen to fuse. During the first phase, the central star maintains constant luminosity, while at the same time it grows ever hotter, eventually reaching temperatures around 100,000 K. In the second phase, it cools so much that it does not give off enough ultraviolet radiation to ionize the increasingly distant gas cloud. The star becomes a white dwarf, and the expanding gas cloud becomes invisible to us, ending the planetary nebula phase of evolution. For a typical planetary nebula, about 10,000 years passes between its formation and recombination of the resulting plasma. Planetary nebulae may play a very important role in galactic evolution. Newly born stars consist almost entirely of hydrogen and helium, but as stars evolve through the asymptotic giant branch phase, they create heavier elements via nuclear fusion which are eventually expelled by strong stellar winds. Planetary nebulae usually contain larger proportions of elements such as carbon, nitrogen and oxygen, and these are recycled into the interstellar medium via these powerful winds. In this way, planetary nebulae greatly enrich the Milky Way and their nebulae with these heavier elements – collectively known by astronomers as metals and specifically referred to"
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_10",
    "chunk": "by the metallicity parameter Z. Subsequent generations of stars formed from such nebulae also tend to have higher metallicities. Although these metals are present in stars in relatively tiny amounts, they have marked effects on stellar evolution and fusion reactions. When stars formed earlier in the universe they theoretically contained smaller quantities of heavier elements. Known examples are the metal poor Population II stars. (See Stellar population.) Identification of stellar metallicity content is found by spectroscopy. A typical planetary nebula is roughly one light year across, and consists of extremely rarefied gas, with a density generally from 100 to 10,000 particles per cm. (The Earth's atmosphere, by comparison, contains 2.5×10 particles per cm.) Young planetary nebulae have the highest densities, sometimes as high as 10 particles per cm. As nebulae age, their expansion causes their density to decrease. The masses of planetary nebulae range from 0.1 to 1 solar masses. Radiation from the central star heats the gases to temperatures of about 10,000 K. The gas temperature in central regions is usually much higher than at the periphery reaching 16,000–25,000 K. The volume in the vicinity of the central star is often filled with a very hot (coronal) gas having"
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_11",
    "chunk": "the temperature of about 1,000,000 K. This gas originates from the surface of the central star in the form of the fast stellar wind. Nebulae may be described as matter bounded or radiation bounded. In the former case, there is not enough matter in the nebula to absorb all the UV photons emitted by the star, and the visible nebula is fully ionized. In the latter case, there are not enough UV photons being emitted by the central star to ionize all the surrounding gas, and an ionization front propagates outward into the circumstellar envelope of neutral atoms. About 3000 planetary nebulae are now known to exist in our galaxy, out of 200 billion stars. Their very short lifetime compared to total stellar lifetime accounts for their rarity. They are found mostly near the plane of the Milky Way, with the greatest concentration near the Galactic Center. Only about 20% of planetary nebulae are spherically symmetric (for example, see Abell 39). A wide variety of shapes exist with some very complex forms seen. Planetary nebulae are classified by different authors into: stellar, disk, ring, irregular, helical, bipolar, quadrupolar, and other types, although the majority of them belong to just three"
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_12",
    "chunk": "types: spherical, elliptical and bipolar. Bipolar nebulae are concentrated in the galactic plane, probably produced by relatively young massive progenitor stars; and bipolars in the galactic bulge appear to prefer orienting their orbital axes parallel to the galactic plane. On the other hand, spherical nebulae are probably produced by old stars similar to the Sun. The huge variety of the shapes is partially the projection effect—the same nebula when viewed under different angles will appear different. Nevertheless, the reason for the huge variety of physical shapes is not fully understood. Gravitational interactions with companion stars if the central stars are binary stars may be one cause. Another possibility is that planets disrupt the flow of material away from the star as the nebula forms. It has been determined that the more massive stars produce more irregularly shaped nebulae. In January 2005, astronomers announced the first detection of magnetic fields around the central stars of two planetary nebulae, predicted already in 1960s, and hypothesized that the fields might be partly or wholly responsible for their remarkable shapes. Planetary nebulae have been detected as members in four Galactic globular clusters: Messier 15, Messier 22, NGC 6441 and Palomar 6. Evidence also points"
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_13",
    "chunk": "to the potential discovery of planetary nebulae in globular clusters in the galaxy M31. However, there is currently only one case of a planetary nebula discovered in an open cluster that is agreed upon by independent researchers. That case pertains to the planetary nebula PHR 1315-6555 and the open cluster Andrews-Lindsay 1. Indeed, through cluster membership, PHR 1315-6555 possesses among the most precise distances established for a planetary nebula (i.e., a 4% distance solution). The cases of NGC 2818 and NGC 2348 in Messier 46, exhibit mismatched velocities between the planetary nebulae and the clusters, which indicates they are line-of-sight coincidences. A subsample of tentative cases that may potentially be cluster/PN pairs includes Abell 8 and Bica 6, and He 2-86 and NGC 4463. Theoretical models predict that planetary nebulae can form from main-sequence stars of between one and eight solar masses, which puts the progenitor star's age at greater than 40 million years. Although there are a few hundred known open clusters within that age range, a variety of reasons limit the chances of finding a planetary nebula within. For one reason, the planetary nebula phase for more massive stars is on the order of millennia, which is a"
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_14",
    "chunk": "blink of the eye in astronomic terms. Also, partly because of their small total mass, open clusters have relatively poor gravitational cohesion and tend to disperse after a relatively short time, typically from 100 to 600 million years. The distances to planetary nebulae are generally poorly determined, but the Gaia mission is now measuring direct parallactic distances between their central stars and neighboring stars. It is also possible to determine distances to nearby planetary nebula by measuring their expansion rates. High resolution observations taken several years apart will show the expansion of the nebula perpendicular to the line of sight, while spectroscopic observations of the Doppler shift will reveal the velocity of expansion in the line of sight. Comparing the angular expansion with the derived velocity of expansion will reveal the distance to the nebula. The issue of how such a diverse range of nebular shapes can be produced is a debatable topic. It is theorised that interactions between material moving away from the star at different speeds gives rise to most observed shapes. However, some astronomers postulate that close binary central stars might be responsible for the more complex and extreme planetary nebulae. Several have been shown to exhibit"
  },
  {
    "source": "Planetary nebula.txt",
    "chunk_id": "Planetary nebula.txt_15",
    "chunk": "strong magnetic fields, and their interactions with ionized gas could explain some planetary nebulae shapes. There are two main methods of determining metal abundances in nebulae. These rely on recombination lines and collisionally excited lines. Large discrepancies are sometimes seen between the results derived from the two methods. This may be explained by the presence of small temperature fluctuations within planetary nebulae. The discrepancies may be too large to be caused by temperature effects, and some hypothesize the existence of cold knots containing very little hydrogen to explain the observations. However, such knots have yet to be observed."
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_0",
    "chunk": "# Platonic solid In geometry, a Platonic solid is a convex, regular polyhedron in three-dimensional Euclidean space. Being a regular polyhedron means that the faces are congruent (identical in shape and size) regular polygons (all angles congruent and all edges congruent), and the same number of faces meet at each vertex. There are only five such polyhedra: Geometers have studied the Platonic solids for thousands of years. They are named for the ancient Greek philosopher Plato, who hypothesized in one of his dialogues, the Timaeus, that the classical elements were made of these regular solids. The Platonic solids have been known since antiquity. It has been suggested that certain carved stone balls created by the late Neolithic people of Scotland represent these shapes; however, these balls have rounded knobs rather than being polyhedral, the numbers of knobs frequently differed from the numbers of vertices of the Platonic solids, there is no ball whose knobs match the 20 vertices of the dodecahedron, and the arrangement of the knobs was not always symmetrical. The ancient Greeks studied the Platonic solids extensively. Some sources (such as Proclus) credit Pythagoras with their discovery. Other evidence suggests that he may have only been familiar with"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_1",
    "chunk": "the tetrahedron, cube, and dodecahedron and that the discovery of the octahedron and icosahedron belong to Theaetetus, a contemporary of Plato. In any case, Theaetetus gave a mathematical description of all five and may have been responsible for the first known proof that no other convex regular polyhedra exist. The Platonic solids are prominent in the philosophy of Plato, their namesake. Plato wrote about them in the dialogue Timaeus c. 360 B.C. in which he associated each of the four classical elements (earth, air, water, and fire) with a regular solid. Earth was associated with the cube, air with the octahedron, water with the icosahedron, and fire with the tetrahedron. Of the fifth Platonic solid, the dodecahedron, Plato obscurely remarked, \"...the god used [it] for arranging the constellations on the whole heaven\". Aristotle added a fifth element, aither (aether in Latin, \"ether\" in English) and postulated that the heavens were made of this element, but he had no interest in matching it with Plato's fifth solid. Euclid completely mathematically described the Platonic solids in the Elements, the last book (Book XIII) of which is devoted to their properties. Propositions 13–17 in Book XIII describe the construction of the tetrahedron, octahedron,"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_2",
    "chunk": "cube, icosahedron, and dodecahedron in that order. For each solid Euclid finds the ratio of the diameter of the circumscribed sphere to the edge length. In Proposition 18 he argues that there are no further convex regular polyhedra. Andreas Speiser has advocated the view that the construction of the five regular solids is the chief goal of the deductive system canonized in the Elements. Much of the information in Book XIII is probably derived from the work of Theaetetus. In the 16th century, the German astronomer Johannes Kepler attempted to relate the five extraterrestrial planets known at that time to the five Platonic solids. In Mysterium Cosmographicum, published in 1596, Kepler proposed a model of the Solar System in which the five solids were set inside one another and separated by a series of inscribed and circumscribed spheres. Kepler proposed that the distance relationships between the six planets known at that time could be understood in terms of the five Platonic solids enclosed within a sphere that represented the orbit of Saturn. The six spheres each corresponded to one of the planets (Mercury, Venus, Earth, Mars, Jupiter, and Saturn). The solids were ordered with the innermost being the octahedron, followed"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_3",
    "chunk": "by the icosahedron, dodecahedron, tetrahedron, and finally the cube, thereby dictating the structure of the solar system and the distance relationships between the planets by the Platonic solids. In the end, Kepler's original idea had to be abandoned, but out of his research came his three laws of orbital dynamics, the first of which was that the orbits of planets are ellipses rather than circles, changing the course of physics and astronomy. He also discovered the Kepler solids, which are two nonconvex regular polyhedra. For Platonic solids centered at the origin, simple Cartesian coordinates of the vertices are given below. The Greek letter φ {\\displaystyle \\varphi } is used to represent the golden ratio 1 + 5 2 ≈ 1.6180 {\\displaystyle {\\frac {1+{\\sqrt {5}}}{2}}\\approx 1.6180} . The coordinates for the tetrahedron, dodecahedron, and icosahedron are given in two positions such that each can be deduced from the other: in the case of the tetrahedron, by changing all coordinates of sign (central symmetry), or, in the other cases, by exchanging two coordinates (reflection with respect to any of the three diagonal planes). These coordinates reveal certain relationships between the Platonic solids: the vertices of the tetrahedron represent half of those of"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_4",
    "chunk": "the cube, as {4,3} or , one of two sets of 4 vertices in dual positions, as h{4,3} or . Both tetrahedral positions make the compound stellated octahedron. The coordinates of the icosahedron are related to two alternated sets of coordinates of a nonuniform truncated octahedron, t{3,4} or , also called a snub octahedron, as s{3,4} or , and seen in the compound of two icosahedra. Eight of the vertices of the dodecahedron are shared with the cube. Completing all orientations leads to the compound of five cubes. A convex polyhedron is a Platonic solid if and only if all three of the following requirements are met. Each Platonic solid can therefore be assigned a pair {p, q} of integers, where p is the number of edges (or, equivalently, vertices) of each face, and q is the number of faces (or, equivalently, edges) that meet at each vertex. This pair {p, q}, called the Schläfli symbol, gives a combinatorial description of the polyhedron. The Schläfli symbols of the five Platonic solids are given in the table below. All other combinatorial information about these solids, such as total number of vertices (V), edges (E), and faces (F), can be determined from"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_5",
    "chunk": "p and q. Since any edge joins two vertices and has two adjacent faces we must have: This can be proved in many ways. Together these three relationships completely determine V, E, and F: V = 4 p 4 − ( p − 2 ) ( q − 2 ) , E = 2 p q 4 − ( p − 2 ) ( q − 2 ) , F = 4 q 4 − ( p − 2 ) ( q − 2 ) . {\\displaystyle V={\\frac {4p}{4-(p-2)(q-2)}},\\quad E={\\frac {2pq}{4-(p-2)(q-2)}},\\quad F={\\frac {4q}{4-(p-2)(q-2)}}.} Swapping p and q interchanges F and V while leaving E unchanged. For a geometric interpretation of this property, see § Dual polyhedra. The elements of a polyhedron can be expressed in a configuration matrix. The rows and columns correspond to vertices, edges, and faces. The diagonal numbers say how many of each element occur in the whole polyhedron. The nondiagonal numbers say how many of the column's element occur in or at the row's element. Dual pairs of polyhedra have their configuration matrices rotated 180 degrees from each other. The classical result is that only five convex regular polyhedra exist. Two common arguments below demonstrate no"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_6",
    "chunk": "more than five Platonic solids can exist, but positively demonstrating the existence of any given solid is a separate question—one that requires an explicit construction. The following geometric argument is very similar to the one given by Euclid in the Elements: A purely topological proof can be made using only combinatorial information about the solids. The key is Euler's observation that V − E + F = 2, and the fact that pF = 2E = qV, where p stands for the number of edges of each face and q for the number of edges meeting at each vertex. Combining these equations one obtains the equation 2 E q − E + 2 E p = 2. {\\displaystyle {\\frac {2E}{q}}-E+{\\frac {2E}{p}}=2.} 1 q + 1 p = 1 2 + 1 E . {\\displaystyle {1 \\over q}+{1 \\over p}={1 \\over 2}+{1 \\over E}.} 1 q + 1 p > 1 2 . {\\displaystyle {\\frac {1}{q}}+{\\frac {1}{p}}>{\\frac {1}{2}}.} Using the fact that p and q must both be at least 3, one can easily see that there are only five possibilities for {p, q}: There are a number of angles associated with each Platonic solid. The dihedral angle is the interior"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_7",
    "chunk": "angle between any two face planes. The dihedral angle, θ, of the solid {p,q} is given by the formula sin ⁡ ( θ / 2 ) = cos ⁡ ( π / q ) sin ⁡ ( π / p ) . {\\displaystyle \\sin(\\theta /2)={\\frac {\\cos(\\pi /q)}{\\sin(\\pi /p)}}.} tan ⁡ ( θ / 2 ) = cos ⁡ ( π / q ) sin ⁡ ( π / h ) . {\\displaystyle \\tan(\\theta /2)={\\frac {\\cos(\\pi /q)}{\\sin(\\pi /h)}}.} The quantity h (called the Coxeter number) is 4, 6, 6, 10, and 10 for the tetrahedron, cube, octahedron, dodecahedron, and icosahedron respectively. The angular deficiency at the vertex of a polyhedron is the difference between the sum of the face-angles at that vertex and 2π. The defect, δ, at any vertex of the Platonic solids {p,q} is δ = 2 π − q π ( 1 − 2 p ) . {\\displaystyle \\delta =2\\pi -q\\pi \\left(1-{2 \\over p}\\right).} By a theorem of Descartes, this is equal to 4π divided by the number of vertices (i.e. the total defect at all vertices is 4π). The three-dimensional analog of a plane angle is a solid angle. The solid angle, Ω, at the vertex of"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_8",
    "chunk": "a Platonic solid is given in terms of the dihedral angle by This follows from the spherical excess formula for a spherical polygon and the fact that the vertex figure of the polyhedron {p,q} is a regular q-gon. The solid angle of a face subtended from the center of a platonic solid is equal to the solid angle of a full sphere (4π steradians) divided by the number of faces. This is equal to the angular deficiency of its dual. The various angles associated with the Platonic solids are tabulated below. The numerical values of the solid angles are given in steradians. The constant φ = ⁠1 + √5/2⁠ is the golden ratio. Another virtue of regularity is that the Platonic solids all possess three concentric spheres: The radii of these spheres are called the circumradius, the midradius, and the inradius. These are the distances from the center of the polyhedron to the vertices, edge midpoints, and face centers respectively. The circumradius R and the inradius r of the solid {p, q} with edge length a are given by R = a 2 tan ⁡ ( π q ) tan ⁡ ( θ 2 ) r = a 2 cot"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_9",
    "chunk": "⁡ ( π p ) tan ⁡ ( θ 2 ) {\\displaystyle {\\begin{aligned}R&={\\frac {a}{2}}\\tan \\left({\\frac {\\pi }{q}}\\right)\\tan \\left({\\frac {\\theta }{2}}\\right)\\\\[3pt]r&={\\frac {a}{2}}\\cot \\left({\\frac {\\pi }{p}}\\right)\\tan \\left({\\frac {\\theta }{2}}\\right)\\end{aligned}}} ρ = a 2 cos ⁡ ( π p ) csc ( π h ) {\\displaystyle \\rho ={\\frac {a}{2}}\\cos \\left({\\frac {\\pi }{p}}\\right)\\,{\\csc }{\\biggl (}{\\frac {\\pi }{h}}{\\biggr )}} where h is the quantity used above in the definition of the dihedral angle (h = 4, 6, 6, 10, or 10). The ratio of the circumradius to the inradius is symmetric in p and q: R r = tan ⁡ ( π p ) tan ⁡ ( π q ) = csc 2 ( θ 2 ) − cos 2 ( α 2 ) sin ⁡ ( α 2 ) . {\\displaystyle {\\frac {R}{r}}=\\tan \\left({\\frac {\\pi }{p}}\\right)\\tan \\left({\\frac {\\pi }{q}}\\right)={\\frac {\\sqrt {{\\csc ^{2}}{\\Bigl (}{\\frac {\\theta }{2}}{\\Bigr )}-{\\cos ^{2}}{\\Bigl (}{\\frac {\\alpha }{2}}{\\Bigr )}}}{\\sin {\\Bigl (}{\\frac {\\alpha }{2}}{\\Bigr )}}}.} The surface area, A, of a Platonic solid {p, q} is easily computed as area of a regular p-gon times the number of faces F. This is: A = ( a 2 ) 2 F p cot ⁡ ( π p ) . {\\displaystyle A={\\biggl (}{\\frac {a}{2}}{\\biggr )}^{2}Fp\\cot"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_10",
    "chunk": "\\left({\\frac {\\pi }{p}}\\right).} The volume is computed as F times the volume of the pyramid whose base is a regular p-gon and whose height is the inradius r. That is, The following table lists the various radii of the Platonic solids together with their surface area and volume. The overall size is fixed by taking the edge length, a, to be equal to 2. φ = 2 cos ⁡ π 5 = 1 + 5 2 , ξ = 2 sin ⁡ π 5 = 5 − 5 2 = 3 − φ . {\\displaystyle \\varphi =2\\cos {\\pi \\over 5}={\\frac {1+{\\sqrt {5}}}{2}},\\qquad \\xi =2\\sin {\\pi \\over 5}={\\sqrt {\\frac {5-{\\sqrt {5}}}{2}}}={\\sqrt {3-\\varphi }}.} Among the Platonic solids, either the dodecahedron or the icosahedron may be seen as the best approximation to the sphere. The icosahedron has the largest number of faces and the largest dihedral angle, it hugs its inscribed sphere the most tightly, and its surface area to volume ratio is closest to that of a sphere of the same size (i.e. either the same surface area or the same volume). The dodecahedron, on the other hand, has the smallest angular defect, the largest vertex solid angle, and it fills"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_11",
    "chunk": "out its circumscribed sphere the most. For an arbitrary point in the space of a Platonic solid with circumradius R, whose distances to the centroid of the Platonic solid and its n vertices are L and di respectively, and S [ n ] ( 2 m ) = 1 n ∑ i = 1 n d i 2 m {\\displaystyle S_{[n]}^{(2m)}={\\frac {1}{n}}\\sum _{i=1}^{n}d_{i}^{2m}} , S [ 4 ] ( 2 ) = S [ 6 ] ( 2 ) = S [ 8 ] ( 2 ) = S [ 12 ] ( 2 ) = S [ 20 ] ( 2 ) = R 2 + L 2 , S [ 4 ] ( 4 ) = S [ 6 ] ( 4 ) = S [ 8 ] ( 4 ) = S [ 12 ] ( 4 ) = S [ 20 ] ( 4 ) = ( R 2 + L 2 ) 2 + 4 3 R 2 L 2 , S [ 6 ] ( 6 ) = S [ 8 ] ( 6 ) = S [ 12 ] ( 6 ) = S [ 20 ] ( 6 ) = ( R"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_12",
    "chunk": "2 + L 2 ) 3 + 4 R 2 L 2 ( R 2 + L 2 ) , S [ 12 ] ( 8 ) = S [ 20 ] ( 8 ) = ( R 2 + L 2 ) 4 + 8 R 2 L 2 ( R 2 + L 2 ) 2 + 16 5 R 4 L 4 , S [ 12 ] ( 10 ) = S [ 20 ] ( 10 ) = ( R 2 + L 2 ) 5 + 40 3 R 2 L 2 ( R 2 + L 2 ) 3 + 16 R 4 L 4 ( R 2 + L 2 ) . {\\displaystyle {\\begin{aligned}S_{}^{(2)}=S_{}^{(2)}=S_{}^{(2)}=S_{}^{(2)}=S_{}^{(2)}&=R^{2}+L^{2},\\\\[4px]S_{}^{(4)}=S_{}^{(4)}=S_{}^{(4)}=S_{}^{(4)}=S_{}^{(4)}&=\\left(R^{2}+L^{2}\\right)^{2}+{\\frac {4}{3}}R^{2}L^{2},\\\\[4px]S_{}^{(6)}=S_{}^{(6)}=S_{}^{(6)}=S_{}^{(6)}&=\\left(R^{2}+L^{2}\\right)^{3}+4R^{2}L^{2}\\left(R^{2}+L^{2}\\right),\\\\[4px]S_{}^{(8)}=S_{}^{(8)}&=\\left(R^{2}+L^{2}\\right)^{4}+8R^{2}L^{2}\\left(R^{2}+L^{2}\\right)^{2}+{\\frac {16}{5}}R^{4}L^{4},\\\\[4px]S_{}^{(10)}=S_{}^{(10)}&=\\left(R^{2}+L^{2}\\right)^{5}+{\\frac {40}{3}}R^{2}L^{2}\\left(R^{2}+L^{2}\\right)^{3}+16R^{4}L^{4}\\left(R^{2}+L^{2}\\right).\\end{aligned}}} For all five Platonic solids, we have S [ n ] ( 4 ) + 16 9 R 4 = ( S [ n ] ( 2 ) + 2 3 R 2 ) 2 . {\\displaystyle S_{[n]}^{(4)}+{\\frac {16}{9}}R^{4}=\\left(S_{[n]}^{(2)}+{\\frac {2}{3}}R^{2}\\right)^{2}.} If di are the distances from the n vertices of the Platonic solid to any point on its circumscribed sphere, then 4 ( ∑ i = 1 n d i 2 ) 2 = 3 n"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_13",
    "chunk": "∑ i = 1 n d i 4 . {\\displaystyle 4\\left(\\sum _{i=1}^{n}d_{i}^{2}\\right)^{2}=3n\\sum _{i=1}^{n}d_{i}^{4}.} A polyhedron P is said to have the Rupert property if a polyhedron of the same or larger size and the same shape as P can pass through a hole in P. All five Platonic solids have this property. Every polyhedron has a dual (or \"polar\") polyhedron with faces and vertices interchanged. The dual of every Platonic solid is another Platonic solid, so that we can arrange the five solids into dual pairs. If a polyhedron has Schläfli symbol {p, q}, then its dual has the symbol {q, p}. Indeed, every combinatorial property of one Platonic solid can be interpreted as another combinatorial property of the dual. One can construct the dual polyhedron by taking the vertices of the dual to be the centers of the faces of the original figure. Connecting the centers of adjacent faces in the original forms the edges of the dual and thereby interchanges the number of faces and vertices while maintaining the number of edges. More generally, one can dualize a Platonic solid with respect to a sphere of radius d concentric with the solid. The radii (R, ρ, r) of"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_14",
    "chunk": "a solid and those of its dual (R*, ρ*, r*) are related by d 2 = R ∗ r = r ∗ R = ρ ∗ ρ . {\\displaystyle d^{2}=R^{\\ast }r=r^{\\ast }R=\\rho ^{\\ast }\\rho .} Dualizing with respect to the midsphere (d = ρ) is often convenient because the midsphere has the same relationship to both polyhedra. Taking d = Rr yields a dual solid with the same circumradius and inradius (i.e. R* = R and r* = r). In mathematics, the concept of symmetry is studied with the notion of a mathematical group. Every polyhedron has an associated symmetry group, which is the set of all transformations (Euclidean isometries) which leave the polyhedron invariant. The order of the symmetry group is the number of symmetries of the polyhedron. One often distinguishes between the full symmetry group, which includes reflections, and the proper symmetry group, which includes only rotations. The symmetry groups of the Platonic solids are a special class of three-dimensional point groups known as polyhedral groups. The high degree of symmetry of the Platonic solids can be interpreted in a number of ways. Most importantly, the vertices of each solid are all equivalent under the action of the"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_15",
    "chunk": "symmetry group, as are the edges and faces. One says the action of the symmetry group is transitive on the vertices, edges, and faces. In fact, this is another way of defining regularity of a polyhedron: a polyhedron is regular if and only if it is vertex-uniform, edge-uniform, and face-uniform. There are only three symmetry groups associated with the Platonic solids rather than five, since the symmetry group of any polyhedron coincides with that of its dual. This is easily seen by examining the construction of the dual polyhedron. Any symmetry of the original must be a symmetry of the dual and vice versa. The three polyhedral groups are: The orders of the proper (rotation) groups are 12, 24, and 60 respectively – precisely twice the number of edges in the respective polyhedra. The orders of the full symmetry groups are twice as much again (24, 48, and 120). See (Coxeter 1973) for a derivation of these facts. All Platonic solids except the tetrahedron are centrally symmetric, meaning they are preserved under reflection through the origin. The following table lists the various symmetry properties of the Platonic solids. The symmetry groups listed are the full groups with the rotation subgroups"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_16",
    "chunk": "given in parentheses (likewise for the number of symmetries). Wythoff's kaleidoscope construction is a method for constructing polyhedra directly from their symmetry groups. They are listed for reference Wythoff's symbol for each of the Platonic solids. The tetrahedron, cube, and octahedron all occur naturally in crystal structures. These by no means exhaust the numbers of possible forms of crystals. However, neither the regular icosahedron nor the regular dodecahedron are amongst them. One of the forms, called the pyritohedron (named for the group of minerals of which it is typical) has twelve pentagonal faces, arranged in the same pattern as the faces of the regular dodecahedron. The faces of the pyritohedron are, however, not regular, so the pyritohedron is also not regular. Allotropes of boron and many boron compounds, such as boron carbide, include discrete B12 icosahedra within their crystal structures. Carborane acids also have molecular structures approximating regular icosahedra. In the early 20th century, Ernst Haeckel described a number of species of Radiolaria, some of whose skeletons are shaped like various regular polyhedra. Examples include Circoporus octahedrus, Circogonia icosahedra, Lithocubus geometricus and Circorrhegma dodecahedra. The shapes of these creatures should be obvious from their names. Many viruses, such as the"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_17",
    "chunk": "herpes virus, have the shape of a regular icosahedron. Viral structures are built of repeated identical protein subunits and the icosahedron is the easiest shape to assemble using these subunits. A regular polyhedron is used because it can be built from a single basic unit protein used over and over again; this saves space in the viral genome. In meteorology and climatology, global numerical models of atmospheric flow are of increasing interest which employ geodesic grids that are based on an icosahedron (refined by triangulation) instead of the more commonly used longitude/latitude grid. This has the advantage of evenly distributed spatial resolution without singularities (i.e. the poles) at the expense of somewhat greater numerical difficulty. Geometry of space frames is often based on platonic solids. In the MERO system, Platonic solids are used for naming convention of various space frame configurations. For example, ⁠1/2⁠O+T refers to a configuration made of one half of octahedron and a tetrahedron. Several Platonic hydrocarbons have been synthesised, including cubane and dodecahedrane and not tetrahedrane. For the intermediate material phase called liquid crystals, the existence of such symmetries was first proposed in 1981 by H. Kleinert and K. Maki. In aluminum the icosahedral structure was"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_18",
    "chunk": "discovered three years after this by Dan Shechtman, which earned him the Nobel Prize in Chemistry in 2011. Platonic solids are often used to make dice, because dice of these shapes can be made fair. 6-sided dice are very common, but the other numbers are commonly used in role-playing games. Such dice are commonly referred to as dn where n is the number of faces (d8, d20, etc.); see dice notation for more details. These shapes frequently show up in other games or puzzles. Puzzles similar to a Rubik's Cube come in all five shapes – see magic polyhedra. Architects liked the idea of Plato's timeless forms that can be seen by the soul in the objects of the material world, but turned these shapes into more suitable for construction sphere, cylinder, cone, and square pyramid. In particular, one of the leaders of neoclassicism, Étienne-Louis Boullée, was preoccupied with the architects' version of \"Platonic solids\". There exist four regular polyhedra that are not convex, called Kepler–Poinsot polyhedra. These all have icosahedral symmetry and may be obtained as stellations of the dodecahedron and the icosahedron. The next most regular convex polyhedra after the Platonic solids are the cuboctahedron, which is a"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_19",
    "chunk": "rectification of the cube and the octahedron, and the icosidodecahedron, which is a rectification of the dodecahedron and the icosahedron (the rectification of the self-dual tetrahedron is a regular octahedron). These are both quasi-regular, meaning that they are vertex- and edge-uniform and have regular faces, but the faces are not all congruent (coming in two different classes). They form two of the thirteen Archimedean solids, which are the convex uniform polyhedra with polyhedral symmetry. Their duals, the rhombic dodecahedron and rhombic triacontahedron, are edge- and face-transitive, but their faces are not regular and their vertices come in two types each; they are two of the thirteen Catalan solids. The uniform polyhedra form a much broader class of polyhedra. These figures are vertex-uniform and have one or more types of regular or star polygons for faces. These include all the polyhedra mentioned above together with an infinite set of prisms, an infinite set of antiprisms, and 53 other non-convex forms. The Johnson solids are convex polyhedra which have regular faces but are not uniform. Among them are five of the eight convex deltahedra, which have identical, regular faces (all equilateral triangles) but are not uniform. (The other three convex deltahedra are"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_20",
    "chunk": "the Platonic tetrahedron, octahedron, and icosahedron.) The three regular tessellations of the plane are closely related to the Platonic solids. Indeed, one can view the Platonic solids as regular tessellations of the sphere. This is done by projecting each solid onto a concentric sphere. The faces project onto regular spherical polygons which exactly cover the sphere. Spherical tilings provide two infinite additional sets of regular tilings, the hosohedra, {2,n} with 2 vertices at the poles, and lune faces, and the dual dihedra, {n,2} with 2 hemispherical faces and regularly spaced vertices on the equator. Such tesselations would be degenerate in true 3D space as polyhedra. Every regular tessellation of the sphere is characterized by a pair of integers {p, q} with ⁠1/p⁠ + ⁠1/q⁠ > ⁠1/2⁠. Likewise, a regular tessellation of the plane is characterized by the condition ⁠1/p⁠ + ⁠1/q⁠ = ⁠1/2⁠. There are three possibilities: In a similar manner, one can consider regular tessellations of the hyperbolic plane. These are characterized by the condition ⁠1/p⁠ + ⁠1/q⁠ < ⁠1/2⁠. There is an infinite family of such tessellations. In more than three dimensions, polyhedra generalize to polytopes, with higher-dimensional convex regular polytopes being the equivalents of the three-dimensional Platonic"
  },
  {
    "source": "Platonic solid.txt",
    "chunk_id": "Platonic solid.txt_21",
    "chunk": "solids. In the mid-19th century the Swiss mathematician Ludwig Schläfli discovered the four-dimensional analogues of the Platonic solids, called convex regular 4-polytopes. There are exactly six of these figures; five are analogous to the Platonic solids : 5-cell as {3,3,3}, 16-cell as {3,3,4}, 600-cell as {3,3,5}, tesseract as {4,3,3}, and 120-cell as {5,3,3}, and a sixth one, the self-dual 24-cell, {3,4,3}. In all dimensions higher than four, there are only three convex regular polytopes: the simplex as {3,3,...,3}, the hypercube as {4,3,...,3}, and the cross-polytope as {3,3,...,4}. In three dimensions, these coincide with the tetrahedron as {3,3}, the cube as {4,3}, and the octahedron as {3,4}."
  },
  {
    "source": "Plutino.txt",
    "chunk_id": "Plutino.txt_0",
    "chunk": "# Plutino In astronomy, the plutinos are a dynamical group of trans-Neptunian objects that orbit in 2:3 mean-motion resonance with Neptune. This means that for every two orbits a plutino makes, Neptune orbits three times. The dwarf planet Pluto is the largest member as well as the namesake of this group. The next largest members are Orcus, (208996) 2003 AZ84, and Ixion. Plutinos are named after mythological creatures associated with the underworld. Plutinos form the inner part of the Kuiper belt and represent about a quarter of the known Kuiper belt objects. They are also the most populous known class of resonant trans-Neptunian objects (also see adjunct box with hierarchical listing). The first plutino after Pluto itself, (385185) 1993 RO, was discovered on September 16, 1993. It is thought that the objects that are currently in mean orbital resonances with Neptune initially followed a variety of independent heliocentric paths. As Neptune migrated outward early in the Solar System's history (see origins of the Kuiper belt), the bodies it approached would have been scattered; during this process, some of them would have been captured into resonances. The 3:2 resonance is a low-order resonance and is thus the strongest and most stable"
  },
  {
    "source": "Plutino.txt",
    "chunk_id": "Plutino.txt_1",
    "chunk": "among all resonances. This is the primary reason it has a larger population than the other Neptunian resonances encountered in the Kuiper Belt. The cloud of low-inclination bodies beyond 40 AU is the cubewano family, while bodies with higher eccentricities (0.05 to 0.34) and semimajor axes close to the 3:2 Neptune resonance are primarily plutinos. While the majority of plutinos have relatively low orbital inclinations, a significant fraction of these objects follow orbits similar to that of Pluto, with inclinations in the 10–25° range and eccentricities around 0.2–0.25; such orbits result in many of these objects having perihelia close to or even inside Neptune's orbit, while simultaneously having aphelia that bring them close to the main Kuiper belt's outer edge (where objects in a 1:2 resonance with Neptune, the Twotinos, are found). The orbital periods of plutinos cluster around 247.3 years (1.5 × Neptune's orbital period), varying by at most a few years from this value. Pluto's influence on the other plutinos has historically been neglected due to its relatively small mass. However, the resonance width (the range of semi-axes compatible with the resonance) is very narrow and only a few times larger than Pluto's Hill sphere (gravitational influence). Consequently,"
  },
  {
    "source": "Plutino.txt",
    "chunk_id": "Plutino.txt_2",
    "chunk": "depending on the original eccentricity, some plutinos will eventually be driven out of the resonance by interactions with Pluto. Numerical simulations suggest that the orbits of plutinos with an eccentricity 10%–30% smaller or larger than that of Pluto are not stable over Ga timescales."
  },
  {
    "source": "Primordial black hole.txt",
    "chunk_id": "Primordial black hole.txt_0",
    "chunk": "# Primordial black hole In cosmology, primordial black holes (PBHs) are hypothetical black holes that formed soon after the Big Bang. In the inflationary era and early radiation-dominated universe, extremely dense pockets of subatomic matter may have been tightly packed to the point of gravitational collapse, creating primordial black holes without the supernova compression typically needed to make black holes today. Because the creation of primordial black holes would pre-date the first stars, they are not limited to the narrow mass range of stellar black holes. In 1966, Yakov Zeldovich and Igor Novikov first proposed the existence of such black holes, while the first in-depth study was conducted by Stephen Hawking in 1971. However, their existence remains hypothetical. In September 2022, primordial black holes were proposed by some researchers to explain the unexpected very large early galaxies discovered by the James Webb Space Telescope (JWST). PBHs have long been considered possibly important if not nearly exclusive components of dark matter, the latter perspective having been strengthened by both LIGO/Virgo interferometer gravitational wave and JWST observations. Early constraints on PBHs as dark matter usually assumed most black holes would have similar or identical (\"monochromatic\") mass, which was disproven by LIGO/Virgo results,"
  },
  {
    "source": "Primordial black hole.txt",
    "chunk_id": "Primordial black hole.txt_1",
    "chunk": "and further suggestions that the actual black hole mass distribution is broadly platykurtic were evident from JWST observations of early large galaxies. Recent analyses agree, suggesting a broad mass distribution with a mode around one solar mass. Many PBHs may have the mass of an asteroid but the size of a hydrogen atom and be travelling at enormous speeds, with one likely being within the Solar System at any given time. Most likely, such PBHs would pass right through a star \"like a bullet\", without any significant effects on the star. However, the ones traveling slowly would have a chance of being captured by the star. Stephen Hawking proposed that our Sun may harbor such a PBH. Depending on the model, primordial black holes could have initial masses ranging from 10 kg (the so-called Planck relics) to more than thousands of solar masses. However, primordial black holes originally having masses lower than 10 kg would not have survived to the present due to Hawking radiation, which causes complete evaporation in a time much shorter than the age of the Universe. Primordial black holes are non-baryonic, and as such are plausible dark matter candidates. Primordial black holes are also good candidates"
  },
  {
    "source": "Primordial black hole.txt",
    "chunk_id": "Primordial black hole.txt_2",
    "chunk": "for being the seeds of the supermassive black holes at the center of massive galaxies, as well as of intermediate-mass black holes. Primordial black holes belong to the class of massive compact halo objects (MACHOs). They are naturally a good dark matter candidate: they are (nearly) collision-less and stable (if sufficiently massive), they have non-relativistic velocities, and they form very early in the history of the Universe (typically less than one second after the Big Bang). Nevertheless, critics maintain that tight limits on their abundance have been set up from various astrophysical and cosmological observations, which would exclude that they contribute significantly to dark matter over most of the plausible mass range. However, new research has provided for the possibility again, whereby these black holes would sit in clusters with a 30-solar-mass primordial black hole at the center. In March 2016, one month after the announcement of the detection by Advanced LIGO/VIRGO of gravitational waves emitted by the merging of two 30 solar mass black holes (about 6×10 kg), three groups of researchers proposed independently that the detected black holes had a primordial origin. Two of the groups found that the merging rates inferred by LIGO are consistent with a"
  },
  {
    "source": "Primordial black hole.txt",
    "chunk_id": "Primordial black hole.txt_3",
    "chunk": "scenario in which all the dark matter is made of primordial black holes, if a non-negligible fraction of them are somehow clustered within halos such as faint dwarf galaxies or globular clusters, as expected by the standard theory of cosmic structure formation. The third group claimed that these merging rates are incompatible with an all-dark-matter scenario and that primordial black holes could only contribute to less than one percent of the total dark matter. The unexpected large mass of the black holes detected by LIGO has strongly revived interest in primordial black holes with masses in the range of 1 to 100 solar masses. It is still debated whether this range is excluded or not by other observations, such as the absence of micro-lensing of stars, the cosmic microwave background anisotropies, the size of faint dwarf galaxies, and the absence of correlation between X-ray and radio sources toward the galactic center. In May 2016, Alexander Kashlinsky suggested that the observed spatial correlations in the unresolved gamma-ray and X-ray background radiations could be due to primordial black holes with similar masses, if their abundance is comparable to that of dark matter. In August 2019, a study was published opening up the"
  },
  {
    "source": "Primordial black hole.txt",
    "chunk_id": "Primordial black hole.txt_4",
    "chunk": "possibility of making up all dark matter with asteroid-mass primordial black holes (3.5 × 10 – 4 × 10 solar masses, or 7 × 10 – 8 × 10 kg). In September 2019, a report by James Unwin and Jakub Scholtz proposed the possibility of a primordial black hole (PBH) with mass 5–15 ME (Earth masses), about the diameter of a tennis ball, existing in the extended Kuiper Belt to explain the orbital anomalies that are theorized to be the result of a 9th planet in the solar system. In October 2019, Derek Inman and Yacine Ali-Haïmoud published an article in which they discovered that the nonlinear velocities that arise from the structure formation are too small to significantly affect the constraints that arise from CMB anisotropies In September 2021, the NANOGrav collaboration announced that they had found a low-frequency signal that could be attributed to gravitational waves and potentially could be associated with PBHs. In September 2022, primordial black holes were used to explain the unexpected very large early (high redshift) galaxies discovered by the James Webb Space Telescope. On 26 November 2023, evidence, for the first time, of an overmassive black hole galaxy (O.B.G.), the result of \"heavy"
  },
  {
    "source": "Primordial black hole.txt",
    "chunk_id": "Primordial black hole.txt_5",
    "chunk": "black hole seed formation from direct collapse\", an alternative way of producing a black hole other than the collapse of a dead star, was reported. This discovery was found in studies of UHZ1, a very early galaxy containing a quasar, by the Chandra X-ray Observatory and James Webb Space Telescope. In 2024, a review by Bernard Carr and colleagues concluded that PBHs formed in the quantum chromodynamics (QCD) epoch prior to 10 seconds after the Big Bang, resulting in a broadly platykurtic mass distribution today, \"with a number of distinct bumps, the most prominent one being at around one solar mass.\" Primordial black holes could have formed in the very early Universe (less than one second after the Big Bang) during the inflationary era, or in the very early radiation-dominated era. The essential ingredient for the formation of a primordial black hole is a fluctuation in the density of the Universe, inducing its gravitational collapse. One typically requires density contrasts δ ρ / ρ ∼ 0.1 {\\displaystyle \\delta \\rho /\\rho \\sim 0.1} (where ρ {\\displaystyle \\rho } is the density of the Universe) to form a black hole. There are several mechanisms able to produce such inhomogeneities in the context"
  },
  {
    "source": "Primordial black hole.txt",
    "chunk_id": "Primordial black hole.txt_6",
    "chunk": "of cosmic inflation (in hybrid inflation models). Some examples include: Axion inflation is a theoretical model in which the axion acts as an inflaton field. Because of the time period it is created at, the field is oscillating at its minimal potential energy. These oscillations are responsible for the energy density fluctuations in the early universe. For a review of production mechanisms arising from various models of inflation, see Ref. Reheating is the transitory process between the inflationary and the hot, dense, radiation-dominated period. During this time the inflaton field decays into other particles. These particles begin to interact in order to reach thermal equilibrium. However, if this process is incomplete it creates density fluctuations, and if these are big enough they could be responsible for the formation of PBH. Cosmological phase transitions may cause inhomogeneities in different ways depending on the specific details of each transition. For example, one mechanism is concerned with the collapse of overdense regions that arise from these phase transitions, while another mechanism involves highly energetic particles that are produced in these phase transitions and then go through gravitational collapse forming PBHs. The dark matter problem, proposed in 1933 by Swiss-American astronomer Fritz Zwicky, refers"
  },
  {
    "source": "Primordial black hole.txt",
    "chunk_id": "Primordial black hole.txt_7",
    "chunk": "to the fact that scientists still do not know what form dark matter takes. PBH can solve that in a few ways. First, if PBHs accounted for all or a significant amount of the dark matter in the universe, this could explain the gravitational effects seen in galaxies and galactic clusters. Secondly, PBHs have different proposed production mechanisms. Unlike WIMPs, they can emit gravitational waves that interact with regular matter. Finally, the discovery of PBHs could explain some of the observed gravitational lensing effects that couldn't arise from ordinary matter. While evidence that primordial black holes may constitute dark matter is inconclusive as of 2023, researchers such as Bernard Carr and others are strong proponents. Since primordial black holes do not necessarily have to be small (they can have any size), they may have contributed to formation of galaxies, such as those earlier than expected. The cosmological domain wall problem, proposed in 1974 by Soviet physicist Yakov Zeldovich, discussed the formation of domain walls during phase transitions of the early universe and what could arise from their large energy densities. PBHs could serve as a solution to this problem in various ways. One explanation could be that PBHs can prevent"
  },
  {
    "source": "Primordial black hole.txt",
    "chunk_id": "Primordial black hole.txt_8",
    "chunk": "the formation of domain walls due to them exerting gravitational forces on the surrounding matter making it clump and theoretically preventing the formation of said walls. Another explanation could be that PBHs could decay domain walls; if these were formed in the early universe before PBHs then due to gravitational interactions these could eventually collapse into PBHs. Finally, a third explanation could be that PBHs do not violate the observational constraints; if PBHs in the 10–10 kg mass range were to be detected then these would have the right density to make up all dark matter in the universe without violating constraints, thus the domain wall problem wouldn't arise. The Cosmological monopole problem, also proposed by Yakov Zeldovich in the late 1970s, consisted of the absence of magnetic monopoles nowadays. PBHs can also serve as a solution to this problem. To start, if magnetic monopoles did exist in the early universe these could have gravitationally interacted with PBHs and been absorbed thus explaining their absence. Another explanation due to PBHs could be that PBHs would have exerted gravitational forces on matter causing it to clump and dilute the density of magnetic monopoles. General relativity predicts the smallest primordial black holes"
  },
  {
    "source": "Primordial black hole.txt",
    "chunk_id": "Primordial black hole.txt_9",
    "chunk": "would have evaporated by now, but if there were a fourth spatial dimension – as predicted by string theory – it would affect how gravity acts on small scales and \"slow down the evaporation quite substantially\". In essence, the energy stored in the fourth spatial dimension as a stationary wave would bestow a significant rest mass to the object when regarded in the conventional four-dimensional space-time. This could mean there are several thousand primordial black holes in our galaxy. To test this theory, scientists will use the Fermi Gamma-ray Space Telescope, which was put into orbit by NASA on June 11, 2008. If they observe specific small interference patterns within gamma-ray bursts, it could be the first indirect evidence for primordial black holes and string theory. A variety of observations have been interpreted to place limits on the abundance and mass of primordial black holes: Lifetime, Hawking radiation and gamma-rays: One way to detect primordial black holes, or to constrain their mass and abundance, is by their Hawking radiation. Stephen Hawking theorized in 1974 that large numbers of such smaller primordial black holes might exist in the Milky Way in our galaxy's halo region. All black holes are theorized to"
  },
  {
    "source": "Primordial black hole.txt",
    "chunk_id": "Primordial black hole.txt_10",
    "chunk": "emit Hawking radiation at a rate inversely proportional to their mass. Since this emission further decreases their mass, black holes with very small mass would experience runaway evaporation, creating a burst of radiation at the final phase, equivalent to a hydrogen bomb yielding millions of megatons of explosive force. A regular black hole (of about 3 solar masses) cannot lose all of its mass within the current age of the universe (they would take about 10 years to do so, even without any matter falling in). However, since primordial black holes are not formed by stellar core collapse, they may be of any size. A black hole with a mass of about 10 kg would have a lifetime about equal to the age of the universe. If such low-mass black holes were created in sufficient number in the Big Bang, we should be able to observe explosions by some of those that are relatively nearby in our own Milky Way galaxy. NASA's Fermi Gamma-ray Space Telescope satellite, launched in June 2008, was designed in part to search for such evaporating primordial black holes. Fermi data set up the limit that less than one percent of dark matter could be made"
  },
  {
    "source": "Primordial black hole.txt",
    "chunk_id": "Primordial black hole.txt_11",
    "chunk": "of primordial black holes with masses up to 10 kg. Evaporating primordial black holes would have also had an impact on the Big Bang nucleosynthesis and change the abundances of light elements in the Universe. However, if theoretical Hawking radiation does not actually exist, such primordial black holes would be extremely difficult, if not impossible, to detect in space due to their small size and lack of large gravitational influence. Temperature anisotropies in the cosmic microwave background: Accretion of matter onto primordial black holes in the early Universe should lead to energy injection in the medium that affects the recombination history of the Universe. This effect induces signatures in the statistical distribution of the cosmic microwave background (CMB) anisotropies. The Planck observations of the CMB exclude that primordial black holes with masses in the range 100–10 solar masses contribute importantly to the dark matter, at least in the simplest conservative model. It is still debated whether the constraints are stronger or weaker in more realistic or complex scenarios. Gamma-ray signatures from annihilating dark matter: If the dark matter in the Universe is in the form of weakly interacting massive particles or WIMPs, primordial black holes would accrete a halo of"
  },
  {
    "source": "Primordial black hole.txt",
    "chunk_id": "Primordial black hole.txt_12",
    "chunk": "WIMPs around them in the early universe. The annihilation of WIMPs in the halo leads to a signal in the gamma-ray spectrum which is potentially detectable by dedicated instruments such as the Fermi Gamma-ray Space Telescope. None of these facilities are focused on the direct detection of PBH due to them being a theoretical phenomenon, but the information collected in each respective experiment provides secondary data which can help provide insight and constraints on the nature of PBHs. A direct collapse black hole is the result of the collapse of unusually dense and large regions of gas, after the radiation-dominated era, while primordial black holes would have resulted from the direct collapse of energy, ionized matter, or both, during the inflationary or radiation-dominated eras."
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_0",
    "chunk": "# Proto-Indo-European language Proto-Indo-European (PIE) is the reconstructed common ancestor of the Indo-European language family. No direct record of Proto-Indo-European exists; its proposed features have been derived by linguistic reconstruction from documented Indo-European languages. Far more work has gone into reconstructing PIE than any other proto-language, and it is the best understood of all proto-languages of its age. The majority of linguistic work during the 19th century was devoted to the reconstruction of PIE and its daughter languages, and many of the modern techniques of linguistic reconstruction (such as the comparative method) were developed as a result. PIE is hypothesized to have been spoken as a single language from approximately 4500 BCE to 2500 BCE during the Late Neolithic to Early Bronze Age, though estimates vary by more than a thousand years. According to the prevailing Kurgan hypothesis, the original homeland of the Proto-Indo-Europeans may have been in the Pontic–Caspian steppe of eastern Europe. The linguistic reconstruction of PIE has provided insight into the pastoral culture and patriarchal religion of its speakers. As speakers of Proto-Indo-European became isolated from each other through the Indo-European migrations, the regional dialects of Proto-Indo-European spoken by the various groups diverged, as each dialect underwent"
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_1",
    "chunk": "shifts in pronunciation (the Indo-European sound laws), morphology, and vocabulary. Over many centuries, these dialects transformed into the known ancient Indo-European languages. From there, further linguistic divergence led to the evolution of their current descendants, the modern Indo-European languages. PIE is believed to have had an elaborate system of morphology that included inflectional suffixes (analogous to English child, child's, children, children's) as well as ablaut (vowel alterations, as preserved in English sing, sang, sung, song) and accent. PIE nominals and pronouns had a complex system of declension, and verbs similarly had a complex system of conjugation. The PIE phonology, particles, numerals, and copula are also well-reconstructed. Asterisks are used by linguists as a conventional mark of reconstructed words, such as *wódr̥, *ḱwn̥tós, or *tréyes; these forms are the reconstructed ancestors of the modern English words water, hound, and three, respectively. No direct evidence of PIE exists; scholars have reconstructed PIE from its present-day descendants using the comparative method. For example, compare the pairs of words in Italian and English: piede and foot, padre and father, pesce and fish. Since there is a consistent correspondence of the initial consonants (p and f) that emerges far too frequently to be coincidental, one"
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_2",
    "chunk": "can infer that these languages stem from a common parent language. Detailed analysis suggests a system of sound laws to describe the phonetic and phonological changes from the hypothetical ancestral words to the modern ones. These laws have become so detailed and reliable as to support the Neogrammarian hypothesis: the Indo-European sound laws apply without exception. William Jones, an Anglo-Welsh philologist and puisne judge in Bengal, caused an academic sensation when in 1786 he postulated the common ancestry of Sanskrit, Greek, Latin, Gothic, the Celtic languages, and Old Persian, but he was not the first to state such a hypothesis. In the 16th century, European visitors to the Indian subcontinent became aware of similarities between Indo-Iranian languages and European languages, and as early as 1653, Marcus Zuerius van Boxhorn had published a proposal for a proto-language (\"Scythian\") for the following language families: Germanic, Romance, Greek, Baltic, Slavic, Celtic, and Iranian. In a memoir sent to the Académie des Inscriptions et Belles-Lettres in 1767, Gaston-Laurent Coeurdoux, a French Jesuit who spent most of his life in India, had specifically demonstrated the analogy between Sanskrit and European languages. According to current academic consensus, Jones's famous work of 1786 was less accurate than"
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_3",
    "chunk": "his predecessors', as he erroneously included Egyptian, Japanese and Chinese in the Indo-European languages, while omitting Hindi. In 1818, Danish linguist Rasmus Christian Rask elaborated the set of correspondences in his prize essay Undersøgelse om det gamle Nordiske eller Islandske Sprogs Oprindelse ('Investigation of the Origin of the Old Norse or Icelandic Language'), where he argued that Old Norse was related to the Germanic languages, and had even suggested a relation to the Baltic, Slavic, Greek, Latin and Romance languages. In 1816, Franz Bopp published On the System of Conjugation in Sanskrit, in which he investigated the common origin of Sanskrit, Persian, Greek, Latin, and German. In 1833, he began publishing the Comparative Grammar of Sanskrit, Zend, Greek, Latin, Lithuanian, Old Slavic, Gothic, and German. In 1822, Jacob Grimm formulated what became known as Grimm's law as a general rule in his Deutsche Grammatik. Grimm showed correlations between the Germanic and other Indo-European languages and demonstrated that sound change systematically transforms all words of a language. From the 1870s, the Neogrammarians proposed that sound laws have no exceptions, as illustrated by Verner's law, published in 1876, which resolved apparent exceptions to Grimm's law by exploring the role of accent (stress)"
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_4",
    "chunk": "in language change. August Schleicher's A Compendium of the Comparative Grammar of the Indo-European, Sanskrit, Greek and Latin Languages (1874–77) represented an early attempt to reconstruct the Proto-Indo-European language. By the early 1900s, Indo-Europeanists had developed well-defined descriptions of PIE which scholars still accept today. Later, the discovery of the Anatolian and Tocharian languages added to the corpus of descendant languages. A subtle new principle won wide acceptance: the laryngeal theory, which explained irregularities in the reconstruction of Proto-Indo-European phonology as the effects of hypothetical sounds which no longer exist in all languages documented prior to the excavation of cuneiform tablets in Anatolian. This theory was first proposed by Ferdinand de Saussure in 1879 on the basis of internal reconstruction only, and progressively won general acceptance after Jerzy Kuryłowicz's discovery of consonantal reflexes of these reconstructed sounds in Hittite. Julius Pokorny's Indogermanisches etymologisches Wörterbuch ('Indo-European Etymological Dictionary', 1959) gave a detailed, though conservative, overview of the lexical knowledge accumulated by 1959. Jerzy Kuryłowicz's 1956 Apophonie gave a better understanding of Indo-European ablaut. From the 1960s, knowledge of Anatolian became robust enough to establish its relationship to PIE. In The Oxford Introduction to Proto-Indo-European and the Proto-Indo-European World, Mallory and Adams"
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_5",
    "chunk": "illustrate the resemblance with the following examples of cognate forms (with the addition of Old English for further comparison): Scholars have proposed multiple hypotheses about when, where, and by whom PIE was spoken. The Kurgan hypothesis, first put forward in 1956 by Marija Gimbutas, has become the most popular. It proposes that the original speakers of PIE were the Yamnaya culture associated with the kurgans (burial mounds) on the Pontic–Caspian steppe north of the Black Sea. According to the theory, they were nomadic pastoralists who domesticated the horse, which allowed them to migrate across Europe and Asia in wagons and chariots. By the early 3rd millennium BCE, they had expanded throughout the Pontic–Caspian steppe and into eastern Europe. Other theories include the Anatolian hypothesis, which posits that PIE spread out from Anatolia with agriculture beginning c. 7500–6000 BCE, the Armenian hypothesis, the Paleolithic continuity paradigm, and the indigenous Aryans theory. The last two of these theories are not regarded as credible within academia. Out of all the theories for a PIE homeland, the Kurgan and Anatolian hypotheses are the ones most widely accepted, and also the ones most debated against each other. Following the publication of several studies on ancient"
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_6",
    "chunk": "DNA in 2015, Colin Renfrew, the original author and proponent of the Anatolian hypothesis, has accepted the reality of migrations of populations speaking one or several Indo-European languages from the Pontic steppe towards Northwestern Europe. The antiquity of the earliest attestation (in units of 500 years) of each Indo-European group is: 2000–1500 BCE for Anatolian; 1500–1000 BCE for Indo-Aryan and Greek; 1000–500 BCE for Iranian, Celtic, Italic, Phrygian, Illyrian, Messapic, South Picene, and Venetic; 500–1 BCE for Thracian and Ancient Macedonian; 1–500 CE for Germanic, Armenian, Lusitanian, and Tocharian; 500–1000 CE for Slavic; 1500–2000 CE for Albanian and Baltic. The table lists the main Indo-European language families, comprising the languages descended from Proto-Indo-European. Commonly proposed subgroups of Indo-European languages include Italo-Celtic, Graeco-Aryan, Graeco-Armenian, Graeco-Phrygian, Daco-Thracian, and Thraco-Illyrian. There are numerous lexical similarities between the Proto-Indo-European and Proto-Kartvelian languages due to early language contact, as well as some morphological similarities—notably the Indo-European ablaut, which is remarkably similar to the root ablaut system reconstructible for Proto-Kartvelian. The Lusitanian language was a marginally attested language spoken in areas near the border between present-day Portugal and Spain. The Venetic and Liburnian languages known from the North Adriatic region are sometimes classified as Italic. Albanian"
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_7",
    "chunk": "and Greek are the only surviving Indo-European descendants of a Paleo-Balkan language area, named for their occurrence in or in the vicinity of the Balkan peninsula. Most of the other languages of this area—including Illyrian, Thracian, and Dacian—do not appear to be members of any other subfamilies of PIE, but are so poorly attested that proper classification of them is not possible. Forming an exception, Phrygian is sufficiently well-attested to allow proposals of a particularly close affiliation with Greek, and a Graeco-Phrygian branch of Indo-European is becoming increasingly accepted. Proto-Indo-European phonology has been reconstructed in some detail. Notable features of the most widely accepted (but not uncontroversial) reconstruction include: All sonorants (i.e. nasals, liquids and semivowels) can appear in syllabic position. The syllabic allophones of *y and *w are realized as the surface vowels *i and *u respectively. The Proto-Indo-European accent is reconstructed today as having had variable lexical stress, which could appear on any syllable and whose position often varied among different members of a paradigm (e.g. between singular and plural of a verbal paradigm). Stressed syllables received a higher pitch and it is often said that PIE had a pitch accent. The location of the stress is associated"
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_8",
    "chunk": "with ablaut variations, especially between full-grade vowels (/e/ and /o/) and zero-grade (i.e. lack of a vowel), but not entirely predictable from it. The accent is best preserved in Vedic Sanskrit and (in the case of nouns) Ancient Greek, and indirectly attested in a number of phenomena in other IE languages, such as Verner's Law in the Germanic branch. Sources for Indo-European accentuation are also the Balto-Slavic accentual system and plene spelling in Hittite cuneiform. To account for mismatches between the accent of Vedic Sanskrit and Ancient Greek, as well as a few other phenomena, a few historical linguists prefer to reconstruct PIE as a tone language where each morpheme had an inherent tone; the sequence of tones in a word then evolved, according to that hypothesis, into the placement of lexical stress in different ways in different IE branches. Proto-Indo-European, like its earliest attested descendants, was a highly inflected, fusional language. Suffixation and ablaut were the main methods of marking inflection, both for nominals and verbs. The subject of a sentence was in the nominative case and agreed in number and person with the verb, which was additionally marked for voice, tense, aspect, and mood. Proto-Indo-European nominals and verbs"
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_9",
    "chunk": "were primarily composed of roots – affix-lacking morphemes that carried the core lexical meaning of a word. They were used to derive related words (cf. the English root \"-friend-\", from which are derived related words such as friendship, friendly, befriend, and newly coined words such as unfriend). As a rule, roots were monosyllabic, and had the structure (s)(C)CVC(C), where the symbols C stand for consonants, V stands for a variable vowel, and optional components are in parentheses. All roots ended in a consonant and, although less certain, they appear to have started with a consonant as well. A root plus a suffix formed a word stem, and a word stem plus an inflectional ending formed a word. Proto-Indo-European was a fusional language, in which inflectional morphemes signaled the grammatical relationships between words. This dependence on inflectional morphemes means that roots in PIE, unlike those in English, were rarely used without affixes. Many morphemes in Proto-Indo-European had short e as their inherent vowel; the Indo-European ablaut is the change of this short e to short o, long e (ē), long o (ō), or no vowel. The forms are referred to as the \"ablaut grades\" of the morpheme—the e-grade, o-grade, zero-grade (no"
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_10",
    "chunk": "vowel), etc. This variation in vowels occurred both within inflectional morphology (e.g., different grammatical forms of a noun or verb may have different vowels) and derivational morphology (e.g., a verb and an associated abstract verbal noun may have different vowels). Categories that PIE distinguished through ablaut were often also identifiable by contrasting endings, but the loss of these endings in some later Indo-European languages has led them to use ablaut alone to identify grammatical categories, as in the Modern English words sing, sang, sung. This system is probably derived from an older two-gender system, attested in Anatolian languages: common (or animate) and neuter (or inanimate) gender. The feminine gender only arose in the later period of the language. Neuter nouns collapsed the nominative, vocative and accusative into a single form, the plural of which used a special collective suffix *-h2 (manifested in most descendants as -a). This same collective suffix in extended forms *-eh2 and *-ih2 (respectively on thematic and athematic nouns, becoming -ā and -ī in the early daughter languages) became used to form feminine nouns from masculines. These numbers were also distinguished in verbs (see below), requiring agreement with their subject nominal. Proto-Indo-European pronouns are difficult to reconstruct,"
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_11",
    "chunk": "owing to their variety in later languages. PIE had personal pronouns in the first and second grammatical person, but not the third person, where demonstrative pronouns were used instead. The personal pronouns had their own unique forms and endings, and some had two distinct stems; this is most obvious in the first person singular where the two stems are still preserved in English I and me. There were also two varieties for the accusative, genitive and dative cases, a stressed and an enclitic form. The most basic categorisation for the reconstructed Indo-European verb is grammatical aspect. Verbs are classed as: Verbs were probably marked by a highly developed system of participles, one for each combination of tense and voice, and an assorted array of verbal nouns and adjectival formations. The following table shows a possible reconstruction of the PIE verb endings from Sihler, which largely represents the current consensus among Indo-Europeanists. Rather than specifically 100, *ḱm̥tóm may originally have meant \"a large number\". Proto-Indo-European particles were probably used both as adverbs and as postpositions. These postpositions became prepositions in most daughter languages. Reconstructed particles include for example, *upo \"under, below\"; the negators *ne, *mē; the conjunctions *kʷe \"and\", *wē \"or\""
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_12",
    "chunk": "and others; and an interjection, *wai!, expressing woe or agony. Proto-Indo-European employed various means of deriving words from other words, or directly from verb roots. Internal derivation was a process that derived new words through changes in accent and ablaut alone. It was not as productive as external (affixing) derivation, but is firmly established by the evidence of various later languages. Possessive or associated adjectives were probably created from nouns through internal derivation. Such words could be used directly as adjectives, or they could be turned back into a noun without any change in morphology, indicating someone or something characterised by the adjective. They were probably also used as the second elements in compounds. If the first element was a noun, this created an adjective that resembled a present participle in meaning, e.g. \"having much rice\" or \"cutting trees\". When turned back into nouns, such compounds were Bahuvrihis or semantically resembled agent nouns. In thematic stems, creating a possessive adjective seems to have involved shifting the accent one syllable to the right, for example: In athematic stems, there was a change in the accent/ablaut class. The reconstructed four classes followed an ordering in which a derivation would shift the class"
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_13",
    "chunk": "one to the right: The reason for this particular ordering of the classes in derivation is not known. Some examples: A vrddhi derivation, named after the Sanskrit grammatical term, signifying \"of, belonging to, descended from\". It was characterised by \"upgrading\" the root grade, from zero to full (e) or from full to lengthened (ē). When upgrading from zero to full grade, the vowel could sometimes be inserted in the \"wrong\" place, creating a different stem from the original full grade. Adjectives with accent on the thematic vowel could be turned into nouns by moving the accent back onto the root. A zero grade root could remain so, or be \"upgraded\" to full grade like in a vrddhi derivative. Some examples: This kind of derivation is likely related to the possessive adjectives, and can be seen as essentially the reverse of it. The syntax of the older Indo-European languages has been studied in earnest since at least the late nineteenth century, by such scholars as Hermann Hirt and Berthold Delbrück. In the second half of the twentieth century, interest in the topic increased and led to reconstructions of Proto-Indo-European syntax. Since all the early attested IE languages were inflectional, PIE is"
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_14",
    "chunk": "thought to have relied primarily on morphological markers, rather than word order, to signal syntactic relationships within sentences. Still, a default (unmarked) word order is thought to have existed in PIE. In 1892, Jacob Wackernagel reconstructed PIE's word order as subject–verb–object (SVO), based on evidence in Vedic Sanskrit. Winfred P. Lehmann (1974), on the other hand, reconstructs PIE as a subject–object–verb (SOV) language. He posits that the presence of person marking in PIE verbs motivated a shift from OV to VO order in later dialects. Many of the descendant languages have VO order: modern Greek, Romance and Albanian prefer SVO, Insular Celtic has VSO as the default order, and even the Anatolian languages show some signs of this word order shift. Tocharian and Indo-Iranian, meanwhile, retained the conservative OV order. Lehmann attributes the context-dependent order preferences in Baltic, Slavic and Germanic to outside influences. Donald Ringe (2006), however, attributes these to internal developments instead. Paul Friedrich (1975) disagrees with Lehmann's analysis. He reconstructs PIE with the following syntax: Friedrich notes that even among those Indo-European languages with basic OV word order, none of them are rigidly OV. He also notes that these non-rigid OV languages mainly occur in parts of"
  },
  {
    "source": "Proto-Indo-European language.txt",
    "chunk_id": "Proto-Indo-European language.txt_15",
    "chunk": "the IE area that overlap with OV languages from other families (such as Uralic and Dravidian), whereas VO is predominant in the central parts of the IE area. For these reasons, among others, he argues for a VO common ancestor. Hans Henrich Hock (2015) reports that the SVO hypothesis still has some adherents, but the \"broad consensus\" among PIE scholars is that PIE would have been an SOV language. The SOV default word order with other orders used to express emphasis (e.g., verb–subject–object to emphasise the verb) is attested in Old Indo-Aryan, Old Iranian, Old Latin and Hittite, while traces of it can be found in the enclitic personal pronouns of the Tocharian languages."
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_0",
    "chunk": "# Protocell A protocell (or protobiont) is a self-organized, endogenously ordered, spherical collection of lipids proposed as a rudimentary precursor to cells during the origin of life. A central question in evolution is how simple protocells first arose and how their progeny could diversify, thus enabling the accumulation of novel biological emergences over time (i.e. biological evolution). Although a functional protocell has not yet been achieved in a laboratory setting, the goal to understand the process appears well within reach. A protocell is a pre-cell in abiogenesis, and was a contained system consisting of simple biologically relevant molecules like ribozymes, and encapsulated in a simple membrane structure – isolating the entity from the environment and other individuals – thought to consist of simple fatty acids, mineral structures, or rock-pore structures. Compartmentalization was important in the origin of life. Membranes form enclosed compartments that are separate from the external environment, thus providing the cell with functionally specialized aqueous spaces. As the lipid bilayer of membranes is impermeable to most hydrophilic molecules (dissolved by water), modern cells have membrane transport-systems that achieve nutrient uptake as well as the export of waste. Prior to the development of these molecular assemblies, protocells likely employed"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_1",
    "chunk": "vesicle dynamics that are relevant to cellular functions, such as membrane trafficking and self-reproduction, using amphiphilic molecules. On the primitive Earth, numerous chemical reactions of organic compounds produced the ingredients of life. Of these substances, amphiphilic molecules might be the first player in the evolution from molecular assembly to cellular life. Vesicle dynamics could progress towards protocells with the development of self-replication coupled with early metabolism. It is possible that protocells might have had a primitive metabolic system (Wood-Ljungdahl pathway) at alkaline hydrothermal vents or other geological environments like impact crater lakes from meteorites, which are known to be composed of elements found in the Wood-Ljungdahl pathway. Another conceptual model of a protocell relates to the term \"chemoton\" (short for 'chemical automaton') which refers to the fundamental unit of life introduced by Hungarian theoretical biologist Tibor Gánti. It is the oldest known computational abstract of a protocell. Gánti conceived the basic idea in 1952 and formulated the concept in 1971 in his book The Principles of Life (originally written in Hungarian, and translated to English only in 2003). He surmised the chemoton as the original ancestor of all organisms, or the last universal common ancestor. The basic assumption of the"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_2",
    "chunk": "chemoton model is that life should fundamentally and essentially have three properties: metabolism, self-replication, and a bilipid membrane. The metabolic and replication functions together form an autocatalytic subsystem necessary for the basic functions of life, and a membrane encloses this subsystem to separate it from the surrounding environment. Therefore, any system having such properties may be regarded as alive, and will contain self sustaining cellular information that is subject to natural selection. Some consider this model a significant contribution to origin of life as it provides a philosophy of evolutionary units. Self-assembled vesicles are essential components of primitive cells. The second law of thermodynamics requires that the universe becomes increasingly disordered (entropy), yet life is distinguished by its great degree of organization. Therefore, a boundary is needed to separate life processes from non-living matter. This fundamental necessity is underpinned by the universality of the cell membrane which is the only cellular structure found in all organisms on Earth. In the aqueous environment in which all known cells function, a non-aqueous barrier is required to surround a cell and separate it from its surroundings. This non-aqueous membrane establishes a barrier to free diffusion, allowing for regulation of the internal environment within"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_3",
    "chunk": "the barrier. The necessity of thermodynamically isolating a subsystem is an irreducible condition of life. In modern biology, such isolation is ordinarily accomplished by amphiphilic bilayers of a thickness of around 10 meters. Researchers including Irene A. Chen and Jack W. Szostak have demonstrated that simple physicochemical properties of elementary protocells can give rise to simpler conceptual analogues of essential cellular behaviors, including primitive forms of Darwinian competition and energy storage. Such cooperative interactions between the membrane and encapsulated contents could greatly simplify the transition from replicating molecules to true cells. Competition for membrane molecules would favor stabilized membranes, suggesting a selective advantage for the evolution of cross-linked fatty acids and even the phospholipids of today. This micro-encapsulation allowed for metabolism within the membrane, exchange of small molecules and prevention of passage of large substances across it. The main advantages of encapsulation include increased solubility of the cargo and creating energy in the form of chemical gradients. Energy is thus often said to be stored by cells in molecular structures such as carbohydrates (including sugars), lipids, and proteins, which release energy when chemically combined with oxygen during cellular respiration. When phospholipids or simple lipids like fatty acids are placed in"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_4",
    "chunk": "water, the molecules spontaneously arrange such that the hydrophobic tails are shielded from the water, resulting in the formation of membrane structures such as bilayers, vesicles, and micelles. In modern cells, vesicles are involved in metabolism, transport, buoyancy control, and enzyme storage. They can also act as natural chemical reaction chambers. A typical vesicle or micelle in aqueous solution forms an aggregate with the hydrophilic \"head\" regions in contact with surrounding solvent, sequestering the hydrophobic single-tail regions in the micelle center. This phase is caused by the packing behavior of single-tail lipids in a bilayer. Although the spontaneous self-assembly process that form lipid monolayer vesicles and micelles in nature resemble the kinds of primordial vesicles or protocells that might have existed at the beginning of evolution, they are not as sophisticated as the bilayer membranes of today's living organisms. However, in a prebiotic context, electrostatic interactions induced by short, positively charged, hydrophobic peptides containing seven amino acids in length or fewer, can attach RNA to a vesicle membrane, the basic cell membrane. Rather than being made up of phospholipids, early membranes may have formed from monolayers or bilayers of simple fatty acids, which may have formed more readily in a"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_5",
    "chunk": "prebiotic environment. Fatty acids have been synthesized in laboratories under a variety of prebiotic conditions and have been found on meteorites, suggesting their natural synthesis in nature. Oleic acid vesicles represent good models of membrane protocells Cohen et al. (2022) suggest that plausible prebiotic production of fatty acids — leading to the development of early protocell membranes — is enriched on metal-rich mineral surfaces, possibly from impact craters, increasing the prebiotic environmental mass of lipids by 10 times. They evaluate three different possible synthesis pathways of fatty acids in the Hadean, and found that these metal surfaces could produce 10 - 10 kg of 6-18 carbon fatty acids. Of these products, the 8-18C fatty acids are compatible with membrane formation. They also propose that alternative amphiphiles like alcohols are co-synthesized with fatty acid, and can help improve membrane stability. However, despite this production, the authors state that net fatty acid synthesis would not yield sufficient concentrations for spontaneous membrane formation without significant evaporation of Earth's aqueous environments. For cellular organisms, the transport of specific molecules across compartmentalizing membrane barriers is essential in order to exchange content with their environment and with other individuals. For example, content exchange between individuals enables"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_6",
    "chunk": "the exchange of genes between individuals (horizontal gene transfer), an important factor in the evolution of cellular life. While modern cells can rely on complicated protein machineries to catalyze these crucial processes, protocells must have accomplished this using more simple mechanisms. Protocells composed of fatty acids would have been able to easily exchange small molecules and ions with their environment. Modern phospholipid bilayer cell membranes exhibit low permeability, but contain complex molecular assemblies which both actively and passively transport relevant molecules across the membrane in a highly specific manner. In the absence of these complex assemblies, simple fatty acid based protocell membranes would be more permeable and allow for greater non-specific transport across membranes. Molecules that would be highly permeable across protocell membranes include nucleoside monophosphate (NMP), nucleoside diphosphate (NDP), and nucleoside triphosphate (NTP), and may withstand millimolar concentrations of Mg. Osmotic pressure can also play a significant role regarding this passive membrane transport. Environmental effects have been suggested to trigger conditions under which a transport of larger molecules, such as DNA and RNA, across the membranes of protocells is possible. For example, it has been proposed that electroporation resulting from lightning strikes could enable such transport. Electroporation is the"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_7",
    "chunk": "rapid increase in bilayer permeability induced by the application of a large artificial electric field across the membrane. During electroporation, the lipid molecules in the membrane shift position, opening up a pore (hole) that acts as a conductive pathway through which hydrophobic molecules like nucleic acids can pass the lipid bilayer. A similar transfer of content across protocells and with the surrounding solution can be caused by freezing and subsequent thawing. This could, for instance, occur in an environment in which day and night cycles cause recurrent freezing. Laboratory experiments have shown that such conditions allow an exchange of genetic information between populations of protocells. This can be explained by the fact that membranes are highly permeable at temperatures slightly below their phase transition temperature. If this point is reached during the freeze-thaw cycle, even large and highly charged molecules can temporarily pass the protocell membrane. Some molecules or particles are too large or too hydrophilic to pass through a lipid bilayer even under these conditions, but can be moved across the membrane through fusion or budding of vesicles, events which have also been observed for freeze-thaw cycles. This may eventually have led to mechanisms that facilitate movement of molecules"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_8",
    "chunk": "to the inside of the protocell (endocytosis) or to release its contents into the extracellular space (exocytosis). See also: Abiogenesis: Suitable Geologic Environment, RNA World: Prebiotic RNA Synthesis It has been proposed that life began in hydrothermal vents in the deep sea, but a 2012 study suggests that hot springs have the ideal characteristics for the origin of life. The conclusion is based mainly on the chemistry of modern cells, where the cytoplasm is rich in potassium, zinc, manganese, and phosphate ions, not widespread in marine environments. Such conditions, the researchers argue, are found only where hot hydrothermal fluid brings the ions to the surface—places such as geysers, mud pots, fumaroles and other geothermal features. Within these fuming and bubbling basins, water laden with zinc and manganese ions could have collected, cooled and condensed in shallow pools. However, a recent discovery of alkaline hydrothermal vents with an ionic concentration of sodium lower than in seawater suggests that high concentrations of potassium can be found at marine environments. A study in the 1990s showed that montmorillonite clay can help create RNA chains of as many as 50 nucleotides joined together spontaneously into a single RNA molecule. Later, in 2002, it was"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_9",
    "chunk": "discovered that by adding montmorillonite to a solution of fatty acid micelles (lipid spheres), the clay sped up the rate of vesicle formation 100-fold. Some minerals can catalyze the stepwise formation of hydrocarbon tails of fatty acids from hydrogen and carbon monoxide gases—gases that may have been released from hydrothermal vents or geysers. Fatty acids of various lengths are eventually released into the surrounding water, but vesicle formation requires a higher concentration of fatty acids, so it is suggested that protocell formation started at land-bound hydrothermal freshwater environments such as geysers, mud pots, fumaroles and other geothermal features where water evaporates and concentrates the solute. In 2019, Nick Lane and colleagues show that vesicles form readily in seawater conditions at pH between 6.5 and >12 and temperatures 70 °C, meant to mimic the conditions of alkaline hydrothermal vents, with the presence of lipid mixtures, however a prebiotic source to such mixtures is unclear in those environments. Simple amphiphilic compounds in seawater do not assemble into vesicles because of the high concentration of ionic solutes. Research has shown that vesicles can be bound and stabilized by prebiotic amino acids even while in the presence of salt ions and magnesium ions. In"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_10",
    "chunk": "hot spring conditions, self-assembly of vesicles occurs, which have a lower concentration of ionic solutes. Scientists oligomerized RNA in alkaline hydrothermal vent conditions in the laboratory. Although they were estimated to be 4 units in length, it implies RNA polymers possibly were synthesized at such environments. Experimental research at hot springs gave higher yields of RNA-like polymers than in the laboratory. The polymers were encapsulated in fatty acid vesicles when rehydrated, further supporting the hot spring hypothesis of abiogenesis. These wet-dry cycles also improved vesicle stability and binding. UV exposure has also been shown to promote the synthesis of stable biomolecules like nucleotides. In the origin of chemiosmosis, if early cells originated at alkaline hydrothermal vents, proton gradients can be maintained by the acidic ocean and alkaline water from white smokers while an inorganic membranous structure is in a rock cavity. If early cells originated in terrestrial pools such as hot springs, quinones present in meteorites like the Murchison meteorite would promote the development of proton gradients by coupled redox reactions if the ferricyanide, the electron acceptor, was within the vesicle and an electron donor like a sulfur compound was outside of the lipid membrane. Because of the \"water problem\","
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_11",
    "chunk": "a primitive ATP synthase and other biomolecules would go through hydrolysis due to the absence of wet-dry cycles at hydrothermal vents, unlike at terrestrial pools. Other researchers propose hydrothermal pore systems coated in mineral gels at deep sea hydrothermal vents to an alternative compartment of membranous structures, promote biochemical reactions of biopolymers, and could solve the \"water problem\". David Deamer and Bruce Damer argue that biomolecules would become trapped within these pore systems upon polymerization and would not undergo combinatorial selection. Catalytic FeS and NiS walls at alkaline hydrothermal vents has also been suggested to have promoted polymerization. However, Jackson (2016) evaluates how the pH gradient between alkaline hydrothermal vents and acidic Hadean seawater might influence prebiotic synthesis. Three main criticisms emerge from this evaluation. Firstly, the maintenance and stability of membranes positioned suitably between turbulent pH gradients seemed implausible. They claim that the proposition of CaCO3 and Mg(OH)2 precipitates interacting with fluid mixing in subsurface pores do not produce satisfactory environments. Secondly, they suggest that the molecular assemblies required to utilize key energetic gradients available at hydrothermal systems were too complex to have been relevant at the origin of life. Lastly, they argue that even if a molecular assembly"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_12",
    "chunk": "could have harvested available hydrothermal energy, those assemblies would have been too large to operate within the proposed membrane thicknesses accepted by proponents of the hydrothermal vent hypothesis. In 2017, Jackson takes a further stance, suggesting that even if an organism successfully originated in alkaline hydrothermal pores, exploiting natural pH gradients for energy, it would not be able to withstand the drastic change of environment after emergence from the vent environment in which it had solely evolved. This emergence, however, is essential to the niche differentiation of life, allowing for the diversification of habitats and energetic strategies. Counters to these arguments suggest that the close resemblance between biochemical pathways and geochemical systems at alkaline hydrothermal vents gives merit to the hypothesis, and that selection on these protocells would improve resilience to environmental change, allowing for emergence and distribution. It has been considered by other researchers that life originating in hydrothermal volcanic ponds exposed to UV radiation, zinc sulfide photocatalysis, and occurrence of continuous wet-dry cycling would not resemble modern biochemistry. Maximal ATP synthesis is shown to occur at high water activity and low ion concentrations. Despite this, hydrothermal vents are still considered to be a feasible environment as some shallow"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_13",
    "chunk": "hydrothermal vents emit freshwater and the concentration of divalent cations in Hadean oceans were likely lower than in modern oceans. Nick Lane and coauthors state that \"alkaline hydrothermal systems tend to precipitate Ca and Mg ions as aragonite and brucite, so their concentrations are typically much lower than mean ocean values. Modelling work in relation to Hadean systems indicates that hydrothermal concentrations of Ca and Mg would likely have been <1 mM, which is in the range that enhanced phosphorylation here. Other conditions considered here, including salinity and high pressure, would have only limited effects on ATP synthesis in submarine hydrothermal systems (which typically have pressures in the range of 100 to 300 Bars). Alkaline hydrothermal systems might also have generated Fe in situ for ADP phosphorylation. Thermodynamic modelling shows that the mixing of alkaline hydrothermal fluids with seawater in submarine systems can promote continuous cycling between ferrous and ferric iron, potentially forming soluble hydrous ferric chlorides, which our experiments show have the same effect as ferric sulphate\". Another group suggests that primitive cells might have formed inside inorganic clay microcompartments, which can provide an ideal container for the synthesis and compartmentalization of complex organic molecules. Clay-armored bubbles form naturally"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_14",
    "chunk": "when particles of montmorillonite clay collect on the outer surface of air bubbles under water. This creates a semi permeable vesicle from materials that are readily available in the environment. The authors remark that montmorillonite is known to serve as a chemical catalyst, encouraging lipids to form membranes and single nucleotides to join into strands of RNA. Primitive reproduction can be envisioned when the clay bubbles burst, releasing the lipid membrane-bound product into the surrounding medium. Another way to form primitive compartments that may lead to the formation of a protocell is polyesters membraneless structures that have the ability to host biochemicals (proteins and RNA) and/or scaffold the assemblies of lipids around them. While these droplets are leaky towards genetic materials, this leakiness could have facilitated the progenote hypothesis. Researchers have also proposed early encapsulation in aqueous phase-separated droplets called coacervates. These droplets are driven by the accumulation of macromolecules, producing a distinct dense phase liquid droplet within a more dilute liquid medium. These droplets can propagate, retaining their internal composition, through shear forces and turbulence in the medium, and could have acted as a means of replicating encapsulation for an early protocell. However, replication was highly disordered and droplet"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_15",
    "chunk": "fusion is common, calling into question coacervates true potential for distinct compartmentalization leading to competition and early Darwinian-selection. Eigen et al. and Woese proposed that the genomes of early protocells were composed of single-stranded RNA, and that individual genes corresponded to separate RNA segments, rather than being linked end-to-end as in present-day DNA genomes. A protocell that was haploid (one copy of each RNA gene) would be vulnerable to damage, since a single lesion in any RNA segment would be potentially lethal to the protocell (e.g. by blocking replication or inhibiting the function of an essential gene). Vulnerability to damage could be reduced by maintaining two or more copies of each RNA segment in each protocell, i.e. by maintaining diploidy or polyploidy. Genome redundancy would allow a damaged RNA segment to be replaced by an additional replication of its homolog. For such a simple organism, the proportion of available resources tied up in the genetic material would be a large fraction of the total resource budget. Under limited resource conditions, the protocell reproductive rate would likely be inversely related to ploidy number, and the protocell's fitness would be reduced by the costs of redundancy. Consequently, coping with damaged RNA genes"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_16",
    "chunk": "while minimizing the costs of redundancy would likely have been a fundamental problem for early protocells. A cost-benefit analysis was carried out in which the costs of maintaining redundancy were balanced against the costs of genome damage. This analysis led to the conclusion that, under a wide range of circumstances, the selected strategy would be for each protocell to be haploid, but to periodically fuse with another haploid protocell to form a transient diploid. The retention of the haploid state maximizes the growth rate. The periodic fusions permit mutual reactivation of otherwise lethally damaged protocells. If at least one damage-free copy of each RNA gene is present in the transient diploid, viable progeny can be formed. For two, rather than one, viable daughter cells to be produced would require an extra replication of the intact RNA gene homologous to any RNA gene that had been damaged prior to the division of the fused protocell. The cycle of haploid reproduction, with occasional fusion to a transient diploid state, followed by splitting to the haploid state, can be considered to be the sexual cycle in its most primitive form. In the absence of this sexual cycle, haploid protocells with damage in an"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_17",
    "chunk": "essential RNA gene would simply die. This model for the early sexual cycle is hypothetical, but it is very similar to the known sexual behavior of the segmented RNA viruses, which are among the simplest organisms known. Influenza virus, whose genome consists of 8 physically separated single-stranded RNA segments, is an example of this type of virus. In segmented RNA viruses, \"mating\" can occur when a host cell is infected by at least two virus particles. If these viruses each contain an RNA segment with a lethal damage, multiple infection can lead to reactivation providing that at least one undamaged copy of each virus gene is present in the infected cell. This phenomenon is known as \"multiplicity reactivation\". Multiplicity reactivation has been reported to occur in influenza virus infections after induction of RNA damage by UV-irradiation, and ionizing radiation. Starting with a technique commonly used to deposit molecules on a solid surface, Langmuir–Blodgett deposition, scientists are able to assemble phospholipid membranes of arbitrary complexity layer by layer. These artificial phospholipid membranes support functional insertion both of purified and of in situ expressed membrane proteins. The technique could help astrobiologists understand how the first living cells originated. Jeewanu protocells are synthetic"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_18",
    "chunk": "chemical particles that possess cell-like structure and seem to have some functional living properties. First synthesized in 1963 from simple minerals and basic organics while exposed to sunlight, it is still reported to have some metabolic capabilities, the presence of semipermeable membrane, amino acids, phospholipids, carbohydrates and RNA-like molecules. The nature and properties of the Jeewanu remains to be clarified. In a similar synthesis experiment a frozen mixture of water, methanol, ammonia and carbon monoxide was exposed to ultraviolet (UV) radiation. This combination yielded large amounts of organic material that self-organised to form globules or vesicles when immersed in water. The investigating scientist considered these globules to resemble cell membranes that enclose and concentrate the chemistry of life, separating their interior from the outside world. The globules were between 10 and 40 micrometres (0.00039 and 0.00157 in), or about the size of red blood cells. Remarkably, the globules fluoresced, or glowed, when exposed to UV light. Absorbing UV and converting it into visible light in this way was considered one possible way of providing energy to a primitive cell. If such globules played a role in the origin of life, the fluorescence could have been a precursor to primitive photosynthesis."
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_19",
    "chunk": "Such fluorescence also provides the benefit of acting as a sunscreen, diffusing any damage that otherwise would be inflicted by UV radiation. Such a protective function would have been vital for life on the early Earth, since the ozone layer, which blocks out the sun's most destructive UV rays, did not form until after photosynthetic life began to produce oxygen. The synthesis of three kinds of \"jeewanu\" have been reported; two of them were organic, and the other was inorganic. Other similar inorganic structures have also been produced. The investigating scientist (V. O. Kalinenko) referred to them as \"bio-like structures\" and \"artificial cells\". Formed in distilled water (as well as on agar gel) under the influence of an electric field, they lack protein, amino acids, purine or pyrimidine bases, and certain enzyme activities. According to NASA researchers, \"presently known scientific principles of biology and biochemistry cannot account for living inorganic units\" and \"the postulated existence of these living units has not been proved\". In March 2014, NASA's Jet Propulsion Laboratory demonstrated a unique way to study the origins of life: fuel cells. Fuel cells are similar to biological cells in that electrons are also transferred to and from molecules. In"
  },
  {
    "source": "Protocell.txt",
    "chunk_id": "Protocell.txt_20",
    "chunk": "both cases, this results in electricity and power. The study of fuel cells suggest that an important factor in protocell development was that the Earth provides electrical energy at the seafloor. \"This energy could have kick-started life and could have sustained life after it arose. Now, we have a way of testing different materials and environments that could have helped life arise not just on Earth, but possibly on Mars, Europa and other places in the Solar System.\" Protocell research has created controversy and opposing opinions, including criticism of vague definitions of \"artificial life\". The creation of a basic unit of life is the most pressing ethical concern, although the most widespread worry about protocells is their potential threat to human health and the environment through uncontrolled replication. Additionally, postulation into the conditions for protocellular origins of life on Earth remain debated. Scientists in the field emphasize the importance of further hypothesis based experimentation over theoretical conjecture to more concretely constrain the prebiotic plausibility of different protocell morphologies, geologic conditions, and synthetic schemes."
  },
  {
    "source": "PubMed Central.txt",
    "chunk_id": "PubMed Central.txt_0",
    "chunk": "# PubMed Central PubMed Central (PMC) is a free digital repository that archives open access full-text scholarly articles that have been published in biomedical and life sciences journals. As one of the major research databases developed by the National Center for Biotechnology Information (NCBI), PubMed Central is more than a document repository. Submissions to PMC are indexed and formatted for enhanced metadata, medical ontology, and unique identifiers which enrich the XML structured data for each article. Content within PMC can be linked to other NCBI databases and accessed via Entrez search and retrieval systems, further enhancing the public's ability to discover, read and build upon its biomedical knowledge. PubMed Central is distinct from PubMed. PubMed Central is a free digital archive of full articles, accessible to anyone from anywhere via a web browser (with varying provisions for reuse). Conversely, although PubMed is a searchable database of biomedical citations and abstracts, the full-text article resides elsewhere (in print or online, free or behind a subscriber paywall). PubMed Central began as E-biomed, initially proposed in May 1999 by then-NIH director Harold Varmus. The idea came to him \"abruptly\" in December 1998, inspired by the early use of arXiv for preprints after a"
  },
  {
    "source": "PubMed Central.txt",
    "chunk_id": "PubMed Central.txt_1",
    "chunk": "presentation from Pat Brown of Stanford and David Lipman, director of NCBI: But my views broadened abruptly one morning in December of 1998 when I met Pat Brown for coffee, at the café that was formerly the famed Tassajara Bakery, on the corner of Cole and Parnassus, during a visit to San Francisco. [...] A few weeks before our coffee, Pat had learned about the methods being used by the physicist Paul Ginsparg and his colleagues at Los Alamos to allow physicists and mathematicians to share their work with one another over the Internet. They were posting \"preprints\" (articles not yet submitted or accepted for publication) at a publicly accessible website (called LanX or arXiv) for anyone to read and critique. [...] The more I thought about this, the more I was convinced that a radical restructuring of methods for publishing, transmitting, storing, and using biomedical research reports might be possible and beneficial. In a spirit of enthusiasm and political innocence, I wrote a lengthy manifesto, proposing the creation of an NIH-supported online system, called E-biomed. The goal of E-biomed was to provide free access to all biomedical research. Papers submitted to E-biomed could take one of two routes: either"
  },
  {
    "source": "PubMed Central.txt",
    "chunk_id": "PubMed Central.txt_2",
    "chunk": "immediately published as a preprint, or through a traditional peer review process. The peer review process was to resemble contemporary overlay journals, with an external editorial board retaining control over the process of reviewing, curating, and listing papers which would otherwise be freely accessible on the central E-biomed server. Varmus intended to realize the new possibilities presented by communicating scientific results digitally, imagining continuous conversation about published work, versioned documents, and enriched \"layered\" formats allowing for multiple levels of detail. The proposal to create a central index of biomedical research was a radical departure from prevailing publishing norms. Prior to the internet, publication indexes operated largely like ISBNs: allocated by registration agencies to secondary publishers. The idea that anyone could own their own address space via a domain name and create their own indexing system was a wholly new idea. Major commercial publishers had begun experimenting with an indexing system for scientific papers shared across publishers as early as 1993, and were spurred to action following the E-biomed proposal. At the October 1999 STM Annual Frankfurt Conference, several publishers led by Springer-Verlag reached a hurried conference room consensus to launch their competitor prototype: At the Board meeting of the STM"
  },
  {
    "source": "PubMed Central.txt",
    "chunk_id": "PubMed Central.txt_3",
    "chunk": "association, held the afternoon of Monday, October 11, before the fair's Wednesday opening, discussion focused on an emerging U.S. National Library of Medicine (NLM) initiative called E-Biomed (later PubMed Central) that had been proposed by Harold Varmus of the National Institutes of Health in the spring of 1999. Varmus envisioned a digital archive of journals, accessible free of charge and with the added value of reference linking. \"Our consensus was that publishers should be the ones doing the linking,\" said Bob Campbell, who chaired the meeting. \"Since we were 'higher up the stream,' so to speak, we should be able to link our articles ahead of the NLM as part of the process of producing them. Stefan von Holtzbrinck then set the ball rolling by offering to link Nature publications with anyone else's. We decided to issue an announcement of a broad STM reference linking initiative. It was, of course, a strategic move only, since we had neither plan nor prototype.\" A small group led by Arnoud de Kemp of Springer-Verlag met in an adjacent room immediately following the Board meeting to draft the announcement, which was distributed to all attendees of the STM annual meeting the following day and"
  },
  {
    "source": "PubMed Central.txt",
    "chunk_id": "PubMed Central.txt_4",
    "chunk": "published in an STM membership publication. [...] The potential benefit of the service that would become CrossRef was immediately apparent. Organizations such as AIP and IOP (Institute of Physics) had begun to link to each other's publications, and the impossibility of replicating such one-off arrangements across the industry was obvious. As Tim Ingoldsby later put it, \"All those linking agreements were going to kill us.\" Under pressure from vigorous lobbying from commercial publishers and scientific societies who feared for lost profits, NIH officials announced a revised PubMed Central proposal in August 1999. PMC would receive submissions from publishers, rather than from authors as in E-biomed. Publications were allowed time-embargoed paywalls up to one year. PMC would only allow peer-reviewed work — no preprints. The then-unnamed publisher-led linking system shortly thereafter became CrossRef and the larger DOI system. Varmus, Brown, and others including Michael Eisen went on to found the Public Library of Science (PLoS) in 2001, reaching the conclusion \"that if we really want to change the publication of scientific research, we must do the publishing ourselves.\" Launched in February 2000, the repository has grown rapidly as the NIH Public Access Policy is designed to make all research funded by"
  },
  {
    "source": "PubMed Central.txt",
    "chunk_id": "PubMed Central.txt_5",
    "chunk": "the National Institutes of Health (NIH) freely accessible to anyone, and, in addition, many publishers are working cooperatively with the NIH to provide free access to their works. In late 2007, the Consolidated Appropriations Act of 2008 (H.R. 2764) was signed into law and included a provision requiring the NIH to modify its policies and require inclusion into PubMed Central complete electronic copies of their peer-reviewed research and findings from NIH-funded research. These articles are required to be included within 12 months of publication. This is the first time the US government has required an agency to provide open access to research and is an evolution from the 2005 policy, in which the NIH asked researchers to voluntarily add their research to PubMed Central. A UK version of the PubMed Central system, UK PubMed Central (UKPMC), has been developed by the Wellcome Trust and the British Library as part of a nine-strong group of UK research funders. This system went live in January 2007. On 1 November 2012, it became Europe PubMed Central. The Canadian member of the PubMed Central International network, PubMed Central Canada, was launched in October 2009. The National Library of Medicine \"NLM Journal Publishing Tag Set\""
  },
  {
    "source": "PubMed Central.txt",
    "chunk_id": "PubMed Central.txt_6",
    "chunk": "journal article markup language is freely available. The Association of Learned and Professional Society Publishers comments that \"it is likely to become the standard for preparing scholarly content for both books and journals\". A related DTD is available for books. The Library of Congress and the British Library have announced support for the NLM DTD. It has also been popular with journal service providers. With the release of public access plans for many agencies beyond NIH, PMC is in the process of becoming the repository for a wider variety of articles. This includes NASA content, with the interface branded as \"PubSpace\". As of December 2018, the PMC archive contained over 5.2 million articles, with contributions coming from publishers or authors depositing their manuscripts into the repository per the NIH Public Access Policy. Earlier data shows that from January 2013 to January 2014 author-initiated deposits exceeded 103,000 papers during a 12-month period. PMC identifies about 4,000 journals which participate in some capacity to deposit their published content into the PMC repository. Some publishers delay the release of their articles on PubMed Central for a set time after publication, referred to as an \"embargo period\", ranging from a few months to a"
  },
  {
    "source": "PubMed Central.txt",
    "chunk_id": "PubMed Central.txt_7",
    "chunk": "few years depending on the journal. (Embargoes of six to twelve months are the most common.) PubMed Central is a key example of \"systematic external distribution by a third party\", which some publishers purport to prohibit through private contracts. Articles are sent to PubMed Central by publishers in XML or SGML, using a variety of article DTDs. Older and larger publishers may have their own established in-house DTDs, but many publishers use the NLM Journal Publishing DTD (see above). Received articles are converted via XSLT to the very similar NLM Archiving and Interchange DTD. This process may reveal errors that are reported back to the publisher for correction. Graphics are also converted to standard formats and sizes. The original and converted forms are archived. The converted form is moved into a relational database, along with associated files for graphics, multimedia, or other associated data. Many publishers also provide PDF of their articles, and these are made available without change. Bibliographic citations are parsed and automatically linked to the relevant abstracts in PubMed, articles in PubMed Central, and resources on publishers' Web sites. PubMed links also lead to PubMed Central. Unresolvable references, such as to journals or particular articles not yet"
  },
  {
    "source": "PubMed Central.txt",
    "chunk_id": "PubMed Central.txt_8",
    "chunk": "available at one of these sources, are tracked in the database and automatically come \"live\" when the resources become available. An in-house indexing system provides search capability, and is aware of biological and medical terminology, such as generic vs. proprietary drug names, and alternate names for organisms, diseases and anatomical parts. When a user accesses a journal issue, a table of contents is automatically generated by retrieving all articles, letters, editorials, etc. for that issue. When an actual item such as an article is reached, PubMed Central converts the NLM markup to HTML for delivery, and provides links to related data objects. This is feasible because the variety of incoming data has first been converted to standard DTDs and graphic formats. In a separate submission stream, NIH-funded authors may deposit articles into PubMed Central using the NIH Manuscript Submission (NIHMS). Articles thus submitted typically go through XML markup in order to be converted to NLM DTD. Reactions to PubMed Central among the scholarly publishing community range between a genuine enthusiasm by some, to cautious concern by others. While PMC is a welcome partner to open access publishers in its ability to augment the discovery and dissemination of biomedical knowledge, that"
  },
  {
    "source": "PubMed Central.txt",
    "chunk_id": "PubMed Central.txt_9",
    "chunk": "same truth causes others to worry about traffic being diverted from the published version of record, the economic consequences of less readership, as well as the effect on maintaining a community of scholars within learned societies. A 2013 analysis found strong evidence that public repositories of published articles were responsible for \"drawing significant numbers of readers away from journal websites\" and that \"the effect of PMC is growing over time\". Libraries, universities, open access supporters, consumer health advocacy groups, and patient rights organizations have applauded PubMed Central, and hope to see similar public access repositories developed by other federal funding agencies so to freely share any research publications that were the result of taxpayer support. The Antelman study of open access publishing found that in philosophy, political science, electrical and electronic engineering and mathematics, open access papers had a greater research impact. A randomised trial found an increase in content downloads of open access papers, with no citation advantage over subscription access one year after publication. The NIH policy and open access repository work has inspired a 2013 presidential directive which has sparked action in other federal agencies as well. In March 2020, PubMed Central accelerated its deposit procedures for"
  },
  {
    "source": "PubMed Central.txt",
    "chunk_id": "PubMed Central.txt_10",
    "chunk": "the full text of publications on coronavirus. The NLM did so upon request from the White House Office of Science and Technology Policy and international scientists to improve access for scientists, healthcare providers, data mining innovators, AI healthcare researchers, and the general public. The PMCID (PubMed Central identifier), also known as the PMC reference number, is a bibliographic identifier for the PubMed Central open access database, much like the PMID is the bibliographic identifier for the PubMed database. The two identifiers are distinct however. It consists of \"PMC\" followed by a string of numbers. The format is:"
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_0",
    "chunk": "# Qi In the Sinosphere, qi (/ˈtʃiː/ CHEE) is traditionally believed to be a vital force part of all living entities. Literally meaning 'vapor', 'air', or 'breath', the word qi is polysemous, often translated as 'vital energy', 'vital force', 'material energy', or simply 'energy'. Qi is also a concept in traditional Chinese medicine and in Chinese martial arts. The attempt to cultivate and balance qi is called qigong. Believers in qi describe it as a vital force, with one's good health requiring its flow to be unimpeded. Originally prescientific, today it is a pseudoscientific concept, i.e. not corresponding to the concept of energy as used in the physical sciences. Chinese gods and immortals, especially anthropomorphic gods, are sometimes thought to have qi and be a reflection of the microcosm of qi in humans, both having qi that can concentrate in certain body parts. The cultural keyword qì is analyzable in terms of Chinese and Sino-Xenic pronunciations. Possible etymologies include the logographs 氣, 气, and 気 with various meanings ranging from \"vapor\" to \"anger\", and the English loanword qi or ch'i. The logograph 氣 is read with two Chinese pronunciations: the usual qì 氣 \"air; vital energy\" and the rare archaic"
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_1",
    "chunk": "xì 氣 \"to present food\" (later disambiguated with 餼). Hackett Publishing Company, Philip J. Ivanhoe, and Bryan W. Van Norden theorize that the word qi possibly came from a term that referred to \"the mist that arose from heated sacrificial offerings\". Pronunciations of 氣 in modern varieties of Chinese with standardized IPA equivalents include: Standard Chinese qì /t͡ɕʰi˥˩/, Wu Chinese qi /t͡ɕʰi˧˦/, Southern Min khì /kʰi˨˩/, Eastern Min ké /kʰɛi˨˩˧/, Standard Cantonese hei /hei̯˧/, and Hakka Chinese hi /hi˥/. Pronunciations of 氣 in Sino-Xenic borrowings include: Japanese ki, Korean gi, and Vietnamese khí. Reconstructions of the Middle Chinese pronunciation of 氣 standardized to IPA transcription include: /kʰe̯i/ (Bernard Karlgren), /kʰĭəi/ (Wang Li), /kʰiəi/ (Li Rong), /kʰɨj/ (Edwin Pulleyblank), and /kʰɨi/ (Zhengzhang Shangfang). Axel Schuessler's reconstruction of the Later Han Chinese pronunciation of 氣 is /kɨs/. Reconstructions of the Old Chinese pronunciation of 氣 standardized to IPA transcription include: */kʰɯds/ (Zhengzhang Shangfang), */C.qʰəp-s/ (William H. Baxter and Laurent Sagart), and */kə(t)s/ (Axel Schuessler). The etymology of qì interconnects with Kharia kʰis \"anger\", Sora kissa \"move with great effort\", Khmer kʰɛs \"strive after; endeavor\", and Gyalrongic kʰɐs \"anger\". In addition, qì 炁 is an uncommon character especially used in writing Daoist talismans."
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_2",
    "chunk": "Historically, the word qì was generally written as 气 until the Han dynasty (206 BCE–220 CE), when it was replaced by the 氣 graph clarified with mǐ 米 \"rice\" indicating \"steam (rising from rice as it cooks.)\" and depicting the Traditional Chinese view of the transformative, changeable nature of existence and the universe. This primary logograph 气, the earliest written character for qì, consisted of three wavy horizontal lines seen in Shang dynasty (c. 1600–1046 BCE) oracle bone script, Zhou dynasty (1046–256 BCE) bronzeware script and large seal script, and Qin dynasty (221–206 BCE) small seal script. These oracle, bronze, and seal scripts logographs 气 were used in ancient times as a phonetic loan character to write qǐ 乞 \"plead for; beg; ask\" which did not have an early character. The vast majority of Chinese characters are classified as radical-phonetic characters. Such characters combine a semantically suggestive \"radical characters\" with a phonetic element approximating ancient pronunciation. For example, the widely known word dào 道 \"the Dao; the way\" graphically combines the \"walk\" radical 辶 with a shǒu 首 \"head\" phonetic. Although the modern dào and shǒu pronunciations are dissimilar, the Old Chinese *lˤuʔ-s 道 and *l̥uʔ-s 首 were alike. The"
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_3",
    "chunk": "regular script character qì 氣 is unusual because qì 气 is both the \"air radical\" and the phonetic, with mǐ 米 \"rice\" semantically indicating \"steam; vapor\". This qì 气 \"air/gas radical\" was only used in a few native Chinese characters like yīnyūn 氤氲 \"thick mist/smoke\", but was also used to create new scientific characters for gaseous chemical elements. Some examples are based on pronunciations in European languages: fú 氟 (with a fú 弗 phonetic) \"fluorine\" and nǎi 氖 (with a nǎi 乃 phonetic) \"neon\". Others are based on semantics: qīng 氫 (with a jīng 巠 phonetic, abbreviating qīng 輕 \"light-weight\") \"hydrogen (the lightest element)\" and lǜ 氯 (with a lù 彔 phonetic, abbreviating lǜ 綠 \"green\") \"(greenish-yellow) chlorine\". Qì 氣 is the phonetic element in a few characters such as kài 愾 \"hate\" with the \"heart-mind radical\" 忄or 心, xì 熂 \"set fire to weeds\" with the \"fire radical\" 火, and xì 餼 \"to present food\" with the \"food radical\" 食. The first Chinese dictionary of characters, the Shuowen Jiezi(121 CE) notes that the primary qì 气 is a pictographic character depicting 雲气 \"cloudy vapors\", and that the full 氣 combines 米 \"rice\" with the phonetic qi 气, meaning 饋客芻米"
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_4",
    "chunk": "\"present provisions to guests\" (later disambiguated as xì 餼). Qi is a polysemous word. The unabridged Chinese-Chinese character dictionary Hanyu Da Cidian defines it as \"present food or provisions\" for the xì pronunciation but also lists 23 meanings for the qì pronunciation. The modern ABC Chinese-English Comprehensive Dictionary, which enters xì 餼 \"grain; animal feed; make a present of food\", and a qì 氣 entry with seven translation equivalents for the noun, two for bound morphemes, and three equivalents for the verb. n. ① air; gas ② smell ③ spirit; vigor; morale ④ vital/material energy (in Ch[inese] metaphysics) ⑤ tone; atmosphere; attitude ⑥ anger ⑦ breath; respiration b.f. ① weather 天氣 tiānqì ② [linguistics] aspiration 送氣 sòngqì v. ① anger ② get angry ③ bully; insult. Qi was also thought of as meaning \"'forces in nature'\" that deity could control and magicians and occultists could harness. Qi was an early Chinese loanword in English. It was romanized as k'i in Church Romanization in the early-19th century, as ch'i in Wade–Giles in the mid-19th century (sometimes misspelled chi omitting the apostrophe), and as qi in Pinyin in the mid-20th century. The Oxford English Dictionary entry for qi gives the pronunciation as"
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_5",
    "chunk": "/tʃi/, the etymology from Chinese qì \"air; breath\", and a definition of \"The physical life-force postulated by certain Chinese philosophers; the material principle.\" It also gives eight usage examples, with the first recorded example of k'í in 1850 (The Chinese Repository), of ch'i in 1917 (The Encyclopaedia Sinica), and qi in 1971 (Felix Mann's Acupuncture) The word qi is very frequently used in word games—such as Scrabble—due to containing a letter Q without a letter U. References to concepts analogous to qi are found in many Asian belief systems. Philosophical conceptions of qi from the earliest records of Chinese philosophy (5th century BCE) correspond to Western notions of humours and to the ancient Hindu yogic concept of prana. An early form of qi comes from the writings of the Chinese philosopher Mencius (4th century BCE). Within the framework of Chinese thought, no notion may attain such a degree of abstraction from empirical data as to correspond perfectly to one of our modern universal concepts. Nevertheless, the term qi comes as close as possible to constituting a generic designation equivalent to our word \"energy\". When Chinese thinkers are unwilling or unable to fix the quality of an energetic phenomenon, the character"
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_6",
    "chunk": "qi (氣) inevitably flows from their brushes. The ancient Chinese described qi as \"life force\". They believed it permeated everything and linked their surroundings together. Qi was also linked to the flow of energy around and through the body, forming a cohesive functioning unit. By understanding the rhythm and flow of qi, they believed they could guide exercises and treatments to provide stability and longevity. Although the concept has been important within many Chinese philosophies, over the centuries the descriptions of qi have varied and have sometimes been in conflict. Until China came into contact with Western scientific and philosophical ideas, the Chinese had not categorized all things in terms of matter and energy. Qi and li (理: \"pattern\") were 'fundamental' categories similar to matter and energy. \"In later Chinese philosophy, qi was thought of as the fundamental 'stuff' out of which everything in the universe condenses and into which it eventually dissipates.\" Fairly early on, some Chinese thinkers began to believe that there were different fractions of qi—the coarsest and heaviest fractions formed solids, lighter fractions formed liquids, and the most ethereal fractions were the \"lifebreath\" that animated living beings. Yuanqi is a notion of innate or prenatal qi"
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_7",
    "chunk": "which is distinguished from acquired qi that a person may develop over their lifetime. The earliest texts that speak of qi give some indications of how the concept developed. In the Analects of Confucius, qi could mean \"breath\". Combining it with the Chinese word for blood (making 血氣, xue–qi, blood and breath), the concept could be used to account for motivational characteristics: The [morally] noble man guards himself against three things. When he is young, his xue–qi has not yet stabilized, so he guards himself against sexual passion. When he reaches his prime, his xue–qi is not easily subdued, so he guards himself against combativeness. When he reaches old age, his xue–qi is already depleted, so he guards himself against acquisitiveness. The philosopher Mozi used the word qi to refer to noxious vapors that would eventually arise from a corpse were it not buried at a sufficient depth. He reported that early civilized humans learned how to live in houses to protect their qi from the moisture that troubled them when they lived in caves. He also associated maintaining one's qi with providing oneself with adequate nutrition. In regard to another kind of qi, he recorded how some people performed"
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_8",
    "chunk": "a kind of prognostication by observing qi (clouds) in the sky. Mencius described a kind of qi that might be characterized as an individual's vital energies. This qi was necessary to activity and it could be controlled by a well-integrated willpower. When properly nurtured, this qi was said to be capable of extending beyond the human body to reach throughout the universe. It could also be augmented by means of careful exercise of one's moral capacities. On the other hand, the qi of an individual could be degraded by adverse external forces that succeed in operating on that individual. Living things were not the only things believed to have qi. Zhuangzi indicated that wind is the qi of the Earth. Moreover, cosmic yin and yang \"are the greatest of qi\". He described qi as \"issuing forth\" and creating profound effects. He also said \"Human beings are born [because of] the accumulation of qi. When it accumulates there is life. When it dissipates there is death... There is one qi that connects and pervades everything in the world.\" The Guanzi essay Neiye (Inward Training) is the oldest received writing on the subject of the cultivation of vapor [qi] and meditation techniques."
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_9",
    "chunk": "The essay was probably composed at the Jixia Academy in Qi in the late fourth century B.C. Xun Zi, another Confucian scholar of the Jixia Academy, followed in later years. At 9:69/127, Xun Zi says, \"Fire and water have qi but do not have life. Grasses and trees have life but do not have perceptivity. Fowl and beasts have perceptivity but do not have yi (sense of right and wrong, duty, justice). Men have qi, life, perceptivity, and yi.\" Chinese people at such an early time had no concept of radiant energy, but they were aware that one can be heated by a campfire from a distance away from the fire. They accounted for this phenomenon by claiming \"qi\" radiated from fire. At 18:62/122, he also uses \"qi\" to refer to the vital forces of the body that decline with advanced age. Among the animals, the gibbon and the crane were considered experts at inhaling the qi. The Confucian scholar Dong Zhongshu (ca. 150 BC) wrote in Luxuriant Dew of the Spring and Autumn Annals: \"The gibbon resembles a macaque, but he is larger, and his color is black. His forearms being long, he lives eight hundred years, because he"
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_10",
    "chunk": "is expert in controlling his breathing.\" (\"猿似猴。大而黑。長前臂。所以壽八百。好引氣也。\") Later, the syncretic text assembled under the direction of Liu An, the Huai Nan Zi, or \"Masters of Huainan\", has a passage that presages most of what is given greater detail by the Neo-Confucians: Heaven (seen here as the ultimate source of all being) falls (duo 墮, i.e., descends into proto-immanence) as the formless. Fleeting, fluttering, penetrating, amorphous it is, and so it is called the Supreme Luminary. The dao begins in the Void Brightening. The Void Brightening produces the universe (yu–zhou). The universe produces qi. Qi has bounds. The clear, yang [qi] was ethereal and so formed heaven. The heavy, turbid [qi] was congealed and impeded and so formed earth. The conjunction of the clear, yang [qi] was fluid and easy. The conjunction of the heavy, turbid [qi] was strained and difficult. So heaven was formed first and earth was made fast later. The pervading essence (xi–jing) of heaven and earth becomes yin and yang. The concentrated (zhuan) essences of yin and yang become the four seasons. The dispersed (san) essences of the four seasons become the myriad creatures. The hot qi of yang in accumulating produces fire. The essence (jing) of"
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_11",
    "chunk": "the fire-qi becomes the sun. The cold qi of yin in accumulating produces water. The essence of the water-qi becomes the moon. The essences produced by coitus (yin) of the sun and moon become the stars and celestial markpoints (chen, planets). Qi is linked to East Asian thought on magic, and certain body parts were important to magic traditions such as some Taoist sects. The Huangdi Neijing (\"The Yellow Emperor's Classic of Medicine\", circa 2nd century BCE) is historically credited with first establishing the pathways, called meridians, through which qi allegedly circulates in the human body. In traditional Chinese medicine, symptoms of various illnesses are believed to be either the product of disrupted, blocked, and unbalanced qi movement through meridians or deficiencies and imbalances of qi in the Zang Fu organs. Traditional Chinese medicine often seeks to relieve these imbalances by adjusting the circulation of qi using a variety of techniques including herbology, food therapy, physical training regimens (qigong, tai chi, and other martial arts training), moxibustion, tui na, or acupuncture.The cultivation of Heavenly and Earthly qi allow for the maintenance of psychological actions The nomenclature of Qi in the human body is different depending on its sources, roles, and"
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_12",
    "chunk": "locations. For sources there is a difference between so-called \"Primordial Qi\" (acquired at birth from one's parents) and Qi acquired throughout one's life. Or again Chinese medicine differentiates between Qi acquired from the air we breathe (so called \"Clean Air\") and Qi acquired from food and drinks (so-called \"Grain Qi\"). Looking at roles Qi is divided into \"Defensive Qi\" and \"Nutritive Qi\". Defensive Qi's role is to defend the body against invasions while Nutritive Qi's role is to provide sustenance for the body. To protect against said invasions, medicines have four types of qi; cold, hot, warm, and cool. Cold qi medicines are used to treat invasions hot in nature, while hot qi medicines are used to treat invasions cold in nature. looking at locations, Qi is also named after the Zang-Fu organ or the Meridian in which it resides: \"Liver Qi\", \"Spleen Qi\", etc. Lastly, prolonged exposure to the three evil qi (wind, cold, and wetness) can result in the penetration of evil qi through surface body parts, eventually reaching Zang-Fu organs. A qi field (chu-chong) refers to the cultivation of an energy field by a group, typically for healing or other benevolent purposes. A qi field is believed"
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_13",
    "chunk": "to be produced by visualization and affirmation. They are an important component of Wisdom Healing Qigong (Zhineng Qigong), founded by Grandmaster Ming Pang. The existence of Qi has not been proven scientifically. A 1998 consensus statement on acupuncture by the United States National Institutes of Health noted that concepts such as qi \"are difficult to reconcile with contemporary biomedical information\". The traditional Chinese art of geomancy, the placement and arrangement of space called feng shui, is based on calculating the balance of qi, interactions between the five elements, yin and yang, and other factors. The retention or dissipation of qi is believed to affect the health, wealth, energy level, luck, and many other aspects of the occupants. Attributes of each item in a space affect the flow of qi by slowing it down, redirecting it or accelerating it. This is said to influence the energy level of the occupants. Positive qi flows in curved lines, whereas negative qi travels in straight lines. In order for qi to be nourishing and positive, it must continue to flow not too quickly or too slowly. In addition, qi should not be blocked abruptly, because it would become stagnant and turn destructive. One use"
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_14",
    "chunk": "for a luopan is to detect the flow of qi. The quality of qi may rise and fall over time. Feng shui with a compass might be considered a form of divination that assesses the quality of the local environment. There are three kinds of qi, known as heaven qi (tian qi 天气), Earth qi (di qi 地气), and human qi (ren qi 人气). Heaven qi is composed of natural forces including the sun and rain. Earth qi is affected by heaven qi. For example, too much sun would lead to drought, and a lack of sun would cause plants to die off. Human qi is affected by earth qi, because the environment has effects on human beings. Feng shui is the balancing of heaven, Earth, and human qi. Reiki is a form of alternative medicine called energy healing. Reiki practitioners use a technique called palm healing or hands-on healing through which a \"universal energy\" is said to be transferred through the palms of the practitioner to the patient in order to encourage emotional or physical healing. Reiki is a pseudoscience, and is used as an illustrative example of pseudoscience in scholarly texts and academic journal articles. It is based"
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_15",
    "chunk": "on qi (\"chi\"), which practitioners say is a universal life force, although there is no empirical evidence that such a life force exists. Clinical research has not shown reiki to be effective as a treatment for any medical condition. There has been no proof of the effectiveness of reiki therapy compared to the placebo effect. An overview of reiki investigations found that studies reporting positive effects had methodological flaws. The American Cancer Society stated that reiki should not replace conventional cancer treatment, a sentiment echoed by Cancer Research UK and the National Center for Complementary and Integrative Health. Developed in Japan in 1922 by Mikao Usui, it has been adapted into varying cultural traditions across the world. According to its believers, Reiki healing occurs by laying hands over or on an individual's area of pain and controlling the universal Qi flow of the nearby space, sending into the area of malaise and purifying it. There is no regulation of the practicing of Reiki in the United States and generally no central world organization that has authority over it. Qìgōng (气功 or 氣功) involves coordinated breathing, movement, and awareness. It is traditionally viewed as a practice to cultivate and balance qi."
  },
  {
    "source": "Qi.txt",
    "chunk_id": "Qi.txt_16",
    "chunk": "With roots in traditional Chinese medicine, philosophy and martial arts, qigong is now practiced worldwide for exercise, healing, meditation, and training for martial arts. Typically a qigong practice involves rhythmic breathing, slow and stylized movement, practicing mindfulness, and visualization of guiding qi. Qi is a didactic concept in Chinese, Vietnamese, Korean, and Japanese martial arts. Martial qigong is a feature of both internal and external training systems in China and other East Asian cultures. The most notable of the qi-focused \"internal\" force (jin) martial arts are Baguazhang, Xingyiquan, tai chi, Southern Praying Mantis, Snake Kung Fu, Southern Dragon Kung Fu, Aikido, Kendo, Hapkido, Aikijujutsu, Luohanquan, and Liuhebafa. Demonstrations of qi or ki are popular in some martial arts and may include the unraisable body, the unbendable arm, and other feats of power. These feats can be explained using biomechanics and physics. Acupuncture is a part of traditional Chinese medicine that involves insertion of needles or the application of pinching/gripping into/onto superficial structures of the body (skin, subcutaneous tissue, muscles) at acupuncture points to balance the flow of qi. This is often accompanied by moxibustion, a treatment that involves burning mugwort on or near the skin at an acupuncture point."
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_0",
    "chunk": "# Quaoar Quaoar (minor-planet designation: 50000 Quaoar) is a large ringed Kuiper belt object, one of many icy planetesimals beyond Neptune. It has an elongated ellipsoidal shape with an average diameter of 1,090 km (680 mi), about half the size of the dwarf planet Pluto, and is a possible dwarf planet. The object was discovered by American astronomers Chad Trujillo and Michael Brown at the Palomar Observatory on 4 June 2002. Quaoar's surface contains crystalline water ice and ammonia hydrate, which suggests that it might have experienced cryovolcanism. A small amount of methane is present on its surface, which is only retained by the largest Kuiper belt objects. Quaoar has one known moon, Weywot, which was discovered by Brown in February 2007. Both objects were named after mythological figures from the Native American Tongva people in Southern California. Quaoar is the Tongva creator deity and Weywot is his son. In 2023, astronomers announced the discovery of two thin rings orbiting Quaoar outside its Roche limit, which defies theoretical expectations that rings outside the Roche limit should not be stable. Quaoar was discovered on 4 June 2002 by American astronomers Chad Trujillo and Michael Brown at the Palomar Observatory in the"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_1",
    "chunk": "Palomar Mountain Range in San Diego County, California. The discovery formed part of the Caltech Wide Area Sky Survey, which was designed to search for the brightest Kuiper belt objects using the Palomar Observatory's 1.22-meter Samuel Oschin telescope. Quaoar was first identified in images by Trujillo on 5 June 2002, when he noticed a dim, 18.6-magnitude object slowly moving among the stars of the constellation Ophiuchus. Quaoar appeared relatively bright for a distant object, suggesting that it could have a size comparable to the diameter of Pluto. To ascertain Quaoar's orbit, Brown and Trujillo initiated a search for archival precovery images. They obtained several precovery images taken by the Near-Earth Asteroid Tracking survey from various observatories in 1996 and 2000–2002. In particular, they had also found two archival photographic plates taken by astronomer Charles T. Kowal in May 1983, who at the time was searching for the hypothesized Planet X at the Palomar Observatory. From these precovery images, Brown and Trujillo were able to calculate Quaoar's orbit and distance. Additional precovery images of Quaoar have been later identified, with the earliest known found by Edward Rhoads on a photographic plate imaged on 25 May 1954 from the Palomar Observatory Sky"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_2",
    "chunk": "Survey. Before announcing the discovery of Quaoar, Brown had planned to conduct follow-up observations using the Hubble Space Telescope to measure Quaoar's size. He had also planned to announce the discovery as soon as possible and found it necessary to keep the discovery information confidential during the follow-up observations. Rather than submitting his Hubble proposal under peer review, Brown submitted his proposal directly to one of Hubble's operators, who promptly allocated time to Brown. While setting up the observing algorithm for Hubble, Brown had also planned to use one of the Keck telescopes in Mauna Kea, Hawaii, as a part of a study on cryovolcanism on the moons of Uranus. This provided him additional time for follow-up observations and took advantage of the whole observing session in July to analyze Quaoar's spectrum and characterize its surface composition. The discovery of Quaoar was formally announced by the Minor Planet Center in a Minor Planet Electronic Circular on 7 October 2002. It was given the provisional designation 2002 LM60, indicating that its discovery took place during the first half of June 2002. Quaoar was the 1,512th object discovered in the first half of June, as indicated by the preceding letter and numbers"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_3",
    "chunk": "in its provisional designation. On that same day, Trujillo and Brown reported their scientific results from observations of Quaoar at the 34th annual meeting of the American Astronomical Society's Division for Planetary Sciences in Birmingham, Alabama. They announced Quaoar was the largest Kuiper belt object found yet, surpassing previous record holders 20000 Varuna and 2002 AW197. Quaoar's discovery has been cited by Brown as having contributed to the reclassification of Pluto as a dwarf planet. Since then, Brown has contributed to the discovery of larger trans-Neptunian objects, including Haumea, Eris, Makemake and Gonggong. Upon Quaoar's discovery, it was initially given the temporary nickname \"Object X\" as a reference to Planet X, due to its potentially large size and unknown nature. At the time, Quaoar's size was uncertain, and its brightness led the discovery team to speculate that it may be a tenth planet. After measuring Quaoar's size with the Hubble Space Telescope in July, the team began considering names for the object, particularly those from local Native American mythologies. Following the International Astronomical Union's (IAU) naming convention for minor planets, non-resonant Kuiper belt objects are to be named after creation deities. The team settled on the name Kwawar, the creator"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_4",
    "chunk": "god of the Tongva people indigenous to the Los Angeles Basin, where Brown's institute, the California Institute of Technology, was located. According to Brown, the name \"Quaoar\" is pronounced with three syllables, and Trujillo's website on Quaoar gives a three-syllable pronunciation, /ˈkwɑːoʊ(w)ɑːr/, as an approximation of the Tongva pronunciation [ˈkʷaʔuwar]. The name can be also pronounced as two syllables, /ˈkwɑːwɑːr/, reflecting the usual English spelling and pronunciation of the deity Kwawar. In Tongva mythology, Kwawar is the genderless creation force of the universe, singing and dancing deities into existence. He first sings and dances to create Weywot (Sky Father), then they together sing Chehooit (Earth Mother) and Tamit (Grandfather Sun) into existence. As they did this, the creation force became more complex as each new deity joined the singing and dancing. Eventually, after reducing chaos to order, they created the seven great giants that upheld the world, then the animals and finally the first man and woman, Tobohar and Pahavit. Upon their investigation of names from Tongva mythology, Brown and Trujillo realized that there were contemporary members of the Tongva people, whom they contacted for permission to use the name. They consulted tribal historian Marc Acuña, who confirmed that the"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_5",
    "chunk": "name Kwawar would indeed be an appropriate name for the newly discovered object. However, the Tongva preferred the spelling Qua-o-ar, which Brown and Trujillo adopted, though with the hyphens omitted. The name and discovery of Quaoar were publicly announced in October, though Brown had not sought approval of the name by the IAU's Committee on Small Body Nomenclature (CSBN). Indeed, Quaoar's name was announced before the official numbering of the object, which Brian Marsden—the head of the Minor Planet Center—remarked in 2004 to be a violation of the protocol. Despite this, the name was approved by the CSBN, and the naming citation, along with Quaoar's official numbering, was published in a Minor Planet Circular on 20 November 2002. Quaoar was given the minor planet number 50000, which was not by coincidence but to commemorate its large size, being that it was found in the search for a Pluto-sized object in the Kuiper belt. The large Kuiper belt object 20000 Varuna was similarly numbered for a similar occasion. However, subsequent even larger discoveries such as 136199 Eris were simply numbered according to the order in which their orbits were confirmed. The usage of planetary symbols is no longer recommended in astronomy,"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_6",
    "chunk": "so Quaoar never received a symbol in the astronomical literature. A symbol ⟨⟩, used mostly among astrologers, is included in Unicode as U+1F77E. The symbol was designed by Denis Moskowitz, a software engineer in Massachusetts; it combines the letter Q (for 'Quaoar') with a canoe, and is stylized to recall angular Tongva rock art. Quaoar orbits the Sun at an average distance of 43.7 AU (6.54 billion km; 4.06 billion mi), taking 288.8 years to complete one full orbit around the Sun. With an orbital eccentricity of 0.04, Quaoar follows a nearly circular orbit, only slightly varying in distance from 42 AU at perihelion to 45 AU at aphelion. At such distances, light from the Sun takes more than 5 hours to reach Quaoar. Quaoar has last passed aphelion in late 1932 and is currently approaching the Sun at a rate of 0.035 AU per year, or about 170 meters per second (380 mph). Quaoar will reach perihelion around February 2075. Because Quaoar has a nearly circular orbit, it does not approach close to Neptune such that its orbit can become significantly perturbed under the gravitational influence of Neptune. Quaoar's minimum orbit intersection distance from Neptune is only 12.3 AU—it"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_7",
    "chunk": "does not approach Neptune within this distance over the course of its orbit, as it is not in a mean-motion orbital resonance with Neptune. Simulations by the Deep Ecliptic Survey show that the perihelion and aphelion distances of Quaoar's orbit do not change significantly over the next ten million years; Quaoar's orbit appears to be stable over the long term. Quaoar is a trans-Neptunian object. It is classified as a distant minor planet by the Minor Planet Center. Because Quaoar is not in a mean-motion resonance with Neptune, it is also classified as a classical Kuiper belt object (cubewano) by the Minor Planet Center and Deep Ecliptic Survey. Quaoar's orbit is moderately inclined to the ecliptic plane by 8 degrees, relatively high when compared to the inclinations of Kuiper belt objects within the dynamically cold population. Because Quaoar's orbital inclination is greater than 4 degrees, it is part of the dynamically hot population of high-inclination classical Kuiper belt objects. The high inclinations of hot classical Kuiper belt objects such as Quaoar are thought to have resulted from gravitational scattering by Neptune during its outward migration in the early Solar System. As of 2024, measurements of Quaoar's shape from its rotational"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_8",
    "chunk": "light curve and stellar occultations show that Quaoar is a triaxial ellipsoid with an average diameter of 1,090 km (680 mi). Quaoar's diameter is roughly half that of Pluto and is slightly smaller than Pluto's moon Charon. At the time of its discovery in 2002, Quaoar was the largest object found in the Solar System since the discovery of Pluto. Quaoar was also the first trans-Neptunian object to be measured directly from Hubble Space Telescope images. Quaoar's far-infrared thermal emission and brightness in visible light both vary significantly (visible light curve amplitude 0.12–0.16 magnitudes) as Quaoar rotates every 17.68 hours, which most likely indicates Quaoar is elongated along its equator. A 2024 analysis of Quaoar's visible and far-infrared rotational light curve by Csaba Kiss and collaborators determined that the lengths of Quaoar's equatorial axes differ by 19% (a/b = 1.19) and the lengths of Quaoar's polar and shortest equatorial axis differ by 16% (b/c = 1.16), which corresponds to ellipsoid dimensions of 1,286 km × 1,080 km × 932 km (799 mi × 671 mi × 579 mi). The ellipsoidal shape of Quaoar matches the size and shape measurements from previous stellar occultations, and also explains why the size and"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_9",
    "chunk": "shape of Quaoar appeared to change in these occultations. Quaoar's elongated shape contradicts theoretical expectations that it should be in hydrostatic equilibrium, because of its large size and slow rotation. According to Michael Brown, rocky bodies around 900 km (560 mi) in diameter should relax into hydrostatic equilibrium, whereas icy bodies relax into hydrostatic equilibrium somewhere between 200 km (120 mi) and 400 km (250 mi). Slowly-rotating objects in hydrostatic equilibrium are expected to be oblate spheroids (Maclaurin spheroids), whereas rapidly-rotating objects in hydrostatic equilibrium, such as Haumea which rotates in nearly 4 hours, are expected to be flattened and elongated ellipsoids (Jacobi ellipsoids). To explain Quaoar's non-equilibrium shape, Kiss and collaborators hypothesized that Quaoar originally had a rapid rotation and was in hydrostatic equilibrium, but its shape became \"frozen in\" and did not change as Quaoar spun down due to tidal forces from its moon Weywot. This would resemble the situation of Saturn's moon Iapetus, which is too oblate for its current rotation rate. Quaoar has a mass of 1.2×10 kg, which was determined from Weywot's orbit using Kepler's third law. Measurements of Quaoar's diameter and mass as of 2024 indicate it has a density between 1.66–1.77 g/cm, which"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_10",
    "chunk": "suggests its interior is composed of roughly 70% rock and 30% ice with low porosity. Quaoar's density was previously thought to be much higher, between 2–4 g/cm, because early measurements inaccurately suggested that Quaoar had a smaller diameter and a higher mass. These early high-density estimates for Quaoar led researchers to hypothesize that the object might be a rocky planetary core exposed by a large impact event, but these hypotheses have since become obsolete as newer estimates indicate a lower density for Quaoar. Quaoar has a dark surface that reflects about 12% of the visible light it receives from the Sun. This may indicate that fresh ice has disappeared from Quaoar's surface. The surface is moderately red, meaning that Quaoar reflects longer (redder) wavelengths of light more than shorter (bluer) wavelengths. Many Kuiper belt objects such as 20000 Varuna and 28978 Ixion share a similar moderately red color. Spectroscopic observations by David Jewitt and Jane Luu in 2004 revealed signs of crystalline water ice and ammonia hydrate on Quaoar's surface. These substances are expected to gradually break down due to solar and cosmic radiation, and crystalline water ice can only form in warm temperatures of at least 110 K (−163"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_11",
    "chunk": "°C), so the presence of crystalline water ice on Quaoar's surface indicates that it was heated to this temperature sometime in the last ten million years. For context, Quaoar's present-day surface temperature is less than 50 K (−223.2 °C). Jewitt and Luu proposed two hypotheses for Quaoar's heating, which are impact events and radiogenic heating. The latter hypothesis allows for the possibility of cryovolcanism on Quaoar, which is supported by the presence of ammonia hydrate on Quaoar's surface. Ammonia hydrate is believed to be cryovolcanically deposited onto Quaoar's surface. A 2006 study by Hauke Hussmann and collaborators suggested that radiogenic heating alone may not be capable of sustaining an internal ocean of liquid water at Quaoar's mantle–core boundary. More precise observations of Quaoar's near infrared spectrum in 2007 indicated the presence of small quantities (5%) of solid methane and ethane. Given its boiling point of 112 K (−161 °C), methane is a volatile ice at average surface temperatures of Quaoar, unlike water ice or ethane. Both models and observations suggest that only a few larger bodies (Pluto, Eris and Makemake) can retain the volatile ices whereas the dominant population of small trans-Neptunian objects lost them. Quaoar, with only small amounts"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_12",
    "chunk": "of methane, appears to be in an intermediary category. In 2022, low-resolution near-infrared (0.7–5 μm) spectroscopic observations by the James Webb Space Telescope (JWST) revealed the presence of carbon dioxide ice, complex organics, and significant amounts of ethane ice on Quaoar's surface. Other possible chemical compounds include hydrogen cyanide and carbon monoxide. JWST also took medium-resolution near-infrared spectra of Quaoar and found evidence of small amounts of methane on Quaoar's surface. However, both JWST's low- and medium-resolution spectra of Quaoar did not show conclusive signs of ammonia hydrates. The presence of methane and other volatiles on Quaoar's surface suggest that it may support a tenuous atmosphere produced from the sublimation of volatiles. With a measured mean temperature of approximately 44 K (−229.2 °C), the upper limit of Quaoar's atmospheric pressure is expected to be in the range of a few microbars. Due to Quaoar's small size and mass, the possibility of Quaoar having an atmosphere of nitrogen and carbon monoxide has been ruled out, since the gases would escape from Quaoar. The possibility of a methane atmosphere, with the upper limit being less than 1 microbar, was considered until 2013, when Quaoar occulted a 15.8-magnitude star and revealed no sign"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_13",
    "chunk": "of a substantial atmosphere, placing an upper limit to at least 20 nanobars, under the assumption that Quaoar's mean temperature is 42 K (−231.2 °C) and that its atmosphere consists of mostly methane. The upper limit of atmosphere pressure was tightened to 10 nanobars after another stellar occultation in 2019. Quaoar has one known moon, Weywot (full designation (50000) Quaoar I Weywot), discovered in 2006 and named after the sky god Weywot, son of Quaoar. It orbits Quaoar at distance of about 13,300 km and is thought to be approximately 170 km (110 mi) in diameter. Besides accurately determining sizes and shapes, stellar occultation campaigns were planned on a long-term basis to search for rings and/or atmospheres around small bodies of the outer solar system. These campaigns agglomerated efforts of various teams in France, Spain and Brazil and were conducted under the umbrella of the European Research Council project Lucky Star. The discovery of Quaoar's first known ring, Q1R, involved various instruments used during stellar occultations observed between 2018 and 2021: the robotic ATOM telescope of the High Energy Stereoscopic System (HESS) in Namibia, the 10.4-m Gran Telescopio Canarias (La Palma Island, Spain); the ESA CHEOPS space telescope, and several"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_14",
    "chunk": "stations run by citizen astronomers in Australia where a report of a Neptune-like ring originated and a dense arc in Q1R was first observed. Taken together, these observations reveal the presence of a partly dense, mostly tenuous and uniquely distant ring around Quaoar, a discovery announced in February 2023. In April 2023, astronomers of the Lucky Star project published the discovery of another ring of Quaoar, Q2R. The Q2R ring was detected by the highly-sensitive 8.2-m Gemini North and the 4.0-m Canada-France-Hawaii Telescope in Mauna Kea, Hawaii, during an observing campaign to confirm Quaoar's Q1R ring in a stellar occultation on 9 August 2022. Quaoar is the fourth minor planet known and confirmed to have a ring system, after 10199 Chariklo, 2060 Chiron, and Haumea. Quaoar possesses two narrow rings, provisionally named Q1R and Q2R by order of discovery, which are confined at radial distances where their orbital periods are integer ratios of Quaoar's rotational period. That is, the rings of Quaoar are in spin-orbit resonances. The outer ring, Q1R, orbits Quaoar at a distance of 4,057 ± 6 km (2,521 ± 4 mi), over seven times the radius of Quaoar and more than double the theoretical maximum distance of"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_15",
    "chunk": "the Roche limit. The Q1R ring is not uniform and is strongly irregular around its circumference, being more opaque (and denser) where it is narrow and less opaque where it is broader. The Q1R ring's radial width ranges from 5 to 300 km (3 to 200 mi) while its optical depth ranges from 0.004 to 0.7. The irregular width of the Q1R ring resembles Saturn's frequently-perturbed F ring or Neptune's ring arcs, which may imply the presence of small, kilometer-sized moonlets embedded within the Q1R ring and gravitationally perturbing the material. The Q1R ring likely consists of icy particles that elastically collide with each other without accreting into a larger mass. Q1R is located in between the 6:1 mean-motion orbital resonance with Quaoar's moon Weywot at 4,021 ± 57 km (2,499 ± 35 mi) and Quaoar's 1:3 spin-orbit resonance at 4,197 ± 58 km (2,608 ± 36 mi). The Q1R ring's coincidental location at these resonances implies they play a key role in maintaining the ring without having it accrete into a single moon. In particular, the confinement of rings to the 1:3 spin-orbit resonance may be common among ringed small Solar System bodies, as it has been previously seen"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_16",
    "chunk": "in Chariklo and Haumea. The inner ring, Q2R, orbits Quaoar at a distance of 2,520 ± 20 km (1,566 ± 12 mi), about four and a half times Quaoar's radius and also outside Quaoar's Roche limit. The Q2R ring's location coincides with Quaoar's 5:7 spin-orbit resonance at 2,525 ± 58 km (1,569 ± 36 mi). Compared to Q1R, the Q2R ring appears relatively uniform with a radial width of 10 km (6.2 mi). With an optical depth of 0.004, the Q2R ring is very tenuous and its opacity is comparable to the least dense part of the Q1R ring. It has been calculated that a flyby mission to Quaoar using a Jupiter gravity assist would take 13.6 years, for launch dates of 25 December 2026, 22 November 2027, 22 December 2028, 22 January 2030 and 20 December 2040. Quaoar would be 41 to 43 AU from the Sun when the spacecraft arrived. In July 2016, the Long Range Reconnaissance Imager (LORRI) aboard the New Horizons spacecraft took a sequence of four images of Quaoar from a distance of about 14 AU. Interstellar Probe, a concept by Pontus Brandt and his colleagues at Johns Hopkins Applied Physics Laboratory would potentially fly"
  },
  {
    "source": "Quaoar.txt",
    "chunk_id": "Quaoar.txt_17",
    "chunk": "by Quaoar in the 2030s before continuing to the interstellar medium, and it has been proposed as a potential flyby target of the first of China National Space Administration's proposed Shensuo probes, which are designed to explore the heliosphere. Quaoar has been chosen as a flyby target for missions like these particularly for its escaping methane atmosphere and possible cryovolcanism, as well as its close proximity to the heliospheric nose."
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_0",
    "chunk": "# Redshift In physics, a redshift is an increase in the wavelength, and corresponding decrease in the frequency and photon energy, of electromagnetic radiation (such as light). The opposite change, a decrease in wavelength and increase in frequency and energy, is known as a blueshift. The terms derive from the colours red and blue which form the extremes of the visible light spectrum. Three forms of redshift occur in astronomy and cosmology: Doppler redshifts due to the relative motions of radiation sources, gravitational redshift as radiation escapes from gravitational potentials, and cosmological redshifts of all light sources proportional to their distances from Earth, a fact known as Hubble's law that implies the universe is expanding. All redshifts can be understood under the umbrella of frame transformation laws. Gravitational waves, which also travel at the speed of light, are subject to the same redshift phenomena. The value of a redshift is often denoted by the letter z, corresponding to the fractional change in wavelength (positive for redshifts, negative for blueshifts), and by the wavelength ratio 1 + z (which is greater than 1 for redshifts and less than 1 for blueshifts). Examples of strong redshifting are a gamma ray perceived as"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_1",
    "chunk": "an X-ray, or initially visible light perceived as radio waves. The initial heat from the Big Bang has redshifted far down to become the cosmic microwave background. Subtler redshifts are seen in the spectroscopic observations of astronomical objects, and are used in terrestrial technologies such as Doppler radar and radar guns. Other physical processes exist that can lead to a shift in the frequency of electromagnetic radiation, including scattering and optical effects; however, the resulting changes are distinguishable from (astronomical) redshift and are not generally referred to as such (see section on physical optics and radiative transfer). Using a telescope and a spectrometer, the variation in intensity of star light with frequency can be measured. The resulting spectrum can be compared to the spectrum from hot gases expected in stars, such as hydrogen, in a laboratory on Earth. As illustrated with the idealized spectrum in the top-right, to determine the redshift, features in the two spectra such as absorption lines, emission lines, or other variations in light intensity may be shifted. Redshift (and blueshift) may be characterized by the relative difference between the observed and emitted wavelengths (or frequency) of an object. In astronomy, it is customary to refer to"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_2",
    "chunk": "this change using a dimensionless quantity called z. If λ represents wavelength and f represents frequency (note, λf = c where c is the speed of light), then z is defined by the equations: Doppler effect blueshifts (z < 0) are associated with objects approaching (moving closer to) the observer with the light shifting to greater energies. Conversely, Doppler effect redshifts (z > 0) are associated with objects receding (moving away) from the observer with the light shifting to lower energies. Likewise, gravitational blueshifts are associated with light emitted from a source residing within a weaker gravitational field as observed from within a stronger gravitational field, while gravitational redshifting implies the opposite conditions. The history of the subject began in the 19th century, with the development of classical wave mechanics and the exploration of phenomena which are associated with the Doppler effect. The effect is named after the Austrian mathematician, Christian Doppler, who offered the first known physical explanation for the phenomenon in 1842. In 1845, the hypothesis was tested and confirmed for sound waves by the Dutch scientist Christophorus Buys Ballot. Doppler correctly predicted that the phenomenon would apply to all waves and, in particular, suggested that the varying"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_3",
    "chunk": "colors of stars could be attributed to their motion with respect to the Earth. Unaware of Doppler's work, French physicist Hippolyte Fizeau in 1848, suggested that a shift in spectral lines from stars might be used to measure their motion relative to Earth. In 1850 François-Napoléon-Marie Moigno analyzed about both Doppler's and Fizeau's ideas in a publication read by both James Clerk Maxwell and William Huggins, who initially stuck to the idea that the color of stars related to their chemistry, however by 1868, Huggins was the first to determine the velocity of a star moving away from the Earth by the analysis of spectral shifts. In 1871, optical redshift was confirmed when the phenomenon was observed in Fraunhofer lines, using solar rotation, about 0.1 Å in the red. In 1887, Vogel and Scheiner discovered the \"annual Doppler effect\", the yearly change in the Doppler shift of stars located near the ecliptic, due to the orbital velocity of the Earth. In 1901, Aristarkh Belopolsky verified optical redshift in the laboratory using a system of rotating mirrors. Beginning with observations in 1912, Vesto Slipher discovered that the Andromeda Galaxy had a blue shift, indicating that it was moving towards the Earth."
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_4",
    "chunk": "Slipher first reported on his measurement in the inaugural volume of the Lowell Observatory Bulletin. Three years later, he wrote a review in the journal Popular Astronomy. In it he stated that \"the early discovery that the great Andromeda spiral had the quite exceptional velocity of –300 km(/s) showed the means then available, capable of investigating not only the spectra of the spirals but their velocities as well.\" Slipher reported the velocities for 15 spiral nebulae spread across the entire celestial sphere, all but three having observable \"positive\" (that is recessional) velocities. Until 1923 the nature of the nebulae was unclear. By that year Edwin Hubble had established that these were galaxies and worked out a procedure to measure distance based on the period-luminosity relation of variable Cepheids stars. This make it possible to test a prediction by Willem de Sitter in 1917 that redshift would be correlated with distance. In 1929 Hubble combined his distance estimates with redshift data from Slipher's reports and measurements by Milton Humason to report an approximate relationship between the redshift and distance, a result now called Hubble's law. Theories relating to the redshift-distance relation also evolved during the decade of the 1920s. The solution"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_5",
    "chunk": "to the equations of general relativity described by de Sitter contained no matter, but in 1922 Alexander Friedmann's derived dynamic solutions, now called the Friedmann–equations, based on frictionless fluid models. Independently Georges Lemaître derived similar equations in 1927 and his analysis became widely known around the time of Hubble's key publication. By early 1930 the combination of the redshift measurements and theoretical models established a major breakthrough in the new science of cosmology: the universe had a history and its expansion could be investigated with physical models backed up with observational astronomy. Arthur Eddington used the term \"red-shift\" as early as 1923, although the word does not appear unhyphenated until about 1934, when Willem de Sitter used it. In the 1960s the discovery of quasars, which appear as very blue point sources and thus were initially thought to be unusual stars, lead to the idea that they were as bright as they were because they were closer than their redshift data indicated. A flurry of theoretical and observational work concluded that these objects were very powerful but distant astronomical objects. Redshifts are differences between two wavelength measurements and wavelengths are a property of both the photons and the measuring equipment."
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_6",
    "chunk": "Thus redshifts characterize differences between two measurement locations. These differences are commonly organized in three groups, attributed to relative motion between the source and the observer, to the expansion of the universe, and to gravity. The following sections explain these groups. If a source of the light is moving away from an observer, then redshift (z > 0) occurs; if the source moves towards the observer, then blueshift (z < 0) occurs. This is true for all electromagnetic waves and is explained by the Doppler effect. Consequently, this type of redshift is called the Doppler redshift. If the source moves away from the observer with velocity v, which is much less than the speed of light (v ≪ c), the redshift is given by where c is the speed of light. In the classical Doppler effect, the frequency of the source is not modified, but the recessional motion causes the illusion of a lower frequency. A more complete treatment of the Doppler redshift requires considering relativistic effects associated with motion of sources close to the speed of light. A complete derivation of the effect can be found in the article on the relativistic Doppler effect. In brief, objects moving close"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_7",
    "chunk": "to the speed of light will experience deviations from the above formula due to the time dilation of special relativity which can be corrected for by introducing the Lorentz factor γ into the classical Doppler formula as follows (for motion solely in the line of sight): This phenomenon was first observed in a 1938 experiment performed by Herbert E. Ives and G.R. Stilwell, called the Ives–Stilwell experiment. Since the Lorentz factor is dependent only on the magnitude of the velocity, this causes the redshift associated with the relativistic correction to be independent of the orientation of the source movement. In contrast, the classical part of the formula is dependent on the projection of the movement of the source into the line-of-sight which yields different results for different orientations. If θ is the angle between the direction of relative motion and the direction of emission in the observer's frame (zero angle is directly away from the observer), the full form for the relativistic Doppler effect becomes: For the special case that the light is moving at right angle (θ = 90°) to the direction of relative motion in the observer's frame, the relativistic redshift is known as the transverse redshift, and"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_8",
    "chunk": "a redshift: is measured, even though the object is not moving away from the observer. Even when the source is moving towards the observer, if there is a transverse component to the motion then there is some speed at which the dilation just cancels the expected blueshift and at higher speed the approaching source will be redshifted. The observations of increasing redshifts from more and more distant galaxies can be modeled assuming a homogeneous and isotropic universe combined with general relativity. This cosmological redshift can be written as a function of a, the time-dependent cosmic scale factor: The scale factor is monotonically increasing as time passes. Thus z is positive, close to zero for local stars, and increasing for distant galaxies that appear redshifted. Using a Friedmann-Robertson-Walker model of the expansion of the universe, redshift can be related to the age of an observed object, the so-called cosmic time–redshift relation. Denote a density ratio as Ω0: with ρcrit the critical density demarcating a universe that eventually crunches from one that simply expands. This density is about three hydrogen atoms per cubic meter of space. At large redshifts, 1 + z > Ω0, one finds: The cosmological redshift is commonly attributed"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_9",
    "chunk": "to stretching of the wavelengths of photons due to the stretching of space. This interpretation can be misleading. As required by general relativity, the cosmological expansion of space has no effect on local physics. There is no term related to expansion in Maxwell's equations that govern light propagation. The cosmological redshift can be interpreted as an accumulation of infinitesimal Doppler shifts along the trajectory of the light. There are several websites for calculating various times and distances from redshift, as the precise calculations require numerical integrals for most values of the parameters. The redshift of a galaxy includes both a component related to recessional velocity from expansion of the universe, and a component related to the peculiar motion of the galaxy with respect to its local universe. The redshift due to expansion of the universe depends upon the recessional velocity in a fashion determined by the cosmological model chosen to describe the expansion of the universe, which is very different from how Doppler redshift depends upon local velocity. Describing the cosmological expansion origin of redshift, cosmologist Edward Robert Harrison said, \"Light leaves a galaxy, which is stationary in its local region of space, and is eventually received by observers who"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_10",
    "chunk": "are stationary in their own local region of space. Between the galaxy and the observer, light travels through vast regions of expanding space. As a result, all wavelengths of the light are stretched by the expansion of space. It is as simple as that...\" Steven Weinberg clarified, \"The increase of wavelength from emission to absorption of light does not depend on the rate of change of a(t) [the scale factor] at the times of emission or absorption, but on the increase of a(t) in the whole period from emission to absorption.\" If the universe were contracting instead of expanding, we would see distant galaxies blueshifted by an amount proportional to their distance instead of redshifted. In the theory of general relativity, there is time dilation within a gravitational well. Light emitted within the well will appear to have fewer cycles per second when measured outside of the well, due to differences in the two clocks. This is known as the gravitational redshift or Einstein Shift. The theoretical derivation of this effect follows from the Schwarzschild solution of the Einstein equations which yields the following formula for redshift associated with a photon traveling in the gravitational field of an uncharged, nonrotating,"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_11",
    "chunk": "spherically symmetric mass: This gravitational redshift result can be derived from the assumptions of special relativity and the equivalence principle; the full theory of general relativity is not required. The effect is very small but measurable on Earth using the Mössbauer effect and was first observed in the Pound–Rebka experiment. However, it is significant near a black hole, and as an object approaches the event horizon the red shift becomes infinite. It is also the dominant cause of large angular-scale temperature fluctuations in the cosmic microwave background radiation (see Sachs–Wolfe effect). Several important special-case formulae for redshift in certain special spacetime geometries, as summarized in the following table. In all cases the magnitude of the shift (the value of z) is independent of the wavelength. The redshift observed in astronomy can be measured because the emission and absorption spectra for atoms are distinctive and well known, calibrated from spectroscopic experiments in laboratories on Earth. When the redshifts of various absorption and emission lines from a single astronomical object are measured, z is found to be remarkably constant. Although distant objects may be slightly blurred and lines broadened, it is by no more than can be explained by thermal or mechanical"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_12",
    "chunk": "motion of the source. For these reasons and others, the consensus among astronomers is that the redshifts they observe are due to some combination of the three established forms of Doppler-like redshifts. Alternative hypotheses and explanations for redshift such as tired light are not generally considered plausible. Spectroscopy, as a measurement, is considerably more difficult than simple photometry, which measures the brightness of astronomical objects through certain filters. When photometric data is all that is available (for example, the Hubble Deep Field and the Hubble Ultra Deep Field), astronomers rely on a technique for measuring photometric redshifts. Due to the broad wavelength ranges in photometric filters and the necessary assumptions about the nature of the spectrum at the light-source, errors for these sorts of measurements can range up to δz = 0.5, and are much less reliable than spectroscopic determinations. However, photometry does at least allow a qualitative characterization of a redshift. For example, if a Sun-like spectrum had a redshift of z = 1, it would be brightest in the infrared (1000nm) rather than at the blue-green (500nm) color associated with the peak of its blackbody spectrum, and the light intensity will be reduced in the filter by a"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_13",
    "chunk": "factor of four, (1 + z). Both the photon count rate and the photon energy are redshifted. (See K correction for more details on the photometric consequences of redshift.) Determining the redshift of an object with spectroscopy requires the wavelength of the emitted light in the rest frame of the source. Astronomical applications rely on distinct spectral lines. Redshifts cannot be calculated by looking at unidentified features whose rest-frame frequency is unknown, or with a spectrum that is featureless or white noise (random fluctuations in a spectrum). Thus gamma-ray bursts themselves cannot be used for reliable redshift measurements, but optical afterglow associated with the burst can be analyzed for redshifts. In nearby objects (within our Milky Way galaxy) observed redshifts are almost always related to the line-of-sight velocities associated with the objects being observed. Observations of such redshifts and blueshifts enable astronomers to measure velocities and parametrize the masses of the orbiting stars in spectroscopic binaries. Similarly, small redshifts and blueshifts detected in the spectroscopic measurements of individual stars are one way astronomers have been able to diagnose and measure the presence and characteristics of planetary systems around other stars and have even made very detailed differential measurements of redshifts"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_14",
    "chunk": "during planetary transits to determine precise orbital parameters. Some approaches are able to track the redshift variations in multiple objects at once. Finely detailed measurements of redshifts are used in helioseismology to determine the precise movements of the photosphere of the Sun. Redshifts have also been used to make the first measurements of the rotation rates of planets, velocities of interstellar clouds, the rotation of galaxies, and the dynamics of accretion onto neutron stars and black holes which exhibit both Doppler and gravitational redshifts. The temperatures of various emitting and absorbing objects can be obtained by measuring Doppler broadening—effectively redshifts and blueshifts over a single emission or absorption line. By measuring the broadening and shifts of the 21-centimeter hydrogen line in different directions, astronomers have been able to measure the recessional velocities of interstellar gas, which in turn reveals the rotation curve of our Milky Way. Similar measurements have been performed on other galaxies, such as Andromeda. As a diagnostic tool, redshift measurements are one of the most important spectroscopic measurements made in astronomy. The most distant objects exhibit larger redshifts corresponding to the Hubble flow of the universe. The largest-observed redshift, corresponding to the greatest distance and furthest back"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_15",
    "chunk": "in time, is that of the cosmic microwave background radiation; the numerical value of its redshift is about z = 1089 (z = 0 corresponds to present time), and it shows the state of the universe about 13.8 billion years ago, and 379,000 years after the initial moments of the Big Bang. The luminous point-like cores of quasars were the first \"high-redshift\" (z > 0.1) objects discovered before the improvement of telescopes allowed for the discovery of other high-redshift galaxies. For galaxies more distant than the Local Group and the nearby Virgo Cluster, but within a thousand megaparsecs or so, the redshift is approximately proportional to the galaxy's distance. This correlation was first observed by Edwin Hubble and has come to be known as Hubble's law. Vesto Slipher was the first to discover galactic redshifts, in about 1912, while Hubble correlated Slipher's measurements with distances he measured by other means to formulate his Law. Hubble's law follows in part from the Copernican principle. Because it is usually not known how luminous objects are, measuring the redshift is easier than more direct distance measurements, so redshift is sometimes in practice converted to a crude distance measurement using Hubble's law. Gravitational interactions"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_16",
    "chunk": "of galaxies with each other and clusters cause a significant scatter in the normal plot of the Hubble diagram. The peculiar velocities associated with galaxies superimpose a rough trace of the mass of virialized objects in the universe. This effect leads to such phenomena as nearby galaxies (such as the Andromeda Galaxy) exhibiting blueshifts as we fall towards a common barycenter, and redshift maps of clusters showing a fingers of god effect due to the scatter of peculiar velocities in a roughly spherical distribution. This added component gives cosmologists a chance to measure the masses of objects independent of the mass-to-light ratio (the ratio of a galaxy's mass in solar masses to its brightness in solar luminosities), an important tool for measuring dark matter. The Hubble law's linear relationship between distance and redshift assumes that the rate of expansion of the universe is constant. However, when the universe was much younger, the expansion rate, and thus the Hubble \"constant\", was larger than it is today. For more distant galaxies, then, whose light has been travelling to us for much longer times, the approximation of constant expansion rate fails, and the Hubble law becomes a non-linear integral relationship and dependent on"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_17",
    "chunk": "the history of the expansion rate since the emission of the light from the galaxy in question. Observations of the redshift-distance relationship can be used, then, to determine the expansion history of the universe and thus the matter and energy content. While it was long believed that the expansion rate has been continuously decreasing since the Big Bang, observations beginning in 1988 of the redshift-distance relationship using Type Ia supernovae have suggested that in comparatively recent times the expansion rate of the universe has begun to accelerate. The most reliable redshifts are from spectroscopic data, and the highest-confirmed spectroscopic redshift of a galaxy is that of JADES-GS-z14-0 with a redshift of z = 14.32, corresponding to 290 million years after the Big Bang. The previous record was held by GN-z11, with a redshift of z = 11.1, corresponding to 400 million years after the Big Bang, and by UDFy-38135539 at a redshift of z = 8.6, corresponding to 600 million years after the Big Bang. Slightly less reliable are Lyman-break redshifts, the highest of which is the lensed galaxy A1689-zD1 at a redshift z = 7.5 and the next highest being z = 7.0. The most distant-observed gamma-ray burst with"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_18",
    "chunk": "a spectroscopic redshift measurement was GRB 090423, which had a redshift of z = 8.2. The most distant-known quasar, ULAS J1342+0928, is at z = 7.54. The highest-known redshift radio galaxy (TGSS1530) is at a redshift z = 5.72 and the highest-known redshift molecular material is the detection of emission from the CO molecule from the quasar SDSS J1148+5251 at z = 6.42. Extremely red objects (EROs) are astronomical sources of radiation that radiate energy in the red and near infrared part of the electromagnetic spectrum. These may be starburst galaxies that have a high redshift accompanied by reddening from intervening dust, or they could be highly redshifted elliptical galaxies with an older (and therefore redder) stellar population. Objects that are even redder than EROs are termed hyper extremely red objects (HEROs). The cosmic microwave background has a redshift of z = 1089, corresponding to an age of approximately 379,000 years after the Big Bang and a proper distance of more than 46 billion light-years. This redshift corresponds to a shift in average temperature from 3000K down to 3K. The yet-to-be-observed first light from the oldest Population III stars, not long after atoms first formed and the CMB ceased to"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_19",
    "chunk": "be absorbed almost completely, may have redshifts in the range of 20 < z < 100. Other high-redshift events predicted by physics but not presently observable are the cosmic neutrino background from about two seconds after the Big Bang (and a redshift in excess of z > 10) and the cosmic gravitational wave background emitted directly from inflation at a redshift in excess of z > 10. In June 2015, astronomers reported evidence for Population III stars in the Cosmos Redshift 7 galaxy at z = 6.60. Such stars are likely to have existed in the very early universe (i.e., at high redshift), and may have started the production of chemical elements heavier than hydrogen that are needed for the later formation of planets and life as we know it. With advent of automated telescopes and improvements in spectroscopes, a number of collaborations have been made to map the universe in redshift space. By combining redshift with angular position data, a redshift survey maps the 3D distribution of matter within a field of the sky. These observations are used to measure properties of the large-scale structure of the universe. The Great Wall, a vast supercluster of galaxies over 500 million"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_20",
    "chunk": "light-years wide, provides a dramatic example of a large-scale structure that redshift surveys can detect. The first redshift survey was the CfA Redshift Survey, started in 1977 with the initial data collection completed in 1982. More recently, the 2dF Galaxy Redshift Survey determined the large-scale structure of one section of the universe, measuring redshifts for over 220,000 galaxies; data collection was completed in 2002, and the final data set was released 30 June 2003. The Sloan Digital Sky Survey (SDSS), is ongoing as of 2013 and aims to measure the redshifts of around 3 million objects. SDSS has recorded redshifts for galaxies as high as 0.8, and has been involved in the detection of quasars beyond z = 6. The DEEP2 Redshift Survey uses the Keck telescopes with the new \"DEIMOS\" spectrograph; a follow-up to the pilot program DEEP1, DEEP2 is designed to measure faint galaxies with redshifts 0.7 and above, and it is therefore planned to provide a high-redshift complement to SDSS and 2dF. The interactions and phenomena summarized in the subjects of radiative transfer and physical optics can result in shifts in the wavelength and frequency of electromagnetic radiation. In such cases, the shifts correspond to a physical"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_21",
    "chunk": "energy transfer to matter or other photons rather than being by a transformation between reference frames. Such shifts can be from such physical phenomena as coherence effects or the scattering of electromagnetic radiation whether from charged elementary particles, from particulates, or from fluctuations of the index of refraction in a dielectric medium as occurs in the radio phenomenon of radio whistlers. While such phenomena are sometimes referred to as \"redshifts\" and \"blueshifts\", in astrophysics light-matter interactions that result in energy shifts in the radiation field are generally referred to as \"reddening\" rather than \"redshifting\" which, as a term, is normally reserved for the effects discussed above. In many circumstances scattering causes radiation to redden because entropy results in the predominance of many low-energy photons over few high-energy ones (while conserving total energy). Except possibly under carefully controlled conditions, scattering does not produce the same relative change in wavelength across the whole spectrum; that is, any calculated z is generally a function of wavelength. Furthermore, scattering from random media generally occurs at many angles, and z is a function of the scattering angle. If multiple scattering occurs, or the scattering particles have relative motion, then there is generally distortion of spectral"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_22",
    "chunk": "lines as well. In interstellar astronomy, visible spectra can appear redder due to scattering processes in a phenomenon referred to as interstellar reddening—similarly Rayleigh scattering causes the atmospheric reddening of the Sun seen in the sunrise or sunset and causes the rest of the sky to have a blue color. This phenomenon is distinct from redshifting because the spectroscopic lines are not shifted to other wavelengths in reddened objects and there is an additional dimming and distortion associated with the phenomenon due to photons being scattered in and out of the line of sight. The opposite of a redshift is a blueshift. A blueshift is any decrease in wavelength (increase in energy), with a corresponding increase in frequency, of an electromagnetic wave. In visible light, this shifts a color towards the blue end of the spectrum. Doppler blueshift is caused by movement of a source towards the observer. The term applies to any decrease in wavelength and increase in frequency caused by relative motion, even outside the visible spectrum. Only objects moving at near-relativistic speeds toward the observer are noticeably bluer to the naked eye, but the wavelength of any reflected or emitted photon or other particle is shortened in"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_23",
    "chunk": "the direction of travel. Unlike the relative Doppler blueshift, caused by movement of a source towards the observer and thus dependent on the received angle of the photon, gravitational blueshift is absolute and does not depend on the received angle of the photon: Photons climbing out of a gravitating object become less energetic. This loss of energy is known as a \"redshifting\", as photons in the visible spectrum would appear more red. Similarly, photons falling into a gravitational field become more energetic and exhibit a blueshifting. ... Note that the magnitude of the redshifting (blueshifting) effect is not a function of the emitted angle or the received angle of the photon—it depends only on how far radially the photon had to climb out of (fall into) the potential well. It is a natural consequence of conservation of energy and mass–energy equivalence, and was confirmed experimentally in 1959 with the Pound–Rebka experiment. Gravitational blueshift contributes to cosmic microwave background (CMB) anisotropy via the Sachs–Wolfe effect: when a gravitational well evolves while a photon is passing, the amount of blueshift on approach will differ from the amount of gravitational redshift as it leaves the region. There are faraway active galaxies that show"
  },
  {
    "source": "Redshift.txt",
    "chunk_id": "Redshift.txt_24",
    "chunk": "a blueshift in their [O III] emission lines. One of the largest blueshifts is found in the narrow-line quasar, PG 1543+489, which has a relative velocity of −1150 km/s. These types of galaxies are called \"blue outliers\". In a hypothetical universe undergoing a runaway Big Crunch contraction, a cosmological blueshift would be observed, with galaxies further away being increasingly blueshifted—the exact opposite of the actually observed cosmological redshift in the present expanding universe."
  },
  {
    "source": "Regiomontanus.txt",
    "chunk_id": "Regiomontanus.txt_0",
    "chunk": "# Regiomontanus Johannes Müller von Königsberg (6 June 1436 – 6 July 1476), better known as Regiomontanus (/ˌriːdʒioʊmɒnˈteɪnəs/), was a mathematician, astrologer and astronomer of the German Renaissance, active in Vienna, Buda and Nuremberg. His contributions were instrumental in the development of Copernican heliocentrism in the decades following his death. Regiomontanus wrote under the Latinized name of Ioannes de Monteregio (or Monte Regio; Regio Monte); the toponym Regiomontanus was first used by Philipp Melanchthon in 1534. He is named after Königsberg in Lower Franconia, not the larger Königsberg (modern Kaliningrad) in Prussia. Although little is known of Regiomontanus' early life, it is believed that at eleven years of age, he became a student at the University of Leipzig, Saxony. In 1451 he continued his studies at Alma Mater Rudolfina, the university in Vienna, in the Duchy of Austria, where he became a pupil and friend of Georg von Peuerbach. In 1452 he was awarded his bachelor's degree (baccalaureus), and he was awarded his master's degree (magister artium) at the age of 21 in 1457. He lectured in optics and ancient literature. In 1460 the papal legate Basilios Bessarion came to Vienna on a diplomatic mission. Being a humanist scholar with"
  },
  {
    "source": "Regiomontanus.txt",
    "chunk_id": "Regiomontanus.txt_1",
    "chunk": "a great interest in the mathematical sciences, Bessarion sought out Peuerbach's company. George of Trebizond who was Bessarion's philosophical rival had recently produced a new Latin translation of Ptolemy's Almagest from the Greek, which Bessarion, correctly, regarded as inaccurate and badly translated, so he asked Peuerbach to produce a new one. Peuerbach's Greek was not good enough to do a translation but he knew the Almagest intimately so instead he started work on a modernised, improved abridgement of the work. Bessarion also invited Peuerbach to become part of his household and to accompany him back to Italy when his work in Vienna was finished. Peuerbach accepted the invitation on the condition that Regiomontanus could also accompany them. However Peuerbach fell ill in 1461 and died having completed only the first six books of his abridgement of the Almagest. On his death bed Peuerbach made Regiomontanus promise to finish the book and publish it. In 1461 Regiomontanus left Vienna with Bessarion and spent the next four years travelling around Northern Italy as a member of Bessarion's household, looking for and copying mathematical and astronomical manuscripts for Bessarion, who possessed the largest private library in Europe at the time. Regiomontanus also made"
  },
  {
    "source": "Regiomontanus.txt",
    "chunk_id": "Regiomontanus.txt_2",
    "chunk": "the acquaintance of the leading Italian mathematicians of the age such as Giovanni Bianchini and Paolo dal Pozzo Toscanelli who had also been friends of Peuerbach during his prolonged stay in Italy more than twenty years earlier. In 1467, he went to work for János Vitéz, archbishop of Esztergom. There he calculated extensive astronomical tables and built astronomical instruments. Next he went to Buda, and the court of Matthias Corvinus of Hungary, for whom he built an astrolabe, and where he collated Greek manuscripts for a handsome salary. The trigonometric tables that he created while living in Hungary, his Tabulae directionum profectionumque (printed posthum., 1490), were designed for astrology, including finding astrological houses. The Tabulae also contained several tangent tables. In 1471 Regiomontanus moved to the Free City of Nuremberg, in Franconia, then one of the Empire's important seats of learning, publication, commerce and art, where he worked with the humanist and merchant Bernhard Walther. Here he founded the world's first scientific printing press, and in 1472 he published the first printed astronomical textbook, the Theoricae novae Planetarum of his teacher Georg von Peurbach. Regiomontanus and Bernhard Walther observed the comet of 1472. Regiomontanus tried to estimate its distance from"
  },
  {
    "source": "Regiomontanus.txt",
    "chunk_id": "Regiomontanus.txt_3",
    "chunk": "Earth, using the angle of parallax. According to David A. Seargeant: In agreement with the prevailing Aristotelian theory on comets as atmospheric phenomena, he estimated its distance to be at least 8,200 miles (13,120 km) and, from this, estimated the central condensation as 26, and the entire coma as 81 miles (41.6 and 129.6 km respectively) in diameter. These values, of course, fail by orders of magnitude, but he is to be commended for this attempt at determining the physical dimensions of the comet. The 1472 comet was visible from Christmas Day 1471 to 1 March 1472 (Julian Calendar), a total of 59 days. In 1475, Regiomontanus was called to Rome by Pope Sixtus IV on to work on the planned calendar reform. Sixtus promised substantial rewards, including the title of bishop of Regensburg, but it is unlikely that he was actually appointed to the role. On his way to Rome, stopping in Venice, he commissioned the publication of his Calendarium with Erhard Ratdolt (printed in 1476). Regiomontanus reached Rome, but he died there after only a few months, in his 41st year, on 6 July 1476. According to a rumor repeated by Gassendi in his Regiomontanus biography, he was"
  },
  {
    "source": "Regiomontanus.txt",
    "chunk_id": "Regiomontanus.txt_4",
    "chunk": "poisoned by relatives of George of Trebizond whom he had criticized in his writing; it is however considered more likely that he died from the plague. During his time in Italy he completed Peuerbach's abridgement of Almagest, Epytoma in almagesti Ptolemei. In 1464, he completed De triangulis omnimodis (\"On Triangles of All Kinds\"). De triangulis omnimodis was one of the first textbooks presenting the current state of trigonometry and included lists of questions for review of individual chapters. In it he wrote: You who wish to study great and wonderful things, who wonder about the movement of the stars, must read these theorems about triangles. Knowing these ideas will open the door to all of astronomy and to certain geometric problems. In Epytoma in almagesti Ptolemei, he critiqued the translation of Almagest by George of Trebizond, pointing out inaccuracies. Later Nicolaus Copernicus would refer to this book as an influence on his own work. A prolific author, Regiomontanus was internationally famous in his lifetime. Despite having completed only a quarter of what he had intended to write, he left a substantial body of work. Nicolaus Copernicus' teacher, Domenico Maria Novara da Ferrara, referred to Regiomontanus as having been his own"
  },
  {
    "source": "Regiomontanus.txt",
    "chunk_id": "Regiomontanus.txt_5",
    "chunk": "teacher. There is speculation that Regiomontanus had arrived at a theory of heliocentrism before he died; a manuscript shows particular attention to the heliocentric theory of the Pythagorean Aristarchus, mention was also given to the motion of the earth in a letter to a friend. Much of the material on spherical trigonometry in Regiomontanus' On Triangles was taken directly from the twelfth-century work of Jabir ibn Aflah otherwise known as Geber, as noted in the sixteenth century by Gerolamo Cardano. Simon Stevin, in his book describing decimal representation of fractions (De Thiende), cites the trigonometric tables of Regiomontanus as suggestive of positional notation. Regiomontanus designed his own astrological house system, which became one of the most popular systems in Europe. In 1561, Daniel Santbech compiled a collected edition of the works of Regiomontanus, De triangulis planis et sphaericis libri quinque (first published in 1533) and Compositio tabularum sinum recto, as well as Santbech's own Problematum astronomicorum et geometricorum sectiones septem. It was published in Basel by Henrich Petri and Petrus Perna. There is an image of him in Hartmann Schedel's 1493 Nuremberg Chronicle. He is holding an astrolabe. Yet, although there are thirteen illustrations of comets in the Chronicle (from"
  },
  {
    "source": "Regiomontanus.txt",
    "chunk_id": "Regiomontanus.txt_6",
    "chunk": "471 to 1472), they are stylized, rather than representing the actual objects."
  },
  {
    "source": "Regulation.txt",
    "chunk_id": "Regulation.txt_0",
    "chunk": "# Regulation Regulation is the management of complex systems according to a set of rules and trends. In systems theory, these types of rules exist in various fields of biology and society, but the term has slightly different meanings according to context. For example: Regulation in the social, political, psychological, and economic domains can take many forms: legal restrictions promulgated by a government authority, contractual obligations (for example, contracts between insurers and their insureds), self-regulation in psychology, social regulation (e.g. norms), co-regulation, third-party regulation, certification, accreditation or market regulation. State-mandated regulation is government intervention in the private market in an attempt to implement policy and produce outcomes which might not otherwise occur, ranging from consumer protection to faster growth or technological advancement. The regulations may prescribe or proscribe conduct (\"command-and-control\" regulation), calibrate incentives (\"incentive\" regulation), or change preferences (\"preferences shaping\" regulation). Common examples of regulation include limits on environmental pollution, laws against child labor or other employment regulations, minimum wages laws, regulations requiring truthful labelling of the ingredients in food and drugs, and food and drug safety regulations establishing minimum standards of testing and quality for what can be sold, and zoning and development approvals regulation. Much less common are"
  },
  {
    "source": "Regulation.txt",
    "chunk_id": "Regulation.txt_1",
    "chunk": "controls on market entry, or price regulation. One critical question in regulation is whether the regulator or government has sufficient information to make ex-ante regulation more efficient than ex-post liability for harm and whether industry self-regulation might be preferable. The economics of imposing or removing regulations relating to markets is analysed in empirical legal studies, law and economics, political science, environmental science, health economics, and regulatory economics. Power to regulate should include the power to enforce regulatory decisions. Monitoring is an important tool used by national regulatory authorities in carrying out the regulated activities. In some countries (in particular the Scandinavian countries) industrial relations are to a very high degree regulated by the labour market parties themselves (self-regulation) in contrast to state regulation of minimum wages etc. Regulation can be assessed for different countries through various quantitative measures. The Global Indicators of Regulatory Governance by World Bank's Global Indicators Group scores 186 countries on transparency around proposed regulations, consultation on their content, the use of regulatory impact assessments and the access to enacted laws on a scale from 0 to 5. The V-Dem Democracy indices include the regulatory quality indicator. The QuantGov project at the Mercatus Center tracks the count"
  },
  {
    "source": "Regulation.txt",
    "chunk_id": "Regulation.txt_2",
    "chunk": "of regulations by topic for United States, Canada, and Australia. The length of Code of Federal Regulations of the United States increased over time. Regulation of businesses existed in the ancient early Egyptian, Indian, Greek, and Roman civilizations. Standardized weights and measures existed to an extent in the ancient world, and gold may have operated to some degree as an international currency. In China, a national currency system existed and paper currency was invented. Sophisticated law existed in Ancient Rome. In the European Early Middle Ages, law and standardization declined with the Roman Empire, but regulation existed in the form of norms, customs, and privileges; this regulation was aided by the unified Christian identity and a sense of honor regarding contracts. Modern industrial regulation can be traced to the Railway Regulation Act 1844 in the United Kingdom, and succeeding Acts. Beginning in the late 19th and 20th centuries, much of regulation in the United States was administered and enforced by regulatory agencies which produced their own administrative law and procedures under the authority of statutes. Legislators created these agencies to require experts in the industry to focus their attention on the issue. At the federal level, one of the earliest"
  },
  {
    "source": "Regulation.txt",
    "chunk_id": "Regulation.txt_3",
    "chunk": "institutions was the Interstate Commerce Commission which had its roots in earlier state-based regulatory commissions and agencies. Later agencies include the Federal Trade Commission, Securities and Exchange Commission, Civil Aeronautics Board, and various other institutions. These institutions vary from industry to industry and at the federal and state level. Individual agencies do not necessarily have clear life-cycles or patterns of behavior, and they are influenced heavily by their leadership and staff as well as the organic law creating the agency. In the 1930s, lawmakers believed that unregulated business often led to injustice and inefficiency; in the 1960s and 1970s, concern shifted to regulatory capture, which led to extremely detailed laws creating the United States Environmental Protection Agency and Occupational Safety and Health Administration."
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_0",
    "chunk": "# Schwarzschild metric In Einstein's theory of general relativity, the Schwarzschild metric (also known as the Schwarzschild solution) is an exact solution to the Einstein field equations that describes the gravitational field outside a spherical mass, on the assumption that the electric charge of the mass, angular momentum of the mass, and universal cosmological constant are all zero. The solution is a useful approximation for describing slowly rotating astronomical objects such as many stars and planets, including Earth and the Sun. It was found by Karl Schwarzschild in 1916. According to Birkhoff's theorem, the Schwarzschild metric is the most general spherically symmetric vacuum solution of the Einstein field equations. A Schwarzschild black hole or static black hole is a black hole that has neither electric charge nor angular momentum (non-rotating). A Schwarzschild black hole is described by the Schwarzschild metric, and cannot be distinguished from any other Schwarzschild black hole except by its mass. The Schwarzschild black hole is characterized by a surrounding spherical boundary, called the event horizon, which is situated at the Schwarzschild radius ( r s {\\displaystyle r_{\\text{s}}} ), often called the radius of a black hole. The boundary is not a physical surface, and a person"
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_1",
    "chunk": "who fell through the event horizon (before being torn apart by tidal forces) would not notice any physical surface at that position; it is a mathematical surface which is significant in determining the black hole's properties. Any non-rotating and non-charged mass that is smaller than its Schwarzschild radius forms a black hole. The solution of the Einstein field equations is valid for any mass M, so in principle (within the theory of general relativity) a Schwarzschild black hole of any mass could exist if conditions became sufficiently favorable to allow for its formation. In the vicinity of a Schwarzschild black hole, space curves so much that even light rays are deflected, and very nearby light can be deflected so much that it travels several times around the black hole. The Schwarzschild metric is a spherically symmetric Lorentzian metric (here, with signature convention (+, -, -, -)), defined on (a subset of) R × ( E 3 − O ) ≅ R × ( 0 , ∞ ) × S 2 {\\displaystyle \\mathbb {R} \\times \\left(E^{3}-O\\right)\\cong \\mathbb {R} \\times (0,\\infty )\\times S^{2}} where E 3 {\\displaystyle E^{3}} is 3 dimensional Euclidean space, and S 2 ⊂ E 3 {\\displaystyle S^{2}\\subset E^{3}}"
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_2",
    "chunk": "is the two sphere. The rotation group S O ( 3 ) = S O ( E 3 ) {\\displaystyle \\mathrm {SO} (3)=\\mathrm {SO} (E^{3})} acts on the E 3 − O {\\displaystyle E^{3}-O} or S 2 {\\displaystyle S^{2}} factor as rotations around the center O {\\displaystyle O} , while leaving the first R {\\displaystyle \\mathbb {R} } factor unchanged. The Schwarzschild metric is a solution of Einstein's field equations in empty space, meaning that it is valid only outside the gravitating body. That is, for a spherical body of radius R {\\displaystyle R} the solution is valid for r > R {\\displaystyle r>R} . To describe the gravitational field both inside and outside the gravitating body the Schwarzschild solution must be matched with some suitable interior solution at ⁠ r = R {\\displaystyle r=R} ⁠, such as the interior Schwarzschild metric. In Schwarzschild coordinates ( t , r , θ , ϕ ) {\\displaystyle (t,r,\\theta ,\\phi )} the Schwarzschild metric (or equivalently, the line element for proper time) has the form d s 2 = c 2 d τ 2 = ( 1 − r s r ) c 2 d t 2 − ( 1 − r s"
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_3",
    "chunk": "r ) − 1 d r 2 − r 2 d Ω 2 , {\\displaystyle {ds}^{2}=c^{2}\\,{d\\tau }^{2}=\\left(1-{\\frac {r_{\\mathrm {s} }}{r}}\\right)c^{2}\\,dt^{2}-\\left(1-{\\frac {r_{\\mathrm {s} }}{r}}\\right)^{-1}\\,dr^{2}-r^{2}{d\\Omega }^{2},} where d Ω 2 {\\displaystyle {d\\Omega }^{2}} is the metric on the two sphere, i.e. ⁠ d Ω 2 = ( d θ 2 + sin 2 ⁡ θ d ϕ 2 ) {\\displaystyle {d\\Omega }^{2}=\\left(d\\theta ^{2}+\\sin ^{2}\\theta \\,d\\phi ^{2}\\right)} ⁠. Furthermore, The Schwarzschild metric has a singularity for r = 0, which is an intrinsic curvature singularity. It also seems to have a singularity on the event horizon r = rs. Depending on the point of view, the metric is therefore defined only on the exterior region r > r s {\\displaystyle r>r_{\\text{s}}} , only on the interior region r < r s {\\displaystyle r<r_{\\text{s}}} or their disjoint union. However, the metric is actually non-singular across the event horizon, as one sees in suitable coordinates (see below). For ⁠ r ≫ r s {\\displaystyle r\\gg r_{\\text{s}}} ⁠, the Schwarzschild metric is asymptotic to the standard Lorentz metric on Minkowski space. For almost all astrophysical objects, the ratio r s R {\\displaystyle {\\frac {r_{\\text{s}}}{R}}} is extremely small. For example, the Schwarzschild radius r s ( Earth"
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_4",
    "chunk": ") {\\displaystyle r_{\\text{s}}^{({\\text{Earth}})}} of the Earth is roughly 8.9 mm, while the Sun, which is 3.3×10 times as massive has a Schwarzschild radius r s ( Sun ) {\\displaystyle r_{\\text{s}}^{({\\text{Sun}})}} of approximately 3.0 km. The ratio becomes large only in close proximity to black holes and other ultra-dense objects such as neutron stars. The radial coordinate turns out to have physical significance as the \"proper distance between two events that occur simultaneously relative to the radially moving geodesic clocks, the two events lying on the same radial coordinate line\". The Schwarzschild solution is analogous to a classical Newtonian theory of gravity that corresponds to the gravitational field around a point particle. Even at the surface of the Earth, the corrections to Newtonian gravity are only one part in a billion. The Schwarzschild solution is named in honour of Karl Schwarzschild, who found the exact solution in 1915 and published it in January 1916, a little more than a month after the publication of Einstein's theory of general relativity. It was the first exact solution of the Einstein field equations other than the trivial flat space solution. Schwarzschild died shortly after his paper was published, as a result of a disease"
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_5",
    "chunk": "(thought to be pemphigus) he developed while serving in the German army during World War I. Johannes Droste in 1916 independently produced the same solution as Schwarzschild, using a simpler, more direct derivation. In the early years of general relativity there was a lot of confusion about the nature of the singularities found in the Schwarzschild and other solutions of the Einstein field equations. In Schwarzschild's original paper, he put what we now call the event horizon at the origin of his coordinate system. In this paper he also introduced what is now known as the Schwarzschild radial coordinate (r in the equations above), as an auxiliary variable. In his equations, Schwarzschild was using a different radial coordinate that was zero at the Schwarzschild radius. A more complete analysis of the singularity structure was given by David Hilbert in the following year, identifying the singularities both at r = 0 and r = rs. Although there was general consensus that the singularity at r = 0 was a 'genuine' physical singularity, the nature of the singularity at r = rs remained unclear. In 1921, Paul Painlevé and in 1922 Allvar Gullstrand independently produced a metric, a spherically symmetric solution of"
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_6",
    "chunk": "Einstein's equations, which we now know is coordinate transformation of the Schwarzschild metric, Gullstrand–Painlevé coordinates, in which there was no singularity at r = rs. They, however, did not recognize that their solutions were just coordinate transforms, and in fact used their solution to argue that Einstein's theory was wrong. In 1924 Arthur Eddington produced the first coordinate transformation (Eddington–Finkelstein coordinates) that showed that the singularity at r = rs was a coordinate artifact, although he also seems to have been unaware of the significance of this discovery. Later, in 1932, Georges Lemaître gave a different coordinate transformation (Lemaître coordinates) to the same effect and was the first to recognize that this implied that the singularity at r = rs was not physical. In 1939 Howard Robertson showed that a free falling observer descending in the Schwarzschild metric would cross the r = rs singularity in a finite amount of proper time even though this would take an infinite amount of time in terms of coordinate time t. In 1950, John Synge produced a paper that showed the maximal analytic extension of the Schwarzschild metric, again showing that the singularity at r = rs was a coordinate artifact and that"
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_7",
    "chunk": "it represented two horizons. A similar result was later rediscovered by George Szekeres, and independently Martin Kruskal. The new coordinates nowadays known as Kruskal–Szekeres coordinates were much simpler than Synge's but both provided a single set of coordinates that covered the entire spacetime. However, perhaps due to the obscurity of the journals in which the papers of Lemaître and Synge were published their conclusions went unnoticed, with many of the major players in the field including Einstein believing that the singularity at the Schwarzschild radius was physical. Synge's later derivation of the Kruskal–Szekeres metric solution, which was motivated by a desire to avoid \"using 'bad' [Schwarzschild] coordinates to obtain 'good' [Kruskal–Szekeres] coordinates\", has been generally under-appreciated in the literature, but was adopted by Chandrasekhar in his black hole monograph. Real progress was made in the 1960s when the mathematically rigorous formulation cast in terms of differential geometry entered the field of general relativity, allowing more exact definitions of what it means for a Lorentzian manifold to be singular. This led to definitive identification of the r = rs singularity in the Schwarzschild metric as an event horizon, i.e., a hypersurface in spacetime that can be crossed in only one direction."
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_8",
    "chunk": "The Schwarzschild solution appears to have singularities at r = 0 and r = rs; some of the metric components \"blow up\" (entail division by zero or multiplication by infinity) at these radii. Since the Schwarzschild metric is expected to be valid only for those radii larger than the radius R of the gravitating body, there is no problem as long as R > rs. For ordinary stars and planets this is always the case. For example, the radius of the Sun is approximately 700000 km, while its Schwarzschild radius is only 3 km. The singularity at r = rs divides the Schwarzschild coordinates in two disconnected patches. The exterior Schwarzschild solution with r > rs is the one that is related to the gravitational fields of stars and planets. The interior Schwarzschild solution with 0 ≤ r < rs, which contains the singularity at r = 0, is completely separated from the outer patch by the singularity at r = rs. The Schwarzschild coordinates therefore give no physical connection between the two patches, which may be viewed as separate solutions. The singularity at r = rs is an illusion however; it is an instance of what is called a"
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_9",
    "chunk": "coordinate singularity. As the name implies, the singularity arises from a bad choice of coordinates or coordinate conditions. When changing to a different coordinate system (for example Lemaître coordinates, Eddington–Finkelstein coordinates, Kruskal–Szekeres coordinates, Novikov coordinates, or Gullstrand–Painlevé coordinates) the metric becomes regular at r = rs and can extend the external patch to values of r smaller than rs. Using a different coordinate transformation one can then relate the extended external patch to the inner patch. The case r = 0 is different, however. If one asks that the solution be valid for all r one runs into a true physical singularity, or gravitational singularity, at the origin. To see that this is a true singularity one must look at quantities that are independent of the choice of coordinates. One such important quantity is the Kretschmann invariant, which is given by At r = 0 the curvature becomes infinite, indicating the presence of a singularity. At this point the metric cannot be extended in a smooth manner (the Kretschmann invariant involves second derivatives of the metric), spacetime itself is then no longer well-defined. Furthermore, Sbierski showed the metric cannot be extended even in a continuous manner. For a long time"
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_10",
    "chunk": "it was thought that such a solution was non-physical. However, a greater understanding of general relativity led to the realization that such singularities were a generic feature of the theory and not just an exotic special case. The Schwarzschild solution, taken to be valid for all r > 0, is called a Schwarzschild black hole. It is a perfectly valid solution of the Einstein field equations, although (like other black holes) it has rather bizarre properties. For r < rs the Schwarzschild radial coordinate r becomes timelike and the time coordinate t becomes spacelike. A curve at constant r is no longer a possible worldline of a particle or observer, not even if a force is exerted to try to keep it there; this occurs because spacetime has been curved so much that the direction of cause and effect (the particle's future light cone) points into the singularity. The surface r = rs demarcates what is called the event horizon of the black hole. It represents the point past which light can no longer escape the gravitational field. Any physical object whose radius R becomes less than or equal to the Schwarzschild radius has undergone gravitational collapse and become a"
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_11",
    "chunk": "black hole. The Schwarzschild solution can be expressed in a range of different choices of coordinates besides the Schwarzschild coordinates used above. Different choices tend to highlight different features of the solution. The table below shows some popular choices. In table above, some shorthand has been introduced for brevity. The speed of light c has been set to one. The notation is used for the metric of a unit radius 2-dimensional sphere. Moreover, in each entry R and T denote alternative choices of radial and time coordinate for the particular coordinates. Note, the R or T may vary from entry to entry. The Kruskal–Szekeres coordinates have the form to which the Belinski–Zakharov transform can be applied. This implies that the Schwarzschild black hole is a form of gravitational soliton. The spatial curvature of the Schwarzschild solution for r > rs can be visualized as the graphic shows. Consider a constant time equatorial slice H through the Schwarzschild solution by fixing θ = ⁠π/2⁠, t = constant, and letting the remaining Schwarzschild coordinates (r, φ) vary. Imagine now that there is an additional Euclidean dimension w, which has no physical reality (it is not part of spacetime). Then replace the (r,"
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_12",
    "chunk": "φ) plane with a surface dimpled in the w direction according to the equation (Flamm's paraboloid) This surface has the property that distances measured within it match distances in the Schwarzschild metric, because with the definition of w above, Thus, Flamm's paraboloid is useful for visualizing the spatial curvature of the Schwarzschild metric. It should not, however, be confused with a gravity well. No ordinary (massive or massless) particle can have a worldline lying on the paraboloid, since all distances on it are spacelike (this is a cross-section at one moment of time, so any particle moving on it would have an infinite velocity). A tachyon could have a spacelike worldline that lies entirely on a single paraboloid. However, even in that case its geodesic path is not the trajectory one gets through a \"rubber sheet\" analogy of gravitational well: in particular, if the dimple is drawn pointing upward rather than downward, the tachyon's geodesic path still curves toward the central mass, not away. See the gravity well article for more information. Flamm's paraboloid may be derived as follows. The Euclidean metric in the cylindrical coordinates (r, φ, w) is written Letting the surface be described by the function w"
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_13",
    "chunk": "= w(r), the Euclidean metric can be written as Comparing this with the Schwarzschild metric in the equatorial plane (θ = π/2) at a fixed time (t = constant, dt = 0), A particle orbiting in the Schwarzschild metric can have a stable circular orbit with r > 3rs. Circular orbits with r between 1.5rs and 3rs are unstable, and no circular orbits exist for r < 1.5rs. The circular orbit of minimum radius 1.5rs corresponds to an orbital velocity approaching the speed of light. It is possible for a particle to have a constant value of r between rs and 1.5rs, but only if some force acts to keep it there. Noncircular orbits, such as Mercury's, dwell longer at small radii than would be expected in Newtonian gravity. This can be seen as a less extreme version of the more dramatic case in which a particle passes through the event horizon and dwells inside it forever. Intermediate between the case of Mercury and the case of an object falling past the event horizon, there are exotic possibilities such as knife-edge orbits, in which the satellite can be made to execute an arbitrarily large number of nearly circular orbits, after"
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_14",
    "chunk": "which it flies back outward. The isometry group of the Schwarzchild metric is ⁠ R × O ( 3 ) × { ± 1 } {\\displaystyle \\mathbb {R} \\times \\mathrm {O} (3)\\times \\{\\pm 1\\}} ⁠, where O ( 3 ) {\\displaystyle \\mathrm {O} (3)} is the orthogonal group of rotations and reflections in three dimensions, R {\\displaystyle \\mathbb {R} } comprises the time translations, and { ± 1 } {\\displaystyle \\{\\pm 1\\}} is the group generated by time reversal. This is thus the subgroup of the ten-dimensional Poincaré group which takes the time axis (trajectory of the star) to itself. It omits the spatial translations (three dimensions) and boosts (three dimensions). It retains the time translations (one dimension) and rotations (three dimensions). Thus it has four dimensions. Like the Poincaré group, it has four connected components: the component of the identity; the time reversed component; the spatial inversion component; and the component which is both time reversed and spatially inverted. The Ricci curvature scalar and the Ricci curvature tensor are both zero. Non-zero components of the Riemann curvature tensor are given by from which one can see that R γ α γ β = 0 {\\displaystyle R^{\\gamma }{}_{\\alpha \\gamma \\beta"
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_15",
    "chunk": "}=0} . Six of these formulas are Eq. 5.13 in Carroll and imply the other 6 by R α β γ δ = g α κ g β λ R λ κ δ γ {\\displaystyle R^{\\alpha }{}_{\\beta \\gamma \\delta }=g^{\\alpha \\kappa }g_{\\beta \\lambda }R^{\\lambda }{}_{\\kappa \\delta \\gamma }} . Components which are obtainable by other symmetries of the Riemann tensor are not displayed. To understand the physical meaning of these quantities, it is useful to express the curvature tensor in an orthonormal basis. In an orthonormal basis of an observer the non-zero components in geometric units are Again, components which are obtainable by the symmetries of the Riemann tensor are not displayed. These results are invariant to any Lorentz boost, thus the components do not change for non-static observers. The geodesic deviation equation shows that the tidal acceleration between two observers separated by ξ j ^ {\\displaystyle \\xi ^{\\hat {j}}} is D 2 ξ j ^ / D τ 2 = − R j ^ t ^ k ^ t ^ ξ k ^ {\\displaystyle D^{2}\\xi ^{\\hat {j}}/D\\tau ^{2}=-R^{\\hat {j}}{}_{{\\hat {t}}{\\hat {k}}{\\hat {t}}}\\xi ^{\\hat {k}}} , so a body of length L {\\displaystyle L} is stretched in the radial direction"
  },
  {
    "source": "Schwarzschild metric.txt",
    "chunk_id": "Schwarzschild metric.txt_16",
    "chunk": "by an apparent acceleration ( r s / r 3 ) c 2 L {\\displaystyle (r_{\\text{s}}/r^{3})c^{2}L} and squeezed in the perpendicular directions by − ( r s / ( 2 r 3 ) ) c 2 L {\\displaystyle -(r_{\\text{s}}/(2r^{3}))c^{2}L} ."
  },
  {
    "source": "Schwarzschild radius.txt",
    "chunk_id": "Schwarzschild radius.txt_0",
    "chunk": "# Schwarzschild radius The Schwarzschild radius or the gravitational radius is a physical parameter in the Schwarzschild solution to Einstein's field equations that corresponds to the radius defining the event horizon of a Schwarzschild black hole. It is a characteristic radius associated with any quantity of mass. The Schwarzschild radius was named after the German astronomer Karl Schwarzschild, who calculated this exact solution for the theory of general relativity in 1916. The Schwarzschild radius is given as r s = 2 G M c 2 , {\\displaystyle r_{\\text{s}}={\\frac {2GM}{c^{2}}},} where G is the gravitational constant, M is the object mass, and c is the speed of light. In 1916, Karl Schwarzschild obtained the exact solution to Einstein's field equations for the gravitational field outside a non-rotating, spherically symmetric body with mass M {\\displaystyle M} (see Schwarzschild metric). The solution contained terms of the form 1 − r s / r {\\displaystyle 1-{r_{\\text{s}}}/r} and 1 1 − r s / r {\\displaystyle {\\frac {1}{1-{r_{\\text{s}}}/r}}} , which become singular at r = 0 {\\displaystyle r=0} and r = r s {\\displaystyle r=r_{\\text{s}}} respectively. The r s {\\displaystyle r_{\\text{s}}} has come to be known as the Schwarzschild radius. The physical significance of these"
  },
  {
    "source": "Schwarzschild radius.txt",
    "chunk_id": "Schwarzschild radius.txt_1",
    "chunk": "singularities was debated for decades. It was found that the one at r = r s {\\displaystyle r=r_{\\text{s}}} is a coordinate singularity, meaning that it is an artifact of the particular system of coordinates that was used; while the one at r = 0 {\\displaystyle r=0} is a spacetime singularity and cannot be removed. The Schwarzschild radius is nonetheless a physically relevant quantity, as noted above and below. This expression had previously been calculated, using Newtonian mechanics, as the radius of a spherically symmetric body at which the escape velocity was equal to the speed of light. It had been identified in the 18th century by John Michell and Pierre-Simon Laplace. The Schwarzschild radius of an object is proportional to its mass. Accordingly, the Sun has a Schwarzschild radius of approximately 3.0 km (1.9 mi), whereas Earth's is approximately 9 mm (0.35 in) and the Moon's is approximately 0.1 mm (0.0039 in). The simplest way of deriving the Schwarzschild radius comes from the equality of the modulus of a spherical solid mass' rest energy with its gravitational energy: Any object whose radius is smaller than its Schwarzschild radius is called a black hole. The surface at the Schwarzschild radius acts"
  },
  {
    "source": "Schwarzschild radius.txt",
    "chunk_id": "Schwarzschild radius.txt_2",
    "chunk": "as an event horizon in a non-rotating body (a rotating black hole operates slightly differently). Neither light nor particles can escape through this surface from the region inside, hence the name \"black hole\". Black holes can be classified based on their Schwarzschild radius, or equivalently, by their density, where density is defined as mass of a black hole divided by the volume of its Schwarzschild sphere. As the Schwarzschild radius is linearly related to mass, while the enclosed volume corresponds to the third power of the radius, small black holes are therefore much more dense than large ones. The volume enclosed in the event horizon of the most massive black holes has an average density lower than main sequence stars. A supermassive black hole (SMBH) is the largest type of black hole, though there are few official criteria on how such an object is considered so, on the order of hundreds of thousands to billions of solar masses. (Supermassive black holes up to 21 billion (2.1 × 10) M☉ have been detected, such as NGC 4889.) Unlike stellar mass black holes, supermassive black holes have comparatively low average densities. (Note that a (non-rotating) black hole is a spherical region in"
  },
  {
    "source": "Schwarzschild radius.txt",
    "chunk_id": "Schwarzschild radius.txt_3",
    "chunk": "space that surrounds the singularity at its center; it is not the singularity itself.) With that in mind, the average density of a supermassive black hole can be less than the density of water. The Schwarzschild radius of a body is proportional to its mass and therefore to its volume, assuming that the body has a constant mass-density. In contrast, the physical radius of the body is proportional to the cube root of its volume. Therefore, as the body accumulates matter at a given fixed density (in this example, 997 kg/m, the density of water), its Schwarzschild radius will increase more quickly than its physical radius. When a body of this density has grown to around 136 million solar masses (1.36 × 10 M☉), its physical radius would be overtaken by its Schwarzschild radius, and thus it would form a supermassive black hole. It is thought that supermassive black holes like these do not form immediately from the singular collapse of a cluster of stars. Instead they may begin life as smaller, stellar-sized black holes and grow larger by the accretion of matter, or even of other black holes. The Schwarzschild radius of the supermassive black hole at the Galactic"
  },
  {
    "source": "Schwarzschild radius.txt",
    "chunk_id": "Schwarzschild radius.txt_4",
    "chunk": "Center of the Milky Way is approximately 12 million kilometres. Its mass is about 4.1 million M☉. Stellar black holes have much greater average densities than supermassive black holes. If one accumulates matter at nuclear density (the density of the nucleus of an atom, about 10 kg/m; neutron stars also reach this density), such an accumulation would fall within its own Schwarzschild radius at about 3 M☉ and thus would be a stellar black hole. A small mass has an extremely small Schwarzschild radius. A black hole of mass similar to that of Mount Everest, 6.3715×10 kg, would have a Schwarzschild radius much smaller than a nanometre. The Schwarzschild radius would be 2 × 6.6738×10 m⋅kg⋅s × 6.3715×10 kg / (299792458 m⋅s) = 9.46×10 m = 9.46×10 nm. Its average density at that size would be so high that no known mechanism could form such extremely compact objects. Such black holes might possibly be formed in an early stage of the evolution of the universe, just after the Big Bang, when densities of matter were extremely high. Therefore, these hypothetical miniature black holes are called primordial black holes. When moving to the Planck scale ℓ P = ( G /"
  },
  {
    "source": "Schwarzschild radius.txt",
    "chunk_id": "Schwarzschild radius.txt_5",
    "chunk": "c 3 ) ℏ {\\displaystyle \\ell _{P}={\\sqrt {(G/c^{3})\\,\\hbar }}} ≈ 10 m, it is convenient to write the gravitational radius in the form r s = 2 ( G / c 3 ) M c {\\displaystyle r_{s}=2\\,(G/c^{3})Mc} , (see also virtual black hole). Gravitational time dilation near a large, slowly rotating, nearly spherical body, such as the Earth or Sun can be reasonably approximated as follows: t r t = 1 − r s r {\\displaystyle {\\frac {t_{r}}{t}}={\\sqrt {1-{\\frac {r_{\\mathrm {s} }}{r}}}}} where: The Schwarzschild radius ( 2 G M / c 2 {\\displaystyle 2GM/c^{2}} ) and the Compton wavelength ( 2 π ℏ / M c {\\displaystyle 2\\pi \\hbar /Mc} ) corresponding to a given mass are similar when the mass is around one Planck mass ( M = ℏ c / G {\\textstyle M={\\sqrt {\\hbar c/G}}} ), when both are of the same order as the Planck length ( ℏ G / c 3 {\\textstyle {\\sqrt {\\hbar G/c^{3}}}} ). Thus, r s r ∼ ℓ P 2 {\\displaystyle r_{s}r\\sim \\ell _{P}^{2}} or Δ r s Δ r ≥ ℓ P 2 {\\displaystyle \\Delta r_{s}\\Delta r\\geq \\ell _{P}^{2}} , which is another form of the Heisenberg uncertainty principle on"
  },
  {
    "source": "Schwarzschild radius.txt",
    "chunk_id": "Schwarzschild radius.txt_6",
    "chunk": "the Planck scale. (See also Virtual black hole). The Schwarzschild radius equation can be manipulated to yield an expression that gives the largest possible radius from an input density that doesn't form a black hole. Taking the input density as ρ, For example, the density of water is 1000 kg/m. This means the largest amount of water you can have without forming a black hole would have a radius of 400 920 754 km (about 2.67 AU)."
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_0",
    "chunk": "# Scientific law Scientific laws or laws of science are statements, based on repeated experiments or observations, that describe or predict a range of natural phenomena. The term law has diverse usage in many cases (approximate, accurate, broad, or narrow) across all fields of natural science (physics, chemistry, astronomy, geoscience, biology). Laws are developed from data and can be further developed through mathematics; in all cases they are directly or indirectly based on empirical evidence. It is generally understood that they implicitly reflect, though they do not explicitly assert, causal relationships fundamental to reality, and are discovered rather than invented. Scientific laws summarize the results of experiments or observations, usually within a certain range of application. In general, the accuracy of a law does not change when a new theory of the relevant phenomenon is worked out, but rather the scope of the law's application, since the mathematics or statement representing the law does not change. As with other kinds of scientific knowledge, scientific laws do not express absolute certainty, as mathematical laws do. A scientific law may be contradicted, restricted, or extended by future observations. A law can often be formulated as one or several statements or equations, so"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_1",
    "chunk": "that it can predict the outcome of an experiment. Laws differ from hypotheses and postulates, which are proposed during the scientific process before and during validation by experiment and observation. Hypotheses and postulates are not laws, since they have not been verified to the same degree, although they may lead to the formulation of laws. Laws are narrower in scope than scientific theories, which may entail one or several laws. Science distinguishes a law or theory from facts. Calling a law a fact is ambiguous, an overstatement, or an equivocation. The nature of scientific laws has been much discussed in philosophy, but in essence scientific laws are simply empirical conclusions reached by the scientific method; they are intended to be neither laden with ontological commitments nor statements of logical absolutes. Social sciences such as economics have also attempted to formulate scientific laws, though these generally have much less predictive power. A scientific law always applies to a physical system under repeated conditions, and it implies that there is a causal relationship involving the elements of the system. Factual and well-confirmed statements like \"Mercury is liquid at standard temperature and pressure\" are considered too specific to qualify as scientific laws. A"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_2",
    "chunk": "central problem in the philosophy of science, going back to David Hume, is that of distinguishing causal relationships (such as those implied by laws) from principles that arise due to constant conjunction. Laws differ from scientific theories in that they do not posit a mechanism or explanation of phenomena: they are merely distillations of the results of repeated observation. As such, the applicability of a law is limited to circumstances resembling those already observed, and the law may be found to be false when extrapolated. Ohm's law only applies to linear networks; Newton's law of universal gravitation only applies in weak gravitational fields; the early laws of aerodynamics, such as Bernoulli's principle, do not apply in the case of compressible flow such as occurs in transonic and supersonic flight; Hooke's law only applies to strain below the elastic limit; Boyle's law applies with perfect accuracy only to the ideal gas, etc. These laws remain useful, but only under the specified conditions where they apply. Many laws take mathematical forms, and thus can be stated as an equation; for example, the law of conservation of energy can be written as Δ E = 0 {\\displaystyle \\Delta E=0} , where E {\\displaystyle"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_3",
    "chunk": "E} is the total amount of energy in the universe. Similarly, the first law of thermodynamics can be written as d U = δ Q − δ W {\\displaystyle \\mathrm {d} U=\\delta Q-\\delta W\\,} , and Newton's second law can be written as F = d p d t . {\\displaystyle \\textstyle F={\\frac {dp}{dt}}.} While these scientific laws explain what our senses perceive, they are still empirical (acquired by observation or scientific experiment) and so are not like mathematical theorems which can be proved purely by mathematics. Like theories and hypotheses, laws make predictions; specifically, they predict that new observations will conform to the given law. Laws can be falsified if they are found in contradiction with new data. Some laws are only approximations of other more general laws, and are good approximations with a restricted domain of applicability. For example, Newtonian dynamics (which is based on Galilean transformations) is the low-speed limit of special relativity (since the Galilean transformation is the low-speed approximation to the Lorentz transformation). Similarly, the Newtonian gravitation law is a low-mass approximation of general relativity, and Coulomb's law is an approximation to quantum electrodynamics at large distances (compared to the range of weak interactions). In"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_4",
    "chunk": "such cases it is common to use the simpler, approximate versions of the laws, instead of the more accurate general laws. Laws are constantly being tested experimentally to increasing degrees of precision, which is one of the main goals of science. The fact that laws have never been observed to be violated does not preclude testing them at increased accuracy or in new kinds of conditions to confirm whether they continue to hold, or whether they break, and what can be discovered in the process. It is always possible for laws to be invalidated or proven to have limitations, by repeatable experimental evidence, should any be observed. Well-established laws have indeed been invalidated in some special cases, but the new formulations created to explain the discrepancies generalize upon, rather than overthrow, the originals. That is, the invalidated laws have been found to be only close approximations, to which other terms or factors must be added to cover previously unaccounted-for conditions, e.g. very large or very small scales of time or space, enormous speeds or masses, etc. This, rather than unchanging knowledge, physical laws are better viewed as a series of improving and more precise generalizations. Scientific laws are typically conclusions"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_5",
    "chunk": "based on repeated scientific experiments and observations over many years and which have become accepted universally within the scientific community. A scientific law is \"inferred from particular facts, applicable to a defined group or class of phenomena, and expressible by the statement that a particular phenomenon always occurs if certain conditions be present\". The production of a summary description of our environment in the form of such laws is a fundamental aim of science. Several general properties of scientific laws, particularly when referring to laws in physics, have been identified. Scientific laws are: The term \"scientific law\" is traditionally associated with the natural sciences, though the social sciences also contain laws. For example, Zipf's law is a law in the social sciences which is based on mathematical statistics. In these cases, laws may describe general trends or expected behaviors rather than being absolutes. In natural science, impossibility assertions come to be widely accepted as overwhelmingly probable rather than considered proved to the point of being unchallengeable. The basis for this strong acceptance is a combination of extensive evidence of something not occurring, combined with an underlying theory, very successful in making predictions, whose assumptions lead logically to the conclusion that"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_6",
    "chunk": "something is impossible. While an impossibility assertion in natural science can never be absolutely proved, it could be refuted by the observation of a single counterexample. Such a counterexample would require that the assumptions underlying the theory that implied the impossibility be re-examined. Some examples of widely accepted impossibilities in physics are perpetual motion machines, which violate the law of conservation of energy, exceeding the speed of light, which violates the implications of special relativity, the uncertainty principle of quantum mechanics, which asserts the impossibility of simultaneously knowing both the position and the momentum of a particle, and Bell's theorem: no physical theory of local hidden variables can ever reproduce all of the predictions of quantum mechanics. Some laws reflect mathematical symmetries found in nature (e.g. the Pauli exclusion principle reflects identity of electrons, conservation laws reflect homogeneity of space, time, and Lorentz transformations reflect rotational symmetry of spacetime). Many fundamental physical laws are mathematical consequences of various symmetries of space, time, or other aspects of nature. Specifically, Noether's theorem connects some conservation laws to certain symmetries. For example, conservation of energy is a consequence of the shift symmetry of time (no moment of time is different from any other),"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_7",
    "chunk": "while conservation of momentum is a consequence of the symmetry (homogeneity) of space (no place in space is special, or different from any other). The indistinguishability of all particles of each fundamental type (say, electrons, or photons) results in the Dirac and Bose quantum statistics which in turn result in the Pauli exclusion principle for fermions and in Bose–Einstein condensation for bosons. Special relativity uses rapidity to express motion according to the symmetries of hyperbolic rotation, a transformation mixing space and time. Symmetry between inertial and gravitational mass results in general relativity. The inverse square law of interactions mediated by massless bosons is the mathematical consequence of the 3-dimensionality of space. One strategy in the search for the most fundamental laws of nature is to search for the most general mathematical symmetry group that can be applied to the fundamental interactions. Conservation laws are fundamental laws that follow from the homogeneity of space, time and phase, in other words symmetry. Conservation laws can be expressed using the general continuity equation (for a conserved quantity) can be written in differential form as: where ρ is some quantity per unit volume, J is the flux of that quantity (change in quantity per"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_8",
    "chunk": "unit time per unit area). Intuitively, the divergence (denoted ∇⋅) of a vector field is a measure of flux diverging radially outwards from a point, so the negative is the amount piling up at a point; hence the rate of change of density in a region of space must be the amount of flux leaving or collecting in some region (see the main article for details). In the table below, the fluxes flows for various physical quantities in transport, and their associated continuity equations, are collected for comparison. More general equations are the convection–diffusion equation and Boltzmann transport equation, which have their roots in the continuity equation. Classical mechanics, including Newton's laws, Lagrange's equations, Hamilton's equations, etc., can be derived from the following principle: where S {\\displaystyle {\\mathcal {S}}} is the action; the integral of the Lagrangian of the physical system between two times t1 and t2. The kinetic energy of the system is T (a function of the rate of change of the configuration of the system), and potential energy is V (a function of the configuration and its rate of change). The configuration of a system which has N degrees of freedom is defined by generalized coordinates q"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_9",
    "chunk": "= (q1, q2, ... qN). There are generalized momenta conjugate to these coordinates, p = (p1, p2, ..., pN), where: The action and Lagrangian both contain the dynamics of the system for all times. The term \"path\" simply refers to a curve traced out by the system in terms of the generalized coordinates in the configuration space, i.e. the curve q(t), parameterized by time (see also parametric equation for this concept). The action is a functional rather than a function, since it depends on the Lagrangian, and the Lagrangian depends on the path q(t), so the action depends on the entire \"shape\" of the path for all times (in the time interval from t1 to t2). Between two instants of time, there are infinitely many paths, but one for which the action is stationary (to the first order) is the true path. The stationary value for the entire continuum of Lagrangian values corresponding to some path, not just one value of the Lagrangian, is required (in other words it is not as simple as \"differentiating a function and setting it to zero, then solving the equations to find the points of maxima and minima etc\", rather this idea is applied"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_10",
    "chunk": "to the entire \"shape\" of the function, see calculus of variations for more details on this procedure). Notice L is not the total energy E of the system due to the difference, rather than the sum: The following general approaches to classical mechanics are summarized below in the order of establishment. They are equivalent formulations. Newton's is commonly used due to simplicity, but Hamilton's and Lagrange's equations are more general, and their range can extend into other branches of physics with suitable modifications. Equations describing fluid flow in various situations can be derived, using the above classical equations of motion and often conservation of mass, energy and momentum. Some elementary examples follow. Some of the more famous laws of nature are found in Isaac Newton's theories of (now) classical mechanics, presented in his Philosophiae Naturalis Principia Mathematica, and in Albert Einstein's theory of relativity. The two postulates of special relativity are not \"laws\" in themselves, but assumptions of their nature in terms of relative motion. They can be stated as \"the laws of physics are the same in all inertial frames\" and \"the speed of light is constant and has the same value in all inertial frames\". The said postulates"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_11",
    "chunk": "lead to the Lorentz transformations – the transformation law between two frame of references moving relative to each other. For any 4-vector this replaces the Galilean transformation law from classical mechanics. The Lorentz transformations reduce to the Galilean transformations for low velocities much less than the speed of light c. The magnitudes of 4-vectors are invariants – not \"conserved\", but the same for all inertial frames (i.e. every observer in an inertial frame will agree on the same value), in particular if A is the four-momentum, the magnitude can derive the famous invariant equation for mass–energy and momentum conservation (see invariant mass): General relativity is governed by the Einstein field equations, which describe the curvature of space-time due to mass–energy equivalent to the gravitational field. Solving the equation for the geometry of space warped due to the mass distribution gives the metric tensor. Using the geodesic equation, the motion of masses falling along the geodesics can be calculated. In a relatively flat spacetime due to weak gravitational fields, gravitational analogues of Maxwell's equations can be found; the GEM equations, to describe an analogous gravitomagnetic field. They are well established by the theory, and experimental tests form ongoing research. Kepler's laws,"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_12",
    "chunk": "though originally discovered from planetary observations (also due to Tycho Brahe), are true for any central forces. Maxwell's equations give the time-evolution of the electric and magnetic fields due to electric charge and current distributions. Given the fields, the Lorentz force law is the equation of motion for charges in the fields. These equations can be modified to include magnetic monopoles, and are consistent with our observations of monopoles either existing or not existing; if they do not exist, the generalized equations reduce to the ones above, if they do, the equations become fully symmetric in electric and magnetic charges and currents. Indeed, there is a duality transformation where electric and magnetic charges can be \"rotated into one another\", and still satisfy Maxwell's equations. These laws were found before the formulation of Maxwell's equations. They are not fundamental, since they can be derived from Maxwell's equations. Coulomb's law can be found from Gauss's law (electrostatic form) and the Biot–Savart law can be deduced from Ampere's law (magnetostatic form). Lenz's law and Faraday's law can be incorporated into the Maxwell–Faraday equation. Nonetheless, they are still very effective for simple calculations. Classically, optics is based on a variational principle: light travels from"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_13",
    "chunk": "one point in space to another in the shortest time. In geometric optics laws are based on approximations in Euclidean geometry (such as the paraxial approximation). In actuality, optical properties of matter are significantly more complex and require quantum mechanics. Quantum mechanics has its roots in postulates. This leads to results which are not usually called \"laws\", but hold the same status, in that all of quantum mechanics follows from them. These postulates can be summarized as follows: These postulates in turn imply many other phenomena, e.g., uncertainty principles and the Pauli exclusion principle. Applying electromagnetism, thermodynamics, and quantum mechanics, to atoms and molecules, some laws of electromagnetic radiation and light are as follows. Chemical laws are those laws of nature relevant to chemistry. Historically, observations led to many empirical laws, though now it is known that chemistry has its foundations in quantum mechanics. The most fundamental concept in chemistry is the law of conservation of mass, which states that there is no detectable change in the quantity of matter during an ordinary chemical reaction. Modern physics shows that it is actually energy that is conserved, and that energy and mass are related; a concept which becomes important in nuclear"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_14",
    "chunk": "chemistry. Conservation of energy leads to the important concepts of equilibrium, thermodynamics, and kinetics. Additional laws of chemistry elaborate on the law of conservation of mass. Joseph Proust's law of definite composition says that pure chemicals are composed of elements in a definite formulation; we now know that the structural arrangement of these elements is also important. Dalton's law of multiple proportions says that these chemicals will present themselves in proportions that are small whole numbers; although in many systems (notably biomacromolecules and minerals) the ratios tend to require large numbers, and are frequently represented as a fraction. The law of definite composition and the law of multiple proportions are the first two of the three laws of stoichiometry, the proportions by which the chemical elements combine to form chemical compounds. The third law of stoichiometry is the law of reciprocal proportions, which provides the basis for establishing equivalent weights for each chemical element. Elemental equivalent weights can then be used to derive atomic weights for each element. More modern laws of chemistry define the relationship between energy and its transformations. Whether or not Natural Selection is a “law of nature” is controversial among biologists. Henry Byerly, an American philosopher"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_15",
    "chunk": "known for his work on evolutionary theory, discussed the problem of interpreting a principle of natural selection as a law. He suggested a formulation of natural selection as a framework principle that can contribute to a better understanding of evolutionary theory. His approach was to express relative fitness, the propensity of a genotype to increase in proportionate representation in a competitive environment, as a function of adaptedness (adaptive design) of the organism. Some mathematical theorems and axioms are referred to as laws because they provide logical foundation to empirical laws. Examples of other observed phenomena sometimes described as laws include the Titius–Bode law of planetary positions, Zipf's law of linguistics, and Moore's law of technological growth. Many of these laws fall within the scope of uncomfortable science. Other laws are pragmatic and observational, such as the law of unintended consequences. By analogy, principles in other fields of study are sometimes loosely referred to as \"laws\". These include Occam's razor as a principle of philosophy and the Pareto principle of economics. The observation and detection of underlying regularities in nature date from prehistoric times – the recognition of cause-and-effect relationships implicitly recognises the existence of laws of nature. The recognition of"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_16",
    "chunk": "such regularities as independent scientific laws per se, though, was limited by their entanglement in animism, and by the attribution of many effects that do not have readily obvious causes—such as physical phenomena—to the actions of gods, spirits, supernatural beings, etc. Observation and speculation about nature were intimately bound up with metaphysics and morality. In Europe, systematic theorizing about nature (physis) began with the early Greek philosophers and scientists and continued into the Hellenistic and Roman imperial periods, during which times the intellectual influence of Roman law increasingly became paramount. The formula \"law of nature\" first appears as \"a live metaphor\" favored by Latin poets Lucretius, Virgil, Ovid, Manilius, in time gaining a firm theoretical presence in the prose treatises of Seneca and Pliny. Why this Roman origin? According to [historian and classicist Daryn] Lehoux's persuasive narrative, the idea was made possible by the pivotal role of codified law and forensic argument in Roman life and culture. For the Romans ... the place par excellence where ethics, law, nature, religion and politics overlap is the law court. When we read Seneca's Natural Questions, and watch again and again just how he applies standards of evidence, witness evaluation, argument and proof,"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_17",
    "chunk": "we can recognize that we are reading one of the great Roman rhetoricians of the age, thoroughly immersed in forensic method. And not Seneca alone. Legal models of scientific judgment turn up all over the place, and for example prove equally integral to Ptolemy's approach to verification, where the mind is assigned the role of magistrate, the senses that of disclosure of evidence, and dialectical reason that of the law itself. The precise formulation of what are now recognized as modern and valid statements of the laws of nature dates from the 17th century in Europe, with the beginning of accurate experimentation and the development of advanced forms of mathematics. During this period, natural philosophers such as Isaac Newton (1642–1727) were influenced by a religious view – stemming from medieval concepts of divine law – which held that God had instituted absolute, universal and immutable physical laws. In chapter 7 of The World, René Descartes (1596–1650) described \"nature\" as matter itself, unchanging as created by God, thus changes in parts \"are to be attributed to nature. The rules according to which these changes take place I call the 'laws of nature'.\" The modern scientific method which took shape at this"
  },
  {
    "source": "Scientific law.txt",
    "chunk_id": "Scientific law.txt_18",
    "chunk": "time (with Francis Bacon (1561–1626) and Galileo (1564–1642)) contributed to a trend of separating science from theology, with minimal speculation about metaphysics and ethics. (Natural law in the political sense, conceived as universal (i.e., divorced from sectarian religion and accidents of place), was also elaborated in this period by scholars such as Grotius (1583–1645), Spinoza (1632–1677), and Hobbes (1588–1679).) The distinction between natural law in the political-legal sense and law of nature or physical law in the scientific sense is a modern one, both concepts being equally derived from physis, the Greek word (translated into Latin as natura) for nature."
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_0",
    "chunk": "# Search for extraterrestrial intelligence The search for extraterrestrial intelligence (usually shortened as SETI) is an expression that refers to the diverse efforts and scientific projects intended to detect extraterrestrial signals, or any evidence of intelligent life beyond Earth. Researchers use methods such as monitoring electromagnetic radiation, searching for optical signals, and investigating potential extraterrestrial artifacts for any signs of transmission from civilizations present on other planets. Some initiatives have also attempted to send messages to hypothetical alien civilizations, such as NASA's Golden Record. Modern SETI research began in the early 20th century after the advent of radio, expanding with projects like Project Ozma, the Wow! signal detection, and the Breakthrough Listen initiative; a $100 million, 10-year attempt to detect signals from nearby stars, announced in 2015 by Stephen Hawking, and Yuri Milner. Since the 1980s, international efforts have been ongoing, with community led projects such as SETI@home and Project Argus, engaging in analyzing data. While SETI remains a respected scientific field, it often gets compared to conspiracy theory, UFO research, bringing unawarrented skepticism from the public, despite its reliance on rigorous scientific methods and verifiable data and research. Similar studies on Unidentified Aerial Phenomena (UAP) such as the Avi"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_1",
    "chunk": "Loeb's Galileo Project have brought further attention to SETI research. Despite decades of searching, no confirmed evidence of alien intelligence has been found, bringing criticism onto SETI for being 'overly hopeful'. Critics argue that SETI is speculative and unfalsifiable, while supporters see it as a crucial step in addressing the Fermi Paradox and understanding extraterrestrial technosignature. There have been many earlier searches for extraterrestrial intelligence within the Solar System. In 1896, Nikola Tesla suggested that an extreme version of his wireless electrical transmission system could be used to contact beings on Mars. In 1899, while conducting experiments at his Colorado Springs experimental station, he thought he had detected a signal from Mars since an odd repetitive static signal seemed to cut off when Mars set in the night sky. Analysis of Tesla's research has led to a range of explanations including: In the early 1900s, Guglielmo Marconi, Lord Kelvin and David Peck Todd also stated their belief that radio could be used to contact Martians, with Marconi stating that his stations had also picked up potential Martian signals. On August 21–23, 1924, Mars entered an opposition closer to Earth than at any time in the century before or the next"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_2",
    "chunk": "80 years. In the United States, a \"National Radio Silence Day\" was promoted during a 36-hour period from August 21–23, with all radios quiet for five minutes on the hour, every hour. At the United States Naval Observatory, a radio receiver was lifted 3 kilometres (1.9 miles) above the ground in a dirigible tuned to a wavelength between 8 and 9 km, using a \"radio-camera\" developed by Amherst College and Charles Francis Jenkins. The program was led by David Peck Todd with the military assistance of Admiral Edward W. Eberle (Chief of Naval Operations), with William F. Friedman (chief cryptographer of the United States Army), assigned to translate any potential Martian messages. A 1959 paper by Philip Morrison and Giuseppe Cocconi first pointed out the possibility of searching the microwave spectrum. It proposed frequencies and a set of initial targets. In 1960, Cornell University astronomer Frank Drake performed the first modern SETI experiment, named \"Project Ozma\" after the Queen of Oz in L. Frank Baum's fantasy books. Drake used a radio telescope 26 metres (85 ft) in diameter at Green Bank, West Virginia, to examine the stars Tau Ceti and Epsilon Eridani near the 1.420 gigahertz marker frequency, a region"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_3",
    "chunk": "of the radio spectrum dubbed the \"water hole\" due to its proximity to the hydrogen and hydroxyl radical spectral lines. A 400 kilohertz band around the marker frequency was scanned using a single-channel receiver with a bandwidth of 100 hertz. He found nothing of interest. Soviet scientists took a strong interest in SETI during the 1960s and performed a number of searches with omnidirectional antennas in the hope of picking up powerful radio signals. Soviet astronomer Iosif Shklovsky wrote the pioneering book in the field, Universe, Life, Intelligence (1962), which was expanded upon by American astronomer Carl Sagan as the best-selling book Intelligent Life in the Universe (1966). In the March 1955 issue of Scientific American, John D. Kraus described an idea to scan the cosmos for natural radio signals using a flat-plane radio telescope equipped with a parabolic reflector. Within two years, his concept was approved for construction by Ohio State University. With a total of US$71,000 (equivalent to $794,880 in 2024) in grants from the National Science Foundation, construction began on an 8-hectare (20-acre) plot in Delaware, Ohio. This Ohio State University Radio Observatory telescope was called \"Big Ear\". Later, it began the world's first continuous SETI program,"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_4",
    "chunk": "called the Ohio State University SETI program. In 1971, NASA funded a SETI study that involved Drake, Barney Oliver of Hewlett-Packard Laboratories, and others. The resulting report proposed the construction of an Earth-based radio telescope array with 1,500 dishes known as \"Project Cyclops\". The price tag for the Cyclops array was US$10 billion. Cyclops was not built, but the report formed the basis of much SETI work that followed. The Ohio State SETI program gained fame on August 15, 1977, when Jerry Ehman, a project volunteer, witnessed a startlingly strong signal received by the telescope. He quickly circled the indication on a printout and scribbled the exclamation \"Wow!\" in the margin. Dubbed the Wow! signal, it is considered by some to be the best candidate for a radio signal from an artificial, extraterrestrial source ever discovered, but it has not been detected again in several additional searches. On 24 May 2023, a test extraterrestrial signal, in the form of a \"coded radio signal from Mars\", was transmitted to radio telescopes on Earth, according to a report in The New York Times. In 1980, Carl Sagan, Bruce Murray, and Louis Friedman founded the U.S. Planetary Society, partly as a vehicle for"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_5",
    "chunk": "SETI studies. In the early 1980s, Harvard University physicist Paul Horowitz took the next step and proposed the design of a spectrum analyzer specifically intended to search for SETI transmissions. Traditional desktop spectrum analyzers were of little use for this job, as they sampled frequencies using banks of analog filters and so were restricted in the number of channels they could acquire. However, modern integrated-circuit digital signal processing (DSP) technology could be used to build autocorrelation receivers to check far more channels. This work led in 1981 to a portable spectrum analyzer named \"Suitcase SETI\" that had a capacity of 131,000 narrow band channels. After field tests that lasted into 1982, Suitcase SETI was put into use in 1983 with the 26-meter (85 ft) Harvard/Smithsonian radio telescope at Oak Ridge Observatory in Harvard, Massachusetts. This project was named \"Sentinel\" and continued into 1985. Even 131,000 channels were not enough to search the sky in detail at a fast rate, so Suitcase SETI was followed in 1985 by Project \"META\", for \"Megachannel Extra-Terrestrial Assay\". The META spectrum analyzer had a capacity of 8.4 million channels and a channel resolution of 0.05 hertz. An important feature of META was its use of"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_6",
    "chunk": "frequency Doppler shift to distinguish between signals of terrestrial and extraterrestrial origin. The project was led by Horowitz with the help of the Planetary Society, and was partly funded by movie maker Steven Spielberg. A second such effort, META II, was begun in Argentina in 1990, to search the southern sky, receiving an equipment upgrade in 1996–1997. The follow-on to META was named \"BETA\", for \"Billion-channel Extraterrestrial Assay\", and it commenced observation on October 30, 1995. The heart of BETA's processing capability consisted of 63 dedicated fast Fourier transform (FFT) engines, each capable of performing a 2-point complex FFTs in two seconds, and 21 general-purpose personal computers equipped with custom digital signal processing boards. This allowed BETA to receive 250 million simultaneous channels with a resolution of 0.5 hertz per channel. It scanned through the microwave spectrum from 1.400 to 1.720 gigahertz in eight hops, with two seconds of observation per hop. An important capability of the BETA search was rapid and automatic re-observation of candidate signals, achieved by observing the sky with two adjacent beams, one slightly to the east and the other slightly to the west. A successful candidate signal would first transit the east beam, and then"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_7",
    "chunk": "the west beam and do so with a speed consistent with Earth's sidereal rotation rate. A third receiver observed the horizon to veto signals of obvious terrestrial origin. On March 23, 1999, the 26-meter radio telescope on which Sentinel, META and BETA were based was blown over by strong winds and seriously damaged. This forced the BETA project to cease operation. In 1978, the NASA SETI program had been heavily criticized by Senator William Proxmire, and funding for SETI research was removed from the NASA budget by Congress in 1981; however, funding was restored in 1982, after Carl Sagan talked with Proxmire and convinced him of the program's value. In 1992, the U.S. government funded an operational SETI program, in the form of the NASA Microwave Observing Program (MOP). MOP was planned as a long-term effort to conduct a general survey of the sky and also carry out targeted searches of 800 specific nearby stars. MOP was to be performed by radio antennas associated with the NASA Deep Space Network, as well as the 140-foot (43 m) radio telescope of the National Radio Astronomy Observatory at Green Bank, West Virginia and the 1,000-foot (300 m) radio telescope at the Arecibo"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_8",
    "chunk": "Observatory in Puerto Rico. The signals were to be analyzed by spectrum analyzers, each with a capacity of 15 million channels. These spectrum analyzers could be grouped together to obtain greater capacity. Those used in the targeted search had a bandwidth of 1 hertz per channel, while those used in the sky survey had a bandwidth of 30 hertz per channel. MOP drew the attention of the United States Congress, where the program met opposition and canceled one year after its start. SETI advocates continued without government funding, and in 1995 the nonprofit SETI Institute of Mountain View, California resurrected the MOP program under the name of Project \"Phoenix\", backed by private sources of funding. In 2012 it cost around $2 million per year to maintain SETI research at the SETI Institute and around 10 times that to support different SETI activities globally. Project Phoenix, under the direction of Jill Tarter, was a continuation of the targeted search program from MOP and studied roughly 1,000 nearby Sun-like stars until approximately 2015. From 1995 through March 2004, Phoenix conducted observations at the 64-meter (210 ft) Parkes radio telescope in Australia, the 140-foot (43 m) radio telescope of the National Radio Astronomy"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_9",
    "chunk": "Observatory in Green Bank, West Virginia, and the 1,000-foot (300 m) radio telescope at the Arecibo Observatory in Puerto Rico. The project observed the equivalent of 800 stars over the available channels in the frequency range from 1200 to 3000 MHz. The search was sensitive enough to pick up transmitters with 1 GW EIRP to a distance of about 200 light-years. Many radio frequencies penetrate Earth's atmosphere quite well, and this led to radio telescopes that investigate the cosmos using large radio antennas. Furthermore, human endeavors emit considerable electromagnetic radiation as a byproduct of communications such as television and radio. These signals would be easy to recognize as artificial due to their repetitive nature and narrow bandwidths. Earth has been sending radio waves from broadcasts into space for over 100 years. These signals have reached over 1,000 stars, most notably Vega, Aldebaran, Barnard's Star, Sirius, and Proxima Centauri. If intelligent alien life exists on any planet orbiting these nearby stars, these signals could be heard and deciphered, even though some of the signal is garbled by the Earth's ionosphere. Many international radio telescopes are currently being used for radio SETI searches, including the Low Frequency Array (LOFAR) in Europe, the"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_10",
    "chunk": "Murchison Widefield Array (MWA) in Australia, and the Lovell Telescope in the United Kingdom. The SETI Institute collaborated with the Radio Astronomy Laboratory at the Berkeley SETI Research Center to develop a specialized radio telescope array for SETI studies, similar to a mini-cyclops array. Formerly known as the One Hectare Telescope (1HT), the concept was renamed the \"Allen Telescope Array\" (ATA) after the project's benefactor, Paul Allen. Its sensitivity is designed to be equivalent to a single large dish more than 100 meters in diameter, if fully completed. Presently, the array has 42 operational dishes at the Hat Creek Radio Observatory in rural northern California. The full array (ATA-350) is planned to consist of 350 or more offset-Gregorian radio dishes, each 6.1 meters (20 feet) in diameter. These dishes are the largest producible with commercially available satellite television dish technology. The ATA was planned for a 2007 completion date, at a cost of US$25 million. The SETI Institute provided money for building the ATA while University of California, Berkeley designed the telescope and provided operational funding. The first portion of the array (ATA-42) became operational in October 2007 with 42 antennas. The DSP system planned for ATA-350 is extremely ambitious."
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_11",
    "chunk": "Completion of the full 350 element array will depend on funding and the technical results from ATA-42. ATA-42 (ATA) is designed to allow multiple observers simultaneous access to the interferometer output at the same time. Typically, the ATA snapshot imager (used for astronomical surveys and SETI) is run in parallel with a beamforming system (used primarily for SETI). ATA also supports observations in multiple synthesized pencil beams at once, through a technique known as \"multibeaming\". Multibeaming provides an effective filter for identifying false positives in SETI, since a very distant transmitter must appear at only one point on the sky. SETI Institute's Center for SETI Research (CSR) uses ATA in the search for extraterrestrial intelligence, observing 12 hours a day, 7 days a week. From 2007 to 2015, ATA identified hundreds of millions of technological signals. So far, all these signals have been assigned the status of noise or radio frequency interference because a) they appear to be generated by satellites or Earth-based transmitters, or b) they disappeared before the threshold time limit of ~1 hour. Researchers in CSR are working on ways to reduce the threshold time limit, and to expand ATA's capabilities for detection of signals that may"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_12",
    "chunk": "have embedded messages. Berkeley astronomers used the ATA to pursue several science topics, some of which might have transient SETI signals, until 2011, when the collaboration between the University of California, Berkeley and the SETI Institute was terminated. CNET published an article and pictures about the Allen Telescope Array (ATA) on December 12, 2008. In April 2011, the ATA entered an 8-month \"hibernation\" due to funding shortfalls. Regular operation of the ATA resumed on December 5, 2011. In 2012, the ATA was revitalized with a $3.6 million donation by Franklin Antonio, co-founder and Chief Scientist of QUALCOMM Incorporated. This gift supported upgrades of all the receivers on the ATA dishes to have (2× to 10× over the range 1–8 GHz) greater sensitivity than before and supporting observations over a wider frequency range from 1–18 GHz, though initially the radio frequency electronics only go to 12 GHz. As of July 2013, the first of these receivers was installed and proven, with full installation on all 42 antennas being expected for June 2017. ATA is well suited to the search for extraterrestrial intelligence (SETI) and to discovery of astronomical radio sources, such as heretofore unexplained non-repeating, possibly extragalactic, pulses known as fast"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_13",
    "chunk": "radio bursts or FRBs. SERENDIP (Search for Extraterrestrial Radio Emissions from Nearby Developed Intelligent Populations) is a SETI program launched in 1979 by the Berkeley SETI Research Center. SERENDIP takes advantage of ongoing \"mainstream\" radio telescope observations as a \"piggy-back\" or \"commensal\" program, using large radio telescopes including the NRAO 90m telescope at Green Bank and, formerly, the Arecibo 305m telescope. Rather than having its own observation program, SERENDIP analyzes deep space radio telescope data that it obtains while other astronomers are using the telescopes. The most recently deployed SERENDIP spectrometer, SERENDIP VI, was installed at both the Arecibo Telescope and the Green Bank Telescope in 2014–2015. Breakthrough Listen is a ten-year initiative with $100 million funding begun in July 2015 to actively search for intelligent extraterrestrial communications in the universe, in a substantially expanded way, using resources that had not previously been extensively used for the purpose. It has been described as the most comprehensive search for alien communications to date. The science program for Breakthrough Listen is based at Berkeley SETI Research Center, located in the Astronomy Department at the University of California, Berkeley. Announced in July 2015, the project is observing for thousands of hours every year"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_14",
    "chunk": "on two major radio telescopes, the Green Bank Observatory in West Virginia, and the Parkes Observatory in Australia. Previously, only about 24 to 36 hours of telescope time per year were used in the search for alien life. Furthermore, the Automated Planet Finder at Lick Observatory is searching for optical signals coming from laser transmissions. The massive data rates from the radio telescopes (24 GB/s at Green Bank) necessitated the construction of dedicated hardware at the telescopes to perform the bulk of the analysis. Some of the data are also analyzed by volunteers in the SETI@home volunteer computing network. Founder of modern SETI Frank Drake was one of the scientists on the project's advisory committee. In October 2019, Breakthrough Listen started a collaboration with scientists from the TESS team (Transiting Exoplanet Survey Satellite) to look for signs of advanced extraterrestrial life. Thousands of new planets found by TESS will be scanned for technosignatures by Breakthrough Listen partner facilities across the globe. Data from TESS monitoring of stars will also be searched for anomalies. China's 500 meter Aperture Spherical Telescope (FAST) lists detecting interstellar communication signals as part of its science mission. It is funded by the National Development and Reform"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_15",
    "chunk": "Commission (NDRC) and managed by the National Astronomical observatories (NAOC) of the Chinese Academy of Sciences (CAS). FAST is the first radio observatory built with SETI as a core scientific goal. FAST consists of a fixed 500 m (1,600 ft) diameter spherical dish constructed in a natural depression sinkhole caused by karst processes in the region. It is the world's largest filled-aperture radio telescope. According to its website, FAST can search to 28 light-years, and is able to reach 1,400 stars. If the transmitter's radiated power were to be increased to 1,000,000 MW, FAST would be able to reach one million stars. This is compared to the former Arecibo 305 meter telescope detection distance of 18 light-years. On 14 June 2022, astronomers, working with China's FAST telescope, reported the possibility of having detected artificial (presumably alien) signals, but cautioned that further studies were required to determine if a natural radio interference may be the source. More recently, on 18 June 2022, Dan Werthimer, chief scientist for several SETI-related projects, reportedly noted, \"These signals are from radio interference; they are due to radio pollution from earthlings, not from E.T.\". Since 2016, University of California Los Angeles (UCLA) undergraduate and graduate students"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_16",
    "chunk": "have been participating in radio searches for technosignatures with the Green Bank Telescope. Targets include the Kepler field, TRAPPIST-1, and solar-type stars. The search is sensitive to Arecibo-class transmitters located within 420 light years of Earth and to transmitters that are 1,000 times more powerful than Arecibo located within 13,000 light years of Earth. The SETI@home project used volunteer computing to analyze signals acquired by the SERENDIP project. SETI@home was conceived by David Gedye along with Craig Kasnoff and is a popular volunteer computing project that was launched by the Berkeley SETI Research Center at the University of California, Berkeley, in May 1999. It was originally funded by The Planetary Society and Paramount Pictures, and later by the state of California. The project is run by director David P. Anderson and chief scientist Dan Werthimer. Any individual could become involved with SETI research by downloading the Berkeley Open Infrastructure for Network Computing (BOINC) software program, attaching to the SETI@home project, and allowing the program to run as a background process that uses idle computer power. The SETI@home program itself ran signal analysis on a \"work unit\" of data recorded from the central 2.5 MHz wide band of the SERENDIP IV"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_17",
    "chunk": "instrument. After computation on the work unit was complete, the results were then automatically reported back to SETI@home servers at University of California, Berkeley. By June 28, 2009, the SETI@home project had over 180,000 active participants volunteering a total of over 290,000 computers. These computers gave SETI@home an average computational power of 617 teraFLOPS. In 2004 radio source SHGb02+14a set off speculation in the media that a signal had been detected but researchers noted the frequency drifted rapidly and the detection on three SETI@home computers fell within random chance. By 2010, after 10 years of data collection, SETI@home had listened to that one frequency at every point of over 67 percent of the sky observable from Arecibo with at least three scans (out of the goal of nine scans), which covers about 20 percent of the full celestial sphere. On March 31, 2020, with 91,454 active users, the project stopped sending out new work to SETI@home users, bringing this particular SETI effort to an indefinite hiatus. SETI Network was the only fully operational private search system. The SETI Net station consisted of off-the-shelf, consumer-grade electronics to minimize cost and to allow this design to be replicated as simply as possible."
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_18",
    "chunk": "It had a 3-meter parabolic antenna that could be directed in azimuth and elevation, an LNA that covered 100 MHz of the 1420 MHz spectrum, a receiver to reproduce the wideband audio, and a standard personal computer as the control device and for deploying the detection algorithms. The antenna could be pointed and locked to one sky location in Ra and DEC which enabling the system to integrate on it for long periods. The Wow! signal area was monitored for many long periods. All search data was collected and is available on the Internet archive. SETI Net started operation in the early 1980s as a way to learn about the science of the search, and developed several software packages for the amateur SETI community. It provided an astronomical clock, a file manager to keep track of SETI data files, a spectrum analyzer optimized for amateur SETI, remote control of the station from the Internet, and other packages. SETI Net went dark and was decommissioned on 2021-12-04. The collected data is available on their website. Founded in 1994 in response to the United States Congress cancellation of the NASA SETI program, The SETI League, Incorporated is a membership-supported nonprofit organization with"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_19",
    "chunk": "1,500 members in 62 countries. This grass-roots alliance of amateur and professional radio astronomers is headed by executive director emeritus H. Paul Shuch, the engineer credited with developing the world's first commercial home satellite TV receiver. Many SETI League members are licensed radio amateurs and microwave experimenters. Others are digital signal processing experts and computer enthusiasts. The SETI League pioneered the conversion of backyard satellite TV dishes 3 to 5 m (10–16 ft) in diameter into research-grade radio telescopes of modest sensitivity. The organization concentrates on coordinating a global network of small, amateur-built radio telescopes under Project Argus, an all-sky survey seeking to achieve real-time coverage of the entire sky. Project Argus was conceived as a continuation of the all-sky survey component of the late NASA SETI program (the targeted search having been continued by the SETI Institute's Project Phoenix). There are currently 143 Project Argus radio telescopes operating in 27 countries. Project Argus instruments typically exhibit sensitivity on the order of 10 Watts/square metre, or roughly equivalent to that achieved by the Ohio State University Big Ear radio telescope in 1977, when it detected the landmark \"Wow!\" candidate signal. The name \"Argus\" derives from the mythical Greek guard-beast who"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_20",
    "chunk": "had 100 eyes, and could see in all directions at once. In the SETI context, the name has been used for radio telescopes in fiction (Arthur C. Clarke, \"Imperial Earth\"; Carl Sagan, \"Contact\"), was the name initially used for the NASA study ultimately known as \"Cyclops,\" and is the name given to an omnidirectional radio telescope design being developed at the Ohio State University. While most SETI sky searches have studied the radio spectrum, some SETI researchers have considered the possibility that alien civilizations might be using powerful lasers for interstellar communications at optical wavelengths. The idea was first suggested by R. N. Schwartz and Charles Hard Townes in a 1961 paper published in the journal Nature titled \"Interstellar and Interplanetary Communication by Optical Masers\". However, the 1971 Cyclops study discounted the possibility of optical SETI, reasoning that construction of a laser system that could outshine the bright central star of a remote star system would be too difficult. In 1983, Townes published a detailed study of the idea in the United States journal Proceedings of the National Academy of Sciences, which was met with interest by the SETI community. There are two problems with optical SETI. The first problem"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_21",
    "chunk": "is that lasers are highly \"monochromatic\", that is, they emit light only on one frequency, making it troublesome to figure out what frequency to look for. However, emitting light in narrow pulses results in a broad spectrum of emission; the spread in frequency becomes higher as the pulse width becomes narrower, making it easier to detect an emission. The other problem is that while radio transmissions can be broadcast in all directions, lasers are highly directional. Interstellar gas and dust is almost transparent to near infrared, so these signals can be seen from greater distances, but the extraterrestrial laser signals would need to be transmitted in the direction of Earth in order to be detected. Optical SETI supporters have conducted paper studies of the effectiveness of using contemporary high-energy lasers and a ten-meter diameter mirror as an interstellar beacon. The analysis shows that an infrared pulse from a laser, focused into a narrow beam by such a mirror, would appear thousands of times brighter than the Sun to a distant civilization in the beam's line of fire. The Cyclops study proved incorrect in suggesting a laser beam would be inherently hard to see. Such a system could be made to"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_22",
    "chunk": "automatically steer itself through a target list, sending a pulse to each target at a constant rate. This would allow targeting of all Sun-like stars within a distance of 100 light-years. The studies have also described an automatic laser pulse detector system with a low-cost, two-meter mirror made of carbon composite materials, focusing on an array of light detectors. This automatic detector system could perform sky surveys to detect laser flashes from civilizations attempting contact. Several optical SETI experiments are now in progress. A Harvard-Smithsonian group that includes Paul Horowitz designed a laser detector and mounted it on Harvard's 155-centimeter (61-inch) optical telescope. This telescope is currently being used for a more conventional star survey, and the optical SETI survey is \"piggybacking\" on that effort. Between October 1998 and November 1999, the survey inspected about 2,500 stars. Nothing that resembled an intentional laser signal was detected, but efforts continue. The Harvard-Smithsonian group is now working with Princeton University to mount a similar detector system on Princeton's 91-centimeter (36-inch) telescope. The Harvard and Princeton telescopes will be \"ganged\" to track the same targets at the same time, with the intent being to detect the same signal in both locations as a"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_23",
    "chunk": "means of reducing errors from detector noise. The Harvard-Smithsonian SETI group led by Professor Paul Horowitz built a dedicated all-sky optical survey system along the lines of that described above, featuring a 1.8-meter (72-inch) telescope. The new optical SETI survey telescope is being set up at the Oak Ridge Observatory in Harvard, Massachusetts. The University of California, Berkeley, home of SERENDIP and SETI@home, is also conducting optical SETI searches and collaborates with the NIROSETI program. The optical SETI program at Breakthrough Listen was initially directed by Geoffrey Marcy, an extrasolar planet hunter, and it involves examination of records of spectra taken during extrasolar planet hunts for a continuous, rather than pulsed, laser signal. This survey uses the Automated Planet Finder 2.4-m telescope at the Lick Observatory, situated on the summit of Mount Hamilton, east of San Jose, California. The other Berkeley optical SETI effort is being pursued by the Harvard-Smithsonian group and is being directed by Dan Werthimer of Berkeley, who built the laser detector for the Harvard-Smithsonian group. This survey uses a 76-centimeter (30-inch) automated telescope at Leuschner Observatory and an older laser detector built by Werthimer. The SETI Institute also runs a program called 'Laser SETI' with an"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_24",
    "chunk": "instrument composed of several cameras that continuously survey the entire night sky searching for millisecond singleton laser pulses of extraterrestrial origin. In January 2020, two Pulsed All-sky Near-infrared Optical SETI (PANOSETI) project telescopes were installed in the Lick Observatory Astrograph Dome. The project aims to commence a wide-field optical SETI search and continue prototyping designs for a full observatory. The installation can offer an \"all-observable-sky\" optical and wide-field near-infrared pulsed technosignature and astrophysical transient search for the northern hemisphere. In May 2017, astronomers reported studies related to laser light emissions from stars, as a way of detecting technology-related signals from an alien civilization. The reported studies included Tabby's Star (designated KIC 8462852 in the Kepler Input Catalog), an oddly dimming star in which its unusual starlight fluctuations may be the result of interference by an artificial megastructure, such as a Dyson swarm, made by such a civilization. No evidence was found for technology-related signals from KIC 8462852 in the studies. In a 2020 paper, Berera examined sources of decoherence in the interstellar medium and made the observation that quantum coherence of photons in certain frequency bands could be sustained to interstellar distances. It was suggested this would allow for quantum"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_25",
    "chunk": "communication at these distances. In a 2021 preprint, astronomer Michael Hipke described for the first time how one could search for quantum communication transmissions sent by ETI using existing telescope and receiver technology. He also provides arguments for why future searches of ETI should also target interstellar quantum communication networks. A 2022 paper by Arjun Berera and Jaime Calderón-Figueroa noted that interstellar quantum communication by other civilizations could be possible and may be advantageous, identifying some potential challenges and factors for detecting technosignatures. They may, for example, use X-ray photons for remotely established quantum communication and quantum teleportation as the communication mode. The possibility of using interstellar messenger probes in the search for extraterrestrial intelligence was first suggested by Ronald N. Bracewell in 1960 (see Bracewell probe), and the technical feasibility of this approach was demonstrated by the British Interplanetary Society's starship study Project Daedalus in 1978. Starting in 1979, Robert Freitas advanced arguments for the proposition that physical space-probes are a superior mode of interstellar communication to radio signals (see Voyager Golden Record). In recognition that any sufficiently advanced interstellar probe in the vicinity of Earth could easily monitor the terrestrial Internet, 'Invitation to ETI' was established by Allen"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_26",
    "chunk": "Tough in 1996, as a Web-based SETI experiment inviting such spacefaring probes to establish contact with humanity. The project's 100 signatories includes prominent physical, biological, and social scientists, as well as artists, educators, entertainers, philosophers and futurists. H. Paul Shuch, executive director emeritus of The SETI League, serves as the project's Principal Investigator. Inscribing a message in matter and transporting it to an interstellar destination can be enormously more energy efficient than communication using electromagnetic waves if delays larger than light transit time can be tolerated. That said, for simple messages such as \"hello,\" radio SETI could be far more efficient. If energy requirement is used as a proxy for technical difficulty, then a solarcentric Search for Extraterrestrial Artifacts (SETA) may be a useful supplement to traditional radio or optical searches. Much like the \"preferred frequency\" concept in SETI radio beacon theory, the Earth-Moon or Sun-Earth libration orbits might therefore constitute the most universally convenient parking places for automated extraterrestrial spacecraft exploring arbitrary stellar systems. A viable long-term SETI program may be founded upon a search for these objects. In 1979, Freitas and Valdes conducted a photographic search of the vicinity of the Earth-Moon triangular libration points L4 and L5,"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_27",
    "chunk": "and of the solar-synchronized positions in the associated halo orbits, seeking possible orbiting extraterrestrial interstellar probes, but found nothing to a detection limit of about 14th magnitude. The authors conducted a second, more comprehensive photographic search for probes in 1982 that examined the five Earth-Moon Lagrangian positions and included the solar-synchronized positions in the stable L4/L5 libration orbits, the potentially stable nonplanar orbits near L1/L2, Earth-Moon L3, and also L2 in the Sun-Earth system. Again no extraterrestrial probes were found to limiting magnitudes of 17–19th magnitude near L3/L4/L5, 10–18th magnitude for L1/L2, and 14–16th magnitude for Sun-Earth L2. In June 1983, Valdes and Freitas used the 26 m radiotelescope at Hat Creek Radio Observatory to search for the tritium hyperfine line at 1516 MHz from 108 assorted astronomical objects, with emphasis on 53 nearby stars including all visible stars within a 20 light-year radius. The tritium frequency was deemed highly attractive for SETI work because (1) the isotope is cosmically rare, (2) the tritium hyperfine line is centered in the SETI water hole region of the terrestrial microwave window, and (3) in addition to beacon signals, tritium hyperfine emission may occur as a byproduct of extensive nuclear fusion energy production"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_28",
    "chunk": "by extraterrestrial civilizations. The wideband- and narrowband-channel observations achieved sensitivities of 5–14×10 W/m/channel and 0.7–2×10 W/m/channel, respectively, but no detections were made. Others have speculated, that we might find traces of past civilizations in our very own Solar System, on planets like Venus or Mars, although the traces would be found most likely underground. Technosignatures, including all signs of technology, are a recent avenue in the search for extraterrestrial intelligence. Technosignatures may originate from various sources, from megastructures such as Dyson spheres and space mirrors or space shaders to the atmospheric contamination created by an industrial civilization, or city lights on extrasolar planets, and may be detectable in the future with large hypertelescopes. Technosignatures can be divided into three broad categories: astroengineering projects, signals of planetary origin, and spacecraft within and outside the Solar System. An astroengineering installation such as a Dyson sphere, designed to convert all of the incident radiation of its host star into energy, could be detected through the observation of an infrared excess from a solar analog star, or by the star's apparent disappearance in the visible spectrum over several years. After examining some 100,000 nearby large galaxies, a team of researchers has concluded that none"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_29",
    "chunk": "of them display any obvious signs of highly advanced technological civilizations. Another hypothetical form of astroengineering, the Shkadov thruster, moves its host star by reflecting some of the star's light back on itself, and would be detected by observing if its transits across the star abruptly end with the thruster in front. Asteroid mining within the Solar System is also a detectable technosignature of the first kind. Individual extrasolar planets can be analyzed for signs of technology. Avi Loeb of the Center for Astrophysics | Harvard & Smithsonian has proposed that persistent light signals on the night side of an exoplanet can be an indication of the presence of cities and an advanced civilization. In addition, the excess infrared radiation and chemicals produced by various industrial processes or terraforming efforts may point to intelligence. Light and heat detected from planets need to be distinguished from natural sources to conclusively prove the existence of civilization on a planet. However, as argued by the Colossus team, a civilization heat signature should be within a \"comfortable\" temperature range, like terrestrial urban heat islands, i.e., only a few degrees warmer than the planet itself. In contrast, such natural sources as wild fires, volcanoes, etc."
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_30",
    "chunk": "are significantly hotter, so they will be well distinguished by their maximum flux at a different wavelength. Other than astroengineering, technosignatures such as artificial satellites around exoplanets, particularly such in geostationary orbit, might be detectable even with today's technology and data, and would allow, similar to fossils on Earth, to find traces of extrasolar life from long ago. Extraterrestrial craft are another target in the search for technosignatures. Magnetic sail interstellar spacecraft should be detectable over thousands of light-years of distance through the synchrotron radiation they would produce through interaction with the interstellar medium; other interstellar spacecraft designs may be detectable at more modest distances. In addition, robotic probes within the Solar System are also being sought with optical and radio searches. For a sufficiently advanced civilization, hyper energetic neutrinos from Planck scale accelerators should be detectable at a distance of many Mpc. A notable advancement in technosignature detection is the development of an algorithm for signal reconstruction in zero-knowledge one-way communication channels. This algorithm decodes signals from unknown sources without prior knowledge of the encoding scheme, using principles from Algorithmic Information Theory to identify the geometric and topological dimensions of the encoding space. It successfully reconstructed the Arecibo message"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_31",
    "chunk": "despite significant noise. The work establishes a connection between syntax and semantics in SETI and technosignature detection, enhancing fields like cryptography and Information Theory. Based on fractal theory and the Weierstrass function, a known fractal, another method authored by the same group called fractal messaging offers a framework for space-time scale-free communication. This method leverages properties of self-similarity and scale invariance, enabling spatio-temporal scale-independent and parallel infinite-frequency communication. It also embodies the concept of sending a self-encoding/self-decoding signal as a mathematical formula, equivalent to self-executable computer code that unfolds to read a message at all possible time scales and in all possible channels simultaneously. Italian physicist Enrico Fermi suggested in the 1950s that if technologically advanced civilizations are common in the universe, then they should be detectable in one way or another. According to those who were there, Fermi either asked \"Where are they?\" or \"Where is everybody?\" The Fermi paradox is commonly understood as asking why extraterrestrials have not visited Earth, but the same reasoning applies to the question of why signals from extraterrestrials have not been heard. The SETI version of the question is sometimes referred to as \"the Great Silence\". The size and age of the universe"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_32",
    "chunk": "incline us to believe that many technologically advanced civilizations must exist. However, this belief seems logically inconsistent with our lack of observational evidence to support it. Either (1) the initial assumption is incorrect and technologically advanced intelligent life is much rarer than we believe, or (2) our current observations are incomplete, and we simply have not detected them yet, or (3) our search methodologies are flawed and we are not searching for the correct indicators, or (4) it is the nature of intelligent life to destroy itself. There are multiple explanations proposed for the Fermi paradox, ranging from analyses suggesting that intelligent life is rare (the \"Rare Earth hypothesis\"), to analyses suggesting that although extraterrestrial civilizations may be common, they would not communicate with us, would communicate in a way we have not discovered yet, could not travel across interstellar distances, or destroy themselves before they master the technology of either interstellar travel or communication. The German astrophysicist and radio astronomer Sebastian von Hoerner suggested that the average duration of civilization was 6,500 years. After this time, according to him, it disappears for external reasons (the destruction of life on the planet, the destruction of only rational beings) or internal"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_33",
    "chunk": "causes (mental or physical degeneration). According to his calculations, on a habitable planet (one in three million stars) there is a sequence of technological species over a time distance of hundreds of millions of years, and each of them \"produces\" an average of four technological species. With these assumptions, the average distance between civilizations in the Milky Way is 1,000 light years. Science writer Timothy Ferris has posited that since galactic societies are most likely only transitory, an obvious solution is an interstellar communications network, or a type of library consisting mostly of automated systems. They would store the cumulative knowledge of vanished civilizations and communicate that knowledge through the galaxy. Ferris calls this the \"Interstellar Internet\", with the various automated systems acting as network \"servers\". If such an Interstellar Internet exists, the hypothesis states, communications between servers are mostly through narrow-band, highly directional radio or laser links. Intercepting such signals is, as discussed earlier, very difficult. However, the network could maintain some broadcast nodes in hopes of making contact with new civilizations. Although somewhat dated in terms of \"information culture\" arguments, not to mention the obvious technological problems of a system that could work effectively for billions of years"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_34",
    "chunk": "and requires multiple lifeforms agreeing on certain basics of communications technologies, this hypothesis is actually testable (see below). A significant problem is the vastness of space. Despite piggybacking on the world's most sensitive radio telescope, astronomer and initiator of SERENDIP Charles Stuart Bowyer noted the then world's largest instrument could not detect random radio noise emanating from a civilization like ours, which has been leaking radio and TV signals for less than 100 years. For SERENDIP and most other SETI projects to detect a signal from an extraterrestrial civilization, the civilization would have to be beaming a powerful signal directly at us. It also means that Earth civilization will only be detectable within a distance of 100 light-years. The International Academy of Astronautics (IAA) has a long-standing SETI Permanent Study Group (SPSG, formerly called the IAA SETI Committee), which addresses matters of SETI science, technology, and international policy. The SPSG meets in conjunction with the International Astronautical Congress (IAC), held annually at different locations around the world, and sponsors two SETI Symposia at each IAC. In 2005, the IAA established the SETI: Post-Detection Science and Technology Taskgroup (chairman, Professor Paul Davies) \"to act as a Standing Committee to be available"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_35",
    "chunk": "to be called on at any time to advise and consult on questions stemming from the discovery of a putative signal of extraterrestrial intelligent (ETI) origin.\" However, the protocols mentioned apply only to radio SETI rather than for METI (Active SETI). The intention for METI is covered under the SETI charter \"Declaration of Principles Concerning Sending Communications with Extraterrestrial Intelligence\". In October 2000 astronomers Iván Almár and Jill Tarter presented a paper to The SETI Permanent Study Group in Rio de Janeiro, Brazil which proposed a scale (modelled after the Torino scale) which is an ordinal scale between zero and ten that quantifies the impact of any public announcement regarding evidence of extraterrestrial intelligence; the Rio scale has since inspired the 2005 San Marino Scale (in regard to the risks of transmissions from Earth) and the 2010 London Scale (in regard to the detection of extraterrestrial life). The Rio scale itself was revised in 2018. The SETI Institute does not officially recognize the Wow! signal as of extraterrestrial origin as it was unable to be verified, although in a 2020 Twitter post the organization stated that ''an astronomer might have pinpointed the host star''. The SETI Institute has also publicly"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_36",
    "chunk": "denied that the candidate signal Radio source SHGb02+14a is of extraterrestrial origin. Although other volunteering projects such as Zooniverse credit users for discoveries, there is currently no crediting or early notification by SETI@Home following the discovery of a signal. Some people, including Steven M. Greer, have expressed cynicism that the general public might not be informed in the event of a genuine discovery of extraterrestrial intelligence due to significant vested interests. Some, such as Bruce Jakosky have also argued that the official disclosure of extraterrestrial life may have far reaching and as yet undetermined implications for society, particularly for the world's religions. Active SETI, also known as messaging to extraterrestrial intelligence (METI), consists of sending signals into space in the hope that they will be detected by an alien intelligence. In November 1974, a largely symbolic attempt was made at the Arecibo Observatory to send a message to other worlds. Known as the Arecibo Message, it was sent towards the globular cluster M13, which is 25,000 light-years from Earth. Further IRMs Cosmic Call, Teen Age Message, Cosmic Call 2, and A Message From Earth were transmitted in 1999, 2001, 2003 and 2008 from the Evpatoria Planetary Radar. Whether or not"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_37",
    "chunk": "to attempt to contact extraterrestrials has attracted significant academic debate in the fields of space ethics and space policy. Physicist Stephen Hawking, in his book A Brief History of Time, suggests that \"alerting\" extraterrestrial intelligences to our existence is foolhardy, citing humankind's history of treating its own kind harshly in meetings of civilizations with a significant technology gap, e.g., the extermination of Tasmanian aborigines. He suggests, in view of this history, that we \"lay low\". In one response to Hawking, in September 2016, astronomer Seth Shostak sought to allay such concerns. Astronomer Jill Tarter also disagrees with Hawking, arguing that aliens developed and long-lived enough to communicate and travel across interstellar distances would have evolved a cooperative and less violent intelligence. She however thinks it is too soon for humans to attempt active SETI and that humans should be more advanced technologically first but keep listening in the meantime. As various SETI projects have progressed, some have criticized early claims by researchers as being too \"euphoric\". For example, Peter Schenkel, while remaining a supporter of SETI projects, wrote in 2006 that: [i]n light of new findings and insights, it seems appropriate to put excessive euphoria to rest and to take"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_38",
    "chunk": "a more down-to-earth view [...] We should quietly admit that the early estimates—that there may be a million, a hundred thousand, or ten thousand advanced extraterrestrial civilizations in our galaxy—may no longer be tenable. Critics claim that the existence of extraterrestrial intelligence has no good Popperian criteria for falsifiability, as explained in a 2009 editorial in Nature, which said: Seti... has always sat at the edge of mainstream astronomy. This is partly because, no matter how scientifically rigorous its practitioners try to be, SETI can't escape an association with UFO believers and other such crackpots. But it is also because SETI is arguably not a falsifiable experiment. Regardless of how exhaustively the Galaxy is searched, the null result of radio silence doesn't rule out the existence of alien civilizations. It means only that those civilizations might not be using radio to communicate. Nature added that SETI was \"marked by a hope, bordering on faith\" that aliens were aiming signals at us, that a hypothetical alien SETI project looking at Earth with \"similar faith\" would be \"sorely disappointed\", despite our many untargeted radar and TV signals, and our few targeted Active SETI radio signals denounced by those fearing aliens, and that"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_39",
    "chunk": "it had difficulties attracting even sympathetic working scientists and government funding because it was \"an effort so likely to turn up nothing\". However, Nature also added, \"Nonetheless, a small SETI effort is well worth supporting, especially given the enormous implications if it did succeed\" and that \"happily, a handful of wealthy technologists and other private donors have proved willing to provide that support\". Supporters of the Rare Earth Hypothesis argue that advanced lifeforms are likely to be very rare, and that, if that is so, then SETI efforts will be futile. However, the Rare Earth Hypothesis itself faces many criticisms. In 1993, Roy Mash stated that \"Arguments favoring the existence of extraterrestrial intelligence nearly always contain an overt appeal to big numbers, often combined with a covert reliance on generalization from a single instance\" and concluded that \"the dispute between believers and skeptics is seen to boil down to a conflict of intuitions which can barely be engaged, let alone resolved, given our present state of knowledge\". In response, in 2012, Milan M. Ćirković, then research professor at the Astronomical Observatory of Belgrade and a research associate of the Future of Humanity Institute at the University of Oxford, said that"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_40",
    "chunk": "Mash was unrealistically over-reliant on excessive abstraction that ignored the empirical information available to modern SETI researchers. George Basalla, Emeritus Professor of History at the University of Delaware, is a critic of SETI who argued in 2006 that \"extraterrestrials discussed by scientists are as imaginary as the spirits and gods of religion or myth\", and was in turn criticized by Milan M. Ćirković for, among other things, being unable to distinguish between \"SETI believers\" and \"scientists engaged in SETI\", who are often sceptical (especially about quick detection), such as Freeman Dyson and, at least in their later years, Iosif Shklovsky and Sebastian von Hoerner, and for ignoring the difference between the knowledge underlying the arguments of modern scientists and those of ancient Greek thinkers. Massimo Pigliucci, Professor of Philosophy at CUNY – City College, asked in 2010 whether SETI is \"uncomfortably close to the status of pseudoscience\" due to the lack of any clear point at which negative results cause the hypothesis of Extraterrestrial Intelligence to be abandoned, before eventually concluding that SETI is \"almost-science\", which is described by Milan M. Ćirković as Pigliucci putting SETI in \"the illustrious company of string theory, interpretations of quantum mechanics, evolutionary psychology and"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_41",
    "chunk": "history (of the 'synthetic' kind done recently by Jared Diamond)\", while adding that his justification for doing so with SETI \"is weak, outdated, and reflecting particular philosophical prejudices similar to the ones described above in Mash and Basalla\". Richard Carrigan, a particle physicist at the Fermi National Accelerator Laboratory near Chicago, Illinois, suggested that passive SETI could also be dangerous and that a signal released onto the Internet could act as a computer virus. Computer security expert Bruce Schneier dismissed this possibility as a \"bizarre movie-plot threat\". Ufologist Stanton Friedman has often criticized SETI researchers for, among other reasons, what he sees as their unscientific criticisms of Ufology, but, unlike SETI, Ufology has generally not been embraced by academia as a scientific field of study, and it is usually characterized as a partial or total pseudoscience. In a 2016 interview, Jill Tarter pointed out that it is still a misconception that SETI and UFOs are related. She states, \"SETI uses the tools of the astronomer to attempt to find evidence of somebody else's technology coming from a great distance. If we ever claim detection of a signal, we will provide evidence and data that can be independently confirmed. UFOs—none of"
  },
  {
    "source": "Search for extraterrestrial intelligence.txt",
    "chunk_id": "Search for extraterrestrial intelligence.txt_42",
    "chunk": "the above.\" The Galileo Project headed by Harvard astronomer Avi Loeb is one of the few scientific efforts to study UFOs or UAPs. Loeb criticized that the study of UAP is often dismissed and not sufficiently studied by scientists and should shift from \"occupying the talking points of national security administrators and politicians\" to the realm of science. The Galileo Project's position after the publication of the 2021 UFO Report by the U.S. Intelligence community is that the scientific community needs to \"systematically, scientifically and transparently look for potential evidence of extraterrestrial technological equipment\"."
  },
  {
    "source": "Selenean summit.txt",
    "chunk_id": "Selenean summit.txt_0",
    "chunk": "# Selenean summit Selenean summit refers to the highest point on the Moon, notionally similar to Mount Everest on the Earth. At some 10,786 m (35,387 ft) above the lunar mean, it is nearly twenty percent 'taller' than Earth's relative highest point, Everest. The summit is located along the north-eastern rim of Engel'gardt crater. Although methods of measurement differ somewhat (e.g., the Moon lacks a sea level), since its discovery in 2010 by the LRO teams, nowhere else has surpassed this region's height measurements on the lunar surface. Approximate coordinates for the summit are 5°24′45″N 158°38′01″W﻿ / ﻿5.4125°N 158.6335°W﻿ / 5.4125; -158.6335. Later meassurements put the summit at 5°26′28″N 158°39′22″W﻿ / ﻿5.441°N 158.656°W﻿ / 5.441; -158.656 and 10.629 kilometres (6.605 mi)."
  },
  {
    "source": "Selenography.txt",
    "chunk_id": "Selenography.txt_0",
    "chunk": "# Selenography Selenography is the study of the surface and physical features of the Moon (also known as geography of the Moon, or selenodesy). Like geography and areography, selenography is a subdiscipline within the field of planetary science. Historically, the principal concern of selenographists was the mapping and naming of the lunar terrane identifying maria, craters, mountain ranges, and other various features. This task was largely finished when high resolution images of the near and far sides of the Moon were obtained by orbiting spacecraft during the early space era. Nevertheless, some regions of the Moon remain poorly imaged (especially near the poles) and the exact locations of many features (like crater depths) are uncertain by several kilometers. Today, selenography is considered to be a subdiscipline of selenology, which itself is most often referred to as simply \"lunar science.\" The word selenography is derived from the Greek word Σελήνη (Selene, meaning Moon) and γράφω graphō, meaning to write. The idea that the Moon is not perfectly smooth originates to at least c. 450 BC, when Democritus asserted that the Moon's \"lofty mountains and hollow valleys\" were the cause of its markings. However, not until the end of the 15th century"
  },
  {
    "source": "Selenography.txt",
    "chunk_id": "Selenography.txt_1",
    "chunk": "AD did serious selenography begin. Around AD 1603, William Gilbert made the first lunar drawing based on naked-eye observation. Others soon followed, and when the telescope was invented, initial drawings of poor accuracy were made, but soon thereafter improved in tandem with optics. In the early 18th century, the librations of the Moon were measured, which revealed that more than half of the lunar surface was visible to observers on Earth. In 1750, Johann Meyer produced the first reliable set of lunar coordinates that permitted astronomers to locate lunar features. Lunar mapping became systematic in 1779 when Johann Schröter began meticulous observation and measurement of lunar topography. In 1834 Johann Heinrich von Mädler published the first large cartograph (map) of the Moon, comprising 4 sheets, and he subsequently published The Universal Selenography. All lunar measurement was based on direct observation until March 1840, when J.W. Draper, using a 5-inch reflector, produced a daguerreotype of the Moon and thus introduced photography to astronomy. At first, the images were of very poor quality, but as with the telescope 200 years earlier, their quality rapidly improved. By 1890 lunar photography had become a recognized subdiscipline of astronomy. The 20th century witnessed more advances"
  },
  {
    "source": "Selenography.txt",
    "chunk_id": "Selenography.txt_2",
    "chunk": "in selenography. In 1959, the Soviet spacecraft Luna 3 transmitted the first photographs of the far side of the Moon, giving the first view of it in history. The United States launched the Ranger spacecraft between 1961 and 1965 to photograph the lunar surface until the instant they impacted it, the Lunar Orbiters between 1966 and 1967 to photograph the Moon from orbit, and the Surveyors between 1966 and 1968 to photograph and softly land on the lunar surface. The Soviet Lunokhods 1 (1970) and 2 (1973) traversed almost 50 km of the lunar surface, making detailed photographs of the lunar surface. The Clementine spacecraft obtained the first nearly global cartograph (map) of the lunar topography, and also multispectral images. Successive missions transmitted photographs of increasing resolution. The Moon has been measured by the methods of laser altimetry and stereo image analysis, including data obtained during several missions. The most visible topographical feature is the giant far-side South Pole-Aitken basin, which possesses the lowest elevations of the Moon. The highest elevations are found just to the northeast of this basin, and it has been suggested that this area might represent thick ejecta deposits that were emplaced during an oblique South"
  },
  {
    "source": "Selenography.txt",
    "chunk_id": "Selenography.txt_3",
    "chunk": "Pole-Aitken basin impact event. Other large impact basins, such as the maria Imbrium, Serenitatis, Crisium, Smythii, and Orientale, also possess regionally low elevations and elevated rims. Another distinguishing feature of the Moon's shape is that the elevations are on average about 1.9 km higher on the far side than the near side. If it is assumed that the crust is in isostatic equilibrium, and that the density of the crust is everywhere the same, then the higher elevations would be associated with a thicker crust. Using gravity, topography and seismic data, the crust is thought to be on average about 50 ± 15 km thick, with the far-side crust being on average thicker than the near side by about 15 km. The oldest known illustration of the Moon was found in a passage grave in Knowth, County Meath, Ireland. The tomb was carbon dated to 3330–2790 BC. Leonardo da Vinci made and annotated some sketches of the Moon in c. 1500. William Gilbert made a drawing of the Moon in which he denominated a dozen surface features in the late 16th century; it was published posthumously in De Mondo Nostro Sublunari Philosophia Nova. After the invention of the telescope, Thomas"
  },
  {
    "source": "Selenography.txt",
    "chunk_id": "Selenography.txt_4",
    "chunk": "Harriot (1609), Galileo Galilei (1609), and Christoph Scheiner (1614) made drawings also. Denominations of the surface features of the Moon, based on telescopic observation, were made by Michael van Langren in 1645. Many of his denominations were distinctly Catholic, denominating craters in honor of Catholic royalty and capes and promontories in honor of Catholic saints. The lunar maria were denominated in Latin for terrestrial seas and oceans. Minor craters were denominated in honor of astronomers, mathematicians, and other famous scholars. In 1647, Johannes Hevelius produced the rival work Selenographia, which was the first lunar atlas. Hevelius ignored the nomenclature of Van Langren and instead denominated the lunar topography according to terrestrial features, such that the names of lunar features corresponded to the toponyms of their geographical terrestrial counterparts, especially as the latter were denominated by the ancient Roman and Greek civilizations. This work of Hevelius influenced his contemporary European astronomers, and the Selenographia was the standard reference on selenography for over a century. Giambattista Riccioli, SJ, a Catholic priest and scholar who lived in northern Italy authored the present scheme of Latin lunar nomenclature. His Almagestum novum was published in 1651 as summary of then current astronomical thinking and recent"
  },
  {
    "source": "Selenography.txt",
    "chunk_id": "Selenography.txt_5",
    "chunk": "developments. In particular he outlined the arguments in favor of and against various cosmological models, both heliocentric and geocentric. Almagestum Novum contained scientific reference matter based on contemporary knowledge, and contemporary educators across Europe widely used it. Although this handbook of astronomy has long since been superseded, its system of lunar nomenclature is used even today. The lunar illustrations in the Almagestum novum were drawn by a fellow Jesuit educator named Francesco Grimaldi, SJ. The nomenclature was based on a subdivision of the visible lunar surface into octants that were numbered in Roman style from I to VIII. Octant I referenced the northwest section and subsequent octants proceeded clockwise in alignment with compass directions. Thus Octant VI was to the south and included Clavius and Tycho Craters. The Latin nomenclature had two components: the first denominated the broad features of terrae (lands) and maria (seas) and the second denominated the craters. Riccioli authored lunar toponyms derived from the names of various conditions, including climactic ones, whose causes were historically attributed to the Moon. Thus there were the seas of crises (\"Mare Crisium\"), serenity (\"Mare Serenitatis\"), and fertility (\"Mare Fecunditatis\"). There were also the seas of rain (\"Mare Imbrium\"), clouds (\"Mare"
  },
  {
    "source": "Selenography.txt",
    "chunk_id": "Selenography.txt_6",
    "chunk": "Nubium\"), and cold (\"Mare Frigoris\"). The topographical features between the maria were comparably denominated, but were opposite the toponyms of the maria. Thus there were the lands of sterility (\"Terra Sterilitatis\"), heat (\"Terra Caloris\"), and life (\"Terra Vitae\"). However, these names for the highland regions were supplanted on later cartographs (maps). See List of features on the Moon for a complete list. Many of the craters were denominated topically pursuant to the octant in which they were located. Craters in Octants I, II, and III were primarily denominated based on names from ancient Greece, such as Plato, Atlas, and Archimedes. Toward the middle in Octants IV, V, and VI craters were denominated based on names from the ancient Roman Empire, such as Julius Caesar, Tacitus, and Taruntius. Toward the southern half of the lunar cartograph (map) craters were denominated in honor of scholars, writers, and philosophers of medieval Europe and Arabic regions. The outer extremes of Octants V, VI, and VII, and all of Octant VIII were denominated in honor of contemporaries of Giambattista Riccioli. Features of Octant VIII were also denominated in honor of Copernicus, Kepler, and Galileo. These persons were \"banished\" to it far from the \"ancients,\" as"
  },
  {
    "source": "Selenography.txt",
    "chunk_id": "Selenography.txt_7",
    "chunk": "a gesture to the Catholic Church. Many craters around the Mare Nectaris were denominated in honor of Catholic saints pursuant to the nomenclature of Van Langren. All of them were, however, connected in some mode with astronomy. Later cartographs (maps) removed the \"St.\" from their toponyms. The lunar nomenclature of Giambattista Riccioli was widely used after the publication of his Almagestum Novum, and many of its toponyms are presently used. The system was scientifically inclusive and was considered eloquent and poetic in style, and therefore it appealed widely to his contemporaries. It was also readily extensible with new toponyms for additional features. Thus it replaced the nomenclature of Van Langren and Hevelius. Later astronomers and lunar cartographers augmented the nomenclature with additional toponyms. The most notable among these contributors was Johann H. Schröter, who published a very detailed cartograph (map) of the Moon in 1791 titled the Selenotopografisches Fragmenten. Schröter's adoption of Riccioli's nomenclature perpetuated it as the universally standard lunar nomenclature. A vote of the International Astronomical Union (IAU) in 1935 established the lunar nomenclature of Riccioli, which included 600 lunar toponyms, as universally official and doctrinal. The IAU later expanded and updated the lunar nomenclature in the 1960s,"
  },
  {
    "source": "Selenography.txt",
    "chunk_id": "Selenography.txt_8",
    "chunk": "but new toponyms were limited to toponyms honoring deceased scientists. After Soviet spacecraft photographed the far side of the Moon, many of the newly discovered features were denominated in honor of Soviet scientists and engineers. The IAU assigned all subsequent new lunar toponyms. Some craters were denominated in honor of space explorers. Johann H. Mädler authored the nomenclature for satellite craters. The subsidiary craters surrounding a major crater were identified by a letter. These subsidiary craters were usually smaller than the crater with which they were associated, with some exceptions. The craters could be assigned letters \"A\" through \"Z,\" with \"I\" omitted. Because the great majority of the toponyms of craters were masculine, the major craters were generically denominated \"patronymic\" craters. The assignment of the letters to satellite craters was originally somewhat haphazard. Letters were typically assigned to craters in order of significance rather than location. Precedence depended on the angle of illumination from the Sun at the time of the telescopic observation, which could change during the lunar day. In many cases the assignments were seemingly random. In a number of cases the satellite crater was located closer to a major crater with which it was not associated. To"
  },
  {
    "source": "Selenography.txt",
    "chunk_id": "Selenography.txt_9",
    "chunk": "identify the patronymic crater, Mädler placed the identifying letter to the side of the midpoint of the feature that was closest to the associated major crater. This also had the advantage of permitting omission of the toponyms of the major craters from the cartographs (maps) when their subsidiary features were labelled. Over time, lunar observers assigned many of the satellite craters an eponym. The International Astronomical Union (IAU) assumed authority to denominate lunar features in 1919. The commission for denominating these features formally adopted the convention of using capital Roman letters to identify craters and valleys. When suitable maps of the far side of the Moon became available by 1966, Ewen Whitaker denominated satellite features based on the angle of their location relative to the major crater with which they were associated. A satellite crater located due north of the major crater was identified as \"Z\". The full 360° circle around the major crater was then subdivided evenly into 24 parts, like a 24-hour clock. Each \"hour\" angle, running clockwise, was assigned a letter, beginning with \"A\" at 1 o'clock. The letters \"I\" and \"O\" were omitted, resulting in only 24 letters. Thus a crater due south of its major"
  },
  {
    "source": "Selenography.txt",
    "chunk_id": "Selenography.txt_10",
    "chunk": "crater was identified as \"M\". The Moon obviously lacks any mean sea level to be used as vertical datum. The USGS's Lunar Orbiter Laser Altimeter (LOLA), an instrument on NASA's Lunar Reconnaissance Orbiter (LRO), employs a digital elevation model (DEM) that uses the nominal lunar radius of 1,737.4 km (1,079.6 mi). The selenoid (the geoid for the Moon) has been measured gravimetrically by the GRAIL twin satellites. The following historically notable lunar maps and atlases are arranged in chronological order by publication date."
  },
  {
    "source": "Semantic Scholar.txt",
    "chunk_id": "Semantic Scholar.txt_0",
    "chunk": "# Semantic Scholar Semantic Scholar is a research tool for scientific literature. It is developed at the Allen Institute for AI and was publicly released in November 2015. Semantic Scholar uses modern techniques in natural language processing to support the research process, for example by providing automatically generated summaries of scholarly papers. The Semantic Scholar team is actively researching the use of artificial intelligence in natural language processing, machine learning, human–computer interaction, and information retrieval. Semantic Scholar began as a database for the topics of computer science, geoscience, and neuroscience. In 2017, the system began including biomedical literature in its corpus. As of September 2022, it includes over 200 million publications from all fields of science. Semantic Scholar provides a one-sentence summary of scientific literature. One of its aims was to address the challenge of reading numerous titles and lengthy abstracts on mobile devices. It also seeks to ensure that the three million scientific papers published yearly reach readers, since it is estimated that only half of this literature is ever read. Artificial intelligence is used to capture the essence of a paper, generating it through an \"abstractive\" technique. The project uses a combination of machine learning, natural language processing,"
  },
  {
    "source": "Semantic Scholar.txt",
    "chunk_id": "Semantic Scholar.txt_1",
    "chunk": "and machine vision to add a layer of semantic analysis to the traditional methods of citation analysis, and to extract relevant figures, tables, entities, and venues from papers. Another key AI-powered feature is Research Feeds, an adaptive research recommender that uses AI to quickly learn what papers users care about reading and recommends the latest research to help scholars stay up to date. It uses a state-of-the-art paper embedding model trained using contrastive learning to find papers similar to those in each Library folder. Semantic Scholar also offers Semantic Reader, an augmented reader with the potential to revolutionize scientific reading by making it more accessible and richly contextual. Semantic Reader provides in-line citation cards that allow users to see citations with TLDR (short for Too Long, Didn't Read) automatically generated short summaries as they read and skimming highlights that capture key points of a paper so users can digest faster. In contrast with Google Scholar and PubMed, Semantic Scholar is designed to highlight the most important and influential elements of a paper. The AI technology is designed to identify hidden connections and links between research topics. Like the previously cited search engines, Semantic Scholar also exploits graph structures, which include"
  },
  {
    "source": "Semantic Scholar.txt",
    "chunk_id": "Semantic Scholar.txt_2",
    "chunk": "the Microsoft Academic Knowledge Graph, Springer Nature's SciGraph, and the Semantic Scholar Corpus (originally a 45 million papers corpus in computer science, neuroscience and biomedicine). Each paper hosted by Semantic Scholar is assigned a unique identifier called the Semantic Scholar Corpus ID (abbreviated S2CID). The following entry is an example: Liu, Ying; Gayle, Albert A; Wilder-Smith, Annelies; Rocklöv, Joacim (March 2020). \"The reproductive number of COVID-19 is higher compared to SARS coronavirus\". Journal of Travel Medicine. 27 (2). doi:10.1093/jtm/taaa021. PMID 32052846. S2CID 211099356. Semantic Scholar is free to use and unlike similar search engines (i.e. Google Scholar) does not search for material that is behind a paywall. One study compared the index scope of Semantic Scholar to Google Scholar, and found that for the papers cited by secondary studies in computer science, the two indices had comparable coverage, each only missing a handful of the papers. As of January 2018, following a 2017 project that added biomedical papers and topic summaries, the Semantic Scholar corpus included more than 40 million papers from computer science and biomedicine. In March 2018, Doug Raymond, who developed machine learning initiatives for the Amazon Alexa platform, was hired to lead the Semantic Scholar project. As"
  },
  {
    "source": "Semantic Scholar.txt",
    "chunk_id": "Semantic Scholar.txt_3",
    "chunk": "of August 2019, the number of included papers metadata (not the actual PDFs) had grown to more than 173 million after the addition of the Microsoft Academic Graph records. In 2020, a partnership between Semantic Scholar and the University of Chicago Press Journals made all articles published under the University of Chicago Press available in the Semantic Scholar corpus. At the end of 2020, Semantic Scholar had indexed 190 million papers. In 2020, Semantic Scholar reached seven million users per month."
  },
  {
    "source": "Silicon-burning process.txt",
    "chunk_id": "Silicon-burning process.txt_0",
    "chunk": "# Silicon-burning process In astrophysics, silicon burning is a very brief sequence of nuclear fusion reactions that occur in massive stars with a minimum of about 8–11 solar masses. Silicon burning is the final stage of fusion for massive stars that have run out of the fuels that power them for their long lives in the main sequence on the Hertzsprung–Russell diagram. It follows the previous stages of hydrogen, helium, carbon, neon and oxygen burning processes. Silicon burning begins when gravitational contraction raises the star's core temperature to 2.7–3.5 billion kelvins (GK). The exact temperature depends on mass. When a star has completed the silicon-burning phase, no further fusion is possible. The star catastrophically collapses and may explode in what is known as a Type II supernova. After a star completes the oxygen-burning process, its core is composed primarily of silicon and sulfur. If it has sufficiently high mass, it further contracts until its core reaches temperatures in the range of 2.7–3.5 GK (230–300 keV). At these temperatures, silicon and other elements can photodisintegrate, emitting a proton or an alpha particle. Silicon burning proceeds by photodisintegration rearrangement, which creates new elements by the alpha process, adding one of these freed"
  },
  {
    "source": "Silicon-burning process.txt",
    "chunk_id": "Silicon-burning process.txt_1",
    "chunk": "alpha particles (the equivalent of a helium nucleus) per capture step in the following sequence (photoejection of alphas not shown): The chain could theoretically continue, as adding further alphas continues to be exothermic all the way to tin-100. However, the steps after nickel-56 are much less exothermic and the temperature is so high that photodisintegration prevents further progress. The silicon-burning sequence lasts about one day before being struck by the shock wave that was launched by the core collapse. Burning then becomes much more rapid at the elevated temperature and stops only when the rearrangement chain has been converted to nickel-56 or is stopped by supernova ejection and cooling. The nickel-56 decays first to cobalt-56 and then to iron-56, with half-lives of 6 and 77 days respectively, but this happens later, because only minutes are available within the core of a massive star. The star has run out of nuclear fuel and within minutes its core begins to contract. During this phase of the contraction, the potential energy of gravitational contraction heats the interior to 5 GK (430 keV) and this opposes and delays the contraction. However, since no additional heat energy can be generated via new fusion reactions, the"
  },
  {
    "source": "Silicon-burning process.txt",
    "chunk_id": "Silicon-burning process.txt_2",
    "chunk": "final unopposed contraction rapidly accelerates into a collapse lasting only a few seconds. The central portion of the star is now crushed into a neutron core with the temperature soaring further to 100 GK (8.6 MeV) that quickly cools down into a neutron star if the mass of the star is below 20 M☉. Between 20 M☉ and 40–50 M☉, fallback of the material will make the neutron core collapse further into a black hole. The outer layers of the star are blown off in an explosion known as a Type II supernova that lasts days to months. The supernova explosion releases a large burst of neutrons, which may synthesize in about one second roughly half of the supply of elements in the universe that are heavier than iron, via a rapid neutron-capture sequence known as the r-process (where the \"r\" stands for \"rapid\" neutron capture). This graph shows the binding energy per nucleon of various nuclides. The binding energy is the difference between the energy of free protons and neutrons and the energy of the nuclide. If the product or products of a reaction have higher binding energy per nucleon than the reactant or reactants, then the reaction is"
  },
  {
    "source": "Silicon-burning process.txt",
    "chunk_id": "Silicon-burning process.txt_3",
    "chunk": "exothermic (releases energy) and can go forward, though this is valid only for reactions that do not change the number of protons or neutrons (no weak force reactions). As can be seen, light nuclides such as deuterium or helium release large amounts of energy (a big increase in binding energy) when combined to form heavier elements—the process of fusion. Conversely, heavy elements such as uranium release energy when broken into lighter elements—the process of nuclear fission. In stars, rapid nucleosynthesis proceeds by adding helium nuclei (alpha particles) to heavier nuclei. As mentioned above, this process ends around atomic mass 56. Decay of nickel-56 explains the large amount of iron-56 seen in metallic meteorites and the cores of rocky planets."
  },
  {
    "source": "Smart Lander for Investigating Moon.txt",
    "chunk_id": "Smart Lander for Investigating Moon.txt_0",
    "chunk": "# Smart Lander for Investigating Moon Smart Lander for Investigating Moon (SLIM), dubbed \"Moon Sniper\", was a lunar lander mission of the Japan Aerospace Exploration Agency (JAXA). The lander's initial launch date in 2021 was postponed until 2023 due to delays in its rideshare, the X-Ray Imaging and Spectroscopy Mission (XRISM). On 6 September 2023 at 23:42 UTC, XRISM launched, and SLIM separated from it later that day. On 1 October 2023, SLIM executed its trans-lunar orbit injection burns. The lander entered lunar orbit on 25 December 2023 and landed on 19 January 2024 at 15:20 UTC, making Japan the fifth country to soft-land a spacecraft on the Moon. News reports of technical difficulties made it to Earth, saying that the lander's solar panels were not oriented to the Sun; however, on 29 January, the lander became operational after conditions shifted. It has survived three lunar nights, awakening again in April. SLIM's operation on the Moon was terminated at 22:40 on August 23, 2024 (JST). SLIM, having survived three lunar nights, broke the world record for longevity among devices on the Moon that do not have an RTG. The main purpose of Japan's first lunar surface mission was to demonstrate"
  },
  {
    "source": "Smart Lander for Investigating Moon.txt",
    "chunk_id": "Smart Lander for Investigating Moon.txt_1",
    "chunk": "precision lunar landing. During its descent, the lander recognized lunar craters by applying technology from facial recognition systems, and determined its current location from observation data collected by the SELENE (Kaguya) lunar orbiter mission. SLIM aimed to perform a soft landing with an accuracy range of 100 m (330 ft). In comparison, the accuracy of the 1969 piloted Apollo 11 Lunar Module Eagle was an elliptic which was 20 km (12 mi) long in downrange and 5 km (3.1 mi) wide in crossrange. According to Yoshifumi Inatani, deputy director general of the JAXA Institute of Space and Astronautical Science (ISAS), succeeding in this extremely precise landing will lead to enhanced quality of space exploration. The expected cost for developing this project is 18 billion yen, or US$121.5 million. The proposal came to be known as the Small Lunar Landing Experiment Satellite (小型月着陸実験衛星) (SLIM). On 27 December 2013, ISAS called for proposals for its next \"Competitively-Chosen Medium-Sized Focused Mission\", and SLIM was among the seven proposals submitted. In June 2014, SLIM passed the semi-final selection along with the DESTINY+ technology demonstration mission, and in February 2015 SLIM was ultimately selected. From April 2016, SLIM gained project status within JAXA. In May"
  },
  {
    "source": "Smart Lander for Investigating Moon.txt",
    "chunk_id": "Smart Lander for Investigating Moon.txt_2",
    "chunk": "2016, Mitsubishi Electric (MELCO) was awarded the contract for building the spacecraft. SLIM was not the first Japanese lunar lander built for operation on the Moon's surface; on 27 May 2016 NASA announced that the OMOTENASHI (Outstanding Moon exploration Technologies demonstrated by Nano Semi-Hard Impactor) CubeSat lander jointly developed by JAXA and the University of Tokyo was to be launched as a secondary payload on Space Launch System (SLS) Artemis 1. OMOTENASHI was meant to deploy a mini lunar lander weighing 1 kg; however, on November 21, 2022, JAXA announced that attempts to communicate with the spacecraft had ceased, because the solar cells failed to generate power when facing away from the Sun. They did not face the Sun again until March 2023. In 2017, funding difficulties for developing XRISM led to SLIM's launch being switched from its own dedicated Epsilon flight to a rideshare H-IIA flight. The resulting cost savings will be transferred to develop other satellites that are behind schedule due to XRISM. Lunar Excursion Vehicle 1 (LEV-1) is a lunar rover which moves by hopping. It has its own direct-to-Earth communication equipment, two wide-angle visible light cameras, and electrical components and UHF band antennas courtesy of the"
  },
  {
    "source": "Smart Lander for Investigating Moon.txt",
    "chunk_id": "Smart Lander for Investigating Moon.txt_3",
    "chunk": "MINERVA and OMOTENASHI projects. Lunar Excursion Vehicle 2 (LEV-2) or Sora-Q, is a tiny rover developed by JAXA in joint cooperation with the toy company Tomy, Sony, and Doshisha University. The baseball-sized rover has a mass of 250 g and is equipped with two small cameras. LEV-2 extends its shape to crawl on the lunar surface using two wheels at its sides, a method of locomotion inspired by frogs and sea turtles; it can \"run\" for approximately two hours. It is the second rover of its kind to attempt operations on the lunar surface; the first was on Hakuto-R Mission 1, which crashed before it could be deployed. SLIM was successfully launched together with the X-Ray Imaging and Spectroscopy Mission (XRISM) space telescope on 6 September 2023 at 23:42 UTC (7 September 08:42 Japan Standard Time) planning to land near Shioli crater (13.3°S, 25.2°E) via weak stability boundary-like trajectory. SLIM entered lunar orbit 25 December JST. The lunar lander, nicknamed Moon Sniper for its extremely accurate landing precision within the projected 100 meters (330 ft) long landing ellipse, touched down onto the Moon on 19 January 2024 at 15:20 UTC, at the Sea of Nectar (Mare Nectaris), south of the"
  },
  {
    "source": "Smart Lander for Investigating Moon.txt",
    "chunk_id": "Smart Lander for Investigating Moon.txt_4",
    "chunk": "Theophilus crater. Japan thus became the fifth nation to successfully soft land an operational spacecraft on the Moon, after the Soviet Union, United States, China, and India. Although SLIM landed successfully, it landed on its side with the solar panels oriented westwards facing opposite the Sun at the start of the lunar day, thereby failing to generate enough power. The lander was able to operate on internal battery power for a short period of time, but was manually powered down on 19 January 2024 at 17:57 UTC (20 January 02:57 Japan Standard Time) to prevent over-discharge of the battery. The two lunar rovers, deployed while the lander was hovering just before it touched down, worked as planned, with LEV-1 communicating independently to ground stations. LEV-1 conducted seven hops over 107 minutes on the lunar surface. Images autonomously taken by Sora-Q (a capability it shares with its sister rover) showed the SLIM had landed at a 90-degree angle, effectively on its nose, and there had been the loss of an engine nozzle during descent and even possible damage to its Earth-oriented antenna. Irrespective of wrong attitude and loss of communication with the lander, the mission is already successful given confirmation of"
  },
  {
    "source": "Smart Lander for Investigating Moon.txt",
    "chunk_id": "Smart Lander for Investigating Moon.txt_5",
    "chunk": "its primary goal: landing within 100 m (330 ft) of its landing spot. JAXA gave itself 60 out of 100 for the landing. After the shutdown on 19 January 2024, the mission's operators still hoped that the lander would wake up in a few days when the Sun would be correctly oriented so sunlight would hit the now askew solar panels. The two rovers, LEV-1 and Sora-Q, continued to operate autonomously as planned. On 25 January JAXA informed the LEV-1 rover has completed its planned operational period on the lunar surface, depleted its designated power, and in a standby state on the lunar surface. While the capability to resume activity exists contingent on solar power generation from changes in the direction of the Sun, efforts will be maintained to continue receiving signals from LEV-1. On 28 January, the lander resumed operations after being shut for a week. JAXA said it re-established contact with the lander and its solar cells were working again after a shift in lighting conditions allowed it to catch sunlight. After that, SLIM was put in sleep mode for the impending harsh lunar night. SLIM was expected to operate only for one lunar daylight period, or 14"
  },
  {
    "source": "Smart Lander for Investigating Moon.txt",
    "chunk_id": "Smart Lander for Investigating Moon.txt_6",
    "chunk": "Earth days, and the on-board electronics were not designed to withstand the −120 °C (−184 °F) nighttime temperatures on the Moon. On 25 February 2024, JAXA sent wake-up calls and found SLIM had successfully survived the night on the lunar surface while maintaining communication capabilities. Since it was midday of the lunar day on 25 February 2024, the temperature of the communications payload was extremely high, so communication was terminated after only a short period of time. JAXA was preparing for resumed operations, once the temperature has fallen sufficiently. This feat of surviving the lunar night without a radioisotope heater unit had previously been achieved only by some landers in the Surveyor Program. On 27 March 2024, SLIM survived its second lunar night and woke up, sending more images back to Earth, showing \"perseverance.\" JAXA said \"According to the acquired data, some temperature sensors and unused battery cells are starting to malfunction, but the majority of functions that survived the first lunar night was maintained even after the second lunar night.\" SLIM completed its second overnight operation in the early hours of March 30th, and went dormant again. On 23 April 2024, it survived its third lunar night and woke"
  },
  {
    "source": "Smart Lander for Investigating Moon.txt",
    "chunk_id": "Smart Lander for Investigating Moon.txt_7",
    "chunk": "up sending more images back to Earth. JAXA said on the social media platform X that SLIM’s key functions were still working despite repeated harsh cycles of temperature changes. The Agency stated that they \"plan to attempt to resume operation again in mid to late May, when SLIM’s solar cells start generating electricity.\" JAXA sent commands for recovery on 24 and 25 May, when it was assumed that power had been restored, but the radio waves from SLIM could not be confirmed. JAXA conducted the operation again on the night of the 27th, but there was no response from SLIM. As the sun has set around SLIM and power generation is no longer possible, this lunar day's operation has unsuccessfully ended. JAXA planned to try operating SLIM again the next month, when it was expected to have sufficient solar power. However, all attempts to re-establish communication with the craft failed, and as of August 26th, the mission was officially concluded."
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_0",
    "chunk": "# SN 1054 SN 1054 is a supernova that was first observed on c. 10 July [O.S. c. 4 July] 1054, and remained visible until c. 12 April [O.S. c. 6 April] 1056. The event was recorded in contemporary Chinese astronomy, and references to it are also found in a later (13th-century) Japanese document and in a document from the Islamic world. Furthermore, there are a number of proposed references from European sources recorded in the 15th century, as well as a pictograph associated with the Ancestral Puebloan culture found near the Peñasco Blanco site in New Mexico, United States. The pyramids at Cahokia in the midwestern United States may have been built in response to the supernova's appearance in the sky. The remnant of SN 1054, which consists of debris ejected during the explosion, is known as the Crab Nebula. It is located in the sky near the star Zeta Tauri (ζ Tauri). The core of the exploding star formed a pulsar, called the Crab Pulsar (or PSR B0531+21). The nebula and the pulsar that it contains are some of the most studied astronomical objects outside the Solar System. It is one of the few Galactic supernovae where the"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_1",
    "chunk": "date of the explosion is well known. The two objects are the most luminous in their respective categories. For these reasons, and because of the important role it has repeatedly played in the modern era, SN 1054 is one of the best known supernovae in the history of astronomy. The Crab Nebula is easily observed by amateur astronomers thanks to its brightness, and was also catalogued early on by professional astronomers, long before its true nature was understood and identified. When the French astronomer Charles Messier watched for the return of Halley's Comet in 1758, he confused the nebula for the comet, as he was unaware of the former's existence. Motivated by this error, he created his catalogue of non-cometary nebulous objects, the Messier Catalogue, to avoid such mistakes in the future. The nebula is catalogued as the first Messier object, or M1. The Crab Nebula was identified as the supernova remnant of SN 1054 between 1921 and 1942, at first speculatively (1920s), with some plausibility by 1939, and beyond reasonable doubt by Jan Oort in 1942. In 1921, Carl Otto Lampland was the first to announce that he had seen changes in the structure of the Crab Nebula. This"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_2",
    "chunk": "announcement occurred at a time when the nature of the nebulae in the sky was completely unknown. Their nature, size and distance were subject to debate. Observing changes in such objects allows astronomers to determine whether their spatial extension is \"small\" or \"large\", in the sense that notable fluctuations to an object as vast as our Milky Way cannot be seen over a small time period, such as a few years, whereas such substantial changes are possible if the size of the object does not exceed a diameter of a few light-years. Lampland's comments were confirmed some weeks later by John Charles Duncan, an astronomer at the Mount Wilson Observatory. He benefited from photographic material obtained with equipment and emulsions that had not changed since 1909; as a result the comparison with older snapshots was easy and emphasized a general expansion of the cloud. The points were moving away from the centre, and did so faster as they got further from it. Also in 1921, Knut Lundmark compiled the data for the \"guest stars\" mentioned in the Chinese chronicles known in the West. He based this on older works, having analysed various sources such as the Wenxian Tongkao, studied for"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_3",
    "chunk": "the first time from an astronomical perspective by Jean-Baptiste Biot in the middle of the 19th century. Lundmark gives a list of 60 suspected novae, then the generic term for a stellar explosion, in fact covering what is now understood as two distinct phenomena, novae and supernovae. The nova of 1054, already mentioned by the Biots in 1843, is part of the list. It stipulates the location of this guest star in a note at the bottom of the page as being \"close to NGC 1952\", one of the names for the Crab Nebula, but it does not seem to create an explicit link between them. In 1928, Edwin Hubble was the first to note that the changing aspect of the Crab Nebula, which was growing bigger in size, suggests that it is the remains of a stellar explosion. He realised that the apparent speed of change in its size signifies that the explosion which it comes from occurred only nine centuries ago (as observed on Earth), which puts the date of the explosion in the period covered by Lundmark's compilation. He also noted that the only possible nova in the region of Taurus (where the cloud is located) is"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_4",
    "chunk": "that of 1054, whose age is estimated to correspond to an explosion dating from the start of the second millennium. Hubble therefore deduced, correctly, that this cloud was the remains of the explosion which was observed by Chinese astronomers. Hubble's comment remained relatively unknown as the physical phenomenon of the explosion was not known at the time. Eleven years later, when the fact that supernovae are very bright phenomena was highlighted by Walter Baade and Fritz Zwicky and when their nature was suggested by Zwicky, Nicholas Mayall proposed that the star of 1054 was actually a supernova, based on the speed of expansion of the cloud, measured by spectroscopy, which allows astronomers to determine its physical size and distance, which he estimated at 5000 light-years. This was under the assumption that the velocities of expansion along the line of sight and perpendicularly to it were identical. Based on the reference to the brightness of the star which featured in the first documents discovered in 1934, he deduced that it was a supernova rather than a nova. This deduction was subsequently refined, which pushed Mayall and Jan Oort in 1942 to analyse historic accounts relating to the guest star more closely"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_5",
    "chunk": "(see § Historical records below). These new accounts, globally and mutually concordant, confirm the initial conclusions by Mayall and Oort in 1939 and the identification of the guest star of 1054 is established beyond all reasonable doubt. Most other historical supernovas are not confirmed so conclusively: supernovas of the first millennium (SN 185, SN 386 and SN 393) are established on the basis of a single document each, and so they cannot be confirmed; in relation to the supposed historical supernova which followed the one in 1054, SN 1181, there are legitimate doubts concerning the proposed remnant (3C58) and an object of less than 1000 years of age. Other historical supernovae of which there are written accounts which precede the invention of the telescope (SN 1006, SN 1572 and SN 1604) are however established with certitude. Telescope-era supernovae are of course associated with full certitude with their remnant, when one is observed, but none is known within the Milky Way. SN 1054 is one of eight supernovae in the Milky Way that can be identified because written testimony describing the explosion has survived. In the nineteenth century, astronomers began to take an interest in the historic records. They compiled and"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_6",
    "chunk": "examined the records as part of their research on recent novae, comets, and later, the supernovae. The first Westerners to attempt a systematic compilation of records from China were the father and son Biot. In 1843, the sinologist Édouard Biot translated for his father, the astronomer and physicist Jean-Baptiste Biot, passages from the astronomical treatise of the 348-volume Chinese encyclopaedia, the Wenxian Tongkao. Almost 80 years later in 1921, Knut Lundmark undertook a similar effort based on a greater number of sources. In 1942, Jan Oort, convinced that the Crab Nebula was the \"guest star\" of 1054 described by the Chinese, asked sinologist J.J.L. Duyvendak to help him compile new evidence on the observation of the event. Star-like objects that appeared temporarily in the sky were generically called \"guest stars\" (kè xīng 客星) by Chinese astronomers. The guest star of 1054 occurred during the reign of the Emperor Renzong of the Song dynasty (960–1279). The relevant year is recorded in Chinese documents as \"the first year of the Zhihe era\". Zhihe was an era name used during the reign of Emperor Renzong, and corresponds to the years 1054–1056, so the first year of the Zhihe era corresponds to the year"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_7",
    "chunk": "1054. Some of the Chinese accounts are well preserved and detailed. The oldest and most detailed accounts are from Song Huiyao and Song Shi, historiographical works of which the extant text was redacted perhaps within a few decades of the event. There are also some later records, redacted in the 13th century, which are not necessarily independent of the older ones. Three accounts are apparently related because they describe the angular distance from the guest star to Zeta Tauri as \"perhaps several inches away\", but they are in apparent disagreement about the date of appearance of the star. The older two mention the day jichou 己丑, but the third, the Xu Zizhi Tongjian Changbian, the day yichou 乙丑. These terms refer to the Chinese sexagenary cycle, corresponding to numbers 26 and 2 of the cycle, which corresponds, in the context where they are cited, respectively, to 4 July and 10 June. As the redaction of the third source is of considerably later date (1280) and the two characters are similar, this is easily explained as a transcription error, the historical date being jichou 己丑, 4 July. The description of the guest star's location as \"to the south-east of Tianguan, perhaps"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_8",
    "chunk": "several inches away\" has perplexed modern astronomers, because the Crab Nebula is not situated in the south-east, but to the north-west of Zeta Tauri. The duration of visibility is explicitly mentioned in chapter 12 of Song Shi, and slightly less accurately, in the Song Huiyao. The last sighting was on 6 April 1056, after a total period of visibility of 642 days. This duration is supported by the Song Shi. The Song Huiyao by contrast mentions a visibility of the guest star of only 23 days, but this is after mentioning visibility during daylight. This period of 23 days applies in all likelihood solely to visibility during the day, which naturally was much shorter. The Song Huiyao (literally \"Collected important documents of the Song dynasty\") covers the period 960–1220. Huiyao is a traditional form of history books in China which aimed mainly to preserve primary sources, and as such are important sources supplementing the official Twenty-Four Histories. The Song dynasty had a specific government department dedicated to compiling the Huiyao, and some 2,200 volumes were published in ten batches during the Song dynasty. However, most of these documents were lost by the time of the Qing Dynasty except for the"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_9",
    "chunk": "synopsis and a relatively small portion preserved as part of the imperial Yongle Encyclopedia. In 1809, the portion preserved in the Yongle Encyclopedia was extracted and re-published as the Song Huiyao Jigao (the \"draft extract of the Song Huiyao\"). Subsequent scholars have worked on the project further and the current edition dates from 1936. This document recounts the observation of the guest star, focusing on the astrological aspect but also giving important information on the visibility of the star, by day and by night. Zhihe era, first year, seventh lunar month, 22nd day. [...] Yang Weide declared: \"I humbly observe that a guest star has appeared; above the star there is a feeble yellow glimmer. If one examines the divination regarding the Emperor, the interpretation [of the presence of this guest star] is the following: The fact that the star has not overrun Bi and that its brightness must represent a person of great value. I demand that the Office of Historiography is informed of this.\" All officials congratulated the Emperor, who ordered his congratulations be [back] forwarded to the Office of Historiography. First year of the era of Jiayou, third lunar month, the director of the Astronomical Office said"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_10",
    "chunk": "\"The guest star has disappeared, which means the departure of the host [that it represents].\" Previously, during the first year of the Zhihe era, during the fifth lunar month, it had appeared at dawn, in the direction of the east, under the watch of Tiānguān (天關, Zeta Tauri). It had been seen in daylight, like Venus. It had rays stemming in all directions, and its colour was reddish white. Altogether visible for 23 days. The Song Shi is the official annals of the Song dynasty. Chapter 12 mentions the guest star, not its appearance but rather the moment of its disappearance. The corresponding entry dated 6 April 1056 indicates: Jiayou era, first year, third lunar month, xinwei day, the director of the Office of Astronomy reported during the fifth lunar month of the first year of the Zhihe era, a guest star had appeared at dawn, in the direction of the east, under the watch of Tianguan. Now it has disappeared. In chapter 56 (\"Astronomical treaty\") of the same document, the guest star is again mentioned in a chapter dedicated to this type of phenomenon, this time focusing on its appearance, Zhihe era of the reign, first year, fifth lunar"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_11",
    "chunk": "month, jichou day. A guest star has appeared to the south-east of Tianguan, perhaps several inches away. After a year or more, it gradually disappeared. The Xu Zizhi Tongjian Changbian (\"Long compilation of the continuation of the Zizhi Tongjian\"), a book covering the period of 960–1126 and written 40 years or so later by Li Tao (1114–1183), contains the oldest Chinese testimonies relating to the observation of the star. It was rediscovered in 1970 by the specialist in Chinese civilisations Ho Peng Yoke and collaborators. It is relatively imprecise in the case of the explosion of SN 1054. A loose translation of what was stated: First year of the Zhihe era, fifth lunar month, ji-chou day. A guest star has appeared to the south-east of Tianguan, perhaps several inches away [of this star]. (The star disappeared in the third lunar month of the first year of the Jiayou era.) There is an account of the star from the Liao dynasty, which ruled in the area around northeast China from 907 to 1125. The book in question, the Qidan Guo Zhi, was compiled by Ye Longli in 1247. It includes various astronomical notes, some of which are clearly copied from the"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_12",
    "chunk": "Song Shi. This entry referring to the star of 1054 seems unique: Chongxi era of the reign of [King Xingzong], twenty-third year eighth lunar moon, the ruler of the realm is dead. It happened before a solar eclipse at noon, and a guest star appeared. The highest officer at the Office of History, Liu Yishou had said \"These are omens of the death of the King.\" This prediction has been realised. The account of Qidan Guo Zhi alluded to the notable astronomical events that preceded the death of King Xingzong. Various historical documents allow us to establish the date of death of the Emperor Xingzong as 28 August 1055, during the eighth lunar month of the twenty-fourth (and not twenty-third) year of his reign. The dates of the two astronomical events mentioned (the eclipse and the appearance of the guest star) are not specified, but were probably before the obituary (2 or 3 years at most). Two solar eclipses were visible shortly before that date in the Khitan kingdom, on 13 November 1053 and 10 May 1054. Of these, only one occurred around noon, that of 13 November; it seems likely that this is what the document mentions. As for"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_13",
    "chunk": "the guest star, only a rough estimate of location is given, corresponding to the moon mansion Mao. This mansion is situated just east of where the star appeared, as mentioned in the other testimonies. Since no other known significant astronomical event occurred in this region of the sky during the two years that preceded the death of Xingzong, it seems likely that the text is actually referring to the star of 1054. The Wenxian Tongkao is the first East Asian source that came to the attention of Western astronomers; it was translated by Édouard Biot in 1843. This source, compiled by Ma Duanlin in 1280, is relatively brief. The text is very close to that of the Song Shi: Zhihe era of the reign, first year, fifth lunar month, ji-chou day. A guest star has appeared to the south-east of Tiānguān, perhaps several inches away. After a year or more, it gradually disappeared. The asterisms (or \"constellations\") of Chinese astronomy were catalogued around the 2nd century BC. The asterisms with the brightest stars in the sky were compiled in a work called Shi Shi, which also includes Tianguan. Identification of Tianguan is comparatively easy, as it is indicated that it"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_14",
    "chunk": "is located at the foot of the Five Chariots asterism, the nature of which is left in hardly any doubt by representation on maps of the Chinese sky: it consists of a large pentagon containing the bright stars of the Auriga. As Tianguan is also represented to the north of the Three Stars asterism, the composition of which is well known, corresponding to the bright stars of Orion, its possible localisation is strongly restricted to the immediate proximity of the star ζ Tauri, located between \"Five Chariots\" and \"Three Stars\". This star, of medium brightness (apparent magnitude of 3.3), is the only star of its level of brightness in this area of the sky (there is no other star that is brighter than an apparent magnitude of 4.5 within 7 degrees of ζ Tauri), and therefore the only one likely to figure among the asterisms of \"Shi Shi\". All of these elements, along with some others, allow \"Tianguan\" to be confirmed beyond reasonable doubt as corresponding to the star ζ Tauri. Three Chinese documents indicate that the guest star was located \"perhaps a few inches\" South-East of Tianguan. Song Shi and Song Huiyao stipulate that it \"was standing guard\" for"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_15",
    "chunk": "the asterism, corresponding to the star ζ Tauri. The \"South-East\" orientation has a simple astronomical meaning, the celestial sphere having, like the Earth's globe, both north and south celestial poles, the \"South-East\" direction thus corresponding to a \"bottom-left\" location in relation to the reference object (in this case, the star ζ Tauri) when it appears at the South. However, this \"South-East\" direction has long left modern astronomers perplexed in the context of this event: the logical remnant of the supernova corresponding to the guest star is the Crab Nebula, but it is not situated to the southeast of ζ Tauri, rather in the opposite direction, to the northwest. The term \"perhaps a few inches\" (ke shu cun in the Latin transliteration) is relatively uncommon in Chinese astronomical documents. The first term, ke, is translated as \"approximately\" or \"perhaps\", the latter being currently preferred. The second term, shu, means \"several\", and more specifically any number between 3 and 9 (limits included). Finally, cun resembles a unit of measurement for angles translated by the term \"inch\". It is part of a group of three angular units, zhang (also written chang), chi (\"foot\") and cun (\"inch\"). Different astronomical documents indicate without much possible"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_16",
    "chunk": "discussion that a zhang corresponds to ten chi, and that one chi corresponds to ten cun. The angular units are not the ones used to determine stars' coordinates, which are given in terms of du, an angular unit corresponding to the average angular distance travelled by the sun per day, which corresponds to around 360/365.25 degrees, in other words almost one degree. The use of different angular units can be surprising, but it is similar to the current situation in modern astronomy, where the angular unit used to measure angular distances between two points is certainly the same as for declination (the degree), but is different for right ascension (which is expressed in angular hours; an angular hour corresponds to exactly 15 degrees). In Chinese astronomy, right ascension and declination have the same unit, which is not the one used for other angular distances. The reason for this choice to use different units in the Chinese world is not well known. However, the exact value of these new units (zhang, chi and cun) was never stipulated, but can be deduced by the context in which they are used. For example, the spectacular passing of Halley's comet in 837 indicates that"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_17",
    "chunk": "the tail of the comet measured 8 zhang. Even if it is not possible to know the angular size of the comet at the time it passed, it is certain that 8 zhang correspond to 180 degrees at the most (maximum visible angle on the celestial sphere), which means that one zhang can hardly exceed 20 degrees, and therefore one cun cannot exceed 0.2 degrees. A more rigorous estimation was made from 1972 on the basis of references of minimal separations expressed in chi or cun between two stars in the case of various conjunctions. The results suggest that one cun is between 0.1 and 0.2 degrees and that one chi is between 0.44 and 2.8 degrees, a range which is compatible with the estimations for one cun. A more solid estimation error is that it is generally accepted that one chi is in the order of one degree (or one du), and that one cun is in the order of one tenth of a degree. The expression \"perhaps a few inches\" therefore suggests an angular distance in the order of one degree or less. If all the available elements strongly suggest that the star of 1054 was a supernova,"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_18",
    "chunk": "and that in the area next to where the star was seen, there is a remnant of a supernova which has all of the characteristics expected of an object that is around 1,000 years old, a major problem arises: the new star is described as being to the South-East of Tianguan, while the Crab Nebula is to the North-East. This problem has been known since the 1940s and has long been unsolved. In 1972 for example, Ho Peng Yoke and his colleagues suggested that the Crab Nebula was not the product of the explosion of 1054, but that the true remnant was to the South-East, as indicated in several Chinese sources. For this, they envisaged that the angular unit cun corresponds to a considerable angle of 1 or 2 degrees, meaning that the distance from the remnant to ζ Tauri was therefore considerable. Aside from the fact that this theory does not account for the large angular sizes of certain comets, expressed in zhang, it comes up against the fact that there it does not make sense to measure the gap between a guest star and a star located so far away from it, when there are closer asterisms that"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_19",
    "chunk": "could be used. In their controversial article (see European sightings, below) Collins and his colleagues make another suggestion: on the morning of 4 July, the star ζ Tauri was not bright enough and too low on the horizon to be visible. If the guest star, which was located close to it, was visible, it is only because its brightness was comparable to Venus. However, there was another star, brighter and higher on the horizon, which was possibly visible, for reference: Beta Tauri (β Tauri). This star is located at around 8 degrees north-north-west of ζ Tauri. The Crab Nebula is south-south-east of β Tauri. Collins et al. suggest therefore that at the time of its discovery, the star was seen to the south-east of β Tauri, and that as the days passed and visibility improved, astronomers were able to see that it was in fact a lot closer to ζ Tauri, but that the direction \"south-east\" used for the first star was kept in error. The solution to this problem was suggested (without proof) by A. Breen and D. McCarthy in 1995 and proved very convincingly by D. A. Green et F. R. Stephenson (2003). The term \"stand on guard\""
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_20",
    "chunk": "obviously signifies a proximity between the two stars, but also means a general orientation: a guest star \"standing on guard\" for a fixed star is systematically located below it. In order to support this theory, Green and Stephenson investigated other entries in Song Shi, which also includes reference to \"standing on guard\". They selected entries relating to conjunctions betweens the stars identified and planets, of which the trajectory can be calculated without difficulty and with great precision on the indicated dates. Of the 18 conjunctions analysed, spreading from 1172 (the Jupiter–Regulus conjunction on 5 December) to 1245 (the Saturn–Gamma Virginis conjunction on 17 May), the planet was more to the north (in the sense of a lower declination) in 15 cases, and in the three remaining cases, it was never in the south quadrant of the star. In addition, Stephenson and Clark (1977) had already highlighted such an inversion of direction in a planetary conjunction: on 13 September 1253, an entry in the astronomical report Koryo-sa indicated that Mars had hidden the star to the south-east of the twenty-eight mansions sign Ghost (Delta Cancri), while in reality, it approached the star north-west of the asterism (Eta Cancri). The oldest and"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_21",
    "chunk": "most detailed record from Japan is in the Meigetsuki, the diary of Fujiwara no Teika, a poet and courtier. There are two other Japanese documents, presumably dependent on the Meigetsuki: The Meigetsuki places the event in the fourth lunar moon, one month earlier than the Chinese texts. Whatever the exact date during this month, there seems to be a contradiction between this period and the observation of the guest star: the star was close to the sun, making daytime and nighttime observation impossible. The visibility in daylight as described by the Chinese texts is thus validated by the Japanese documents, and is consistent with a period of moderate visibility, which implies that the star's period of diurnal visibility was very short. In contrast, the day of the cycle given in the Chinese documents is compatible with the months that they state, reinforcing the idea that the month on the Japanese document is incorrect. The study of other medieval supernovas (SN 1006 and SN 1181) reveals a proximity in the dates of discovery of a guest star in China and Japan, although clearly based on different sources. Fujiwara no Teika's interest in the guest star seems to have come accidentally whilst"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_22",
    "chunk": "observing a comet in December 1230, which prompted him to search for evidence of past guest stars, among those SN 1054 (as well as SN 1006 and SN 1181, the two other historic supernovas from the early second millennium). The entry relating to SN 1054 can be translated as: Tengi era of the emperor Go-Reizei, second year, fourth lunar month, after the middle period of ten days. At chou [a Chinese term for 1–3am], a guest star appeared in the degrees of the moon mansions of Zuixi and Shen. It has been viewed in the direction of the East and has emerged from the Tianguan star. It was as big as Jupiter. The source used by Fujiwara no Teika is the records of Yasutoshi Abe (Onmyōdō doctor), but it seems to have been based, for all the astronomical events he has recorded, on documents of Japanese origin. The date he gives is prior to the third part of ten days of the lunar month mentioned, which corresponds to the period of between 30 May and 8 June 1054 of the Julian calendar, which is around one month earlier than Chinese documentation. This difference is usually attributed to an error in"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_23",
    "chunk": "the lunar months (fourth place and fifth place). The location of the guest star, clearly straddling the moon mansions Shen and Zuixi, corresponds to what would be expected of a star appearing in the immediate vicinity of Tianguan. While SN 1006, which was significantly brighter than SN 1054, was mentioned by several Arab chroniclers, there exist no Arabic reports relating to the rather faint SN 1181. Only one Arabic account has been found concerning SN 1054, whose brightness is between those of the last two stars mentioned. This account, discovered in 1978, is that of a Nestorian Christian doctor, Ibn Butlan, transcribed in the Uyun al-Anba, a book on detailed biographies of physicians in the Islamic world compiled by Ibn Abi Usaybi'a (1194–1270) in the mid-thirteenth century. This is a translation of the passage in question: I copied the following hand written testimony [that of Ibn Butlan]. He stated: \"One of the famous epidemics of our time has occurred when a spectacular star appeared in [the zodiac star] Gemini, of the year 446 [of the Muslim calendar]. In the autumn of that year, fourteen thousand people were buried in Constantinople. Thereafter, in the middle of the summer of 447, the"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_24",
    "chunk": "majority of the people of Fostat and all foreigners died\". He [Ibn Butlan] continues: \"While this spectacular star appeared in the sign of Gemini [...] it caused the epidemic in Fostat by the Nile being low when it appeared in 445 [sic].\" The three years cited (AH 445, 446, 447) refer, respectively, to: 23 April 1053 – 11 April 1054, 12 April 1054 – 1 April 1055, and 2 April 1055 – 20 March 1056. There is an apparent inconsistency in the year of occurrence of the star, first announced as 446, then 445. This problem is solved by reading other entries in the book, which quite explicitly specify that the Nile was low at 446. This year of the Muslim calendar ran from 12 April 1054 to 1 April 1055, which is compatible with the appearance of the star in July 1054, as its location (admittedly rather vague), is in the astrological sign of Gemini (which, due to axial precession, covers the eastern part of the Constellation Taurus). The date of the event in 446 is harder to determine, but the reference to the level of the Nile refers to the period preceding its annual flood, which happens during"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_25",
    "chunk": "the summer. Since 1980, several European documents have been identified as possible observations of the supernova. The first such suggestion was made in 1980 by Umberto Dall'Olmo (1925–1980). The following passage which reports an astronomical sighting is taken from an account compiled by Jacobus Malvecius in the 15th century: And in those days a star of immense brightness appeared within the circle of the Moon a few days after its separation from the Sun. The date this passage refers to is not explicit, however, and by means of a reference to an earthquake in Brescia 11 April 1064, it would seem ten years too late. Dall'Olmo suggests this is due to a transcription error. Another candidate is the Cronaca Rampona, proposed in 1981, which however also indicates a date several years after the event, in 1058 instead of 1054. The European candidate documents are imprecise, especially lacking in astronomical terms likely due to European scholars having lost many of the astronomical skills of antiquity. In contrast, the Chinese accounts pin-point within a degree where the supernova occurred, as well as how long it lasted and roughly how bright it became. The lack of accounts from European chroniclers has long raised"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_26",
    "chunk": "questions. In fact, it is known that the supernova of 1006 was recorded in a large number of European documents, albeit not in astronomical terms. Among the proposed explanations for the lack of European accounts of SN 1054, its concurrence with the East-West Schism is prominent. In fact, the date of the excommunication of the Patriarch of Constantinople Michael I Cerularius (16 July) corresponds to the star reaching its maximum brightness and being visible in the daytime. Among the six proposed European documents, one does not seem to correspond to the year of the supernova (the chronicle of Jacobus Malvecius). Another (the Cronaca Rampona) has large dating and internal coherence problems. The four others are relatively precisely dated, but they date from Spring and not Summer 1054, that is to say before the conjunction between the supernova and the Sun (although a Khitan document suggests this may have been possible). Three of the documents (the chronicle of Jacobus Malvecius, the Cronaca Rampona and the Armenian chronicle) make reference relatively explicitly to conjunctions between the Moon and stars, of which one is identified (Jupiter, in the Armenian chronicle). The three other documents are very unclear. In 1999, George W. Collins and"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_27",
    "chunk": "his colleagues defended the plausibility of European sighting of SN 1054. They argue that the records suggest that European sightings even predate Chinese and Japanese reports by more than two months (April 1054). These authors emphasize the problems associated with the Chinese reports, especially the position of the supernova relative to Zeta Tauri. They also adduce a Khitan document which they suggest might establish observation of the supernova at the time of the solar eclipse of 10 May 1054 (which would corrobate the \"late\" date of Chinese observation of the event). Conversely, they interpret the European documents, taken in conjunction, as plausibly establishing that an unusual astronomical phenomenon was visible in Europe in the spring of 1054, i.e. even before the Sun's conjunction with Zeta Tauri. They also surmise that the correct year in the report by Ibn Butlan is AH 445 (23 April 1053 – 11 April 1054) rather than AH 446 (12 April 1054 – 1 April 1055). The publication by Collins et al. was criticized by Stephenson and Green (2003). These authors insist that the problems with the Chinese and Japanese documents can easily be resolved philologically (as common copyists' mistakes) and need not indicate unreliability of"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_28",
    "chunk": "the Chinese observations. Stephenson and Green condemn attempts at uncovering European sightings of the supernova as it were at any cost as suffering from confirmation bias, \"anxious to ensure that this event was recorded by Europeans\". They also reject the idea of the Khitan document referring to the supernova as a mistake based in a translation of the document. The European account of a supernova sighting that is considered the most plausible is part of a medieval chronicle from the region of Bologna, the Cronaca Rampona. This text came to astronomers' attention in 1972, and was interpreted as a possible sighting of the supernova in 1981, and again in 1999. The relevant part of the chronicle translates to: In AD 1058, Pope Stephen IX has come to the throne [...] Also in this year of Christ 1058, Henry III reigned [or \"lived\"] for 49 years. He went to Rome for the first time in the month of May. At this time, famine and death was upon the whole world. He stayed in the province of Tibur for three days in the month of June [...] At that time, a very brightly-shining star (stella clarissima) entered into the circle [or the"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_29",
    "chunk": "circuit] of the new moon, in the thirteenth calends at the beginning of the night. Before even looking for potential problems in the astronomical last sentence of the passage, skeptics point out at least two discrepancies in the dating: Pope Stephen IX became Pope in 1057, not 1058, and Emperor Henry III, Holy Roman Emperor was born in 1017, 39 and not 49 years before 1058, his reign having started in 1039 (as King of the Romans, then as emperor of the Romans from 1046 after Pope Clement II consecrated him during his brief pontificate). Henry III died in 1056, and his reign did not overlap with Stephen IX's papacy. It seems likely that the text underwent various alterations, as its date format uses a mix of Roman and Arabic numerals (the number 1058 is for instance written as Ml8) which was common in the 15th century when the Cronaca Rampona was assembled, but not in the 11th century when the events occurred. Associating the stella clarissima with the 1054 supernova also requires assuming that its entry in the Cronaca Rampona is out of order, as the entries are otherwise in chronological order and the two previous entries are later"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_30",
    "chunk": "than 1054 (in order, the previous entries refer to 1046, 1049, 1051, 1055, 1056, all written in a mix of Arab and Roman characters, namely Mxl6, Mxl9, Mli, Mlv and Ml6). Additionally, the date of the new moon is discrepant. Calculating the phase of the moon for every day of 1054 and converting the calends, which refer to the Roman calendar, to our Gregorian calendar shows that no month of that year had a new moon on the thirteenth day of its Calends. All of this strongly contrasts with the general precision of references to eclipse dates in medieval European chronicles: a study of 48 partial or total solar eclipses from 733 to 1544 finds that 42 dates out of 48 are correct, and of the six remaining, three are incorrect by one of two days and the three others give the correct day and month, but a wrong year. Even assuming that the stated event nevertheless corresponds to May or June 1054, and that it describes a conjunction between the already visible supernova and the moon, a final problem arises: the moon did not pass very close to the location of the supernova during two those months. It is"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_31",
    "chunk": "therefore possible that the account instead describes an approach or a concealment of a planet by the Moon, contemporary to the date written in the document (1058). This scenario is corroborated by two perfectly dated contemporary documents which describe a conjunction and a planetary concealment by the Moon in relatively similar terms. These two documents, unearthed by Robert Russell Newton, are taken from the Annales Cavenses, Latin chronicles from la Trinità della Cava (Province of Salerno). They mention \"a bright star that entered into the circle of the (new) moon\" for both 17 February 1086 ([Martii incipiente nocte] stella clarissima in circulum lunae primae ingressa est) and for 6 August 1096 (stella clarissima venit in circulum lunae). The first event can be verified as Venus being eclipsed by the Moon, the second as the Moon passing Jupiter at a distance of less than one degree after a lunar eclipse which was also mentioned in the chronicle. The Cronaca Rampona account is apparently also reflected in the Armenian chronicle of Hayton of Corycus (written before 1307). The relevant passage translated from the Armenian manuscript reads: AD 1048. There was the 5th year, 2nd month, 6th day of Pope Leo in Rome."
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_32",
    "chunk": "Robert Kijart (Robert Guiscard) arrived in Rome and sieged the Tiburtina town. There was starvation over the whole world. That year a bright star appeared within the circle of the Moon, the Moon was new, on 14 May, in the first part of the night. Vahe Gurzadyan's proposal connecting the Hayton of Corycus's chronicle with Cronaca Rampona and SN 1054 dates to 2012. In a work entitled De Obitu Leonis (\"On the Death of [Pope] Leo\") by one subdeacon Libuinus, there is a report of an unusual celestial phenomenon. A certain Albertus, leading a group of pilgrims in the region of Todi, Umbria, reportedly confirmed having seen, on the day that Pope Leo IX died, a phenomenon described as Guidoboni et al. (1994) proposed that this may relate to SN 1054, and was endorsed by Collins et al. (1999). Guidoboni et al. (1994) also proposed a Flemish text as an account of a sighting of the supernova. The text, from Saint Paul's church—no longer extant—in the Flemish town of Oudenburg, describes the death of Pope Leo IX in Spring 1054 (the date described corresponds to 14 April 1054). On the eighteenth calends of May, on the second day of the"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_33",
    "chunk": "week at around midday, the soul [of Pope Leo IX] departed. At the moment it left his body, in Rome, where he rests, but \"also everywhere on earth, a circle of extraordinary brightness appeared in the sky for half an hour.\" McCarthy and Breen (1997) proposed an extract from an Irish chronicle as a possible European sighting of the supernova. This chronicle indicates the following for 1054: A round circle of fire was seen at Ros Ela on the Sunday of the feast day of Saint George over five hours during the day, and countless black birds passed before it, in the centre of which there was a larger bird [...] The date of the event corresponds to 24 April (Saint George's Day is 23 April and fell on a Saturday in 1054; thus the mention of the \"Sunday of Saint George's Day\" corresponds to the next day, 24 April) long before the sighting noted by the Chinese. The astronomical nature of the account remains very uncertain, and interpretation as a solar halo or aurora seems at least as probable. Two Native American paintings in Arizona show a crescent moon located next to a circle that could represent a star."
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_34",
    "chunk": "In 1955, optical engineer and amateur archaeologist William C. Miller proposed that this represents a conjunction between the moon and the supernova, made possible by the fact that, seen from the Earth, the supernova occurred in the path of the Ecliptic. On the morning of 5 July, the moon was located in the immediate proximity of the supernova, and this proximity might have been represented in these paintings. This theory is compatible with the uncertain dating of these paintings but cannot be confirmed. The dating of the paintings is extremely imprecise (between the 10th and 12th century), and only one of them shows the crescent moon with the correct orientation in relation to the supernova on the date of the explosion. Moreover, this type of drawing could well represent a proximity of the moon with Venus or Jupiter. Another, better known document was updated during the 1970s at the Chaco Canyon site (New Mexico), occupied around 1000 AD by the Ancestral Pueblo Peoples. On the flat underside of an overhang, it represents a hand, below which there is a crescent moon facing a star at the bottom-left. On the wall underneath the petroglyph there is a drawing which could be"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_35",
    "chunk": "the core and tail of a comet. Apart from the petroglyph, which could represent the configuration of the moon and supernova on the morning of 5 July 1054, this period corresponds to the apogee of the Ancestral Pueblo civilization. It seems possible to propose an interpretation of the other petroglyph, which, if it is more recent than the other one, could possibly correspond to the passing of Halley's Comet in 1066. Although plausible, this interpretation is impossible to confirm and does not explain why it was the supernova of 1054 that was represented, rather than the supernova of 1006, which was brighter and also visible to this civilisation. The Aboriginal people of the region around Ooldea have passed in oral tradition a detailed account of their mythology of the constellation Orion and the Pleiades. The anthropologist Daisy Bates was the first to attempt to compile records of this story. Work done by her and others has shown that all of the protagonists of the story of Nyeeruna and the Yugarilya correspond to individual stars covering the region around Orion and the Pleiades, with the exception of Baba, the father dingo, which is a major protagonist of the story and of"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_36",
    "chunk": "the yearly re-enactments of the myth by the local people: Again Nyeeruna's magic comes back in great force and brightness, and when Kambugudha sees the strong magic in arm and body, she calls to a father dingo (horn of the Bull) to come and humiliate Nyeeruna and Babba the Dingo rushes over to Nyeeruna and shakes and swings him east and west by his middle and Kambugudha points at him and laughs but her frightened little sisters hide their heads under their little mountain devil neck humps until Babba loosens his hold and returns to his place again. It has been suggested by Leaman and Hamacher that the location usually assigned to Baba by the locals (recorded by Bates as being at the \"horn of the bull\") is more likely to correspond to SN 1054 than to a faint star of that region such as β or ζ Tauri. This is motivated by the reference to Babba \"returning to his place again\" after attacking Nyeeruna which could refer to a transient star, as well as the fact that important characters of the myth are associated with bright stars. However, Leaman and Hamacher clarify there is no solid evidence to support"
  },
  {
    "source": "SN 1054.txt",
    "chunk_id": "SN 1054.txt_37",
    "chunk": "this interpretation, which remains speculative. Hamacher demonstrates the extreme difficulty in identifying supernovae in Indigenous oral traditions. Other elements of the story which have been found to correspond to astronomical elements by these authors include: awareness by the Aboriginal people of the different colors of the stars, possible awareness of the variability of Betelgeuse, observations of meteors in the Orionid meteor shower and the possibility that the rite associated with the myth is held at a time of astronomical significance, corresponding to the few days in the year when due to the Sun's proximity to Orion, it is unseen throughout the night, but is always in the sky during the daytime. The supernova is mentioned in Ayreon's song \"To the Quasar\", from the album Universal Migrator Part 2: Flight of the Migrator. SN 1054 and the lack of European recordings of the event is also mentioned in the historical novel Space by James A. Michener. The popular science book Death by Black Hole by Neil deGrasse Tyson uses SN 1054 to illustrate the relationships between religion, philosophy and human interpretations of astronomical events. The guest star of 1054 is also mentioned in the novel Red Dragon by Thomas Harris."
  },
  {
    "source": "Solar physics.txt",
    "chunk_id": "Solar physics.txt_0",
    "chunk": "# Solar physics Solar physics is the branch of astrophysics that specializes in the study of the Sun. It intersects with many disciplines of pure physics and astrophysics. Because the Sun is uniquely situated for close-range observing (other stars cannot be resolved with anything like the spatial or temporal resolution that the Sun can), there is a split between the related discipline of observational astrophysics (of distant stars) and observational solar physics. The study of solar physics is also important as it provides a \"physical laboratory\" for the study of plasma physics. Babylonians were keeping a record of solar eclipses, with the oldest record originating from the ancient city of Ugarit, in modern-day Syria. This record dates to about 1300 BC. Ancient Chinese astronomers were also observing solar phenomena (such as solar eclipses and visible sunspots) with the purpose of keeping track of calendars, which were based on lunar and solar cycles. Unfortunately, records kept before 720 BC are very vague and offer no useful information. However, after 720 BC, 37 solar eclipses were noted over the course of 240 years. Astronomical knowledge flourished in the Islamic world during medieval times. Many observatories were built in cities from Damascus to"
  },
  {
    "source": "Solar physics.txt",
    "chunk_id": "Solar physics.txt_1",
    "chunk": "Baghdad, where detailed astronomical observations were taken. Particularly, a few solar parameters were measured and detailed observations of the Sun were taken. Solar observations were taken with the purpose of navigation, but mostly for timekeeping. Islam requires its followers to pray five times a day, at specific position of the Sun in the sky. As such, accurate observations of the Sun and its trajectory on the sky were needed. In the late 10th century, Iranian astronomer Abu-Mahmud Khojandi built a massive observatory near Tehran. There, he took accurate measurements of a series of meridian transits of the Sun, which he later used to calculate the obliquity of the ecliptic. Following the fall of the Western Roman Empire, Western Europe was cut from all sources of ancient scientific knowledge, especially those written in Greek. This, plus de-urbanisation and diseases such as the Black Death led to a decline in scientific knowledge in medieval Europe, especially in the early Middle Ages. During this period, observations of the Sun were taken either in relation to the zodiac, or to assist in building places of worship such as churches and cathedrals. In astronomy, the renaissance period started with the work of Nicolaus Copernicus. He"
  },
  {
    "source": "Solar physics.txt",
    "chunk_id": "Solar physics.txt_2",
    "chunk": "proposed that planets revolve around the Sun and not around the Earth, as it was believed at the time. This model is known as the heliocentric model. His work was later expanded by Johannes Kepler and Galileo Galilei. Particularly, Galilei used his new telescope to look at the Sun. In 1610, he discovered sunspots on its surface. In the autumn of 1611, Johannes Fabricius wrote the first book on sunspots, De Maculis in Sole Observatis (\"On the spots observed in the Sun\"). Modern day solar physics is focused towards understanding the many phenomena observed with the help of modern telescopes and satellites. Of particular interest are the structure of the solar photosphere, the coronal heat problem and sunspots. The Solar Physics Division of the American Astronomical Society boasts 555 members (as of May 2007), compared to several thousand in the parent organization. A major thrust of current (2009) effort in the field of solar physics is integrated understanding of the entire Solar System including the Sun and its effects throughout interplanetary space within the heliosphere and on planets and planetary atmospheres. Studies of phenomena that affect multiple systems in the heliosphere, or that are considered to fit within a heliospheric"
  },
  {
    "source": "Solar physics.txt",
    "chunk_id": "Solar physics.txt_3",
    "chunk": "context, are called heliophysics, a new coinage that entered usage in the early years of the current millennium. Helios-A and Helios-B are a pair of spacecraft launched in December 1974 and January 1976 from Cape Canaveral, as a joint venture between the German Aerospace Center and NASA. Their orbits approach the Sun closer than Mercury. They included instruments to measure the solar wind, magnetic fields, cosmic rays, and interplanetary dust. Helios-A continued to transmit data until 1986. The Solar and Heliospheric Observatory, SOHO, is a joint project between NASA and ESA that was launched in December 1995. It was launched to probe the interior of the Sun, make observations of the solar wind and phenomena associated with it and investigate the outer layers of the Sun. A publicly funded mission led by the Japanese Aerospace Exploration Agency, the HINODE satellite, launched in 2006, consists of a coordinated set of optical, extreme ultraviolet and X-ray instruments. These investigate the interaction between the solar corona and the Sun's magnetic field. The Solar Dynamics Observatory (SDO) was launched by NASA in February 2010 from Cape Canaveral. The main goals of the mission are understanding how solar activity arises and how it affects life"
  },
  {
    "source": "Solar physics.txt",
    "chunk_id": "Solar physics.txt_4",
    "chunk": "on Earth by determining how the Sun's magnetic field is generated and structured and how the stored magnetic energy is converted and released into space. The Parker Solar Probe (PSP) was launched in 2018 with the mission of making detailed observations of the outer solar corona. It has made the closest approaches to the Sun of any artificial object. The Advanced Technology Solar Telescope (ATST) is a solar telescope facility that is under construction in Maui. Twenty-two institutions are collaborating on the ATST project, with the main funding agency being the National Science Foundation. Sunspot Solar Observatory (SSO) operates the Richard B. Dunn Solar Telescope (DST) on behalf of the NSF. The Big Bear Solar Observatory in California houses several telescopes including the New Solar Telescope(NTS) which is a 1.6 meter, clear-aperture, off-axis Gregorian telescope. The NTS saw first light in December 2008. Until the ATST comes on line, the NTS remains the largest solar telescope in the world. The Big Bear Observatory is one of several facilities operated by the Center for Solar-Terrestrial Research at New Jersey Institute of Technology (NJIT). The Extreme Ultraviolet Normal Incidence Spectrograph (EUNIS) is a two channel imaging spectrograph that first flew in 2006."
  },
  {
    "source": "Solar physics.txt",
    "chunk_id": "Solar physics.txt_5",
    "chunk": "It observes the solar corona with high spectral resolution. So far, it has provided information on the nature of coronal bright points, cool transients and coronal loop arcades. Data from it also helped calibrating SOHO and a few other telescopes."
  },
  {
    "source": "Solar System model.txt",
    "chunk_id": "Solar System model.txt_0",
    "chunk": "# Solar System model Solar System models, especially mechanical models, called orreries, that illustrate the relative positions and motions of the planets and moons in the Solar System have been built for centuries. While they often showed relative sizes, these models were usually not built to scale. The enormous ratio of interplanetary distances to planetary diameters makes constructing a scale model of the Solar System a challenging task. As one example of the difficulty, the distance between the Earth and the Sun is almost 12,000 times the diameter of the Earth. If the smaller planets are to be easily visible to the naked eye, large outdoor spaces are generally necessary, as is some means for highlighting objects that might otherwise not be noticed from a distance. The Boston Museum of Science had placed bronze models of the planets in major public buildings, all on similar stands with interpretive labels. For example, the model of Jupiter was located in the cavernous South Station waiting area. The properly-scaled, basket-ball-sized model is 1.3 miles (2.14 km) from the model Sun which is located at the museum, graphically illustrating the immense empty space in the Solar System. The objects in such large models do"
  },
  {
    "source": "Solar System model.txt",
    "chunk_id": "Solar System model.txt_1",
    "chunk": "not move. Traditional orreries often did move, and some used clockworks to display the relative speeds of objects accurately. These can be thought of as being correctly scaled in time, instead of distance. Many towns and institutions have built outdoor scale models of the Solar System. Here is a table comparing these models with the actual system."
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_0",
    "chunk": "# Solar System The Solar System is the gravitationally bound system of the Sun and the objects that orbit it. It formed about 4.6 billion years ago when a dense region of a molecular cloud collapsed, forming the Sun and a protoplanetary disc. The Sun is a typical star that maintains a balanced equilibrium by the fusion of hydrogen into helium at its core, releasing this energy from its outer photosphere. Astronomers classify it as a G-type main-sequence star. The largest objects that orbit the Sun are the eight planets. In order from the Sun, they are four terrestrial planets (Mercury, Venus, Earth and Mars); two gas giants (Jupiter and Saturn); and two ice giants (Uranus and Neptune). All terrestrial planets have solid surfaces. Inversely, all giant planets do not have a definite surface, as they are mainly composed of gases and liquids. Over 99.86% of the Solar System's mass is in the Sun and nearly 90% of the remaining mass is in Jupiter and Saturn. There is a strong consensus among astronomers that the Solar System has at least nine dwarf planets: Ceres, Orcus, Pluto, Haumea, Quaoar, Makemake, Gonggong, Eris, and Sedna. There are a vast number of small"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_1",
    "chunk": "Solar System bodies, such as asteroids, comets, centaurs, meteoroids, and interplanetary dust clouds. Some of these bodies are in the asteroid belt (between Mars's and Jupiter's orbit) and the Kuiper belt (just outside Neptune's orbit). Six planets, seven dwarf planets, and other bodies have orbiting natural satellites, which are commonly called 'moons'. The Solar System is constantly flooded by the Sun's charged particles, the solar wind, forming the heliosphere. Around 75–90 astronomical units from the Sun, the solar wind is halted, resulting in the heliopause. This is the boundary of the Solar System to interstellar space. The outermost region of the Solar System is the theorized Oort cloud, the source for long-period comets, extending to a radius of 2,000–200,000 AU. The closest star to the Solar System, Proxima Centauri, is 4.25 light-years (269,000 AU) away. Both stars belong to the Milky Way galaxy. The Solar System formed at least 4.568 billion years ago from the gravitational collapse of a region within a large molecular cloud. This initial cloud was likely several light-years across and probably birthed several stars. As is typical of molecular clouds, this one consisted mostly of hydrogen, with some helium, and small amounts of heavier elements fused"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_2",
    "chunk": "by previous generations of stars. As the pre-solar nebula collapsed, conservation of angular momentum caused it to rotate faster. The center, where most of the mass collected, became increasingly hotter than the surroundings. As the contracting nebula spun faster, it began to flatten into a protoplanetary disc with a diameter of roughly 200 AU and a hot, dense protostar at the center. The planets formed by accretion from this disc, in which dust and gas gravitationally attracted each other, coalescing to form ever larger bodies. Hundreds of protoplanets may have existed in the early Solar System, but they either merged or were destroyed or ejected, leaving the planets, dwarf planets, and leftover minor bodies. Due to their higher boiling points, only metals and silicates could exist in solid form in the warm inner Solar System close to the Sun (within the frost line). They eventually formed the rocky planets of Mercury, Venus, Earth, and Mars. Because these refractory materials only comprised a small fraction of the solar nebula, the terrestrial planets could not grow very large. The giant planets (Jupiter, Saturn, Uranus, and Neptune) formed further out, beyond the frost line, the point between the orbits of Mars and Jupiter"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_3",
    "chunk": "where material is cool enough for volatile icy compounds to remain solid. The ices that formed these planets were more plentiful than the metals and silicates that formed the terrestrial inner planets, allowing them to grow massive enough to capture large atmospheres of hydrogen and helium, the lightest and most abundant elements. Leftover debris that never became planets congregated in regions such as the asteroid belt, Kuiper belt, and Oort cloud. Within 50 million years, the pressure and density of hydrogen in the center of the protostar became great enough for it to begin thermonuclear fusion. As helium accumulates at its core, the Sun is growing brighter; early in its main-sequence life its brightness was 70% that of what it is today. The temperature, reaction rate, pressure, and density increased until hydrostatic equilibrium was achieved: the thermal pressure counterbalancing the force of gravity. At this point, the Sun became a main-sequence star. Solar wind from the Sun created the heliosphere and swept away the remaining gas and dust from the protoplanetary disc into interstellar space. Following the dissipation of the protoplanetary disk, the Nice model proposes that gravitational encounters between planetisimals and the gas giants caused each to migrate into"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_4",
    "chunk": "different orbits. This led to dynamical instability of the entire system, which scattered the planetisimals and ultimately placed the gas giants in their current positions. During this period, the grand tack hypothesis suggests that a final inward migration of Jupiter dispersed much of the asteroid belt, leading to the Late Heavy Bombardment of the inner planets. The Solar System remains in a relatively stable, slowly evolving state by following isolated, gravitationally bound orbits around the Sun. Although the Solar System has been fairly stable for billions of years, it is technically chaotic, and may eventually be disrupted. There is a small chance that another star will pass through the Solar System in the next few billion years. Although this could destabilize the system and eventually lead millions of years later to expulsion of planets, collisions of planets, or planets hitting the Sun, it would most likely leave the Solar System much as it is today. The Sun's main-sequence phase, from beginning to end, will last about 10 billion years for the Sun compared to around two billion years for all other subsequent phases of the Sun's pre-remnant life combined. The Solar System will remain roughly as it is known today"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_5",
    "chunk": "until the hydrogen in the core of the Sun has been entirely converted to helium, which will occur roughly 5 billion years from now. This will mark the end of the Sun's main-sequence life. At that time, the core of the Sun will contract with hydrogen fusion occurring along a shell surrounding the inert helium, and the energy output will be greater than at present. The outer layers of the Sun will expand to roughly 260 times its current diameter, and the Sun will become a red giant. Because of its increased surface area, the surface of the Sun will be cooler (2,600 K (4,220 °F) at its coolest) than it is on the main sequence. The expanding Sun is expected to vaporize Mercury as well as Venus, and render Earth and Mars uninhabitable (possibly destroying Earth as well). Eventually, the core will be hot enough for helium fusion; the Sun will burn helium for a fraction of the time it burned hydrogen in the core. The Sun is not massive enough to commence the fusion of heavier elements, and nuclear reactions in the core will dwindle. Its outer layers will be ejected into space, leaving behind a dense white"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_6",
    "chunk": "dwarf, half the original mass of the Sun but only the size of Earth. The ejected outer layers may form a planetary nebula, returning some of the material that formed the Sun—but now enriched with heavier elements like carbon—to the interstellar medium. Astronomers sometimes divide the Solar System structure into separate regions. The inner Solar System includes Mercury, Venus, Earth, Mars, and the bodies in the asteroid belt. The outer Solar System includes Jupiter, Saturn, Uranus, Neptune, and the bodies in the Kuiper belt. Since the discovery of the Kuiper belt, the outermost parts of the Solar System are considered a distinct region consisting of the objects beyond Neptune. The principal component of the Solar System is the Sun, a G-type main-sequence star that contains 99.86% of the system's known mass and dominates it gravitationally. The Sun's four largest orbiting bodies, the giant planets, account for 99% of the remaining mass, with Jupiter and Saturn together comprising more than 90%. The remaining objects of the Solar System (including the four terrestrial planets, the dwarf planets, moons, asteroids, and comets) together comprise less than 0.002% of the Solar System's total mass. The Sun is composed of roughly 98% hydrogen and helium,"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_7",
    "chunk": "as are Jupiter and Saturn. A composition gradient exists in the Solar System, created by heat and light pressure from the early Sun; those objects closer to the Sun, which are more affected by heat and light pressure, are composed of elements with high melting points. Objects farther from the Sun are composed largely of materials with lower melting points. The boundary in the Solar System beyond which those volatile substances could coalesce is known as the frost line, and it lies at roughly five times the Earth's distance from the Sun. The planets and other large objects in orbit around the Sun lie near the plane of Earth's orbit, known as the ecliptic. Smaller icy objects such as comets frequently orbit at significantly greater angles to this plane. Most of the planets in the Solar System have secondary systems of their own, being orbited by natural satellites called moons. All of the largest natural satellites are in synchronous rotation, with one face permanently turned toward their parent. The four giant planets have planetary rings, thin discs of tiny particles that orbit them in unison. As a result of the formation of the Solar System, planets and most other objects"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_8",
    "chunk": "orbit the Sun in the same direction that the Sun is rotating. That is, counter-clockwise, as viewed from above Earth's north pole. There are exceptions, such as Halley's Comet. Most of the larger moons orbit their planets in prograde direction, matching the direction of planetary rotation; Neptune's moon Triton is the largest to orbit in the opposite, retrograde manner. Most larger objects rotate around their own axes in the prograde direction relative to their orbit, though the rotation of Venus is retrograde. To a good first approximation, Kepler's laws of planetary motion describe the orbits of objects around the Sun. These laws stipulate that each object travels along an ellipse with the Sun at one focus, which causes the body's distance from the Sun to vary over the course of its year. A body's closest approach to the Sun is called its perihelion, whereas its most distant point from the Sun is called its aphelion. With the exception of Mercury, the orbits of the planets are nearly circular, but many comets, asteroids, and Kuiper belt objects follow highly elliptical orbits. Kepler's laws only account for the influence of the Sun's gravity upon an orbiting body, not the gravitational pulls of"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_9",
    "chunk": "different bodies upon each other. On a human time scale, these perturbations can be accounted for using numerical models, but the planetary system can change chaotically over billions of years. The angular momentum of the Solar System is a measure of the total amount of orbital and rotational momentum possessed by all its moving components. Although the Sun dominates the system by mass, it accounts for only about 2% of the angular momentum. The planets, dominated by Jupiter, account for most of the rest of the angular momentum due to the combination of their mass, orbit, and distance from the Sun, with a possibly significant contribution from comets. The radius of the Sun is 0.0047 AU (700,000 km; 400,000 mi). Thus, the Sun occupies 0.00001% (1 part in 10) of the volume of a sphere with a radius the size of Earth's orbit, whereas Earth's volume is roughly 1 millionth (10) that of the Sun. Jupiter, the largest planet, is 5.2 AU from the Sun and has a radius of 71,000 km (0.00047 AU; 44,000 mi), whereas the most distant planet, Neptune, is 30 AU from the Sun. With a few exceptions, the farther a planet or belt is from"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_10",
    "chunk": "the Sun, the larger the distance between its orbit and the orbit of the next nearest object to the Sun. For example, Venus is approximately 0.33 AU farther out from the Sun than Mercury, whereas Saturn is 4.3 AU out from Jupiter, and Neptune lies 10.5 AU out from Uranus. Attempts have been made to determine a relationship between these orbital distances, like the Titius–Bode law and Johannes Kepler's model based on the Platonic solids, but ongoing discoveries have invalidated these hypotheses. Some Solar System models attempt to convey the relative scales involved in the Solar System in human terms. Some are small in scale (and may be mechanical—called orreries)—whereas others extend across cities or regional areas. The largest such scale model, the Sweden Solar System, uses the 110-meter (361-foot) Avicii Arena in Stockholm as its substitute Sun, and, following the scale, Jupiter is a 7.5-meter (25-foot) sphere at Stockholm Arlanda Airport, 40 km (25 mi) away, whereas the farthest current object, Sedna, is a 10 cm (4 in) sphere in Luleå, 912 km (567 mi) away. At that scale, the distance to Proxima Centauri would be roughly 8 times further than the Moon is from Earth. If the Sun–Neptune"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_11",
    "chunk": "distance is scaled to 100 metres (330 ft), then the Sun would be about 3 cm (1.2 in) in diameter (roughly two-thirds the diameter of a golf ball), the giant planets would be all smaller than about 3 mm (0.12 in), and Earth's diameter along with that of the other terrestrial planets would be smaller than a flea (0.3 mm or 0.012 in) at this scale. Besides solar energy, the primary characteristic of the Solar System enabling the presence of life is the heliosphere and planetary magnetic fields (for those planets that have them). These magnetic fields partially shield the Solar System from high-energy interstellar particles called cosmic rays. The density of cosmic rays in the interstellar medium and the strength of the Sun's magnetic field change on very long timescales, so the level of cosmic-ray penetration in the Solar System varies, though by how much is unknown. The zone of habitability of the Solar System is conventionally located in the inner Solar System, where planetary surface or atmospheric temperatures admit the possibility of liquid water. Habitability might be possible in subsurface oceans of various outer Solar System moons. Compared to many extrasolar systems, the Solar System stands out in"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_12",
    "chunk": "lacking planets interior to the orbit of Mercury. The known Solar System lacks super-Earths, planets between one and ten times as massive as the Earth, although the hypothetical Planet Nine, if it does exist, could be a super-Earth orbiting in the edge of the Solar System. Uncommonly, it has only small terrestrial and large gas giants; elsewhere planets of intermediate size are typical—both rocky and gas—so there is no \"gap\" as seen between the size of Earth and of Neptune (with a radius 3.8 times as large). As many of these super-Earths are closer to their respective stars than Mercury is to the Sun, a hypothesis has arisen that all planetary systems start with many close-in planets, and that typically a sequence of their collisions causes consolidation of mass into few larger planets, but in case of the Solar System the collisions caused their destruction and ejection. The orbits of Solar System planets are nearly circular. Compared to many other systems, they have smaller orbital eccentricity. Although there are attempts to explain it partly with a bias in the radial-velocity detection method and partly with long interactions of a quite high number of planets, the exact causes remain undetermined. The"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_13",
    "chunk": "Sun is the Solar System's star and by far its most massive component. Its large mass (332,900 Earth masses), which comprises 99.86% of all the mass in the Solar System, produces temperatures and densities in its core high enough to sustain nuclear fusion of hydrogen into helium. This releases an enormous amount of energy, mostly radiated into space as electromagnetic radiation peaking in visible light. Because the Sun fuses hydrogen at its core, it is a main-sequence star. More specifically, it is a G2-type main-sequence star, where the type designation refers to its effective temperature. Hotter main-sequence stars are more luminous but shorter lived. The Sun's temperature is intermediate between that of the hottest stars and that of the coolest stars. Stars brighter and hotter than the Sun are rare, whereas substantially dimmer and cooler stars, known as red dwarfs, make up about 75% of the fusor stars in the Milky Way. The Sun is a population I star, having formed in the spiral arms of the Milky Way galaxy. It has a higher abundance of elements heavier than hydrogen and helium (\"metals\" in astronomical parlance) than the older population II stars in the galactic bulge and halo. Elements heavier"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_14",
    "chunk": "than hydrogen and helium were formed in the cores of ancient and exploding stars, so the first generation of stars had to die before the universe could be enriched with these atoms. The oldest stars contain few metals, whereas stars born later have more. This higher metallicity is thought to have been crucial to the Sun's development of a planetary system because the planets formed from the accretion of \"metals\". The region of space dominated by the Solar magnetosphere is the heliosphere, which spans much of the Solar System. Along with light, the Sun radiates a continuous stream of charged particles (a plasma) called the solar wind. This stream spreads outwards at speeds from 900,000 kilometres per hour (560,000 mph) to 2,880,000 kilometres per hour (1,790,000 mph), filling the vacuum between the bodies of the Solar System. The result is a thin, dusty atmosphere, called the interplanetary medium, which extends to at least 100 AU. Activity on the Sun's surface, such as solar flares and coronal mass ejections, disturbs the heliosphere, creating space weather and causing geomagnetic storms. Coronal mass ejections and similar events blow a magnetic field and huge quantities of material from the surface of the Sun. The"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_15",
    "chunk": "interaction of this magnetic field and material with Earth's magnetic field funnels charged particles into Earth's upper atmosphere, where its interactions create aurorae seen near the magnetic poles. The largest stable structure within the heliosphere is the heliospheric current sheet, a spiral form created by the actions of the Sun's rotating magnetic field on the interplanetary medium. The inner Solar System is the region comprising the terrestrial planets and the asteroids. Composed mainly of silicates and metals, the objects of the inner Solar System are relatively close to the Sun; the radius of this entire region is less than the distance between the orbits of Jupiter and Saturn. This region is within the frost line, which is a little less than 5 AU from the Sun. The four terrestrial or inner planets have dense, rocky compositions, few or no moons, and no ring systems. They are composed largely of refractory minerals such as silicates—which form their crusts and mantles—and metals such as iron and nickel which form their cores. Three of the four inner planets (Venus, Earth, and Mars) have atmospheres substantial enough to generate weather; all have impact craters and tectonic surface features, such as rift valleys and volcanoes."
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_16",
    "chunk": "Asteroids, except for the largest, Ceres, are classified as small Solar System bodies and are composed mainly of carbonaceous, refractory rocky and metallic minerals, with some ice. They range from a few meters to hundreds of kilometers in size. Many asteroids are divided into asteroid groups and families based on their orbital characteristics. Some asteroids have natural satellites that orbit them, that is, asteroids that orbit larger asteroids. The asteroid belt occupies a torus-shaped region between 2.3 and 3.3 AU from the Sun, which lies between the orbits of Mars and Jupiter. It is thought to be remnants from the Solar System's formation that failed to coalesce because of the gravitational interference of Jupiter. The asteroid belt contains tens of thousands, possibly millions, of objects over one kilometer in diameter. Despite this, the total mass of the asteroid belt is unlikely to be more than a thousandth of that of Earth. The asteroid belt is very sparsely populated; spacecraft routinely pass through without incident. Below are the descriptions of the three largest bodies in the asteroid belt. They are all considered to be relatively intact protoplanets, a precursor stage before becoming a fully-formed planet (see List of exceptional asteroids): Hilda"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_17",
    "chunk": "asteroids are in a 3:2 resonance with Jupiter; that is, they go around the Sun three times for every two Jovian orbits. They lie in three linked clusters between Jupiter and the main asteroid belt. Trojans are bodies located within another body's gravitationally stable Lagrange points: L4, 60° ahead in its orbit, or L5, 60° behind in its orbit. Every planet except Mercury and Saturn is known to possess at least 1 trojan. The Jupiter trojan population is roughly equal to that of the asteroid belt. After Jupiter, Neptune possesses the most confirmed trojans, at 28. The outer region of the Solar System is home to the giant planets and their large moons. The centaurs and many short-period comets orbit in this region. Due to their greater distance from the Sun, the solid objects in the outer Solar System contain a higher proportion of volatiles such as water, ammonia, and methane, than planets of the inner Solar System because their lower temperatures allow these compounds to remain solid, without significant sublimation. The four outer planets, called giant planets or Jovian planets, collectively make up 99% of the mass orbiting the Sun. All four giant planets have multiple moons and a"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_18",
    "chunk": "ring system, although only Saturn's rings are easily observed from Earth. Jupiter and Saturn are composed mainly of gases with extremely low melting points, such as hydrogen, helium, and neon, hence their designation as gas giants. Uranus and Neptune are ice giants, meaning they are largely composed of 'ice' in the astronomical sense (chemical compounds with melting points of up to a few hundred kelvins such as water, methane, ammonia, hydrogen sulfide, and carbon dioxide.) Icy substances comprise the majority of the satellites of the giant planets and small objects that lie beyond Neptune's orbit. The centaurs are icy, comet-like bodies whose semi-major axes are longer than Jupiter's and shorter than Neptune's (between 5.5 and 30 AU). These are former Kuiper belt and scattered disc objects (SDOs) that were gravitationally perturbed closer to the Sun by the outer planets, and are expected to become comets or be ejected out of the Solar System. While most centaurs are inactive and asteroid-like, some exhibit cometary activity, such as the first centaur discovered, 2060 Chiron, which has been classified as a comet (95P) because it develops a coma just as comets do when they approach the Sun. The largest known centaur, 10199 Chariklo,"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_19",
    "chunk": "has a diameter of about 250 km (160 mi) and is one of the few minor planets possessing a ring system. Beyond the orbit of Neptune lies the area of the \"trans-Neptunian region\", with the doughnut-shaped Kuiper belt, home of Pluto and several other dwarf planets, and an overlapping disc of scattered objects, which is tilted toward the plane of the Solar System and reaches much further out than the Kuiper belt. The entire region is still largely unexplored. It appears to consist overwhelmingly of many thousands of small worlds—the largest having a diameter only a fifth that of Earth and a mass far smaller than that of the Moon—composed mainly of rock and ice. This region is sometimes described as the \"third zone of the Solar System\", enclosing the inner and the outer Solar System. The Kuiper belt is a great ring of debris similar to the asteroid belt, but consisting mainly of objects composed primarily of ice. It extends between 30 and 50 AU from the Sun. It is composed mainly of small Solar System bodies, although the largest few are probably large enough to be dwarf planets. There are estimated to be over 100,000 Kuiper belt objects"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_20",
    "chunk": "with a diameter greater than 50 km (30 mi), but the total mass of the Kuiper belt is thought to be only a tenth or even a hundredth the mass of Earth. Many Kuiper belt objects have satellites, and most have orbits that are substantially inclined (~10°) to the plane of the ecliptic. The Kuiper belt can be roughly divided into the \"classical\" belt and the resonant trans-Neptunian objects. The latter have orbits whose periods are in a simple ratio to that of Neptune: for example, going around the Sun twice for every three times that Neptune does, or once for every two. The classical belt consists of objects having no resonance with Neptune, and extends from roughly 39.4 to 47.7 AU. Members of the classical Kuiper belt are sometimes called \"cubewanos\", after the first of their kind to be discovered, originally designated 1992 QB1, (and has since been named Albion); they are still in near primordial, low-eccentricity orbits. There is strong consensus among astronomers that five members of the Kuiper belt are dwarf planets. Many dwarf planet candidates are being considered, pending further data for verification. The scattered disc, which overlaps the Kuiper belt but extends out to near"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_21",
    "chunk": "500 AU, is thought to be the source of short-period comets. Scattered-disc objects are believed to have been perturbed into erratic orbits by the gravitational influence of Neptune's early outward migration. Most scattered disc objects have perihelia within the Kuiper belt but aphelia far beyond it (some more than 150 AU from the Sun). SDOs' orbits can be inclined up to 46.8° from the ecliptic plane. Some astronomers consider the scattered disc to be merely another region of the Kuiper belt and describe scattered-disc objects as \"scattered Kuiper belt objects\". Some astronomers classify centaurs as inward-scattered Kuiper belt objects along with the outward-scattered residents of the scattered disc. Currently, there is strong consensus among astronomers that two of the bodies in the scattered disc are dwarf planets: Some objects in the Solar System have a very large orbit, and therefore are much less affected by the known giant planets than other minor planet populations. These bodies are called extreme trans-Neptunian objects, or ETNOs for short. Generally, ETNOs' semi-major axes are at least 150–250 AU wide. For example, 541132 Leleākūhonua orbits the Sun once every ~32,000 years, with a distance of 65–2000 AU from the Sun. This population is divided into"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_22",
    "chunk": "three subgroups by astronomers. The scattered ETNOs have perihelia around 38–45 AU and an exceptionally high eccentricity of more than 0.85. As with the regular scattered disc objects, they were likely formed as result of gravitational scattering by Neptune and still interact with the giant planets. The detached ETNOs, with perihelia approximately between 40–45 and 50–60 AU, are less affected by Neptune than the scattered ETNOs, but are still relatively close to Neptune. The sednoids or inner Oort cloud objects, with perihelia beyond 50–60 AU, are too far from Neptune to be strongly influenced by it. The Sun's stellar-wind bubble, the heliosphere, a region of space dominated by the Sun, has its boundary at the termination shock. Based on the Sun's peculiar motion relative to the local standard of rest, this boundary is roughly 80–100 AU from the Sun upwind of the interstellar medium and roughly 200 AU from the Sun downwind. Here the solar wind collides with the interstellar medium and dramatically slows, condenses and becomes more turbulent, forming a great oval structure known as the heliosheath. The heliosheath has been theorized to look and behave very much like a comet's tail, extending outward for a further 40 AU"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_23",
    "chunk": "on the upwind side but tailing many times that distance downwind to possibly several thousands of AU. Evidence from the Cassini and Interstellar Boundary Explorer spacecraft has suggested that it is forced into a bubble shape by the constraining action of the interstellar magnetic field, but the actual shape remains unknown. The shape and form of the outer edge of the heliosphere is likely affected by the fluid dynamics of interactions with the interstellar medium as well as solar magnetic fields prevailing to the south, e.g. it is bluntly shaped with the northern hemisphere extending 9 AU farther than the southern hemisphere. The heliopause is considered the beginning of the interstellar medium. Beyond the heliopause, at around 230 AU, lies the bow shock: a plasma \"wake\" left by the Sun as it travels through the Milky Way. Large objects outside the heliopause remain gravitationally bound to the Sun, but the flow of matter in the interstellar medium homogenizes the distribution of micro-scale objects. Comets are small Solar System bodies, typically only a few kilometers across, composed largely of volatile ices. They have highly eccentric orbits, generally a perihelion within the orbits of the inner planets and an aphelion far beyond"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_24",
    "chunk": "Pluto. When a comet enters the inner Solar System, its proximity to the Sun causes its icy surface to sublimate and ionise, creating a coma: a long tail of gas and dust often visible to the naked eye. Short-period comets have orbits lasting less than two hundred years. Long-period comets have orbits lasting thousands of years. Short-period comets are thought to originate in the Kuiper belt, whereas long-period comets, such as Hale–Bopp, are thought to originate in the Oort cloud. Many comet groups, such as the Kreutz sungrazers, formed from the breakup of a single parent. Some comets with hyperbolic orbits may originate outside the Solar System, but determining their precise orbits is difficult. Old comets whose volatiles have mostly been driven out by solar warming are often categorized as asteroids. Solid objects smaller than one meter are usually called meteoroids and micrometeoroids (grain-sized), with the exact division between the two categories being debated over the years. By 2017, the IAU designated any solid object having a diameter between ~30 micrometers and 1 meter as meteoroids, and depreciated the micrometeoroid categorization, instead terms smaller particles simply as 'dust particles'. Some meteoroids formed via disintegration of comets and asteroids, while a"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_25",
    "chunk": "few formed via impact debris ejected from planetary bodies. Most meteoroids are made of silicates and heavier metals like nickel and iron. When passing through the Solar System, comets produce a trail of meteoroids; it is hypothesized that this is caused either by vaporization of the comet's material or by simple breakup of dormant comets. When crossing an atmosphere, these meteoroids will produce bright streaks in the sky due to atmospheric entry, called meteors. If a stream of meteoroids enter the atmosphere on parallel trajectories, the meteors will seemingly 'radiate' from a point in the sky, hence the phenomenon's name: meteor shower. The inner Solar System is home to the zodiacal dust cloud, which is visible as the hazy zodiacal light in dark, unpolluted skies. It may be generated by collisions within the asteroid belt brought on by gravitational interactions with the planets; a more recent proposed origin is materials from planet Mars. The outer Solar System hosts a cosmic dust cloud. It extends from about 10 AU to about 40 AU, and was probably created by collisions within the Kuiper belt. Much of the Solar System is still unknown. Regions beyond thousands of AU away are still virtually unmapped"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_26",
    "chunk": "and learning about this region of space is difficult. Study in this region depends upon inferences from those few objects whose orbits happen to be perturbed such that they fall closer to the Sun, and even then, detecting these objects has often been possible only when they happened to become bright enough to register as comets. Many objects may yet be discovered in the Solar System's uncharted regions. The Oort cloud is a theorized spherical shell of up to a trillion icy objects that is thought to be the source for all long-period comets. No direct observation of the Oort cloud is possible with present imaging technology. It is theorized to surround the Solar System at roughly 50,000 AU (~0.9 ly) from the Sun and possibly to as far as 100,000 AU (~1.8 ly). The Oort cloud is thought to be composed of comets that were ejected from the inner Solar System by gravitational interactions with the outer planets. Oort cloud objects move very slowly, and can be perturbed by infrequent events, such as collisions, the gravitational effects of a passing star, or the galactic tide, the tidal force exerted by the Milky Way. As of the 2020s, a few"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_27",
    "chunk": "astronomers have hypothesized that Planet Nine (a planet beyond Neptune) might exist, based on statistical variance in the orbit of extreme trans-Neptunian objects. Their closest approaches to the Sun are mostly clustered around one sector and their orbits are similarly tilted, suggesting that a large planet might be influencing their orbit over millions of years. However, some astronomers said that this observation might be credited to observational biases or just sheer coincidence. An alternative hypothesis has a close flyby of another star disrupting the outer Solar System. The Sun's gravitational field is estimated to dominate the gravitational forces of surrounding stars out to about two light-years (125,000 AU). Lower estimates for the radius of the Oort cloud, by contrast, do not place it farther than 50,000 AU. Most of the mass is orbiting in the region between 3,000 and 100,000 AU. The furthest known objects, such as Comet West, have aphelia around 70,000 AU from the Sun. The Sun's Hill sphere with respect to the galactic nucleus, the effective range of its gravitational influence, is thought to extend up to a thousand times farther and encompasses the hypothetical Oort cloud. It was calculated by G. A. Chebotarev to be 230,000"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_28",
    "chunk": "AU. Within 10 light-years of the Sun there are relatively few stars, the closest being the triple star system Alpha Centauri, which is about 4.4 light-years away and may be in the Local Bubble's G-Cloud. Alpha Centauri A and B are a closely tied pair of Sun-like stars, whereas the closest star to the Sun, the small red dwarf Proxima Centauri, orbits the pair at a distance of 0.2 light-years. In 2016, a potentially habitable exoplanet was found to be orbiting Proxima Centauri, called Proxima Centauri b, the closest confirmed exoplanet to the Sun. The Solar System is surrounded by the Local Interstellar Cloud, although it is not clear if it is embedded in the Local Interstellar Cloud or if it lies just outside the cloud's edge. Multiple other interstellar clouds exist in the region within 300 light-years of the Sun, known as the Local Bubble. The latter feature is an hourglass-shaped cavity or superbubble in the interstellar medium roughly 300 light-years across. The bubble is suffused with high-temperature plasma, suggesting that it may be the product of several recent supernovae. The Local Bubble is a small superbubble compared to the neighboring wider Radcliffe Wave and Split linear structures (formerly"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_29",
    "chunk": "Gould Belt), each of which are some thousands of light-years in length. All these structures are part of the Orion Arm, which contains most of the stars in the Milky Way that are visible to the unaided eye. Groups of stars form together in star clusters, before dissolving into co-moving associations. A prominent grouping that is visible to the naked eye is the Ursa Major moving group, which is around 80 light-years away within the Local Bubble. The nearest star cluster is Hyades, which lies at the edge of the Local Bubble. The closest star-forming regions are the Corona Australis Molecular Cloud, the Rho Ophiuchi cloud complex and the Taurus molecular cloud; the latter lies just beyond the Local Bubble and is part of the Radcliffe wave. Stellar flybys that pass within 0.8 light-years of the Sun occur roughly once every 100,000 years. The closest well-measured approach was Scholz's Star, which approached to ~50,000 AU of the Sun some ~70 thousands years ago, likely passing through the outer Oort cloud. There is a 1% chance every billion years that a star will pass within 100 AU of the Sun, potentially disrupting the Solar System. The Solar System is located in"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_30",
    "chunk": "the Milky Way, a barred spiral galaxy with a diameter of about 100,000 light-years containing more than 100 billion stars. The Sun is part of one of the Milky Way's outer spiral arms, known as the Orion–Cygnus Arm or Local Spur. It is a member of the thin disk population of stars orbiting close to the galactic plane. Its speed around the center of the Milky Way is about 220 km/s, so that it completes one revolution every 240 million years. This revolution is known as the Solar System's galactic year. The solar apex, the direction of the Sun's path through interstellar space, is near the constellation Hercules in the direction of the current location of the bright star Vega. The plane of the ecliptic lies at an angle of about 60° to the galactic plane. The Sun follows a nearly circular orbit around the Galactic Center (where the supermassive black hole Sagittarius A* resides) at a distance of 26,660 light-years, orbiting at roughly the same speed as that of the spiral arms. If it orbited close to the center, gravitational tugs from nearby stars could perturb bodies in the Oort cloud and send many comets into the inner Solar"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_31",
    "chunk": "System, producing collisions with potentially catastrophic implications for life on Earth. In this scenario, the intense radiation of the Galactic Center could interfere with the development of complex life. The Solar System's location in the Milky Way is a factor in the evolutionary history of life on Earth. Spiral arms are home to a far larger concentration of supernovae, gravitational instabilities, and radiation that could disrupt the Solar System, but since Earth stays in the Local Spur and therefore does not pass frequently through spiral arms, this has given Earth long periods of stability for life to evolve. However, according to the controversial Shiva hypothesis, the changing position of the Solar System relative to other parts of the Milky Way could explain periodic extinction events on Earth. Humanity's knowledge of the Solar System has grown incrementally over the centuries. Up to the Late Middle Ages–Renaissance, astronomers from Europe to India believed Earth to be stationary at the center of the universe and categorically different from the divine or ethereal objects that moved through the sky. Although the Greek philosopher Aristarchus of Samos had speculated on a heliocentric reordering of the cosmos, Nicolaus Copernicus was the first person known to have"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_32",
    "chunk": "developed a mathematically predictive heliocentric system. Heliocentrism did not triumph immediately over geocentrism, but the work of Copernicus had its champions, notably Johannes Kepler. Using a heliocentric model that improved upon Copernicus by allowing orbits to be elliptical, and the precise observational data of Tycho Brahe, Kepler produced the Rudolphine Tables, which enabled accurate computations of the positions of the then-known planets. Pierre Gassendi used them to predict a transit of Mercury in 1631, and Jeremiah Horrocks did the same for a transit of Venus in 1639. This provided a strong vindication of heliocentrism and Kepler's elliptical orbits. In the 17th century, Galileo publicized the use of the telescope in astronomy; he and Simon Marius independently discovered that Jupiter had four satellites in orbit around it. Christiaan Huygens followed on from these observations by discovering Saturn's moon Titan and the shape of the rings of Saturn. In 1677, Edmond Halley observed a transit of Mercury across the Sun, leading him to realize that observations of the solar parallax of a planet (more ideally using the transit of Venus) could be used to trigonometrically determine the distances between Earth, Venus, and the Sun. Halley's friend Isaac Newton, in his magisterial Principia"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_33",
    "chunk": "Mathematica of 1687, demonstrated that celestial bodies are not quintessentially different from Earthly ones: the same laws of motion and of gravity apply on Earth and in the skies. The term \"Solar System\" entered the English language by 1704, when John Locke used it to refer to the Sun, planets, and comets. In 1705, Halley realized that repeated sightings of a comet were of the same object, returning regularly once every 75–76 years. This was the first evidence that anything other than the planets repeatedly orbited the Sun, though Seneca had theorized this about comets in the 1st century. Careful observations of the 1769 transit of Venus allowed astronomers to calculate the average Earth–Sun distance as 93,726,900 miles (150,838,800 km), only 0.8% greater than the modern value. Uranus, having occasionally been observed since 1690 and possibly from antiquity, was recognized to be a planet orbiting beyond Saturn by 1783. In 1838, Friedrich Bessel successfully measured a stellar parallax, an apparent shift in the position of a star created by Earth's motion around the Sun, providing the first direct, experimental proof of heliocentrism. Neptune was identified as a planet some years later, in 1846, thanks to its gravitational pull causing a"
  },
  {
    "source": "Solar System.txt",
    "chunk_id": "Solar System.txt_34",
    "chunk": "slight but detectable variation in the orbit of Uranus. Mercury's orbital anomaly observations led to searches for Vulcan, a planet interior of Mercury, but these attempts were quashed with Albert Einstein's theory of general relativity in 1915. In the 20th century, humans began their space exploration around the Solar System, starting with placing telescopes in space since the 1960s. By 1989, all eight planets have been visited by space probes. Probes have returned samples from comets and asteroids, as well as flown through the Sun's corona and visited two dwarf planets (Pluto and Ceres). To save on fuel, some space missions make use of gravity assist maneuvers, such as the two Voyager probes accelerating when flying by planets in the outer Solar System and the Parker Solar Probe decelerating closer towards the Sun after its flyby of Venus. Humans have landed on the Moon during the Apollo program in the 1960s and 1970s and will return to the Moon in the 2020s with the Artemis program. Discoveries in the 20th and 21st century has prompted the redefinition of the term planet in 2006, hence the demotion of Pluto to a dwarf planet, and further interest in trans-Neptunian objects."
  },
  {
    "source": "Solutions of the Einstein field equations.txt",
    "chunk_id": "Solutions of the Einstein field equations.txt_0",
    "chunk": "# Solutions of the Einstein field equations Solutions of the Einstein field equations are metrics of spacetimes that result from solving the Einstein field equations (EFE) of general relativity. Solving the field equations gives a Lorentz manifold. Solutions are broadly classed as exact or non-exact. G μ ν + Λ g μ ν = κ T μ ν , {\\displaystyle G_{\\mu \\nu }+\\Lambda g_{\\mu \\nu }\\,=\\kappa T_{\\mu \\nu },} where G μ ν {\\displaystyle G_{\\mu \\nu }} is the Einstein tensor, Λ {\\displaystyle \\Lambda } is the cosmological constant (sometimes taken to be zero for simplicity), g μ ν {\\displaystyle g_{\\mu \\nu }} is the metric tensor, κ {\\displaystyle \\kappa } is a constant, and T μ ν {\\displaystyle T_{\\mu \\nu }} is the stress–energy tensor. The Einstein field equations relate the Einstein tensor to the stress–energy tensor, which represents the distribution of energy, momentum and stress in the spacetime manifold. The Einstein tensor is built up from the metric tensor and its partial derivatives; thus, given the stress–energy tensor, the Einstein field equations are a system of ten partial differential equations in which the metric tensor can be solved for. It is important to realize that the Einstein field"
  },
  {
    "source": "Solutions of the Einstein field equations.txt",
    "chunk_id": "Solutions of the Einstein field equations.txt_1",
    "chunk": "equations alone are not enough to determine the evolution of a gravitational system in many cases. They depend on the stress–energy tensor, which depends on the dynamics of matter and energy (such as trajectories of moving particles), which in turn depends on the gravitational field. If one is only interested in the weak field limit of the theory, the dynamics of matter can be computed using special relativity methods and/or Newtonian laws of gravity and the resulting stress–energy tensor can then be plugged into the Einstein field equations. But if one requires an exact solution or a solution describing strong fields, the evolution of both the metric and the stress–energy tensor must be solved for at once. To obtain solutions, the relevant equations are the above quoted EFE (in either form) plus the continuity equation (to determine the evolution of the stress–energy tensor): These amount to only 14 equations (10 from the field equations and 4 from the continuity equation) and are by themselves insufficient for determining the 20 unknowns (10 metric components and 10 stress–energy tensor components). The equations of state are missing. In the most general case, it's easy to see that at least 6 more equations are"
  },
  {
    "source": "Solutions of the Einstein field equations.txt",
    "chunk_id": "Solutions of the Einstein field equations.txt_2",
    "chunk": "required, possibly more if there are internal degrees of freedom (such as temperature) which may vary throughout spacetime. In practice, it is usually possible to simplify the problem by replacing the full set of equations of state with a simple approximation. Some common approximations are: Here ρ {\\displaystyle \\rho } is the mass–energy density measured in a momentary co-moving frame, u a {\\displaystyle u_{a}} is the fluid's 4-velocity vector field, and p {\\displaystyle p} is the pressure. For a perfect fluid, another equation of state relating density ρ {\\displaystyle \\rho } and pressure p {\\displaystyle p} must be added. This equation will often depend on temperature, so a heat transfer equation is required or the postulate that heat transfer can be neglected. Next, notice that only 10 of the original 14 equations are independent, because the continuity equation T a b ; b = 0 {\\displaystyle T^{ab}{}_{;b}=0} is a consequence of Einstein's equations. This reflects the fact that the system is gauge invariant (in general, absent some symmetry, any choice of a curvilinear coordinate net on the same system would correspond to a numerically different solution.) A \"gauge fixing\" is needed, i.e. we need to impose 4 (arbitrary) constraints on"
  },
  {
    "source": "Solutions of the Einstein field equations.txt",
    "chunk_id": "Solutions of the Einstein field equations.txt_3",
    "chunk": "the coordinate system in order to obtain unequivocal results. These constraints are known as coordinate conditions. A popular choice of gauge is the so-called \"De Donder gauge\", also known as the harmonic condition or harmonic gauge In numerical relativity, the preferred gauge is the so-called \"3+1 decomposition\", based on the ADM formalism. In this decomposition, metric is written in the form N {\\displaystyle N} and N i {\\displaystyle N^{i}} are functions of spacetime coordinates and can be chosen arbitrarily in each point. The remaining physical degrees of freedom are contained in γ i j {\\displaystyle \\gamma _{ij}} , which represents the Riemannian metric on 3-hypersurfaces with constant t {\\displaystyle t} . For example, a naive choice of N = 1 {\\displaystyle N=1} , N i = 0 {\\displaystyle N_{i}=0} , would correspond to a so-called synchronous coordinate system: one where t-coordinate coincides with proper time for any comoving observer (particle that moves along a fixed x i {\\displaystyle x^{i}} trajectory.) Once equations of state are chosen and the gauge is fixed, the complete set of equations can be solved. Unfortunately, even in the simplest case of gravitational field in the vacuum (vanishing stress–energy tensor), the problem is too complex to"
  },
  {
    "source": "Solutions of the Einstein field equations.txt",
    "chunk_id": "Solutions of the Einstein field equations.txt_4",
    "chunk": "be exactly solvable. To get physical results, we can either turn to numerical methods, try to find exact solutions by imposing symmetries, or try middle-ground approaches such as perturbation methods or linear approximations of the Einstein tensor. Exact solutions are Lorentz metrics that are conformable to a physically realistic stress–energy tensor and which are obtained by solving the EFE exactly in closed form. The simplest family of these exact solutions are vacuum solutions, where T a b = 0 {\\displaystyle T_{ab}\\,=0} . However, other families of solutions exist for different constraints on which fields contribute to T a b {\\displaystyle T_{ab}} . The solutions that are not exact are called non-exact solutions. Such solutions mainly arise due to the difficulty of solving the EFE in closed form and often take the form of approximations to ideal systems. Many non-exact solutions may be devoid of physical content, but serve as useful counterexamples to theoretical conjectures. There are practical as well as theoretical reasons for studying solutions of the Einstein field equations. From a purely mathematical viewpoint, it is interesting to know the set of solutions of the Einstein field equations. Some of these solutions are parametrised by one or more parameters."
  },
  {
    "source": "Solutions of the Einstein field equations.txt",
    "chunk_id": "Solutions of the Einstein field equations.txt_5",
    "chunk": "From a physical standpoint, knowing the solutions of the Einstein Field Equations allows highly-precise modelling of astrophysical phenomena, including black holes, neutron stars, and stellar systems. Predictions can be made analytically about the system analyzed; such predictions include the perihelion precession of Mercury, the existence of a co-rotating region inside spinning black holes, and the orbits of objects around massive bodies."
  },
  {
    "source": "Space station.txt",
    "chunk_id": "Space station.txt_0",
    "chunk": "# Space station A space station (or orbital station) is a spacecraft which remains in orbit and hosts humans for extended periods of time. It therefore is an artificial satellite featuring habitation facilities. The purpose of maintaining a space station varies depending on the program. Most often space stations have been research stations, but they have also served military or commercial uses, such as hosting space tourists. Space stations have been hosting the only continuous presence of humans in space. The first space station was Salyut 1 (1971), hosting the first crew, of the ill-fated Soyuz 11. Consecutively space stations have been operated since Skylab (1973) and occupied since 1987 with the Salyut successor Mir. Uninterrupted occupation has been sustained since the operational transition from the Mir to the International Space Station (ISS), with its first occupation in 2000. Currently there are two fully operational space stations – the ISS and China's Tiangong Space Station (TSS), which have been occupied since October 2000 with Expedition 1 and since June 2022 with Shenzhou 14. The highest number of people at the same time on one space station has been 13, first achieved with the eleven day docking to the ISS of"
  },
  {
    "source": "Space station.txt",
    "chunk_id": "Space station.txt_1",
    "chunk": "the 127th Space Shuttle mission in 2009. The record for most people on all space stations at the same time has been 17, first on May 30, 2023, with 11 people on the ISS and 6 on the TSS. Space stations are often modular, featuring docking ports, through which they are built and maintained, allowing the joining or movement of modules and the docking of other spacecrafts for the exchange of people, supplies and tools. While space stations generally do not leave their orbit, they do feature thrusters for station keeping. The first mention of anything resembling a space station occurred in Edward Everett Hale's 1868 \"The Brick Moon\". The first to give serious, scientifically grounded consideration to space stations were Konstantin Tsiolkovsky and Hermann Oberth about two decades apart in the early 20th century. In 1929, Herman Potočnik's The Problem of Space Travel was published, the first to envision a \"rotating wheel\" space station to create artificial gravity. Conceptualized during the Second World War, the \"sun gun\" was a theoretical orbital weapon orbiting Earth at a height of 8,200 kilometres (5,100 mi). No further research was ever conducted. In 1951, Wernher von Braun published a concept for a rotating"
  },
  {
    "source": "Space station.txt",
    "chunk_id": "Space station.txt_2",
    "chunk": "wheel space station in Collier's Weekly, referencing Potočnik's idea. However, development of a rotating station was never begun in the 20th century. The first human flew to space and concluded the first orbit on April 12, 1961, with Vostok 1. The Apollo program had in its early planning instead of a lunar landing a crewed lunar orbital flight and an orbital laboratory station in orbit of Earth, at times called Project Olympus, as two different possible program goals, until the Kennedy administration sped ahead and made the Apollo program focus on what was originally planned to come after it, the lunar landing. The Project Olympus space station, or orbiting laboratory of the Apollo program, was proposed as an in-space unfolded structure with the Apollo command and service module docking. While never realized, the Apollo command and service module would perform docking maneuvers and eventually become a lunar orbiting module which was used for station-like purposes. But before that the Gemini program paved the way and achieved the first space rendezvous (undocked) with Gemini 6 and Gemini 7 in 1965. Subsequently in 1966 Neil Armstrong performed on Gemini 8 the first ever space docking, while in 1967 Kosmos 186 and Kosmos"
  },
  {
    "source": "Space station.txt",
    "chunk_id": "Space station.txt_3",
    "chunk": "188 were the first spacecrafts that docked automatically. In January 1969, Soyuz 4 and Soyuz 5 performed the first docked, but not internal, crew transfer, and in March, Apollo 9 performed the first ever internal transfer of astronauts between two docked spaceships. In 1971, the Soviet Union developed and launched the world's first space station, Salyut 1. The Almaz and Salyut series were eventually joined by Skylab, Mir, and Tiangong-1 and Tiangong-2. The hardware developed during the initial Soviet efforts remains in use, with evolved variants comprising a considerable part of the ISS, orbiting today. Each crew member stays aboard the station for weeks or months but rarely more than a year. Early stations were monolithic designs that were constructed and launched in one piece, generally containing all their supplies and experimental equipment. A crew would then be launched to join the station and perform research. After the supplies had been consumed, the station was abandoned. The first space station was Salyut 1, which was launched by the Soviet Union on April 19, 1971. The early Soviet stations were all designated \"Salyut\", but among these, there were two distinct types: civilian and military. The military stations, Salyut 2, Salyut 3,"
  },
  {
    "source": "Space station.txt",
    "chunk_id": "Space station.txt_4",
    "chunk": "and Salyut 5, were also known as Almaz stations. The civilian stations Salyut 6 and Salyut 7 were built with two docking ports, which allowed a second crew to visit, bringing a new spacecraft with them; the Soyuz ferry could spend 90 days in space, at which point it needed to be replaced by a fresh Soyuz spacecraft. This allowed for a crew to man the station continually. The American Skylab (1973–1979) was also equipped with two docking ports, like second-generation stations, but the extra port was never used. The presence of a second port on the new stations allowed Progress supply vehicles to be docked to the station, meaning that fresh supplies could be brought to aid long-duration missions. This concept was expanded on Salyut 7, which \"hard docked\" with a TKS tug shortly before it was abandoned; this served as a proof of concept for the use of modular space stations. The later Salyuts may reasonably be seen as a transition between the two groups. Unlike previous stations, the Soviet space station Mir had a modular design; a core unit was launched, and additional modules, generally with a specific role, were later added. This method allows for greater"
  },
  {
    "source": "Space station.txt",
    "chunk_id": "Space station.txt_5",
    "chunk": "flexibility in operation, as well as removing the need for a single immensely powerful launch vehicle. Modular stations are also designed from the outset to have their supplies provided by logistical support craft, which allows for a longer lifetime at the cost of requiring regular support launches. The ISS is divided into two main sections, the Russian Orbital Segment (ROS) and the US Orbital Segment (USOS). The first module of the ISS, Zarya, was launched in 1998. The Russian Orbital Segment's \"second-generation\" modules were able to launch on Proton, fly to the correct orbit, and dock themselves without human intervention. Connections are automatically made for power, data, gases, and propellants. The Russian autonomous approach allows the assembly of space stations prior to the launch of crew. The Russian \"second-generation\" modules are able to be reconfigured to suit changing needs. As of 2009, RKK Energia was considering the removal and reuse of some modules of the ROS on the Orbital Piloted Assembly and Experiment Complex after the end of mission is reached for the ISS. However, in September 2017, the head of Roscosmos said that the technical feasibility of separating the station to form OPSEK had been studied, and there were"
  },
  {
    "source": "Space station.txt",
    "chunk_id": "Space station.txt_6",
    "chunk": "now no plans to separate the Russian segment from the ISS. In contrast, the main US modules launched on the Space Shuttle and were attached to the ISS by crews during EVAs. Connections for electrical power, data, propulsion, and cooling fluids are also made at this time, resulting in an integrated block of modules that is not designed for disassembly and must be deorbited as one mass. Axiom Station is a planned commercial space station that will begin as a single module docked to the ISS. Axiom Space gained NASA approval for the venture in January 2020. The first module, the Payload Power Transfer Module (PPTM), is expected to be launched to the ISS no earlier than 2027. PPTM will remain at the ISS until the launch of Axiom's Habitat One (Hab-1) module about one year later, after which it will detach from the ISS to join with Hab-1. China's first space laboratory, Tiangong-1 was launched in September 2011. The uncrewed Shenzhou 8 then successfully performed an automatic rendezvous and docking in November 2011. The crewed Shenzhou 9 then docked with Tiangong-1 in June 2012, followed by the crewed Shenzhou 10 in 2013. According to the China Manned Space Engineering"
  },
  {
    "source": "Space station.txt",
    "chunk_id": "Space station.txt_7",
    "chunk": "Office, Tiangong-1 reentered over the South Pacific Ocean, northwest of Tahiti, on 2 April 2018 at 00:15 UTC. A second space laboratory Tiangong-2 was launched in September 2016, while a plan for Tiangong-3 was merged with Tiangong-2. The station made a controlled reentry on 19 July 2019 and burned up over the South Pacific Ocean. The Tiangong Space Station (Chinese: 天宫; pinyin: Tiāngōng; lit. 'Heavenly Palace'), the first module of which was launched on 29 April 2021, is in low Earth orbit, 340 to 450 kilometres above the Earth at an orbital inclination of 42° to 43°. The core module was extended in 2022 with two laboratory modules, bringing the total station capacity to six crew members. The station was completed on 5 November 2022. These space stations have been announced by their host entity and are currently in planning, development or production. The launch date listed here may change as more information becomes available. Two types of space stations have been flown: monolithic and modular. Monolithic stations consist of a single vehicle and are launched by one rocket. Modular stations consist of two or more separate vehicles that are launched independently and docked on orbit. Modular stations are currently"
  },
  {
    "source": "Space station.txt",
    "chunk_id": "Space station.txt_8",
    "chunk": "preferred due to lower costs and greater flexibility. A space station is a complex vehicle that must incorporate many interrelated subsystems, including structure, electrical power, thermal control, attitude determination and control, orbital navigation and propulsion, automation and robotics, computing and communications, environmental and life support, crew facilities, and crew and cargo transportation. Stations must serve a useful role, which drives the capabilities required. Space stations are made from durable materials that have to weather space radiation, internal pressure, micrometeoroids, thermal effects of the sun and cold temperatures for long periods of time. They are typically made from stainless steel, titanium and high-quality aluminum alloys, with layers of insulation such as Kevlar as a ballistics shield protection. The International Space Station (ISS) has a single inflatable module, the Bigelow Expandable Activity Module, which was installed in April 2016 after being delivered to the ISS on the SpaceX CRS-8 resupply mission. This module, based on NASA research in the 1990s, weighs 1,400 kilograms (3,100 lb) and was transported while compressed before being attached to the ISS by the space station arm and inflated to provide a 16 cubic metres (21 cu yd) volume. Whilst it was initially designed for a 2 year"
  },
  {
    "source": "Space station.txt",
    "chunk_id": "Space station.txt_9",
    "chunk": "lifetime it was still attached and being used for storage in August 2022. The space station environment presents a variety of challenges to human habitability, including short-term problems such as the limited supplies of air, water, and food and the need to manage waste heat, and long-term ones such as weightlessness and relatively high levels of ionizing radiation. These conditions can create long-term health problems for space-station inhabitants, including muscle atrophy, bone deterioration, balance disorders, eyesight disorders, and elevated risk of cancer. Future space habitats may attempt to address these issues, and could be designed for occupation beyond the weeks or months that current missions typically last. Possible solutions include the creation of artificial gravity by a rotating structure, the inclusion of radiation shielding, and the development of on-site agricultural ecosystems. Some designs might even accommodate large numbers of people, becoming essentially \"cities in space\" where people would reside semi-permanently. Molds that develop aboard space stations can produce acids that degrade metal, glass, and rubber. Despite an expanding array of molecular approaches for detecting microorganisms, rapid and robust means of assessing the differential viability of the microbial cells, as a function of phylogenetic lineage, remain elusive. Like uncrewed spacecraft close"
  },
  {
    "source": "Space station.txt",
    "chunk_id": "Space station.txt_10",
    "chunk": "to the Sun, space stations in the inner Solar System generally rely on solar panels to obtain power. Space station air and water is brought up in spacecraft from Earth before being recycled. Supplemental oxygen can be supplied by a solid fuel oxygen generator. The last military-use space station was the Soviet Salyut 5, which was launched under the Almaz program and orbited between 1976 and 1977. Space stations have harboured so far the only long-duration direct human presence in space. After the first station, Salyut 1 (1971), and its tragic Soyuz 11 crew, space stations have been operated consecutively since Skylab (1973–1974), having allowed a progression of long-duration direct human presence in space. Long-duration resident crews have been joined by visiting crews since 1977 (Salyut 6), and stations have been occupied by consecutive crews since 1987 with the Salyut successor Mir. Uninterrupted occupation of stations has been achieved since the operational transition from the Mir to the ISS, with its first occupation in 2000. The ISS has hosted the highest number of people in orbit at the same time, reaching 13 for the first time during the eleven day docking of STS-127 in 2009. The duration record for a"
  },
  {
    "source": "Space station.txt",
    "chunk_id": "Space station.txt_11",
    "chunk": "single spaceflight is 437.75 days, set by Valeri Polyakov aboard Mir from 1994 to 1995. As of 2021, four cosmonauts have completed single missions of over a year, all aboard Mir. Many spacecraft are used to dock with the space stations. Soyuz flight T-15 in March to July 1986 was the first and as of 2016, only spacecraft to visit two different space stations, Mir and Salyut 7. The Mir space station was in orbit from 1986 to 2001 and was supported and visited by the following spacecraft: Research conducted on the Mir included the first long term space based ESA research project EUROMIR 95 which lasted 179 days and included 35 scientific experiments. During the first 20 years of operation of the International Space Station, there were around 3,000 scientific experiments in the areas of biology and biotech, technology development, educational activities, human research, physical science, and Earth and space science. Space stations provide a useful platform to test the performance, stability, and survivability of materials in space. This research follows on from previous experiments such as the Long Duration Exposure Facility, a free flying experimental platform which flew from April 1984 until January 1990. On the International Space"
  },
  {
    "source": "Space station.txt",
    "chunk_id": "Space station.txt_12",
    "chunk": "Station, guests sometimes pay $50 million to spend the week living as an astronaut. Later, space tourism is slated to expand once launch costs are lowered sufficiently. By the end of the 2020s, space hotels may become relatively common. As it currently costs on average $10,000 to $25,000 per kilogram to launch anything into orbit, space stations remain the exclusive province of government space agencies, which are primarily funded by taxation. In the case of the International Space Station, space tourism makes up a small portion of money to run it."
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_0",
    "chunk": "# Special relativity In physics, the special theory of relativity, or special relativity for short, is a scientific theory of the relationship between space and time. In Albert Einstein's 1905 paper, \"On the Electrodynamics of Moving Bodies\", the theory is presented as being based on just two postulates: The first postulate was first formulated by Galileo Galilei (see Galilean invariance). Special relativity builds upon important physics ideas. The non-technical ideas include: Two observers in relative motion receive information about two events via light signals traveling at constant speed, independent of either observer's speed. Their motion during the transit time causes them to get the information at different times on their local clock. ( interval ) 2 = [ event separation in time ] 2 − [ event separation in space ] 2 {\\displaystyle ({\\text{interval}})^{2}=\\left[{\\text{event separation in time}}\\right]^{2}-\\left[{\\text{event separation in space}}\\right]^{2}} The spacetime interval is an invariant between inertial frames, demonstrating the physical unity of spacetime. Coordinate systems are not invariant between inertial frames and require transformations. Unusual among modern topics in physics, the theory special relativity needs only mathematics at high school level and yet it fundamentally alters our understanding, especially our understanding of the concept of time. Built on"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_1",
    "chunk": "just two postulates or assumptions, many interesting consequences follow. The two postulates both concern observers moving at a constant speed relative to each other. The first postulate, the § principle of relativity, says the laws of physics do not depend on objects being at absolute rest: a observer on a moving train sees natural phenomena on that train that look the same whether the train is moving or not. The second postulate, constant speed of light, says observers on a moving train or on in the train station see light travel at the same speed. A light signal from the station to the train has the same speed, no matter how fast a train goes. In the theory of special relativity, the two postulates combine to change the definition of \"relative speed\". Rather than the simple concept of distance traveled divided by time spent, the new theory incorporates the speed of light as the maximum possible speed. In special relativity, covering ten times more distance on the ground in the same amount of time according to a moving watch does not result in a speed up as seen from the ground by a factor of ten. Special relativity has a"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_2",
    "chunk": "wide range of consequences that have been experimentally verified. The conceptual effects include: Combined with other laws of physics, the two postulates of special relativity predict the equivalence of mass and energy, as expressed in the mass–energy equivalence formula ⁠ E = m c 2 {\\displaystyle E=mc^{2}} ⁠, where c {\\displaystyle c} is the speed of light in vacuum. Special relativity replaced the conventional notion of an absolute, universal time with the notion of a time that is local to each observer. Information about distant objects can arrive no faster than the speed of light so visual observations always report events that have happened in the past. This effect makes visual descriptions of the effects of special relativity especially prone to mistakes. Special relativity also has profound technical consequences. A defining feature of special relativity is the replacement of Euclidean geometry with Lorentzian geometry. Distances in Euclidean geometry are calculated with the Pythagorean theorem and only involved spatial coordinates. In Lorentzian geometry, 'distances' become 'intervals' and include a time coordinate with a minus sign. Unlike spatial distances, the interval between two events has the same value for all observers independent of their relative velocity. When comparing two sets of coordinates"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_3",
    "chunk": "in relative motion is Lorentz transformation replace Galilean transformations of Newtonian mechanics. Other effects include the relativistic corrects to the Doppler effect and the Thomas precession. It also explains how electricity and magnetism are related. The principle of relativity, forming one of the two postulates of special relativity, was described by Galileo Galilei in 1632 using a thought experiment involving observing natural phenomena on a moving ship. His conclusions were summarized as Galilean relativity and used as the basis of Newtonian mechanics. This principle can be expressed as a coordinate transformation, between two coordinate systems. Isaac Newton noted that many transformations, such as those involving rotation or acceleration, will not preserve the observation of physical phenomena. Newton considered only those transformations involving motion with respect to an immovable absolute space, now called transformations between inertial frames. In 1864 James Clerk Maxwell presented a theory of electromagnetism which did not obey Galilean relativity. The theory specifically predicted a constant speed of light in vacuum, no matter the motion (velocity, acceleration, etc.) of the light emitter or receiver or its frequency, wavelength, direction, polarization, or phase. This, as yet untested theory, was thought at the time to be only valid in inertial"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_4",
    "chunk": "frames fixed in an aether Numerous experiments followed, attempting to measure the speed of light as Earth moved through the proposed fixed aether, culminating in the 1887 Michelson-Morley experiment which only confirmed the constant speed of light. Several fixes to the aether theory where proposed, with those of George Francis Fitzgerald, Hendrik Antoon Lorentz, and Jules Henri Poincare all pointing in the direction of a result similar to the theory of special relativity. The final important step was taken by Albert Einstein in a paper published on 26 September 1905 titled \"On the Electrodynamics of Moving Bodies\". Einstein applied the Lorentz transformations known to be compatible with Maxwell's equations for electrodynamics to the classical laws of mechanics. This changed Newton's mechanics situations involving all motions, especially velocities close to that of light (known as relativistic velocities). Another way to describe the advance made by the special theory is to say Einstein extended the Galilean principle so that it accounted for the constant speed of light, a phenomenon that had been observed in the Michelson–Morley experiment. He also postulated that it holds for all the laws of physics, including both the laws of mechanics and of electrodynamics. The theory became essentially"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_5",
    "chunk": "complete in 1907, with Hermann Minkowski's papers on spacetime. Special relativity has proven to be the most accurate model of motion at any speed when gravitational and quantum effects are negligible. Even so, the Newtonian model remains accurate at low velocities relative to the speed of light, for example, everyday motion on Earth. When updating his 1911 book on relativity, to include general relativity in 1920, Robert Daniel Carmichael called the earlier work the \"restricted theory\" as a \"special case\" of the new general theory; he also used the phrase \"special theory of relativity\". In comparing to the general theory in 1923 Einstein specifically called his earlier work \"the special theory of relativity\", saying he meant a restriction to frames uniform motion. Just as Galilean relativity is accepted as an approximation of special relativity that is valid for low speeds, special relativity is considered an approximation of general relativity that is valid for weak gravitational fields, that is, at a sufficiently small scale (e.g., when tidal forces are negligible) and in conditions of free fall. But general relativity incorporates non-Euclidean geometry to represent gravitational effects as the geometric curvature of spacetime. Special relativity is restricted to the flat spacetime known"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_6",
    "chunk": "as Minkowski space. As long as the universe can be modeled as a pseudo-Riemannian manifold, a Lorentz-invariant frame that abides by special relativity can be defined for a sufficiently small neighborhood of each point in this curved spacetime. \"Reflections of this type made it clear to me as long ago as shortly after 1900, i.e., shortly after Planck's trailblazing work, that neither mechanics nor electrodynamics could (except in limiting cases) claim exact validity. Gradually I despaired of the possibility of discovering the true laws by means of constructive efforts based on known facts. The longer and the more desperately I tried, the more I came to the conviction that only the discovery of a universal formal principle could lead us to assured results ... How, then, could such a universal principle be found?\" Einstein discerned two fundamental propositions that seemed to be the most assured, regardless of the exact validity of the (then) known laws of either mechanics or electrodynamics. These propositions were the constancy of the speed of light in vacuum and the independence of physical laws (especially the constancy of the speed of light) from the choice of inertial system. In his initial presentation of special relativity in"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_7",
    "chunk": "1905 he expressed these postulates as: The constancy of the speed of light was motivated by Maxwell's theory of electromagnetism and the lack of evidence for the luminiferous ether. There is conflicting evidence on the extent to which Einstein was influenced by the null result of the Michelson–Morley experiment. In any case, the null result of the Michelson–Morley experiment helped the notion of the constancy of the speed of light gain widespread and rapid acceptance. The derivation of special relativity depends not only on these two explicit postulates, but also on several tacit assumptions (made in almost all theories of physics), including the isotropy and homogeneity of space and the independence of measuring rods and clocks from their past history. Reference frames play a crucial role in relativity theory. The term reference frame as used here is an observational perspective in space that is not undergoing any change in motion (acceleration), from which a position can be measured along 3 spatial axes (so, at rest or constant velocity). In addition, a reference frame has the ability to determine measurements of the time of events using a \"clock\" (any reference device with uniform periodicity). An event is an occurrence that can"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_8",
    "chunk": "be assigned a single unique moment and location in space relative to a reference frame: it is a \"point\" in spacetime. Since the speed of light is constant in relativity irrespective of the reference frame, pulses of light can be used to unambiguously measure distances and refer back to the times that events occurred to the clock, even though light takes time to reach the clock after the event has transpired. For example, the explosion of a firecracker may be considered to be an \"event\". We can completely specify an event by its four spacetime coordinates: The time of occurrence and its 3-dimensional spatial location define a reference point. Let's call this reference frame S. In relativity theory, we often want to calculate the coordinates of an event from differing reference frames. The equations that relate measurements made in different frames are called transformation equations. To gain insight into how the spacetime coordinates measured by observers in different reference frames compare with each other, it is useful to work with a simplified setup with frames in a standard configuration. With care, this allows simplification of the math with no loss of generality in the conclusions that are reached. In Fig."
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_9",
    "chunk": "2-1, two Galilean reference frames (i.e., conventional 3-space frames) are displayed in relative motion. Frame S belongs to a first observer O, and frame S′ (pronounced \"S prime\" or \"S dash\") belongs to a second observer O′. Since there is no absolute reference frame in relativity theory, a concept of \"moving\" does not strictly exist, as everything may be moving with respect to some other reference frame. Instead, any two frames that move at the same speed in the same direction are said to be comoving. Therefore, S and S′ are not comoving. The principle of relativity, which states that physical laws have the same form in each inertial reference frame, dates back to Galileo, and was incorporated into Newtonian physics. But in the late 19th century the existence of electromagnetic waves led some physicists to suggest that the universe was filled with a substance they called \"aether\", which, they postulated, would act as the medium through which these waves, or vibrations, propagated (in many respects similar to the way sound propagates through air). The aether was thought to be an absolute reference frame against which all speeds could be measured, and could be considered fixed and motionless relative to"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_10",
    "chunk": "Earth or some other fixed reference point. The aether was supposed to be sufficiently elastic to support electromagnetic waves, while those waves could interact with matter, yet offering no resistance to bodies passing through it (its one property was that it allowed electromagnetic waves to propagate). The results of various experiments, including the Michelson–Morley experiment in 1887 (subsequently verified with more accurate and innovative experiments), led to the theory of special relativity, by showing that the aether did not exist. Einstein's solution was to discard the notion of an aether and the absolute state of rest. In relativity, any reference frame moving with uniform motion will observe the same laws of physics. In particular, the speed of light in vacuum is always measured to be c, even when measured by multiple systems that are moving at different (but constant) velocities. From the principle of relativity alone without assuming the constancy of the speed of light (i.e., using the isotropy of space and the symmetry implied by the principle of special relativity) it can be shown that the spacetime transformations between inertial frames are either Euclidean, Galilean, or Lorentzian. In the Lorentzian case, one can then obtain relativistic interval conservation and"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_11",
    "chunk": "a certain finite limiting speed. Experiments suggest that this speed is the speed of light in vacuum. In Einstein's own view, the two postulates of relativity and the invariance of the speed of light lead to a single postulate, the Lorentz transformation: The insight fundamental for the special theory of relativity is this: The assumptions relativity and light speed invariance are compatible if relations of a new type (\"Lorentz transformation\") are postulated for the conversion of coordinates and times of events ... The universal principle of the special theory of relativity is contained in the postulate: The laws of physics are invariant with respect to Lorentz transformations (for the transition from one inertial system to any other arbitrarily chosen inertial system). This is a restricting principle for natural laws ... Following Einstein's original presentation of special relativity in 1905, many different sets of postulates have been proposed in various alternative derivations, but Einstein stuck to his approach throughout work. Henri Poincaré provided the mathematical framework for relativity theory by proving that Lorentz transformations are a subset of his Poincaré group of symmetry transformations. Einstein later derived these transformations from his axioms. While the traditional two-postulate approach to special relativity is"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_12",
    "chunk": "presented in innumerable college textbooks and popular presentations, other treatments of special relativity base it on the single postulate of universal Lorentz covariance, or, equivalently, on the single postulate of Minkowski spacetime. Textbooks starting with the single postulate of Minkowski spacetime include those by Taylor and Wheeler and by Callahan. Define an event to have spacetime coordinates (t, x, y, z) in system S and (t′, x′, y′, z′) in a reference frame moving at a velocity v on the x-axis with respect to that frame, S′. Then the Lorentz transformation specifies that these coordinates are related in the following way: t ′ = γ ( t − v x / c 2 ) x ′ = γ ( x − v t ) y ′ = y z ′ = z , {\\displaystyle {\\begin{aligned}t'&=\\gamma \\ (t-vx/c^{2})\\\\x'&=\\gamma \\ (x-vt)\\\\y'&=y\\\\z'&=z,\\end{aligned}}} where γ = 1 1 − v 2 / c 2 {\\displaystyle \\gamma ={\\frac {1}{\\sqrt {1-v^{2}/c^{2}}}}} is the Lorentz factor and c is the speed of light in vacuum, and the velocity v of S′, relative to S, is parallel to the x-axis. For simplicity, the y and z coordinates are unaffected; only the x and t coordinates are transformed. These"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_13",
    "chunk": "Lorentz transformations form a one-parameter group of linear mappings, that parameter being called rapidity. Solving the four transformation equations above for the unprimed coordinates yields the inverse Lorentz transformation: t = γ ( t ′ + v x ′ / c 2 ) x = γ ( x ′ + v t ′ ) y = y ′ z = z ′ . {\\displaystyle {\\begin{aligned}t&=\\gamma (t'+vx'/c^{2})\\\\x&=\\gamma (x'+vt')\\\\y&=y'\\\\z&=z'.\\end{aligned}}} This shows that the unprimed frame is moving with the velocity −v, as measured in the primed frame. There is nothing special about the x-axis. The transformation can apply to the y- or z-axis, or indeed in any direction parallel to the motion (which are warped by the γ factor) and perpendicular; see the article Lorentz transformation for details. A quantity that is invariant under Lorentz transformations is known as a Lorentz scalar. Writing the Lorentz transformation and its inverse in terms of coordinate differences, where one event has coordinates (x1, t1) and (x′1, t′1), another event has coordinates (x2, t2) and (x′2, t′2), and the differences are defined as Spacetime diagrams (Minkowski diagrams) are an extremely useful aid to visualizing how coordinates transform between different reference frames. Although it is not as"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_14",
    "chunk": "easy to perform exact computations using them as directly invoking the Lorentz transformations, their main power is their ability to provide an intuitive grasp of the results of a relativistic scenario. To draw a spacetime diagram, begin by considering two Galilean reference frames, S and S′, in standard configuration, as shown in Fig. 2-1. Fig. 3-1a. Draw the x {\\displaystyle x} and t {\\displaystyle t} axes of frame S. The x {\\displaystyle x} axis is horizontal and the t {\\displaystyle t} (actually c t {\\displaystyle ct} ) axis is vertical, which is the opposite of the usual convention in kinematics. The c t {\\displaystyle ct} axis is scaled by a factor of c {\\displaystyle c} so that both axes have common units of length. In the diagram shown, the gridlines are spaced one unit distance apart. The 45° diagonal lines represent the worldlines of two photons passing through the origin at time t = 0. {\\displaystyle t=0.} The slope of these worldlines is 1 because the photons advance one unit in space per unit of time. Two events, A {\\displaystyle {\\text{A}}} and B , {\\displaystyle {\\text{B}},} have been plotted on this graph so that their coordinates may be compared in"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_15",
    "chunk": "the S and S' frames. Fig. 3-1b. Draw the x ′ {\\displaystyle x'} and c t ′ {\\displaystyle ct'} axes of frame S'. The c t ′ {\\displaystyle ct'} axis represents the worldline of the origin of the S' coordinate system as measured in frame S. In this figure, v = c / 2. {\\displaystyle v=c/2.} Both the c t ′ {\\displaystyle ct'} and x ′ {\\displaystyle x'} axes are tilted from the unprimed axes by an angle α = tan − 1 ⁡ ( β ) , {\\displaystyle \\alpha =\\tan ^{-1}(\\beta ),} where β = v / c . {\\displaystyle \\beta =v/c.} The primed and unprimed axes share a common origin because frames S and S' had been set up in standard configuration, so that t = 0 {\\displaystyle t=0} when t ′ = 0. {\\displaystyle t'=0.} Fig. 3-1c. Units in the primed axes have a different scale from units in the unprimed axes. From the Lorentz transformations, we observe that ( x ′ , c t ′ ) {\\displaystyle (x',ct')} coordinates of ( 0 , 1 ) {\\displaystyle (0,1)} in the primed coordinate system transform to ( β γ , γ ) {\\displaystyle (\\beta \\gamma ,\\gamma )} in"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_16",
    "chunk": "the unprimed coordinate system. Likewise, ( x ′ , c t ′ ) {\\displaystyle (x',ct')} coordinates of ( 1 , 0 ) {\\displaystyle (1,0)} in the primed coordinate system transform to ( γ , β γ ) {\\displaystyle (\\gamma ,\\beta \\gamma )} in the unprimed system. Draw gridlines parallel with the c t ′ {\\displaystyle ct'} axis through points ( k γ , k β γ ) {\\displaystyle (k\\gamma ,k\\beta \\gamma )} as measured in the unprimed frame, where k {\\displaystyle k} is an integer. Likewise, draw gridlines parallel with the x ′ {\\displaystyle x'} axis through ( k β γ , k γ ) {\\displaystyle (k\\beta \\gamma ,k\\gamma )} as measured in the unprimed frame. Using the Pythagorean theorem, we observe that the spacing between c t ′ {\\displaystyle ct'} units equals ( 1 + β 2 ) / ( 1 − β 2 ) {\\textstyle {\\sqrt {(1+\\beta ^{2})/(1-\\beta ^{2})}}} times the spacing between c t {\\displaystyle ct} units, as measured in frame S. This ratio is always greater than 1, and ultimately it approaches infinity as β → 1. {\\displaystyle \\beta \\to 1.} Fig. 3-1d. Since the speed of light is an invariant, the worldlines of two photons"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_17",
    "chunk": "passing through the origin at time t ′ = 0 {\\displaystyle t'=0} still plot as 45° diagonal lines. The primed coordinates of A {\\displaystyle {\\text{A}}} and B {\\displaystyle {\\text{B}}} are related to the unprimed coordinates through the Lorentz transformations and could be approximately measured from the graph (assuming that it has been plotted accurately enough), but the real merit of a Minkowski diagram is its granting us a geometric view of the scenario. For example, in this figure, we observe that the two timelike-separated events that had different x-coordinates in the unprimed frame are now at the same position in space. While the unprimed frame is drawn with space and time axes that meet at right angles, the primed frame is drawn with axes that meet at acute or obtuse angles. This asymmetry is due to unavoidable distortions in how spacetime coordinates map onto a Cartesian plane, but the frames are actually equivalent. The consequences of special relativity can be derived from the Lorentz transformation equations. These transformations, and hence special relativity, lead to different physical predictions than those of Newtonian mechanics at all relative velocities, and most pronounced when relative velocities become comparable to the speed of light. The"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_18",
    "chunk": "speed of light is so much larger than anything most humans encounter that some of the effects predicted by relativity are initially counterintuitive. In Galilean relativity, the spatial separation, (⁠ Δ r {\\displaystyle \\Delta r} ⁠), and the temporal separation, (⁠ Δ t {\\displaystyle \\Delta t} ⁠), between two events are independent invariants, the values of which do not change when observed from different frames of reference. In special relativity, however, the interweaving of spatial and temporal coordinates generates the concept of an invariant interval, denoted as ⁠ Δ s 2 {\\displaystyle \\Delta s^{2}} ⁠: Δ s 2 = def c 2 Δ t 2 − ( Δ x 2 + Δ y 2 + Δ z 2 ) {\\displaystyle \\Delta s^{2}\\;{\\overset {\\text{def}}{=}}\\;c^{2}\\Delta t^{2}-(\\Delta x^{2}+\\Delta y^{2}+\\Delta z^{2})} In considering the physical significance of ⁠ Δ s 2 {\\displaystyle \\Delta s^{2}} ⁠, there are three cases to note: The interweaving of space and time revokes the implicitly assumed concepts of absolute simultaneity and synchronization across non-comoving frames. The form of ⁠ Δ s 2 {\\displaystyle \\Delta s^{2}} ⁠, being the difference of the squared time lapse and the squared spatial distance, demonstrates a fundamental discrepancy between Euclidean and spacetime distances. The"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_19",
    "chunk": "invariance of this interval is a property of the general Lorentz transform (also called the Poincaré transformation), making it an isometry of spacetime. The general Lorentz transform extends the standard Lorentz transform (which deals with translations without rotation, that is, Lorentz boosts, in the x-direction) with all other translations, reflections, and rotations between any Cartesian inertial frame. In the analysis of simplified scenarios, such as spacetime diagrams, a reduced-dimensionality form of the invariant interval is often employed: Δ s 2 = c 2 Δ t 2 − Δ x 2 {\\displaystyle \\Delta s^{2}\\,=\\,c^{2}\\Delta t^{2}-\\Delta x^{2}} Demonstrating that the interval is invariant is straightforward for the reduced-dimensionality case and with frames in standard configuration: c 2 Δ t 2 − Δ x 2 = c 2 γ 2 ( Δ t ′ + v Δ x ′ c 2 ) 2 − γ 2 ( Δ x ′ + v Δ t ′ ) 2 = γ 2 ( c 2 Δ t ′ 2 + 2 v Δ x ′ Δ t ′ + v 2 Δ x ′ 2 c 2 ) − γ 2 ( Δ x ′ 2 + 2 v Δ x ′ Δ t ′"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_20",
    "chunk": "+ v 2 Δ t ′ 2 ) = γ 2 c 2 Δ t ′ 2 − γ 2 v 2 Δ t ′ 2 − γ 2 Δ x ′ 2 + γ 2 v 2 Δ x ′ 2 c 2 = γ 2 c 2 Δ t ′ 2 ( 1 − v 2 c 2 ) − γ 2 Δ x ′ 2 ( 1 − v 2 c 2 ) = c 2 Δ t ′ 2 − Δ x ′ 2 {\\displaystyle {\\begin{aligned}c^{2}\\Delta t^{2}-\\Delta x^{2}&=c^{2}\\gamma ^{2}\\left(\\Delta t'+{\\dfrac {v\\Delta x'}{c^{2}}}\\right)^{2}-\\gamma ^{2}\\ (\\Delta x'+v\\Delta t')^{2}\\\\&=\\gamma ^{2}\\left(c^{2}\\Delta t'^{\\,2}+2v\\Delta x'\\Delta t'+{\\dfrac {v^{2}\\Delta x'^{\\,2}}{c^{2}}}\\right)-\\gamma ^{2}\\ (\\Delta x'^{\\,2}+2v\\Delta x'\\Delta t'+v^{2}\\Delta t'^{\\,2})\\\\&=\\gamma ^{2}c^{2}\\Delta t'^{\\,2}-\\gamma ^{2}v^{2}\\Delta t'^{\\,2}-\\gamma ^{2}\\Delta x'^{\\,2}+\\gamma ^{2}{\\dfrac {v^{2}\\Delta x'^{\\,2}}{c^{2}}}\\\\&=\\gamma ^{2}c^{2}\\Delta t'^{\\,2}\\left(1-{\\dfrac {v^{2}}{c^{2}}}\\right)-\\gamma ^{2}\\Delta x'^{\\,2}\\left(1-{\\dfrac {v^{2}}{c^{2}}}\\right)\\\\&=c^{2}\\Delta t'^{\\,2}-\\Delta x'^{\\,2}\\end{aligned}}} The value of Δ s 2 {\\displaystyle \\Delta s^{2}} is hence independent of the frame in which it is measured. Consider two events happening in two different locations that occur simultaneously in the reference frame of one inertial observer. They may occur non-simultaneously in the reference frame of another inertial observer (lack of absolute simultaneity). From Equation 3 (the forward Lorentz transformation in terms of coordinate differences) Δ t ′ ="
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_21",
    "chunk": "γ ( Δ t − v Δ x c 2 ) {\\displaystyle \\Delta t'=\\gamma \\left(\\Delta t-{\\frac {v\\,\\Delta x}{c^{2}}}\\right)} It is clear that the two events that are simultaneous in frame S (satisfying Δt = 0), are not necessarily simultaneous in another inertial frame S′ (satisfying Δt′ = 0). Only if these events are additionally co-local in frame S (satisfying Δx = 0), will they be simultaneous in another frame S′. The Sagnac effect can be considered a manifestation of the relativity of simultaneity. Since relativity of simultaneity is a first order effect in ⁠ v {\\displaystyle v} ⁠, instruments based on the Sagnac effect for their operation, such as ring laser gyroscopes and fiber optic gyroscopes, are capable of extreme levels of sensitivity. The time lapse between two events is not invariant from one observer to another, but is dependent on the relative speeds of the observers' reference frames. Suppose a clock is at rest in the unprimed system S. The location of the clock on two different ticks is then characterized by Δx = 0. To find the relation between the times between these ticks as measured in both systems, Equation 3 can be used to find: This shows"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_22",
    "chunk": "that the time (Δt′) between the two ticks as seen in the frame in which the clock is moving (S′), is longer than the time (Δt) between these ticks as measured in the rest frame of the clock (S). Time dilation explains a number of physical phenomena; for example, the lifetime of high speed muons created by the collision of cosmic rays with particles in the Earth's outer atmosphere and moving towards the surface is greater than the lifetime of slowly moving muons, created and decaying in a laboratory. Whenever one hears a statement to the effect that \"moving clocks run slow\", one should envision an inertial reference frame thickly populated with identical, synchronized clocks. As a moving clock travels through this array, its reading at any particular point is compared with a stationary clock at the same point. The measurements that we would get if we actually looked at a moving clock would, in general, not at all be the same thing, because the time that we would see would be delayed by the finite speed of light, i.e. the times that we see would be distorted by the Doppler effect. Measurements of relativistic effects must always be understood"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_23",
    "chunk": "as having been made after finite speed-of-light effects have been factored out. Paul Langevin, an early proponent of the theory of relativity, did much to popularize the theory in the face of resistance by many physicists to Einstein's revolutionary concepts. Among his numerous contributions to the foundations of special relativity were independent work on the mass–energy relationship, a thorough examination of the twin paradox, and investigations into rotating coordinate systems. His name is frequently attached to a hypothetical construct called a \"light-clock\" (originally developed by Lewis and Tolman in 1909), which he used to perform a novel derivation of the Lorentz transformation. A light-clock is imagined to be a box of perfectly reflecting walls wherein a light signal reflects back and forth from opposite faces. The concept of time dilation is frequently taught using a light-clock that is traveling in uniform inertial motion perpendicular to a line connecting the two mirrors. (Langevin himself made use of a light-clock oriented parallel to its line of motion.) Consider the scenario illustrated in Fig. 4-3A. Observer A holds a light-clock of length L {\\displaystyle L} as well as an electronic timer with which she measures how long it takes a pulse to make"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_24",
    "chunk": "a round trip up and down along the light-clock. Although observer A is traveling rapidly along a train, from her point of view the emission and receipt of the pulse occur at the same place, and she measures the interval using a single clock located at the precise position of these two events. For the interval between these two events, observer A finds ⁠ t A = 2 L / c {\\displaystyle t_{\\text{A}}=2L/c} ⁠. A time interval measured using a single clock that is motionless in a particular reference frame is called a proper time interval. Fig. 4-3B illustrates these same two events from the standpoint of observer B, who is parked by the tracks as the train goes by at a speed of ⁠ v {\\displaystyle v} ⁠. Instead of making straight up-and-down motions, observer B sees the pulses moving along a zig-zag line. However, because of the postulate of the constancy of the speed of light, the speed of the pulses along these diagonal lines is the same c {\\displaystyle c} that observer A saw for her up-and-down pulses. B measures the speed of the vertical component of these pulses as ± c 2 − v 2 ,"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_25",
    "chunk": "{\\textstyle \\pm {\\sqrt {c^{2}-v^{2}}},} so that the total round-trip time of the pulses is t B = 2 L / c 2 − v 2 = {\\textstyle t_{\\text{B}}=2L{\\big /}{\\sqrt {c^{2}-v^{2}}}={}} ⁠ t A / 1 − v 2 / c 2 {\\displaystyle \\textstyle t_{\\text{A}}{\\big /}{\\sqrt {1-v^{2}/c^{2}}}} ⁠. Note that for observer B, the emission and receipt of the light pulse occurred at different places, and he measured the interval using two stationary and synchronized clocks located at two different positions in his reference frame. The interval that B measured was therefore not a proper time interval because he did not measure it with a single resting clock. In the above description of the Langevin light-clock, the labeling of one observer as stationary and the other as in motion was completely arbitrary. One could just as well have observer B carrying the light-clock and moving at a speed of v {\\displaystyle v} to the left, in which case observer A would perceive B's clock as running slower than her local clock. There is no paradox here, because there is no independent observer C who will agree with both A and B. Observer C necessarily makes his measurements from his own reference"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_26",
    "chunk": "frame. If that reference frame coincides with A's reference frame, then C will agree with A's measurement of time. If C's reference frame coincides with B's reference frame, then C will agree with B's measurement of time. If C's reference frame coincides with neither A's frame nor B's frame, then C's measurement of time will disagree with both A's and B's measurement of time. The reciprocity of time dilation between two observers in separate inertial frames leads to the so-called twin paradox, articulated in its present form by Langevin in 1911. Langevin imagined an adventurer wishing to explore the future of the Earth. This traveler boards a projectile capable of traveling at 99.995% of the speed of light. After making a round-trip journey to and from a nearby star lasting only two years of his own life, he returns to an Earth that is two hundred years older. This result appears puzzling because both the traveler and an Earthbound observer would see the other as moving, and so, because of the reciprocity of time dilation, one might initially expect that each should have found the other to have aged less. In reality, there is no paradox at all, because in"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_27",
    "chunk": "order for the two observers to perform side-by-side comparisons of their elapsed proper times, the symmetry of the situation must be broken: At least one of the two observers must change their state of motion to match that of the other. Knowing the general resolution of the paradox, however, does not immediately yield the ability to calculate correct quantitative results. Many solutions to this puzzle have been provided in the literature and have been reviewed in the Twin paradox article. We will examine in the following one such solution to the paradox. Our basic aim will be to demonstrate that, after the trip, both twins are in perfect agreement about who aged by how much, regardless of their different experiences. Fig 4-4 illustrates a scenario where the traveling twin flies at 0.6 c to and from a star 3 ly distant. During the trip, each twin sends yearly time signals (measured in their own proper times) to the other. After the trip, the cumulative counts are compared. On the outward phase of the trip, each twin receives the other's signals at the lowered rate of ⁠ f ′ = f ( 1 − β ) / ( 1 + β"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_28",
    "chunk": ") {\\displaystyle \\textstyle f'=f{\\sqrt {(1-\\beta )/(1+\\beta )}}} ⁠. Initially, the situation is perfectly symmetric: note that each twin receives the other's one-year signal at two years measured on their own clock. The symmetry is broken when the traveling twin turns around at the four-year mark as measured by her clock. During the remaining four years of her trip, she receives signals at the enhanced rate of ⁠ f ″ = f ( 1 + β ) / ( 1 − β ) {\\displaystyle \\textstyle f''=f{\\sqrt {(1+\\beta )/(1-\\beta )}}} ⁠. The situation is quite different with the stationary twin. Because of light-speed delay, he does not see his sister turn around until eight years have passed on his own clock. Thus, he receives enhanced-rate signals from his sister for only a relatively brief period. Although the twins disagree in their respective measures of total time, we see in the following table, as well as by simple observation of the Minkowski diagram, that each twin is in total agreement with the other as to the total number of signals sent from one to the other. There is hence no paradox. The dimensions (e.g., length) of an object as measured by one observer"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_29",
    "chunk": "may be smaller than the results of measurements of the same object made by another observer (e.g., the ladder paradox involves a long ladder traveling near the speed of light and being contained within a smaller garage). Similarly, suppose a measuring rod is at rest and aligned along the x-axis in the unprimed system S. In this system, the length of this rod is written as Δx. To measure the length of this rod in the system S′, in which the rod is moving, the distances x′ to the end points of the rod must be measured simultaneously in that system S′. In other words, the measurement is characterized by Δt′ = 0, which can be combined with Equation 4 to find the relation between the lengths Δx and Δx′: This shows that the length (Δx′) of the rod as measured in the frame in which it is moving (S′), is shorter than its length (Δx) in its own rest frame (S). Time dilation and length contraction are not merely appearances. Time dilation is explicitly related to our way of measuring time intervals between events that occur at the same place in a given coordinate system (called \"co-local\" events). These"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_30",
    "chunk": "time intervals (which can be, and are, actually measured experimentally by relevant observers) are different in another coordinate system moving with respect to the first, unless the events, in addition to being co-local, are also simultaneous. Similarly, length contraction relates to our measured distances between separated but simultaneous events in a given coordinate system of choice. If these events are not co-local, but are separated by distance (space), they will not occur at the same spatial distance from each other when seen from another moving coordinate system. Consider two frames S and S′ in standard configuration. A particle in S moves in the x direction with velocity vector ⁠ u {\\displaystyle \\mathbf {u} } ⁠. What is its velocity u ′ {\\displaystyle \\mathbf {u'} } in frame S′? Substituting expressions for d x ′ {\\displaystyle dx'} and d t ′ {\\displaystyle dt'} from Equation 5 into Equation 8, followed by straightforward mathematical manipulations and back-substitution from Equation 7 yields the Lorentz transformation of the speed u {\\displaystyle u} to ⁠ u ′ {\\displaystyle u'} ⁠: The inverse relation is obtained by interchanging the primed and unprimed symbols and replacing v {\\displaystyle v} with ⁠ − v {\\displaystyle -v} ⁠. For"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_31",
    "chunk": "u {\\displaystyle \\mathbf {u} } not aligned along the x-axis, we write: Equation 10 and Equation 14 can be interpreted as giving the resultant u {\\displaystyle \\mathbf {u} } of the two velocities v {\\displaystyle \\mathbf {v} } and ⁠ u ′ {\\displaystyle \\mathbf {u'} } ⁠, and they replace the formula ⁠ u = u ′ + v {\\displaystyle \\mathbf {u=u'+v} } ⁠. which is valid in Galilean relativity. Interpreted in such a fashion, they are commonly referred to as the relativistic velocity addition (or composition) formulas, valid for the three axes of S and S′ being aligned with each other (although not necessarily in standard configuration). There is nothing special about the x direction in the standard configuration. The above formalism applies to any direction; and three orthogonal directions allow dealing with all directions in space by decomposing the velocity vectors to their components in these directions. See Velocity-addition formula for details. The composition of two non-collinear Lorentz boosts (i.e., two non-collinear Lorentz transformations, neither of which involve rotation) results in a Lorentz transformation that is not a pure boost but is the composition of a boost and a rotation. Thomas rotation results from the relativity of simultaneity."
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_32",
    "chunk": "In Fig. 4-5a, a rod of length L {\\displaystyle L} in its rest frame (i.e., having a proper length of ⁠ L {\\displaystyle L} ⁠) rises vertically along the y-axis in the ground frame. In Fig. 4-5b, the same rod is observed from the frame of a rocket moving at speed v {\\displaystyle v} to the right. If we imagine two clocks situated at the left and right ends of the rod that are synchronized in the frame of the rod, relativity of simultaneity causes the observer in the rocket frame to observe (not see) the clock at the right end of the rod as being advanced in time by ⁠ L v / c 2 {\\displaystyle Lv/c^{2}} ⁠, and the rod is correspondingly observed as tilted. Unlike second-order relativistic effects such as length contraction or time dilation, this effect becomes quite significant even at fairly low velocities. For example, this can be seen in the spin of moving particles, where Thomas precession is a relativistic correction that applies to the spin of an elementary particle or the rotation of a macroscopic gyroscope, relating the angular velocity of the spin of a particle following a curvilinear orbit to the angular"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_33",
    "chunk": "velocity of the orbital motion. Thomas rotation provides the resolution to the well-known \"meter stick and hole paradox\". In Fig. 4-6, the time interval between the events A (the \"cause\") and B (the \"effect\") is 'timelike'; that is, there is a frame of reference in which events A and B occur at the same location in space, separated only by occurring at different times. If A precedes B in that frame, then A precedes B in all frames accessible by a Lorentz transformation. It is possible for matter (or information) to travel (below light speed) from the location of A, starting at the time of A, to the location of B, arriving at the time of B, so there can be a causal relationship (with A the cause and B the effect). The interval AC in the diagram is 'spacelike'; that is, there is a frame of reference in which events A and C occur simultaneously, separated only in space. There are also frames in which A precedes C (as shown) and frames in which C precedes A. But no frames are accessible by a Lorentz transformation, in which events A and C occur at the same location. If it"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_34",
    "chunk": "were possible for a cause-and-effect relationship to exist between events A and C, paradoxes of causality would result. For example, if signals could be sent faster than light, then signals could be sent into the sender's past (observer B in the diagrams). A variety of causal paradoxes could then be constructed. Consider the spacetime diagrams in Fig. 4-7. A and B stand alongside a railroad track, when a high-speed train passes by, with C riding in the last car of the train and D riding in the leading car. The world lines of A and B are vertical (ct), distinguishing the stationary position of these observers on the ground, while the world lines of C and D are tilted forwards (ct′), reflecting the rapid motion of the observers C and D stationary in their train, as observed from the ground. It is not necessary for signals to be instantaneous to violate causality. Even if the signal from D to C were slightly shallower than the x ′ {\\displaystyle x'} axis (and the signal from A to B slightly steeper than the x {\\displaystyle x} axis), it would still be possible for B to receive his message before he had sent"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_35",
    "chunk": "it. By increasing the speed of the train to near light speeds, the c t ′ {\\displaystyle ct'} and x ′ {\\displaystyle x'} axes can be squeezed very close to the dashed line representing the speed of light. With this modified setup, it can be demonstrated that even signals only slightly faster than the speed of light will result in causality violation. Therefore, if causality is to be preserved, one of the consequences of special relativity is that no information signal or material object can travel faster than light in vacuum. This is not to say that all faster than light speeds are impossible. Various trivial situations can be described where some \"things\" (not actual matter or energy) move faster than light. For example, the location where the beam of a search light hits the bottom of a cloud can move faster than light when the search light is turned rapidly (although this does not violate causality or any other relativistic phenomenon). In 1850, Hippolyte Fizeau and Léon Foucault independently established that light travels more slowly in water than in air, thus validating a prediction of Fresnel's wave theory of light and invalidating the corresponding prediction of Newton's corpuscular theory."
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_36",
    "chunk": "The speed of light was measured in still water. What would be the speed of light in flowing water? In 1851, Fizeau conducted an experiment to answer this question, a simplified representation of which is illustrated in Fig. 5-1. A beam of light is divided by a beam splitter, and the split beams are passed in opposite directions through a tube of flowing water. They are recombined to form interference fringes, indicating a difference in optical path length, that an observer can view. The experiment demonstrated that dragging of the light by the flowing water caused a displacement of the fringes, showing that the motion of the water had affected the speed of the light. According to the theories prevailing at the time, light traveling through a moving medium would be a simple sum of its speed through the medium plus the speed of the medium. Contrary to expectation, Fizeau found that although light appeared to be dragged by the water, the magnitude of the dragging was much lower than expected. If u ′ = c / n {\\displaystyle u'=c/n} is the speed of light in still water, and v {\\displaystyle v} is the speed of the water, and u"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_37",
    "chunk": "± {\\displaystyle u_{\\pm }} is the water-borne speed of light in the lab frame with the flow of water adding to or subtracting from the speed of light, then u ± = c n ± v ( 1 − 1 n 2 ) . {\\displaystyle u_{\\pm }={\\frac {c}{n}}\\pm v\\left(1-{\\frac {1}{n^{2}}}\\right)\\ .} Fizeau's results, although consistent with Fresnel's earlier hypothesis of partial aether dragging, were extremely disconcerting to physicists of the time. Among other things, the presence of an index of refraction term meant that, since n {\\displaystyle n} depends on wavelength, the aether must be capable of sustaining different motions at the same time. A variety of theoretical explanations were proposed to explain Fresnel's dragging coefficient, that were completely at odds with each other. Even before the Michelson–Morley experiment, Fizeau's experimental results were among a number of observations that created a critical situation in explaining the optics of moving bodies. From the point of view of special relativity, Fizeau's result is nothing but an approximation to Equation 10, the relativistic formula for composition of velocities. Because of the finite speed of light, if the relative motions of a source and receiver include a transverse component, then the direction from which"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_38",
    "chunk": "light arrives at the receiver will be displaced from the geometric position in space of the source relative to the receiver. The classical calculation of the displacement takes two forms and makes different predictions depending on whether the receiver, the source, or both are in motion with respect to the medium. (1) If the receiver is in motion, the displacement would be the consequence of the aberration of light. The incident angle of the beam relative to the receiver would be calculable from the vector sum of the receiver's motions and the velocity of the incident light. (2) If the source is in motion, the displacement would be the consequence of light-time correction. The displacement of the apparent position of the source from its geometric position would be the result of the source's motion during the time that its light takes to reach the receiver. The classical explanation failed experimental test. Since the aberration angle depends on the relationship between the velocity of the receiver and the speed of the incident light, passage of the incident light through a refractive medium should change the aberration angle. In 1810, Arago used this expected phenomenon in a failed attempt to measure the"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_39",
    "chunk": "speed of light, and in 1870, George Airy tested the hypothesis using a water-filled telescope, finding that, against expectation, the measured aberration was identical to the aberration measured with an air-filled telescope. A \"cumbrous\" attempt to explain these results used the hypothesis of partial aether-drag, but was incompatible with the results of the Michelson–Morley experiment, which apparently demanded complete aether-drag. Assuming inertial frames, the relativistic expression for the aberration of light is applicable to both the receiver moving and source moving cases. A variety of trigonometrically equivalent formulas have been published. Expressed in terms of the variables in Fig. 5-2, these include The classical Doppler effect depends on whether the source, receiver, or both are in motion with respect to the medium. The relativistic Doppler effect is independent of any medium. Nevertheless, relativistic Doppler shift for the longitudinal case, with source and receiver moving directly towards or away from each other, can be derived as if it were the classical phenomenon, but modified by the addition of a time dilation term, and that is the treatment described here. Assume the receiver and the source are moving away from each other with a relative speed v {\\displaystyle v} as measured by"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_40",
    "chunk": "an observer on the receiver or the source (The sign convention adopted here is that v {\\displaystyle v} is negative if the receiver and the source are moving towards each other). Assume that the source is stationary in the medium. Then f r = ( 1 − v c s ) f s {\\displaystyle f_{r}=\\left(1-{\\frac {v}{c_{s}}}\\right)f_{s}} where c s {\\displaystyle c_{s}} is the speed of sound. For light, and with the receiver moving at relativistic speeds, clocks on the receiver are time dilated relative to clocks at the source. The receiver will measure the received frequency to be f r = γ ( 1 − β ) f s = 1 − β 1 + β f s . {\\displaystyle f_{r}=\\gamma \\left(1-\\beta \\right)f_{s}={\\sqrt {\\frac {1-\\beta }{1+\\beta }}}\\,f_{s}.} where An identical expression for relativistic Doppler shift is obtained when performing the analysis in the reference frame of the receiver with a moving source. The transverse Doppler effect is one of the main novel predictions of the special theory of relativity. Classically, one might expect that if source and receiver are moving transversely with respect to each other with no longitudinal component to their relative motions, that there should be no Doppler"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_41",
    "chunk": "shift in the light arriving at the receiver. Special relativity predicts otherwise. Fig. 5-3 illustrates two common variants of this scenario. Both variants can be analyzed using simple time dilation arguments. In Fig. 5-3a, the receiver observes light from the source as being blueshifted by a factor of ⁠ γ {\\displaystyle \\gamma } ⁠. In Fig. 5-3b, the light is redshifted by the same factor. Time dilation and length contraction are not optical illusions, but genuine effects. Measurements of these effects are not an artifact of Doppler shift, nor are they the result of neglecting to take into account the time it takes light to travel from an event to an observer. Scientists make a fundamental distinction between measurement or observation on the one hand, versus visual appearance, or what one sees. The measured shape of an object is a hypothetical snapshot of all of the object's points as they exist at a single moment in time. But the visual appearance of an object is affected by the varying lengths of time that light takes to travel from different points on the object to one's eye. For many years, the distinction between the two had not been generally appreciated, and"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_42",
    "chunk": "it had generally been thought that a length contracted object passing by an observer would in fact actually be seen as length contracted. In 1959, James Terrell and Roger Penrose independently pointed out that differential time lag effects in signals reaching the observer from the different parts of a moving object result in a fast moving object's visual appearance being quite different from its measured shape. For example, a receding object would appear contracted, an approaching object would appear elongated, and a passing object would have a skew appearance that has been likened to a rotation. A sphere in motion retains the circular outline for all speeds, for any distance, and for all view angles, although the surface of the sphere and the images on it will appear distorted. Both Fig. 5-4 and Fig. 5-5 illustrate objects moving transversely to the line of sight. In Fig. 5-4, a cube is viewed from a distance of four times the length of its sides. At high speeds, the sides of the cube that are perpendicular to the direction of motion appear hyperbolic in shape. The cube is actually not rotated. Rather, light from the rear of the cube takes longer to reach"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_43",
    "chunk": "one's eyes compared with light from the front, during which time the cube has moved to the right. At high speeds, the sphere in Fig. 5-5 takes on the appearance of a flattened disk tilted up to 45° from the line of sight. If the objects' motions are not strictly transverse but instead include a longitudinal component, exaggerated distortions in perspective may be seen. This illusion has come to be known as Terrell rotation or the Terrell–Penrose effect. Another example where visual appearance is at odds with measurement comes from the observation of apparent superluminal motion in various radio galaxies, BL Lac objects, quasars, and other astronomical objects that eject relativistic-speed jets of matter at narrow angles with respect to the viewer. An apparent optical illusion results giving the appearance of faster than light travel. In Fig. 5-6, galaxy M87 streams out a high-speed jet of subatomic particles almost directly towards us, but Penrose–Terrell rotation causes the jet to appear to be moving laterally in the same manner that the appearance of the cube in Fig. 5-4 has been stretched out. Section § Consequences derived from the Lorentz transformation dealt strictly with kinematics, the study of the motion of points,"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_44",
    "chunk": "bodies, and systems of bodies without considering the forces that caused the motion. This section discusses masses, forces, energy and so forth, and as such requires consideration of physical effects beyond those encompassed by the Lorentz transformation itself. Mass–energy equivalence is a consequence of special relativity. The energy and momentum, which are separate in Newtonian mechanics, form a four-vector in relativity, and this relates the time component (the energy) to the space components (the momentum) in a non-trivial way. For an object at rest, the energy–momentum four-vector is (E/c, 0, 0, 0): it has a time component, which is the energy, and three space components, which are zero. By changing frames with a Lorentz transformation in the x direction with a small value of the velocity v, the energy momentum four-vector becomes (E/c, Ev/c, 0, 0). The momentum is equal to the energy multiplied by the velocity divided by c. As such, the Newtonian mass of an object, which is the ratio of the momentum to the velocity for slow velocities, is equal to E/c. The energy and momentum are properties of matter and radiation, and it is impossible to deduce that they form a four-vector just from the two"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_45",
    "chunk": "basic postulates of special relativity by themselves, because these do not talk about matter or radiation, they only talk about space and time. The derivation therefore requires some additional physical reasoning. In his 1905 paper, Einstein used the additional principles that Newtonian mechanics should hold for slow velocities, so that there is one energy scalar and one three-vector momentum at slow velocities, and that the conservation law for energy and momentum is exactly true in relativity. Furthermore, he assumed that the energy of light is transformed by the same Doppler-shift factor as its frequency, which he had previously shown to be true based on Maxwell's equations. The first of Einstein's papers on this subject was \"Does the Inertia of a Body Depend upon its Energy Content?\" in 1905. Although Einstein's argument in this paper is nearly universally accepted by physicists as correct, even self-evident, many authors over the years have suggested that it is wrong. Other authors suggest that the argument was merely inconclusive because it relied on some implicit assumptions. Einstein acknowledged the controversy over his derivation in his 1907 survey paper on special relativity. There he notes that it is problematic to rely on Maxwell's equations for the"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_46",
    "chunk": "heuristic mass–energy argument. The argument in his 1905 paper can be carried out with the emission of any massless particles, but the Maxwell equations are implicitly used to make it obvious that the emission of light in particular can be achieved only by doing work. To emit electromagnetic waves, all you have to do is shake a charged particle, and this is clearly doing work, so that the emission is of energy. In his fourth of his 1905 Annus mirabilis papers, Einstein presented a heuristic argument for the equivalence of mass and energy. Although, as discussed above, subsequent scholarship has established that his arguments fell short of a broadly definitive proof, the conclusions that he reached in this paper have stood the test of time. Einstein took as starting assumptions his recently discovered formula for relativistic Doppler shift, the laws of conservation of energy and conservation of momentum, and the relationship between the frequency of light and its energy as implied by Maxwell's equations. Fig. 6-1 (top). Consider a system of plane waves of light having frequency f {\\displaystyle f} traveling in direction ϕ {\\displaystyle \\phi } relative to the x-axis of reference frame S. The frequency (and hence energy)"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_47",
    "chunk": "of the waves as measured in frame S′ that is moving along the x-axis at velocity v {\\displaystyle v} is given by the relativistic Doppler shift formula that Einstein had developed in his 1905 paper on special relativity: Fig. 6-1 (bottom). Consider an arbitrary body that is stationary in reference frame S. Let this body emit a pair of equal-energy light-pulses in opposite directions at angle ϕ {\\displaystyle \\phi } with respect to the x-axis. Each pulse has energy ⁠ L / 2 {\\displaystyle L/2} ⁠. Because of conservation of momentum, the body remains stationary in S after emission of the two pulses. Let E 0 {\\displaystyle E_{0}} be the energy of the body before emission of the two pulses and E 1 {\\displaystyle E_{1}} after their emission. Next, consider the same system observed from frame S′ that is moving along the x-axis at speed v {\\displaystyle v} relative to frame S. In this frame, light from the forwards and reverse pulses will be relativistically Doppler-shifted. Let H 0 {\\displaystyle H_{0}} be the energy of the body measured in reference frame S′ before emission of the two pulses and H 1 {\\displaystyle H_{1}} after their emission. We obtain the following"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_48",
    "chunk": "relationships: The two differences of form H − E {\\displaystyle H-E} seen in the above equation have a straightforward physical interpretation. Since H {\\displaystyle H} and E {\\displaystyle E} are the energies of the arbitrary body in the moving and stationary frames, H 0 − E 0 {\\displaystyle H_{0}-E_{0}} and H 1 − E 1 {\\displaystyle H_{1}-E_{1}} represents the kinetic energies of the bodies before and after the emission of light (except for an additive constant that fixes the zero point of energy and is conventionally set to zero). Hence, Comparing the above expression with the classical expression for kinetic energy, K.E. = ⁠1/2⁠mv, Einstein then noted: \"If a body gives off the energy L in the form of radiation, its mass diminishes by L/c.\" Rindler has observed that Einstein's heuristic argument suggested merely that energy contributes to mass. In 1905, Einstein's cautious expression of the mass–energy relationship allowed for the possibility that \"dormant\" mass might exist that would remain behind after all the energy of a body was removed. By 1907, however, Einstein was ready to assert that all inertial mass represented a reserve of energy. \"To equate all mass with energy required an act of aesthetic faith, very"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_49",
    "chunk": "characteristic of Einstein.\" Einstein's bold hypothesis has been amply confirmed in the years subsequent to his original proposal. For a variety of reasons, Einstein's original derivation is currently seldom taught. Besides the vigorous debate that continues until this day as to the formal correctness of his original derivation, the recognition of special relativity as being what Einstein called a \"principle theory\" has led to a shift away from reliance on electromagnetic phenomena to purely dynamic methods of proof. Since nothing can travel faster than light, one might conclude that a human can never travel farther from Earth than ~ 100 light years. You would easily think that a traveler would never be able to reach more than the few solar systems that exist within the limit of 100 light years from Earth. However, because of time dilation, a hypothetical spaceship can travel thousands of light years during a passenger's lifetime. If a spaceship could be built that accelerates at a constant 1g, it will, after one year, be travelling at almost the speed of light as seen from Earth. This is described by: v ( t ) = a t 1 + a 2 t 2 / c 2 ,"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_50",
    "chunk": "{\\displaystyle v(t)={\\frac {at}{\\sqrt {1+a^{2}t^{2}/c^{2}}}},} where v(t) is the velocity at a time t, a is the acceleration of the spaceship and t is the coordinate time as measured by people on Earth. Therefore, after one year of accelerating at 9.81 m/s, the spaceship will be travelling at v = 0.712 c and 0.946 c after three years, relative to Earth. After three years of this acceleration, with the spaceship achieving a velocity of 94.6% of the speed of light relative to Earth, time dilation will result in each second experienced on the spaceship corresponding to 3.1 seconds back on Earth. During their journey, people on Earth will experience more time than they do – since their clocks (all physical phenomena) would really be ticking 3.1 times faster than those of the spaceship. A 5-year round trip for the traveller will take 6.5 Earth years and cover a distance of over 6 light-years. A 20-year round trip for them (5 years accelerating, 5 decelerating, twice each) will land them back on Earth having travelled for 335 Earth years and a distance of 331 light years. A full 40-year trip at 1g will appear on Earth to last 58,000 years and cover"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_51",
    "chunk": "a distance of 55,000 light years. A 40-year trip at 1.1 g will take 148000 years and cover about 140000 light years. A one-way 28 year (14 years accelerating, 14 decelerating as measured with the astronaut's clock) trip at 1g acceleration could reach 2,000,000 light-years to the Andromeda Galaxy. This same time dilation is why a muon travelling close to c is observed to travel much farther than c times its half-life (when at rest). Examination of the collision products generated by particle accelerators around the world provides scientists evidence of the structure of the subatomic world and the natural laws governing it. Analysis of the collision products, the sum of whose masses may vastly exceed the masses of the incident particles, requires special relativity. In Newtonian mechanics, analysis of collisions involves use of the conservation laws for mass, momentum and energy. In relativistic mechanics, mass is not independently conserved, because it has been subsumed into the total relativistic energy. We illustrate the differences that arise between the Newtonian and relativistic treatments of particle collisions by examining the simple case of two perfectly elastic colliding particles of equal mass. (Inelastic collisions are discussed in Spacetime#Conservation laws. Radioactive decay may be"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_52",
    "chunk": "considered a sort of time-reversed inelastic collision.) Elastic scattering of charged elementary particles deviates from ideality due to the production of Bremsstrahlung radiation. Fig. 6-2 provides a demonstration of the result, familiar to billiard players, that if a stationary ball is struck elastically by another one of the same mass (assuming no sidespin, or \"English\"), then after collision, the diverging paths of the two balls will subtend a right angle. (a) In the stationary frame, an incident sphere traveling at 2v strikes a stationary sphere. (b) In the center of momentum frame, the two spheres approach each other symmetrically at ±v. After elastic collision, the two spheres rebound from each other with equal and opposite velocities ±u. Energy conservation requires that |u| = |v|. (c) Reverting to the stationary frame, the rebound velocities are v ± u. The dot product (v + u) ⋅ (v − u) = v − u = 0, indicating that the vectors are orthogonal. Consider the elastic collision scenario in Fig. 6-3 between a moving particle colliding with an equal mass stationary particle. Unlike the Newtonian case, the angle between the two particles after collision is less than 90°, is dependent on the angle of"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_53",
    "chunk": "scattering, and becomes smaller and smaller as the velocity of the incident particle approaches the speed of light: The relativistic momentum and total relativistic energy of a particle are given by Conservation of momentum dictates that the sum of the momenta of the incoming particle and the stationary particle (which initially has momentum = 0) equals the sum of the momenta of the emergent particles: Likewise, the sum of the total relativistic energies of the incoming particle and the stationary particle (which initially has total energy mc) equals the sum of the total energies of the emergent particles: Breaking down (6-5) into its components, replacing v {\\displaystyle v} with the dimensionless ⁠ β {\\displaystyle \\beta } ⁠, and factoring out common terms from (6-5) and (6-6) yields the following: For the symmetrical case in which ϕ = θ {\\displaystyle \\phi =\\theta } and ⁠ β 2 = β 3 {\\displaystyle \\beta _{2}=\\beta _{3}} ⁠, (6-12) takes on the simpler form: Lorentz transformations relate coordinates of events in one reference frame to those of another frame. Relativistic composition of velocities is used to add two velocities together. The formulas to perform the latter computations are nonlinear, making them more complex than"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_54",
    "chunk": "the corresponding Galilean formulas. This nonlinearity is an artifact of our choice of parameters. We have previously noted that in an x–ct spacetime diagram, the points at some constant spacetime interval from the origin form an invariant hyperbola. We have also noted that the coordinate systems of two spacetime reference frames in standard configuration are hyperbolically rotated with respect to each other. The natural functions for expressing these relationships are the hyperbolic analogs of the trigonometric functions. Fig. 7-1a shows a unit circle with sin(a) and cos(a), the only difference between this diagram and the familiar unit circle of elementary trigonometry being that a is interpreted, not as the angle between the ray and the x-axis, but as twice the area of the sector swept out by the ray from the x-axis. Numerically, the angle and 2 × area measures for the unit circle are identical. Fig. 7-1b shows a unit hyperbola with sinh(a) and cosh(a), where a is likewise interpreted as twice the tinted area. Fig. 7-2 presents plots of the sinh, cosh, and tanh functions. In the Cartesian plane, rotation of point (x, y) into point (x', y') by angle θ is given by In a spacetime diagram,"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_55",
    "chunk": "the velocity parameter β {\\displaystyle \\beta } is the analog of slope. The rapidity, φ, is defined by The rapidity defined above is very useful in special relativity because many expressions take on a considerably simpler form when expressed in terms of it. For example, rapidity is simply additive in the collinear velocity-addition formula; or in other words, ⁠ ϕ = ϕ 1 + ϕ 2 {\\displaystyle \\phi =\\phi _{1}+\\phi _{2}} ⁠. The Lorentz transformations take a simple form when expressed in terms of rapidity. The γ factor can be written as Transformations describing relative motion with uniform velocity and without rotation of the space coordinate axes are called boosts. Substituting γ and γβ into the transformations as previously presented and rewriting in matrix form, the Lorentz boost in the x-direction may be written as In other words, Lorentz boosts represent hyperbolic rotations in Minkowski spacetime. The advantages of using hyperbolic functions are such that some textbooks such as the classic ones by Taylor and Wheeler introduce their use at a very early stage. Four‑vectors have been mentioned above in context of the energy–momentum 4‑vector, but without any great emphasis. Indeed, none of the elementary derivations of special relativity require"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_56",
    "chunk": "them. But once understood, 4‑vectors, and more generally tensors, greatly simplify the mathematics and conceptual understanding of special relativity. Working exclusively with such objects leads to formulas that are manifestly relativistically invariant, which is a considerable advantage in non-trivial contexts. For instance, demonstrating relativistic invariance of Maxwell's equations in their usual form is not trivial, while it is merely a routine calculation, really no more than an observation, using the field strength tensor formulation. On the other hand, general relativity, from the outset, relies heavily on 4‑vectors, and more generally tensors, representing physically relevant entities. Relating these via equations that do not rely on specific coordinates requires tensors, capable of connecting such 4‑vectors even within a curved spacetime, and not just within a flat one as in special relativity. The study of tensors is outside the scope of this article, which provides only a basic discussion of spacetime. A 4-tuple, ⁠ A = ( A 0 , A 1 , A 2 , A 3 ) {\\displaystyle A=\\left(A_{0},A_{1},A_{2},A_{3}\\right)} ⁠ is a \"4-vector\" if its component Ai transform between frames according to the Lorentz transformation. If using ⁠ ( c t , x , y , z ) {\\displaystyle (ct,x,y,z)} ⁠"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_57",
    "chunk": "coordinates, A is a 4–vector if it transforms (in the x-direction) according to which comes from simply replacing ct with A0 and x with A1 in the earlier presentation of the Lorentz transformation. The last three components of a 4–vector must be a standard vector in three-dimensional space. Therefore, a 4–vector must transform like ⁠ ( c Δ t , Δ x , Δ y , Δ z ) {\\displaystyle (c\\Delta t,\\Delta x,\\Delta y,\\Delta z)} ⁠ under Lorentz transformations as well as rotations. As expected, the final components of the above 4-vectors are all standard 3-vectors corresponding to spatial 3-momentum, 3-force etc. The first postulate of special relativity declares the equivalency of all inertial frames. A physical law holding in one frame must apply in all frames, since otherwise it would be possible to differentiate between frames. Newtonian momenta fail to behave properly under Lorentzian transformation, and Einstein preferred to change the definition of momentum to one involving 4-vectors rather than give up on conservation of momentum. Physical laws must be based on constructs that are frame independent. This means that physical laws may take the form of equations connecting scalars, which are always frame independent. However, equations involving 4-vectors"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_58",
    "chunk": "require the use of tensors with appropriate rank, which themselves can be thought of as being built up from 4-vectors. Special relativity does accommodate accelerations as well as accelerating frames of reference. It is a common misconception that special relativity is applicable only to inertial frames, and that it is unable to handle accelerating objects or accelerating reference frames. It is only when gravitation is significant that general relativity is required. Properly handling accelerating frames does require some care, however. The difference between special and general relativity is that (1) In special relativity, all velocities are relative, but acceleration is absolute. (2) In general relativity, all motion is relative, whether inertial, accelerating, or rotating. To accommodate this difference, general relativity uses curved spacetime. In this section, we analyze several scenarios involving accelerated reference frames. The Dewan–Beran–Bell spaceship paradox (Bell's spaceship paradox) is a good example of a problem where intuitive reasoning unassisted by the geometric insight of the spacetime approach can lead to issues. In Fig. 7-4, two identical spaceships float in space and are at rest relative to each other. They are connected by a string that is capable of only a limited amount of stretching before breaking. At"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_59",
    "chunk": "a given instant in our frame, the observer frame, both spaceships accelerate in the same direction along the line between them with the same constant proper acceleration. Will the string break? When the paradox was new and relatively unknown, even professional physicists had difficulty working out the solution. Two lines of reasoning lead to opposite conclusions. Both arguments, which are presented below, are flawed even though one of them yields the correct answer. The problem with the first argument is that there is no \"frame of the spaceships.\" There cannot be, because the two spaceships measure a growing distance between the two. Because there is no common frame of the spaceships, the length of the string is ill-defined. Nevertheless, the conclusion is correct, and the argument is mostly right. The second argument, however, completely ignores the relativity of simultaneity. A spacetime diagram (Fig. 7-5) makes the correct solution to this paradox almost immediately evident. Two observers in Minkowski spacetime accelerate with constant magnitude k {\\displaystyle k} acceleration for proper time σ {\\displaystyle \\sigma } (acceleration and elapsed time measured by the observers themselves, not some inertial observer). They are comoving and inertial before and after this phase. In Minkowski geometry,"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_60",
    "chunk": "the length along the line of simultaneity A ′ B ″ {\\displaystyle A'B''} turns out to be greater than the length along the line of simultaneity ⁠ A B {\\displaystyle AB} ⁠. The length increase can be calculated with the help of the Lorentz transformation. If, as illustrated in Fig. 7-5, the acceleration is finished, the ships will remain at a constant offset in some frame ⁠ S ′ {\\displaystyle S'} ⁠. If x A {\\displaystyle x_{A}} and x B = x A + L {\\displaystyle x_{B}=x_{A}+L} are the ships' positions in ⁠ S {\\displaystyle S} ⁠, the positions in frame S ′ {\\displaystyle S'} are: The \"paradox\", as it were, comes from the way that Bell constructed his example. In the usual discussion of Lorentz contraction, the rest length is fixed and the moving length shortens as measured in frame ⁠ S {\\displaystyle S} ⁠. As shown in Fig. 7-5, Bell's example asserts the moving lengths A B {\\displaystyle AB} and A ′ B ′ {\\displaystyle A'B'} measured in frame S {\\displaystyle S} to be fixed, thereby forcing the rest frame length A ′ B ″ {\\displaystyle A'B''} in frame S ′ {\\displaystyle S'} to increase. Certain special relativity"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_61",
    "chunk": "problem setups can lead to insight about phenomena normally associated with general relativity, such as event horizons. In the text accompanying Section \"Invariant hyperbola\" of the article Spacetime, the magenta hyperbolae represented actual paths that are tracked by a constantly accelerating traveler in spacetime. During periods of positive acceleration, the traveler's velocity just approaches the speed of light, while, measured in our frame, the traveler's acceleration constantly decreases. Fig. 7-6 details various features of the traveler's motions with more specificity. At any given moment, her space axis is formed by a line passing through the origin and her current position on the hyperbola, while her time axis is the tangent to the hyperbola at her position. The velocity parameter β {\\displaystyle \\beta } approaches a limit of one as c t {\\displaystyle ct} increases. Likewise, γ {\\displaystyle \\gamma } approaches infinity. The shape of the invariant hyperbola corresponds to a path of constant proper acceleration. This is demonstrable as follows: Fig. 7-6 illustrates a specific calculated scenario. Terence (A) and Stella (B) initially stand together 100 light hours from the origin. Stella lifts off at time 0, her spacecraft accelerating at 0.01 c per hour. Every twenty hours, Terence radios"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_62",
    "chunk": "updates to Stella about the situation at home (solid green lines). Stella receives these regular transmissions, but the increasing distance (offset in part by time dilation) causes her to receive Terence's communications later and later as measured on her clock, and she never receives any communications from Terence after 100 hours on his clock (dashed green lines). After 100 hours according to Terence's clock, Stella enters a dark region. She has traveled outside Terence's timelike future. On the other hand, Terence can continue to receive Stella's messages to him indefinitely. He just has to wait long enough. Spacetime has been divided into distinct regions separated by an apparent event horizon. So long as Stella continues to accelerate, she can never know what takes place behind this horizon. Theoretical investigation in classical electromagnetism led to the discovery of wave propagation. Equations generalizing the electromagnetic effects found that finite propagation speed of the E and B fields required certain behaviors on charged particles. The general study of moving charges forms the Liénard–Wiechert potential, which is a step towards special relativity. The Lorentz transformation of the electric field of a moving charge into a non-moving observer's reference frame results in the appearance of"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_63",
    "chunk": "a mathematical term commonly called the magnetic field. Conversely, the magnetic field generated by a moving charge disappears and becomes a purely electrostatic field in a comoving frame of reference. Maxwell's equations are thus simply an empirical fit to special relativistic effects in a classical model of the Universe. As electric and magnetic fields are reference frame dependent and thus intertwined, one speaks of electromagnetic fields. Special relativity provides the transformation rules for how an electromagnetic field in one inertial frame appears in another inertial frame. Maxwell's equations in the 3D form are already consistent with the physical content of special relativity, although they are easier to manipulate in a manifestly covariant form, that is, in the language of tensor calculus. Special relativity can be combined with quantum mechanics to form relativistic quantum mechanics and quantum electrodynamics. How general relativity and quantum mechanics can be unified is one of the unsolved problems in physics; quantum gravity and a \"theory of everything\", which require a unification including general relativity too, are active and ongoing areas in theoretical research. The early Bohr–Sommerfeld atomic model explained the fine structure of alkali metal atoms using both special relativity and the preliminary knowledge on quantum"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_64",
    "chunk": "mechanics of the time. In 1928, Paul Dirac constructed an influential relativistic wave equation, now known as the Dirac equation in his honour, that is fully compatible both with special relativity and with the final version of quantum theory existing after 1926. This equation not only described the intrinsic angular momentum of the electrons called spin, it also led to the prediction of the antiparticle of the electron (the positron), and fine structure could only be fully explained with special relativity. It was the first foundation of relativistic quantum mechanics. On the other hand, the existence of antiparticles leads to the conclusion that relativistic quantum mechanics is not enough for a more accurate and complete theory of particle interactions. Instead, a theory of particles interpreted as quantized fields, called quantum field theory, becomes necessary; in which particles can be created and destroyed throughout space and time. Special relativity in its Minkowski spacetime is accurate only when the absolute value of the gravitational potential is much less than c in the region of interest. In a strong gravitational field, one must use general relativity. General relativity becomes special relativity at the limit of a weak field. At very small scales, such"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_65",
    "chunk": "as at the Planck length and below, quantum effects must be taken into consideration resulting in quantum gravity. But at macroscopic scales and in the absence of strong gravitational fields, special relativity is experimentally tested to extremely high degree of accuracy (10) and thus accepted by the physics community. Experimental results that appear to contradict it are not reproducible and are thus widely believed to be due to experimental errors. Special relativity is mathematically self-consistent, and it is an organic part of all modern physical theories, most notably quantum field theory, string theory, and general relativity (in the limiting case of negligible gravitational fields). Newtonian mechanics mathematically follows from special relativity at small velocities (compared to the speed of light) – thus Newtonian mechanics can be considered as a special relativity of slow moving bodies. See Classical mechanics for a more detailed discussion. Several experiments predating Einstein's 1905 paper are now interpreted as evidence for relativity. Of these it is known Einstein was aware of the Fizeau experiment before 1905, and historians have concluded that Einstein was at least aware of the Michelson–Morley experiment as early as 1899 despite claims he made in his later years that it played no"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_66",
    "chunk": "role in his development of the theory. Particle accelerators accelerate and measure the properties of particles moving at near the speed of light, where their behavior is consistent with relativity theory and inconsistent with the earlier Newtonian mechanics. These machines would simply not work if they were not engineered according to relativistic principles. In addition, a considerable number of modern experiments have been conducted to test special relativity. Some examples: Special relativity uses a \"flat\" 4-dimensional Minkowski space – an example of a spacetime. Minkowski spacetime appears to be very similar to the standard 3-dimensional Euclidean space, but there is a crucial difference with respect to time. In 3D space, the differential of distance (line element) ds is defined by d s 2 = d x ⋅ d x = d x 1 2 + d x 2 2 + d x 3 2 , {\\displaystyle ds^{2}=d\\mathbf {x} \\cdot d\\mathbf {x} =dx_{1}^{2}+dx_{2}^{2}+dx_{3}^{2},} where dx = (dx1, dx2, dx3) are the differentials of the three spatial dimensions. In Minkowski geometry, there is an extra dimension with coordinate X derived from time, such that the distance differential fulfills d s 2 = − d X 0 2 + d X 1 2"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_67",
    "chunk": "+ d X 2 2 + d X 3 2 , {\\displaystyle ds^{2}=-dX_{0}^{2}+dX_{1}^{2}+dX_{2}^{2}+dX_{3}^{2},} where dX = (dX0, dX1, dX2, dX3) are the differentials of the four spacetime dimensions. This suggests a deep theoretical insight: special relativity is simply a rotational symmetry of our spacetime, analogous to the rotational symmetry of Euclidean space (see Fig. 10-1). Just as Euclidean space uses a Euclidean metric, so spacetime uses a Minkowski metric. Basically, special relativity can be stated as the invariance of any spacetime interval (that is the 4D distance between any two events) when viewed from any inertial reference frame. All equations and effects of special relativity can be derived from this rotational symmetry (the Poincaré group) of Minkowski spacetime. The actual form of ds above depends on the metric and on the choices for the X coordinate. To make the time coordinate look like the space coordinates, it can be treated as imaginary: X0 = ict (this is called a Wick rotation). According to Misner, Thorne and Wheeler (1971, §2.3), ultimately the deeper understanding of both special and general relativity will come from the study of the Minkowski metric (described below) and to take X = ct, rather than a \"disguised\""
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_68",
    "chunk": "Euclidean metric using ict as the time coordinate. Some authors use X = t, with factors of c elsewhere to compensate; for instance, spatial coordinates are divided by c or factors of c are included in the metric tensor. These numerous conventions can be superseded by using natural units where c = 1. Then space and time have equivalent units, and no factors of c appear anywhere. If we reduce the spatial dimensions to 2, so that we can represent the physics in a 3D space d s 2 = d x 1 2 + d x 2 2 − c 2 d t 2 , {\\displaystyle ds^{2}=dx_{1}^{2}+dx_{2}^{2}-c^{2}dt^{2},} we see that the null geodesics lie along a dual-cone (see Fig. 10-2) defined by the equation; d s 2 = 0 = d x 1 2 + d x 2 2 − c 2 d t 2 {\\displaystyle ds^{2}=0=dx_{1}^{2}+dx_{2}^{2}-c^{2}dt^{2}} or simply d x 1 2 + d x 2 2 = c 2 d t 2 , {\\displaystyle dx_{1}^{2}+dx_{2}^{2}=c^{2}dt^{2},} which is the equation of a circle of radius c dt. If we extend this to three spatial dimensions, the null geodesics are the 4-dimensional cone: d s 2 = 0 ="
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_69",
    "chunk": "d x 1 2 + d x 2 2 + d x 3 2 − c 2 d t 2 {\\displaystyle ds^{2}=0=dx_{1}^{2}+dx_{2}^{2}+dx_{3}^{2}-c^{2}dt^{2}} so d x 1 2 + d x 2 2 + d x 3 2 = c 2 d t 2 . {\\displaystyle dx_{1}^{2}+dx_{2}^{2}+dx_{3}^{2}=c^{2}dt^{2}.} As illustrated in Fig. 10-3, the null geodesics can be visualized as a set of continuous concentric spheres with radii = c dt. This null dual-cone represents the \"line of sight\" of a point in space. That is, when we look at the stars and say \"The light from that star that I am receiving is X years old\", we are looking down this line of sight: a null geodesic. We are looking at an event a distance d = x 1 2 + x 2 2 + x 3 2 {\\textstyle d={\\sqrt {x_{1}^{2}+x_{2}^{2}+x_{3}^{2}}}} away and a time d/c in the past. For this reason the null dual cone is also known as the \"light cone\". (The point in the lower left of the Fig. 10-2 represents the star, the origin represents the observer, and the line represents the null geodesic \"line of sight\".) The cone in the −t region is the information that"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_70",
    "chunk": "the point is \"receiving\", while the cone in the +t section is the information that the point is \"sending\". The geometry of Minkowski space can be depicted using Minkowski diagrams, which are useful also in understanding many of the thought experiments in special relativity. Above, the Lorentz transformation for the time coordinate and three space coordinates illustrates that they are intertwined. This is true more generally: certain pairs of \"timelike\" and \"spacelike\" quantities naturally combine on equal footing under the same Lorentz transformation. The Lorentz transformation in standard configuration above, that is, for a boost in the x-direction, can be recast into matrix form as follows: ( c t ′ x ′ y ′ z ′ ) = ( γ − β γ 0 0 − β γ γ 0 0 0 0 1 0 0 0 0 1 ) ( c t x y z ) = ( γ c t − γ β x γ x − β γ c t y z ) . {\\displaystyle {\\begin{pmatrix}ct'\\\\x'\\\\y'\\\\z'\\end{pmatrix}}={\\begin{pmatrix}\\gamma &-\\beta \\gamma &0&0\\\\-\\beta \\gamma &\\gamma &0&0\\\\0&0&1&0\\\\0&0&0&1\\end{pmatrix}}{\\begin{pmatrix}ct\\\\x\\\\y\\\\z\\end{pmatrix}}={\\begin{pmatrix}\\gamma ct-\\gamma \\beta x\\\\\\gamma x-\\beta \\gamma ct\\\\y\\\\z\\end{pmatrix}}.} In Newtonian mechanics, quantities that have magnitude and direction are mathematically described as 3d vectors in Euclidean space, and"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_71",
    "chunk": "in general they are parametrized by time. In special relativity, this notion is extended by adding the appropriate timelike quantity to a spacelike vector quantity, and we have 4d vectors, or \"four-vectors\", in Minkowski spacetime. The components of vectors are written using tensor index notation, as this has numerous advantages. The notation makes it clear the equations are manifestly covariant under the Poincaré group, thus bypassing the tedious calculations to check this fact. In constructing such equations, we often find that equations previously thought to be unrelated are, in fact, closely connected being part of the same tensor equation. Recognizing other physical quantities as tensors simplifies their transformation laws. Throughout, upper indices (superscripts) are contravariant indices rather than exponents except when they indicate a square (this should be clear from the context), and lower indices (subscripts) are covariant indices. For simplicity and consistency with the earlier equations, Cartesian coordinates will be used. The simplest example of a four-vector is the position of an event in spacetime, which constitutes a timelike component ct and spacelike component x = (x, y, z), in a contravariant position four-vector with components: X ν = ( X 0 , X 1 , X 2 ,"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_72",
    "chunk": "X 3 ) = ( c t , x , y , z ) = ( c t , x ) . {\\displaystyle X^{\\nu }=(X^{0},X^{1},X^{2},X^{3})=(ct,x,y,z)=(ct,\\mathbf {x} ).} where we define X = ct so that the time coordinate has the same dimension of distance as the other spatial dimensions; so that space and time are treated equally. Now the transformation of the contravariant components of the position 4-vector can be compactly written as: X μ ′ = Λ μ ′ ν X ν {\\displaystyle X^{\\mu '}=\\Lambda ^{\\mu '}{}_{\\nu }X^{\\nu }} where there is an implied summation on ν {\\displaystyle \\nu } from 0 to 3, and Λ μ ′ ν {\\displaystyle \\Lambda ^{\\mu '}{}_{\\nu }} is a matrix. More generally, all contravariant components of a four-vector T ν {\\displaystyle T^{\\nu }} transform from one frame to another frame by a Lorentz transformation: T μ ′ = Λ μ ′ ν T ν {\\displaystyle T^{\\mu '}=\\Lambda ^{\\mu '}{}_{\\nu }T^{\\nu }} Examples of other 4-vectors include the four-velocity ⁠ U μ {\\displaystyle U^{\\mu }} ⁠, defined as the derivative of the position 4-vector with respect to proper time: U μ = d X μ d τ = γ ( v ) ("
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_73",
    "chunk": "c , v x , v y , v z ) = γ ( v ) ( c , v ) . {\\displaystyle U^{\\mu }={\\frac {dX^{\\mu }}{d\\tau }}=\\gamma (v)(c,v_{x},v_{y},v_{z})=\\gamma (v)(c,\\mathbf {v} ).} where the Lorentz factor is: γ ( v ) = 1 1 − v 2 / c 2 v 2 = v x 2 + v y 2 + v z 2 . {\\displaystyle \\gamma (v)={\\frac {1}{\\sqrt {1-v^{2}/c^{2}}}}\\qquad v^{2}=v_{x}^{2}+v_{y}^{2}+v_{z}^{2}.} The relativistic energy E = γ ( v ) m c 2 {\\displaystyle E=\\gamma (v)mc^{2}} and relativistic momentum p = γ ( v ) m v {\\displaystyle \\mathbf {p} =\\gamma (v)m\\mathbf {v} } of an object are respectively the timelike and spacelike components of a contravariant four-momentum vector: P μ = m U μ = m γ ( v ) ( c , v x , v y , v z ) = ( E c , p x , p y , p z ) = ( E c , p ) . {\\displaystyle P^{\\mu }=mU^{\\mu }=m\\gamma (v)(c,v_{x},v_{y},v_{z})=\\left({\\frac {E}{c}},p_{x},p_{y},p_{z}\\right)=\\left({\\frac {E}{c}},\\mathbf {p} \\right).} where m is the invariant mass. The four-acceleration is the proper time derivative of 4-velocity: A μ = d U μ d τ . {\\displaystyle A^{\\mu"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_74",
    "chunk": "}={\\frac {dU^{\\mu }}{d\\tau }}.} The transformation rules for three-dimensional velocities and accelerations are very awkward; even above in standard configuration the velocity equations are quite complicated owing to their non-linearity. On the other hand, the transformation of four-velocity and four-acceleration are simpler by means of the Lorentz transformation matrix. The four-gradient of a scalar field φ transforms covariantly rather than contravariantly: ( 1 c ∂ ϕ ∂ t ′ ∂ ϕ ∂ x ′ ∂ ϕ ∂ y ′ ∂ ϕ ∂ z ′ ) = ( 1 c ∂ ϕ ∂ t ∂ ϕ ∂ x ∂ ϕ ∂ y ∂ ϕ ∂ z ) ( γ + β γ 0 0 + β γ γ 0 0 0 0 1 0 0 0 0 1 ) , {\\displaystyle {\\begin{pmatrix}{\\dfrac {1}{c}}{\\dfrac {\\partial \\phi }{\\partial t'}}&{\\dfrac {\\partial \\phi }{\\partial x'}}&{\\dfrac {\\partial \\phi }{\\partial y'}}&{\\dfrac {\\partial \\phi }{\\partial z'}}\\end{pmatrix}}={\\begin{pmatrix}{\\dfrac {1}{c}}{\\dfrac {\\partial \\phi }{\\partial t}}&{\\dfrac {\\partial \\phi }{\\partial x}}&{\\dfrac {\\partial \\phi }{\\partial y}}&{\\dfrac {\\partial \\phi }{\\partial z}}\\end{pmatrix}}{\\begin{pmatrix}\\gamma &+\\beta \\gamma &0&0\\\\+\\beta \\gamma &\\gamma &0&0\\\\0&0&1&0\\\\0&0&0&1\\end{pmatrix}},} which is the transpose of: ( ∂ μ ′ ϕ ) = Λ μ ′ ν ( ∂ ν ϕ ) ∂ μ ≡ ∂ ∂ x μ ."
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_75",
    "chunk": "{\\displaystyle (\\partial _{\\mu '}\\phi )=\\Lambda _{\\mu '}{}^{\\nu }(\\partial _{\\nu }\\phi )\\qquad \\partial _{\\mu }\\equiv {\\frac {\\partial }{\\partial x^{\\mu }}}.} only in Cartesian coordinates. It is the covariant derivative that transforms in manifest covariance, in Cartesian coordinates this happens to reduce to the partial derivatives, but not in other coordinates. More generally, the covariant components of a 4-vector transform according to the inverse Lorentz transformation: T μ ′ = Λ μ ′ ν T ν , {\\displaystyle T_{\\mu '}=\\Lambda _{\\mu '}{}^{\\nu }T_{\\nu },} where Λ μ ′ ν {\\displaystyle \\Lambda _{\\mu '}{}^{\\nu }} is the reciprocal matrix of ⁠ Λ μ ′ ν {\\displaystyle \\Lambda ^{\\mu '}{}_{\\nu }} ⁠. The postulates of special relativity constrain the exact form the Lorentz transformation matrices take. More generally, most physical quantities are best described as (components of) tensors. So to transform from one frame to another, we use the well-known tensor transformation law T θ ′ ι ′ ⋯ κ ′ α ′ β ′ ⋯ ζ ′ = Λ α ′ μ Λ β ′ ν ⋯ Λ ζ ′ ρ Λ θ ′ σ Λ ι ′ υ ⋯ Λ κ ′ ϕ T σ υ ⋯ ϕ μ ν ⋯ ρ"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_76",
    "chunk": "{\\displaystyle T_{\\theta '\\iota '\\cdots \\kappa '}^{\\alpha '\\beta '\\cdots \\zeta '}=\\Lambda ^{\\alpha '}{}_{\\mu }\\Lambda ^{\\beta '}{}_{\\nu }\\cdots \\Lambda ^{\\zeta '}{}_{\\rho }\\Lambda _{\\theta '}{}^{\\sigma }\\Lambda _{\\iota '}{}^{\\upsilon }\\cdots \\Lambda _{\\kappa '}{}^{\\phi }T_{\\sigma \\upsilon \\cdots \\phi }^{\\mu \\nu \\cdots \\rho }} where Λ χ ′ ψ {\\displaystyle \\Lambda _{\\chi '}{}^{\\psi }} is the reciprocal matrix of ⁠ Λ χ ′ ψ {\\displaystyle \\Lambda ^{\\chi '}{}_{\\psi }} ⁠. All tensors transform by this rule. An example of a four-dimensional second order antisymmetric tensor is the relativistic angular momentum, which has six components: three are the classical angular momentum, and the other three are related to the boost of the center of mass of the system. The derivative of the relativistic angular momentum with respect to proper time is the relativistic torque, also second order antisymmetric tensor. The electromagnetic field tensor is another second order antisymmetric tensor field, with six components: three for the electric field and another three for the magnetic field. There is also the stress–energy tensor for the electromagnetic field, namely the electromagnetic stress–energy tensor. The metric tensor allows one to define the inner product of two vectors, which in turn allows one to assign a magnitude to the vector. Given the"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_77",
    "chunk": "four-dimensional nature of spacetime the Minkowski metric η has components (valid with suitably chosen coordinates), which can be arranged in a 4 × 4 matrix: η α β = ( − 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 ) , {\\displaystyle \\eta _{\\alpha \\beta }={\\begin{pmatrix}-1&0&0&0\\\\0&1&0&0\\\\0&0&1&0\\\\0&0&0&1\\end{pmatrix}},} which is equal to its reciprocal, ⁠ η α β {\\displaystyle \\eta ^{\\alpha \\beta }} ⁠, in those frames. Throughout we use the signs as above, different authors use different conventions – see Minkowski metric alternative signs. The Poincaré group is the most general group of transformations that preserves the Minkowski metric: η α β = η μ ′ ν ′ Λ μ ′ α Λ ν ′ β {\\displaystyle \\eta _{\\alpha \\beta }=\\eta _{\\mu '\\nu '}\\Lambda ^{\\mu '}{}_{\\alpha }\\Lambda ^{\\nu '}{}_{\\beta }} and this is the physical symmetry underlying special relativity. The metric can be used for raising and lowering indices on vectors and tensors. Invariants can be constructed using the metric, the inner product of a 4-vector T with another 4-vector S is: T α S α = T α η α β S β = T α η α β S β ="
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_78",
    "chunk": "invariant scalar {\\displaystyle T^{\\alpha }S_{\\alpha }=T^{\\alpha }\\eta _{\\alpha \\beta }S^{\\beta }=T_{\\alpha }\\eta ^{\\alpha \\beta }S_{\\beta }={\\text{invariant scalar}}} Invariant means that it takes the same value in all inertial frames, because it is a scalar (0 rank tensor), and so no Λ appears in its trivial transformation. The magnitude of the 4-vector T is the positive square root of the inner product with itself: | T | = T α T α {\\displaystyle |\\mathbf {T} |={\\sqrt {T^{\\alpha }T_{\\alpha }}}} One can extend this idea to tensors of higher order, for a second order tensor we can form the invariants: T α α , T α β T β α , T α β T β γ T γ α = invariant scalars , {\\displaystyle T^{\\alpha }{}_{\\alpha },T^{\\alpha }{}_{\\beta }T^{\\beta }{}_{\\alpha },T^{\\alpha }{}_{\\beta }T^{\\beta }{}_{\\gamma }T^{\\gamma }{}_{\\alpha }={\\text{invariant scalars}},} similarly for higher order tensors. Invariant expressions, particularly inner products of 4-vectors with themselves, provide equations that are useful for calculations, because one does not need to perform Lorentz transformations to determine the invariants. The coordinate differentials transform also contravariantly: d X μ ′ = Λ μ ′ ν d X ν {\\displaystyle dX^{\\mu '}=\\Lambda ^{\\mu '}{}_{\\nu }dX^{\\nu }} so the squared length"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_79",
    "chunk": "of the differential of the position four-vector dX constructed using d X 2 = d X μ d X μ = η μ ν d X μ d X ν = − ( c d t ) 2 + ( d x ) 2 + ( d y ) 2 + ( d z ) 2 {\\displaystyle d\\mathbf {X} ^{2}=dX^{\\mu }\\,dX_{\\mu }=\\eta _{\\mu \\nu }\\,dX^{\\mu }\\,dX^{\\nu }=-(c\\,dt)^{2}+(dx)^{2}+(dy)^{2}+(dz)^{2}} is an invariant. Notice that when the line element dX is negative that √−dX is the differential of proper time, while when dX is positive, √dX is differential of the proper distance. The 4-velocity U has an invariant form: U 2 = η ν μ U ν U μ = − c 2 , {\\displaystyle \\mathbf {U} ^{2}=\\eta _{\\nu \\mu }U^{\\nu }U^{\\mu }=-c^{2}\\,,} which means all velocity four-vectors have a magnitude of c. This is an expression of the fact that there is no such thing as being at coordinate rest in relativity: at the least, you are always moving forward through time. Differentiating the above equation by τ produces: 2 η μ ν A μ U ν = 0. {\\displaystyle 2\\eta _{\\mu \\nu }A^{\\mu }U^{\\nu }=0.} So in special relativity, the acceleration"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_80",
    "chunk": "four-vector and the velocity four-vector are orthogonal. The invariant magnitude of the momentum 4-vector generates the energy–momentum relation: P 2 = η μ ν P μ P ν = − ( E c ) 2 + p 2 . {\\displaystyle \\mathbf {P} ^{2}=\\eta ^{\\mu \\nu }P_{\\mu }P_{\\nu }=-\\left({\\frac {E}{c}}\\right)^{2}+p^{2}.} We can work out what this invariant is by first arguing that, since it is a scalar, it does not matter in which reference frame we calculate it, and then by transforming to a frame where the total momentum is zero. P 2 = − ( E rest c ) 2 = − ( m c ) 2 . {\\displaystyle \\mathbf {P} ^{2}=-\\left({\\frac {E_{\\text{rest}}}{c}}\\right)^{2}=-(mc)^{2}.} We see that the rest energy is an independent invariant. A rest energy can be calculated even for particles and systems in motion, by translating to a frame in which momentum is zero. The rest energy is related to the mass according to the celebrated equation discussed above: E rest = m c 2 . {\\displaystyle E_{\\text{rest}}=mc^{2}.} The mass of systems measured in their center of momentum frame (where total momentum is zero) is given by the total energy of the system in this frame. It may"
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_81",
    "chunk": "not be equal to the sum of individual system masses measured in other frames. To use Newton's third law of motion, both forces must be defined as the rate of change of momentum with respect to the same time coordinate. That is, it requires the 3D force defined above. Unfortunately, there is no tensor in 4D that contains the components of the 3D force vector among its components. If a particle is not traveling at c, one can transform the 3D force from the particle's co-moving reference frame into the observer's reference frame. This yields a 4-vector called the four-force. It is the rate of change of the above energy momentum four-vector with respect to proper time. The covariant version of the four-force is: F ν = d P ν d τ = m A ν {\\displaystyle F_{\\nu }={\\frac {dP_{\\nu }}{d\\tau }}=mA_{\\nu }} In the rest frame of the object, the time component of the four-force is zero unless the \"invariant mass\" of the object is changing (this requires a non-closed system in which energy/mass is being directly added or removed from the object) in which case it is the negative of that rate of change of mass, times c."
  },
  {
    "source": "Special relativity.txt",
    "chunk_id": "Special relativity.txt_82",
    "chunk": "In general, though, the components of the four-force are not equal to the components of the three-force, because the three force is defined by the rate of change of momentum with respect to coordinate time, that is, dp/dt while the four-force is defined by the rate of change of momentum with respect to proper time, that is, dp/dτ. In a continuous medium, the 3D density of force combines with the density of power to form a covariant 4-vector. The spatial part is the result of dividing the force on a small cell (in 3-space) by the volume of that cell. The time component is −1/c times the power transferred to that cell divided by the volume of the cell. This will be used below in the section on electromagnetism."
  },
  {
    "source": "Springer Science+Business Media.txt",
    "chunk_id": "Springer Science+Business Media.txt_0",
    "chunk": "# Springer Science+Business Media Springer Science+Business Media, commonly known as Springer, is a German multinational publishing company of books, e-books and peer-reviewed journals in science, humanities, technical and medical (STM) publishing. Originally founded in 1842 in Berlin, it expanded internationally in the 1960s, and through mergers in the 1990s and a sale to venture capitalists it fused with Wolters Kluwer and eventually became part of Springer Nature in 2015. Springer has major offices in Berlin, Heidelberg, Dordrecht, and New York City. Julius Springer founded Springer-Verlag in Berlin in 1842 and his son Ferdinand Springer grew it from a small firm of 4 employees into Germany's then second-largest academic publisher with 65 staff in 1872. In 1964, Springer expanded its business internationally, opening an office in New York City. Offices in Tokyo, Paris, Milan, Hong Kong, and Delhi soon followed. In 1999, the academic publishing company BertelsmannSpringer was formed after the media and entertainment company Bertelsmann bought a majority stake in Springer-Verlag. In 2003, the British investment groups Cinven and Candover bought BertelsmannSpringer from Bertelsmann. They merged the company in 2004 with the Dutch publisher Kluwer Academic Publishers (successor of D. Reidel, Dr. W. Junk, Plenum Publishers, most of Chapman &"
  },
  {
    "source": "Springer Science+Business Media.txt",
    "chunk_id": "Springer Science+Business Media.txt_1",
    "chunk": "Hall, and Baltzer Science Publishers) which they bought from Wolters Kluwer in 2002, to form Springer Science+Business Media. Springer acquired the open-access publisher BioMed Central in October 2008 for an undisclosed amount. In 2009, Cinven and Candover sold Springer to two private equity firms, EQT AB and Government of Singapore Investment Corporation, confirmed in February 2010 after the competition authorities in the US and in Europe approved the transfer. In 2011, Springer acquired Pharma Marketing and Publishing Services (MPS) from Wolters Kluwer. In 2013, the London-based private equity firm BC Partners acquired a majority stake in Springer from EQT and GIC for $4.4 billion. In January 2015, Holtzbrinck Publishing Group / Nature Publishing Group and Springer Science+Business Media announced a merger. in May 2015 they concluded the transaction and formed a new joint venture company, Springer Nature with Holtzbrinck in the majority 53% share and BC Partners retaining 47% interest in the company. In 1996, Springer launched electronic book and journal content on its SpringerLink site. SpringerImages was launched in 2008. In 2009, SpringerMaterials, a platform for accessing the Landolt-Börnstein database of research and information on materials and their properties, was launched. AuthorMapper is a free online tool for visualizing"
  },
  {
    "source": "Springer Science+Business Media.txt",
    "chunk_id": "Springer Science+Business Media.txt_2",
    "chunk": "scientific research that enables document discovery based on author locations and geographic maps, helping users explore patterns in scientific research, identify literature trends, discover collaborative relationships, and locate experts in several scientific/medical fields. Springer Protocols contained a collection of laboratory protocols, recipes that provide step-by-step instructions for conducting experiments, which in 2018 was made available in SpringerLink instead. Book publications include major reference works, textbooks, monographs and book series; more than 168,000 titles are available as e-books in 24 subject collections. Springer is a member of the Open Access Scholarly Publishers Association. For some of its journals, Springer does not require its authors to transfer their copyrights, and allows them to decide whether their articles are published under an open-access license or in the traditional restricted licence model. While open-access publishing typically requires the author to pay a fee for copyright retention, this fee is sometimes covered by a third party. For example, a national institution in Poland allows authors to publish in open-access journals without incurring any personal cost but using public funds. In 1938, Springer-Verlag was pressed to apply Nazi principles on the journal Zentralblatt MATH. Tullio Levi-Civita, who was Jewish, was forced out from the editorial board,"
  },
  {
    "source": "Springer Science+Business Media.txt",
    "chunk_id": "Springer Science+Business Media.txt_3",
    "chunk": "and Otto Neugebauer resigned in protest along with most of the rest of the board. In 2014, it was revealed that 16 papers in conference proceedings published by Springer had been computer-generated using SCIgen. Springer subsequently retracted all papers from these proceedings. IEEE had removed more than 100 fake papers from its conference proceedings. In 2015, Springer retracted 64 papers from 10 of its journals it had published after a fraudulent peer review process was uncovered. According to Goodhart's law and concerned academics like the signatories of the San Francisco Declaration on Research Assessment, commercial academic publishers benefit from manipulation of bibliometrics and scientometrics like the journal impact factor, which is often used as a proxy of prestige and can influence revenues, including public subsidies in the form of subscriptions and free work from academics. Seven Springer Nature journals, which exhibited unusual levels of self-citation, had their journal impact factor of 2019 suspended from Journal Citation Reports in 2020, a sanction which hit 34 journals in total."
  },
  {
    "source": "Star system.txt",
    "chunk_id": "Star system.txt_0",
    "chunk": "# Star system A star system or stellar system is a small number of stars that orbit each other, bound by gravitational attraction. A large group of stars bound by gravitation is generally called a star cluster or galaxy, although, broadly speaking, they are also star systems. Star systems are not to be confused with planetary systems, which include planets and similar bodies (such as comets). A star system of two stars is known as a binary star, binary star system or physical double star. Systems with four or more components are less likely to occur. Multiple-star systems are called triple, ternary, or trinary if they contain three stars; quadruple or quaternary if they contain four stars; quintuple or quintenary with five stars; sextuple or sextenary with six stars; septuple or septenary with seven stars; and octuple or octenary with eight stars. These systems are smaller than open star clusters, which have more complex dynamics and typically have from 100 to 1,000 stars. Binary and multiple star systems are also known as a physical multiple stars, to distinguish them from optical multiple stars, which merely look close together when viewed from Earth. Multiple stars may refer to either optical or"
  },
  {
    "source": "Star system.txt",
    "chunk_id": "Star system.txt_1",
    "chunk": "physical, but optical multiples do not form a star system. Triple stars that are not all gravitationally bound (and thus do not form a triple star system) might comprise a physical binary and an optical companion (such as Beta Cephei) or, in rare cases, a purely optical triple star (such as Gamma Serpentis). Research on binary and multiple stars estimates they make up about a third of the star systems in the Milky Way galaxy, with two-thirds of stars being single. Binary stars are the most common non-single stars. With multiple star systems, the number of known systems decreases exponentially with multiplicity. For example, in the 1999 revision of Tokovinin's catalog of physical multiple stars, 551 out of the 728 systems described are triple. However, because of suspected selection effects, the ability to interpret these statistics is very limited. There are various methods to detect star systems and distinguish them from optical binaries multiples. These include: In systems that satisfy the assumptions of the two-body problem – including having negligible tidal effects, perturbations (from the gravity of other bodies), and transfer of mass between stars – the two stars will trace out a stable elliptical orbit around the barycenter of"
  },
  {
    "source": "Star system.txt",
    "chunk_id": "Star system.txt_2",
    "chunk": "the system. Examples of binary systems are Sirius, Procyon and Cygnus X-1, the last of which probably consists of a star and a black hole. Most multiple-star systems are organized in what is called a hierarchical system: the stars in the system can be divided into two smaller groups, each of which traverses a larger orbit around the system's center of mass. Each of these smaller groups must also be hierarchical, which means that they must be divided into smaller subgroups which themselves are hierarchical, and so on. Each level of the hierarchy can be treated as a two-body problem by considering close pairs as if they were a single star. In these systems there is little interaction between the orbits and the stars' motion will continue to approximate stable Keplerian orbits around the system's center of mass. For example, stable trinary systems consist of two stars in a close binary system, with a third orbiting this pair at a distance much larger than that of the binary orbit. If the inner and outer orbits are comparable in size, the system may become dynamically unstable, leading to a star being ejected from the system. EZ Aquarii is an example of"
  },
  {
    "source": "Star system.txt",
    "chunk_id": "Star system.txt_3",
    "chunk": "a physical hierarchical triple system, which has an outer star orbiting an inner binary composed of two more red dwarf stars. Hierarchical arrangements can be organized by what Evans (1968) called mobile diagrams, which look similar to ornamental mobiles hung from the ceiling. Each level of the mobile illustrates the decomposition of the system into two or more systems with smaller size. Evans calls a diagram multiplex if there is a node with more than two children, i.e. if the decomposition of some subsystem involves two or more orbits with comparable size. Because multiplexes may be unstable, multiple stars are expected to be simplex, meaning that at each level there are exactly two children. Evans calls the number of levels in the diagram its hierarchy. Higher hierarchies are also possible. Most of these higher hierarchies either are stable or suffer from internal perturbations. Others consider complex multiple stars will in time theoretically disintegrate into less complex multiple stars, like more common observed triples or quadruples. Trapezia are usually very young, unstable systems. These are thought to form in stellar nurseries, and quickly fragment into stable multiple stars, which in the process may eject components as galactic high-velocity stars. They are"
  },
  {
    "source": "Star system.txt",
    "chunk_id": "Star system.txt_4",
    "chunk": "named after the multiple star system known as the Trapezium Cluster in the heart of the Orion Nebula. Such systems are not rare, and commonly appear close to or within bright nebulae. These stars have no standard hierarchical arrangements, but compete for stable orbits. This relationship is called interplay. Such stars eventually settle down to a close binary with a distant companion, with the other star(s) previously in the system ejected into interstellar space at high velocities. This dynamic may explain the runaway stars that might have been ejected during a collision of two binary star groups or a multiple system. This event is credited with ejecting AE Aurigae, Mu Columbae and 53 Arietis at above 200 km·s and has been traced to the Trapezium cluster in the Orion Nebula some two million years ago. The components of multiple stars can be specified by appending the suffixes A, B, C, etc., to the system's designation. Suffixes such as AB may be used to denote the pair consisting of A and B. The sequence of letters B, C, etc. may be assigned in order of separation from the component A. Components discovered close to an already known component may be assigned"
  },
  {
    "source": "Star system.txt",
    "chunk_id": "Star system.txt_5",
    "chunk": "suffixes such as Aa, Ba, and so forth. A. A. Tokovinin's Multiple Star Catalogue uses a system in which each subsystem in a mobile diagram is encoded by a sequence of digits. In the mobile diagram (d) above, for example, the widest system would be given the number 1, while the subsystem containing its primary component would be numbered 11 and the subsystem containing its secondary component would be numbered 12. Subsystems which would appear below this in the mobile diagram will be given numbers with three, four, or more digits. When describing a non-hierarchical system by this method, the same subsystem number will be used more than once; for example, a system with three visual components, A, B, and C, no two of which can be grouped into a subsystem, would have two subsystems numbered 1 denoting the two binaries AB and AC. In this case, if B and C were subsequently resolved into binaries, they would be given the subsystem numbers 12 and 13. The current nomenclature for double and multiple stars can cause confusion as binary stars discovered in different ways are given different designations (for example, discoverer designations for visual binary stars and variable star designations"
  },
  {
    "source": "Star system.txt",
    "chunk_id": "Star system.txt_6",
    "chunk": "for eclipsing binary stars), and, worse, component letters may be assigned differently by different authors, so that, for example, one person's A can be another's C. Discussion starting in 1999 resulted in four proposed schemes to address this problem: For a designation system, identifying the hierarchy within the system has the advantage that it makes identifying subsystems and computing their properties easier. However, it causes problems when new components are discovered at a level above or intermediate to the existing hierarchy. In this case, part of the hierarchy will shift inwards. Components which are found to be nonexistent, or are later reassigned to a different subsystem, also cause problems. During the 24th General Assembly of the International Astronomical Union in 2000, the WMC scheme was endorsed and it was resolved by Commissions 5, 8, 26, 42, and 45 that it should be expanded into a usable uniform designation scheme. A sample of a catalog using the WMC scheme, covering half an hour of right ascension, was later prepared. The issue was discussed again at the 25th General Assembly in 2003, and it was again resolved by commissions 5, 8, 26, 42, and 45, as well as the Working Group on"
  },
  {
    "source": "Star system.txt",
    "chunk_id": "Star system.txt_7",
    "chunk": "Interferometry, that the WMC scheme should be expanded and further developed. The sample WMC is hierarchically organized; the hierarchy used is based on observed orbital periods or separations. Since it contains many visual double stars, which may be optical rather than physical, this hierarchy may be only apparent. It uses upper-case letters (A, B, ...) for the first level of the hierarchy, lower-case letters (a, b, ...) for the second level, and numbers (1, 2, ...) for the third. Subsequent levels would use alternating lower-case letters and numbers, but no examples of this were found in the sample."
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_0",
    "chunk": "# Star A star is a luminous spheroid of plasma held together by self-gravity. The nearest star to Earth is the Sun. Many other stars are visible to the naked eye at night; their immense distances from Earth make them appear as fixed points of light. The most prominent stars have been categorised into constellations and asterisms, and many of the brightest stars have proper names. Astronomers have assembled star catalogues that identify the known stars and provide standardized stellar designations. The observable universe contains an estimated 10 to 10 stars. Only about 4,000 of these stars are visible to the naked eye—all within the Milky Way galaxy. A star's life begins with the gravitational collapse of a gaseous nebula of material largely comprising hydrogen, helium, and trace heavier elements. Its total mass mainly determines its evolution and eventual fate. A star shines for most of its active life due to the thermonuclear fusion of hydrogen into helium in its core. This process releases energy that traverses the star's interior and radiates into outer space. At the end of a star's lifetime, fusion ceases and its core becomes a stellar remnant: a white dwarf, a neutron star, or—if it is"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_1",
    "chunk": "sufficiently massive—a black hole. Stellar nucleosynthesis in stars or their remnants creates almost all naturally occurring chemical elements heavier than lithium. Stellar mass loss or supernova explosions return chemically enriched material to the interstellar medium. These elements are then recycled into new stars. Astronomers can determine stellar properties—including mass, age, metallicity (chemical composition), variability, distance, and motion through space—by carrying out observations of a star's apparent brightness, spectrum, and changes in its position in the sky over time. Stars can form orbital systems with other astronomical objects, as in planetary systems and star systems with two or more stars. When two such stars orbit closely, their gravitational interaction can significantly impact their evolution. Stars can form part of a much larger gravitationally bound structure, such as a star cluster or a galaxy. The word \"star\" ultimately derives from the Proto-Indo-European root \"h₂stḗr\" also meaning star, but further analyzable as h₂eh₁s- (\"to burn\", also the source of the word \"ash\") + -tēr (agentive suffix). Compare Latin stella, Greek aster, German Stern. Some scholars believe the word is a borrowing from Akkadian \"istar\" (Venus). \"Star\" is cognate (shares the same root) with the following words: asterisk, asteroid, astral, constellation, Esther. Historically, stars"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_2",
    "chunk": "have been important to civilizations throughout the world. They have been part of religious practices, divination rituals, mythology, used for celestial navigation and orientation, to mark the passage of seasons, and to define calendars. Early astronomers recognized a difference between \"fixed stars\", whose position on the celestial sphere does not change, and \"wandering stars\" (planets), which move noticeably relative to the fixed stars over days or weeks. Many ancient astronomers believed that the stars were permanently affixed to a heavenly sphere and that they were immutable. By convention, astronomers grouped prominent stars into asterisms and constellations and used them to track the motions of the planets and the inferred position of the Sun. The motion of the Sun against the background stars (and the horizon) was used to create calendars, which could be used to regulate agricultural practices. The Gregorian calendar, currently used nearly everywhere in the world, is a solar calendar based on the angle of the Earth's rotational axis relative to its local star, the Sun. The oldest accurately dated star chart was the result of ancient Egyptian astronomy in 1534 BC. The earliest known star catalogues were compiled by the ancient Babylonian astronomers of Mesopotamia in the"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_3",
    "chunk": "late 2nd millennium BC, during the Kassite Period (c. 1531 BC – c. 1155 BC). The first star catalogue in Greek astronomy was created by Aristillus in approximately 300 BC, with the help of Timocharis. The star catalog of Hipparchus (2nd century BC) included 1,020 stars, and was used to assemble Ptolemy's star catalogue. Hipparchus is known for the discovery of the first recorded nova (new star). Many of the constellations and star names in use today derive from Greek astronomy. Despite the apparent immutability of the heavens, Chinese astronomers were aware that new stars could appear. In 185 AD, they were the first to observe and write about a supernova, now known as SN 185. The brightest stellar event in recorded history was the SN 1006 supernova, which was observed in 1006 and written about by the Egyptian astronomer Ali ibn Ridwan and several Chinese astronomers. The SN 1054 supernova, which gave birth to the Crab Nebula, was also observed by Chinese and Islamic astronomers. Medieval Islamic astronomers gave Arabic names to many stars that are still used today and they invented numerous astronomical instruments that could compute the positions of the stars. They built the first large observatory"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_4",
    "chunk": "research institutes, mainly to produce Zij star catalogues. Among these, the Book of Fixed Stars (964) was written by the Persian astronomer Abd al-Rahman al-Sufi, who observed a number of stars, star clusters (including the Omicron Velorum and Brocchi's Clusters) and galaxies (including the Andromeda Galaxy). According to A. Zahoor, in the 11th century, the Persian polymath scholar Abu Rayhan Biruni described the Milky Way galaxy as a multitude of fragments having the properties of nebulous stars, and gave the latitudes of various stars during a lunar eclipse in 1019. According to Josep Puig, the Andalusian astronomer Ibn Bajjah proposed that the Milky Way was made up of many stars that almost touched one another and appeared to be a continuous image due to the effect of refraction from sublunary material, citing his observation of the conjunction of Jupiter and Mars on 500 AH (1106/1107 AD) as evidence. Early European astronomers such as Tycho Brahe identified new stars in the night sky (later termed novae), suggesting that the heavens were not immutable. In 1584, Giordano Bruno suggested that the stars were like the Sun, and may have other planets, possibly even Earth-like, in orbit around them, an idea that had"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_5",
    "chunk": "been suggested earlier by the ancient Greek philosophers, Democritus and Epicurus, and by medieval Islamic cosmologists such as Fakhr al-Din al-Razi. By the following century, the idea of the stars being the same as the Sun was reaching a consensus among astronomers. To explain why these stars exerted no net gravitational pull on the Solar System, Isaac Newton suggested that the stars were equally distributed in every direction, an idea prompted by the theologian Richard Bentley. The Italian astronomer Geminiano Montanari recorded observing variations in luminosity of the star Algol in 1667. Edmond Halley published the first measurements of the proper motion of a pair of nearby \"fixed\" stars, demonstrating that they had changed positions since the time of the ancient Greek astronomers Ptolemy and Hipparchus. William Herschel was the first astronomer to attempt to determine the distribution of stars in the sky. During the 1780s, he established a series of gauges in 600 directions and counted the stars observed along each line of sight. From this, he deduced that the number of stars steadily increased toward one side of the sky, in the direction of the Milky Way core. His son John Herschel repeated this study in the southern"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_6",
    "chunk": "hemisphere and found a corresponding increase in the same direction. In addition to his other accomplishments, William Herschel is noted for his discovery that some stars do not merely lie along the same line of sight, but are physical companions that form binary star systems. The science of stellar spectroscopy was pioneered by Joseph von Fraunhofer and Angelo Secchi. By comparing the spectra of stars such as Sirius to the Sun, they found differences in the strength and number of their absorption lines—the dark lines in stellar spectra caused by the atmosphere's absorption of specific frequencies. In 1865, Secchi began classifying stars into spectral types. The modern version of the stellar classification scheme was developed by Annie J. Cannon during the early 1900s. The first direct measurement of the distance to a star (61 Cygni at 11.4 light-years) was made in 1838 by Friedrich Bessel using the parallax technique. Parallax measurements demonstrated the vast separation of the stars in the heavens. Observation of double stars gained increasing importance during the 19th century. In 1834, Friedrich Bessel observed changes in the proper motion of the star Sirius and inferred a hidden companion. Edward Pickering discovered the first spectroscopic binary in 1899"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_7",
    "chunk": "when he observed the periodic splitting of the spectral lines of the star Mizar in a 104-day period. Detailed observations of many binary star systems were collected by astronomers such as Friedrich Georg Wilhelm von Struve and S. W. Burnham, allowing the masses of stars to be determined from computation of orbital elements. The first solution to the problem of deriving an orbit of binary stars from telescope observations was made by Felix Savary in 1827. The twentieth century saw increasingly rapid advances in the scientific study of stars. The photograph became a valuable astronomical tool. Karl Schwarzschild discovered that the color of a star and, hence, its temperature, could be determined by comparing the visual magnitude against the photographic magnitude. The development of the photoelectric photometer allowed precise measurements of magnitude at multiple wavelength intervals. In 1921 Albert A. Michelson made the first measurements of a stellar diameter using an interferometer on the Hooker telescope at Mount Wilson Observatory. Important theoretical work on the physical structure of stars occurred during the first decades of the twentieth century. In 1913, the Hertzsprung-Russell diagram was developed, propelling the astrophysical study of stars. Successful models were developed to explain the interiors of"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_8",
    "chunk": "stars and stellar evolution. Cecilia Payne-Gaposchkin first proposed that stars were made primarily of hydrogen and helium in her 1925 PhD thesis. The spectra of stars were further understood through advances in quantum physics. This allowed the chemical composition of the stellar atmosphere to be determined. With the exception of rare events such as supernovae and supernova impostors, individual stars have primarily been observed in the Local Group, and especially in the visible part of the Milky Way (as demonstrated by the detailed star catalogues available for the Milky Way galaxy) and its satellites. Individual stars such as Cepheid variables have been observed in the M87 and M100 galaxies of the Virgo Cluster, as well as luminous stars in some other relatively nearby galaxies. With the aid of gravitational lensing, a single star (named Icarus) has been observed at 9 billion light-years away. The concept of a constellation was known to exist during the Babylonian period. Ancient sky watchers imagined that prominent arrangements of stars formed patterns, and they associated these with particular aspects of nature or their myths. Twelve of these formations lay along the band of the ecliptic and these became the basis of astrology. Many of the"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_9",
    "chunk": "more prominent individual stars were given names, particularly with Arabic or Latin designations. As well as certain constellations and the Sun itself, individual stars have their own myths. To the Ancient Greeks, some \"stars\", known as planets (Greek πλανήτης (planētēs), meaning \"wanderer\"), represented various important deities, from which the names of the planets Mercury, Venus, Mars, Jupiter and Saturn were taken. (Uranus and Neptune were Greek and Roman gods, but neither planet was known in Antiquity because of their low brightness. Their names were assigned by later astronomers.) Circa 1600, the names of the constellations were used to name the stars in the corresponding regions of the sky. The German astronomer Johann Bayer created a series of star maps and applied Greek letters as designations to the stars in each constellation. Later a numbering system based on the star's right ascension was invented and added to John Flamsteed's star catalogue in his book \"Historia coelestis Britannica\" (the 1712 edition), whereby this numbering system came to be called Flamsteed designation or Flamsteed numbering. The internationally recognized authority for naming celestial bodies is the International Astronomical Union (IAU). The International Astronomical Union maintains the Working Group on Star Names (WGSN) which catalogs"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_10",
    "chunk": "and standardizes proper names for stars. A number of private companies sell names of stars which are not recognized by the IAU, professional astronomers, or the amateur astronomy community. The British Library calls this an unregulated commercial enterprise, and the New York City Department of Consumer and Worker Protection issued a violation against one such star-naming company for engaging in a deceptive trade practice. Although stellar parameters can be expressed in SI units or Gaussian units, it is often most convenient to express mass, luminosity, and radii in solar units, based on the characteristics of the Sun. In 2015, the IAU defined a set of nominal solar values (defined as SI constants, without uncertainties) which can be used for quoting stellar parameters: The solar mass M☉ was not explicitly defined by the IAU due to the large relative uncertainty (10) of the Newtonian constant of gravitation G. Since the product of the Newtonian constant of gravitation and solar mass together (GM☉) has been determined to much greater precision, the IAU defined the nominal solar mass parameter to be: The nominal solar mass parameter can be combined with the most recent (2014) CODATA estimate of the Newtonian constant of gravitation G"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_11",
    "chunk": "to derive the solar mass to be approximately 1.9885×10 kg. Although the exact values for the luminosity, radius, mass parameter, and mass may vary slightly in the future due to observational uncertainties, the 2015 IAU nominal constants will remain the same SI values as they remain useful measures for quoting stellar parameters. Large lengths, such as the radius of a giant star or the semi-major axis of a binary star system, are often expressed in terms of the astronomical unit—approximately equal to the mean distance between the Earth and the Sun (150 million km or approximately 93 million miles). In 2012, the IAU defined the astronomical constant to be an exact length in meters: 149,597,870,700 m. Stars condense from regions of space of higher matter density, yet those regions are less dense than within a vacuum chamber. These regions—known as molecular clouds—consist mostly of hydrogen, with about 23 to 28 percent helium and a few percent heavier elements. One example of such a star-forming region is the Orion Nebula. Most stars form in groups of dozens to hundreds of thousands of stars. Massive stars in these groups may powerfully illuminate those clouds, ionizing the hydrogen, and creating H II regions."
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_12",
    "chunk": "Such feedback effects, from star formation, may ultimately disrupt the cloud and prevent further star formation. All stars spend the majority of their existence as main sequence stars, fueled primarily by the nuclear fusion of hydrogen into helium within their cores. However, stars of different masses have markedly different properties at various stages of their development. The ultimate fate of more massive stars differs from that of less massive stars, as do their luminosities and the impact they have on their environment. Accordingly, astronomers often group stars by their mass: The formation of a star begins with gravitational instability within a molecular cloud, caused by regions of higher density—often triggered by compression of clouds by radiation from massive stars, expanding bubbles in the interstellar medium, the collision of different molecular clouds, or the collision of galaxies (as in a starburst galaxy). When a region reaches a sufficient density of matter to satisfy the criteria for Jeans instability, it begins to collapse under its own gravitational force. As the cloud collapses, individual conglomerations of dense dust and gas form \"Bok globules\". As a globule collapses and the density increases, the gravitational energy converts into heat and the temperature rises. When the"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_13",
    "chunk": "protostellar cloud has approximately reached the stable condition of hydrostatic equilibrium, a protostar forms at the core. These pre-main-sequence stars are often surrounded by a protoplanetary disk and powered mainly by the conversion of gravitational energy. The period of gravitational contraction lasts about 10 million years for a star like the sun, up to 100 million years for a red dwarf. Early stars of less than 2 M☉ are called T Tauri stars, while those with greater mass are Herbig Ae/Be stars. These newly formed stars emit jets of gas along their axis of rotation, which may reduce the angular momentum of the collapsing star and result in small patches of nebulosity known as Herbig–Haro objects. These jets, in combination with radiation from nearby massive stars, may help to drive away the surrounding cloud from which the star was formed. Early in their development, T Tauri stars follow the Hayashi track—they contract and decrease in luminosity while remaining at roughly the same temperature. Less massive T Tauri stars follow this track to the main sequence, while more massive stars turn onto the Henyey track. Most stars are observed to be members of binary star systems, and the properties of those"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_14",
    "chunk": "binaries are the result of the conditions in which they formed. A gas cloud must lose its angular momentum in order to collapse and form a star. The fragmentation of the cloud into multiple stars distributes some of that angular momentum. The primordial binaries transfer some angular momentum by gravitational interactions during close encounters with other stars in young stellar clusters. These interactions tend to split apart more widely separated (soft) binaries while causing hard binaries to become more tightly bound. This produces the separation of binaries into their two observed populations distributions. Stars spend about 90% of their lifetimes fusing hydrogen into helium in high-temperature-and-pressure reactions in their cores. Such stars are said to be on the main sequence and are called dwarf stars. Starting at zero-age main sequence, the proportion of helium in a star's core will steadily increase, the rate of nuclear fusion at the core will slowly increase, as will the star's temperature and luminosity. The Sun, for example, is estimated to have increased in luminosity by about 40% since it reached the main sequence 4.6 billion (4.6×10) years ago. Every star generates a stellar wind of particles that causes a continual outflow of gas into"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_15",
    "chunk": "space. For most stars, the mass lost is negligible. The Sun loses 10 M☉ every year, or about 0.01% of its total mass over its entire lifespan. However, very massive stars can lose 10 to 10 M☉ each year, significantly affecting their evolution. Stars that begin with more than 50 M☉ can lose over half their total mass while on the main sequence. The time a star spends on the main sequence depends primarily on the amount of fuel it has and the rate at which it fuses it. The Sun is expected to live 10 billion (10) years. Massive stars consume their fuel very rapidly and are short-lived. Low mass stars consume their fuel very slowly. Stars less massive than 0.25 M☉, called red dwarfs, are able to fuse nearly all of their mass while stars of about 1 M☉ can only fuse about 10% of their mass. The combination of their slow fuel-consumption and relatively large usable fuel supply allows low mass stars to last about one trillion (10×10) years; the most extreme of 0.08 M☉ will last for about 12 trillion years. Red dwarfs become hotter and more luminous as they accumulate helium. When they eventually run"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_16",
    "chunk": "out of hydrogen, they contract into a white dwarf and decline in temperature. Since the lifespan of such stars is greater than the current age of the universe (13.8 billion years), no stars under about 0.85 M☉ are expected to have moved off the main sequence. Besides mass, the elements heavier than helium can play a significant role in the evolution of stars. Astronomers label all elements heavier than helium \"metals\", and call the chemical concentration of these elements in a star, its metallicity. A star's metallicity can influence the time the star takes to burn its fuel, and controls the formation of its magnetic fields, which affects the strength of its stellar wind. Older, population II stars have substantially less metallicity than the younger, population I stars due to the composition of the molecular clouds from which they formed. Over time, such clouds become increasingly enriched in heavier elements as older stars die and shed portions of their atmospheres. As stars of at least 0.4 M☉ exhaust the supply of hydrogen at their core, they start to fuse hydrogen in a shell surrounding the helium core. The outer layers of the star expand and cool greatly as they transition"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_17",
    "chunk": "into a red giant. In some cases, they will fuse heavier elements at the core or in shells around the core. As the stars expand, they throw part of their mass, enriched with those heavier elements, into the interstellar environment, to be recycled later as new stars. In about 5 billion years, when the Sun enters the helium burning phase, it will expand to a maximum radius of roughly 1 astronomical unit (150 million kilometres), 250 times its present size, and lose 30% of its current mass. As the hydrogen-burning shell produces more helium, the core increases in mass and temperature. In a red giant of up to 2.25 M☉, the mass of the helium core becomes degenerate prior to helium fusion. Finally, when the temperature increases sufficiently, core helium fusion begins explosively in what is called a helium flash, and the star rapidly shrinks in radius, increases its surface temperature, and moves to the horizontal branch of the HR diagram. For more massive stars, helium core fusion starts before the core becomes degenerate, and the star spends some time in the red clump, slowly burning helium, before the outer convective envelope collapses and the star then moves to the"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_18",
    "chunk": "horizontal branch. After a star has fused the helium of its core, it begins fusing helium along a shell surrounding the hot carbon core. The star then follows an evolutionary path called the asymptotic giant branch (AGB) that parallels the other described red-giant phase, but with a higher luminosity. The more massive AGB stars may undergo a brief period of carbon fusion before the core becomes degenerate. During the AGB phase, stars undergo thermal pulses due to instabilities in the core of the star. In these thermal pulses, the luminosity of the star varies and matter is ejected from the star's atmosphere, ultimately forming a planetary nebula. As much as 50 to 70% of a star's mass can be ejected in this mass loss process. Because energy transport in an AGB star is primarily by convection, this ejected material is enriched with the fusion products dredged up from the core. Therefore, the planetary nebula is enriched with elements like carbon and oxygen. Ultimately, the planetary nebula disperses, enriching the general interstellar medium. Therefore, future generations of stars are made of the \"star stuff\" from past stars. During their helium-burning phase, a star of more than 9 solar masses expands to"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_19",
    "chunk": "form first a blue supergiant and then a red supergiant. Particularly massive stars (exceeding 40 solar masses, like Alnilam, the central blue supergiant of Orion's Belt) do not become red supergiants due to high mass loss. These may instead evolve to a Wolf–Rayet star, characterised by spectra dominated by emission lines of elements heavier than hydrogen, which have reached the surface due to strong convection and intense mass loss, or from stripping of the outer layers. When helium is exhausted at the core of a massive star, the core contracts and the temperature and pressure rises enough to fuse carbon (see Carbon-burning process). This process continues, with the successive stages being fueled by neon (see neon-burning process), oxygen (see oxygen-burning process), and silicon (see silicon-burning process). Near the end of the star's life, fusion continues along a series of onion-layer shells within a massive star. Each shell fuses a different element, with the outermost shell fusing hydrogen; the next shell fusing helium, and so forth. The final stage occurs when a massive star begins producing iron. Since iron nuclei are more tightly bound than any heavier nuclei, any fusion beyond iron does not produce a net release of energy. Some"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_20",
    "chunk": "massive stars, particularly luminous blue variables, are very unstable to the extent that they violently shed their mass into space in events known as supernova impostors, becoming significantly brighter in the process. Eta Carinae is known for having undergone a supernova impostor event, the Great Eruption, in the 19th century. As a star's core shrinks, the intensity of radiation from that surface increases, creating such radiation pressure on the outer shell of gas that it will push those layers away, forming a planetary nebula. If what remains after the outer atmosphere has been shed is less than roughly 1.4 M☉, it shrinks to a relatively tiny object about the size of Earth, known as a white dwarf. White dwarfs lack the mass for further gravitational compression to take place. The electron-degenerate matter inside a white dwarf is no longer a plasma. Eventually, white dwarfs fade into black dwarfs over a very long period of time. In massive stars, fusion continues until the iron core has grown so large (more than 1.4 M☉) that it can no longer support its own mass. This core will suddenly collapse as its electrons are driven into its protons, forming neutrons, neutrinos, and gamma rays"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_21",
    "chunk": "in a burst of electron capture and inverse beta decay. The shockwave formed by this sudden collapse causes the rest of the star to explode in a supernova. Supernovae become so bright that they may briefly outshine the star's entire home galaxy. When they occur within the Milky Way, supernovae have historically been observed by naked-eye observers as \"new stars\" where none seemingly existed before. A supernova explosion blows away the star's outer layers, leaving a remnant such as the Crab Nebula. The core is compressed into a neutron star, which sometimes manifests itself as a pulsar or X-ray burster. In the case of the largest stars, the remnant is a black hole greater than 4 M☉. In a neutron star the matter is in a state known as neutron-degenerate matter, with a more exotic form of degenerate matter, QCD matter, possibly present in the core. The blown-off outer layers of dying stars include heavy elements, which may be recycled during the formation of new stars. These heavy elements allow the formation of rocky planets. The outflow from supernovae and the stellar wind of large stars play an important part in shaping the interstellar medium. Binary stars' evolution may significantly"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_22",
    "chunk": "differ from that of single stars of the same mass. For example, when any star expands to become a red giant, it may overflow its Roche lobe, the surrounding region where material is gravitationally bound to it; if stars in a binary system are close enough, some of that material may overflow to the other star, yielding phenomena including contact binaries, common-envelope binaries, cataclysmic variables, blue stragglers, and type Ia supernovae. Mass transfer leads to cases such as the Algol paradox, where the most-evolved star in a system is the least massive. The evolution of binary star and higher-order star systems is intensely researched since so many stars have been found to be members of binary systems. Around half of Sun-like stars, and an even higher proportion of more massive stars, form in multiple systems, and this may greatly influence such phenomena as novae and supernovae, the formation of certain types of star, and the enrichment of space with nucleosynthesis products. The influence of binary star evolution on the formation of evolved massive stars such as luminous blue variables, Wolf–Rayet stars, and the progenitors of certain classes of core collapse supernova is still disputed. Single massive stars may be unable"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_23",
    "chunk": "to expel their outer layers fast enough to form the types and numbers of evolved stars that are observed, or to produce progenitors that would explode as the supernovae that are observed. Mass transfer through gravitational stripping in binary systems is seen by some astronomers as the solution to that problem. Stars are not spread uniformly across the universe but are normally grouped into galaxies along with interstellar gas and dust. A typical large galaxy like the Milky Way contains hundreds of billions of stars. There are more than 2 trillion (10) galaxies, though most are less than 10% the mass of the Milky Way. Overall, there are likely to be between 10 and 10 stars, which are more stars than all the grains of sand on planet Earth. Most stars are within galaxies, but between 10 and 50% of the starlight in large galaxy clusters may come from stars outside of any galaxy. A multi-star system consists of two or more gravitationally bound stars that orbit each other. The simplest and most common multi-star system is a binary star, but systems of three or more stars exist. For reasons of orbital stability, such multi-star systems are often organized into"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_24",
    "chunk": "hierarchical sets of binary stars. Larger groups are called star clusters. These range from loose stellar associations with only a few stars to open clusters with dozens to thousands of stars, up to enormous globular clusters with hundreds of thousands of stars. Such systems orbit their host galaxy. The stars in an open or globular cluster all formed from the same giant molecular cloud, so all members normally have similar ages and compositions. Many stars are observed, and most or all may have originally formed in gravitationally bound, multiple-star systems. This is particularly true for very massive O and B class stars, 80% of which are believed to be part of multiple-star systems. The proportion of single star systems increases with decreasing star mass, so that only 25% of red dwarfs are known to have stellar companions. As 85% of all stars are red dwarfs, more than two thirds of stars in the Milky Way are likely single red dwarfs. In a 2017 study of the Perseus molecular cloud, astronomers found that most of the newly formed stars are in binary systems. In the model that best explained the data, all stars initially formed as binaries, though some binaries later"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_25",
    "chunk": "split up and leave single stars behind. The nearest star to the Earth, apart from the Sun, is Proxima Centauri, 4.2465 light-years (40.175 trillion kilometres) away. Travelling at the orbital speed of the Space Shuttle, 8 kilometres per second (29,000 kilometres per hour), it would take about 150,000 years to arrive. This is typical of stellar separations in galactic discs. Stars can be much closer to each other in the centres of galaxies and in globular clusters, or much farther apart in galactic halos. Due to the relatively vast distances between stars outside the galactic nucleus, collisions between stars are thought to be rare. In denser regions such as the core of globular clusters or the galactic center, collisions can be more common. Such collisions can produce what are known as blue stragglers. These abnormal stars have a higher surface temperature and thus are bluer than stars at the main sequence turnoff in the cluster to which they belong; in standard stellar evolution, blue stragglers would already have evolved off the main sequence and thus would not be seen in the cluster. Almost everything about a star is determined by its initial mass, including such characteristics as luminosity, size, evolution,"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_26",
    "chunk": "lifespan, and its eventual fate. Most stars are between 1 billion and 10 billion years old. Some stars may even be close to 13.8 billion years old—the observed age of the universe. The oldest star yet discovered, HD 140283, nicknamed Methuselah star, is an estimated 14.46 ± 0.8 billion years old. (Due to the uncertainty in the value, this age for the star does not conflict with the age of the universe, determined by the Planck satellite as 13.799 ± 0.021). The more massive the star, the shorter its lifespan, primarily because massive stars have greater pressure on their cores, causing them to burn hydrogen more rapidly. The most massive stars last an average of a few million years, while stars of minimum mass (red dwarfs) burn their fuel very slowly and can last tens to hundreds of billions of years. When stars form in the present Milky Way galaxy, they are composed of about 71% hydrogen and 27% helium, as measured by mass, with a small fraction of heavier elements. Typically the portion of heavy elements is measured in terms of the iron content of the stellar atmosphere, as iron is a common element and its absorption lines are"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_27",
    "chunk": "relatively easy to measure. The portion of heavier elements may be an indicator of the likelihood that the star has a planetary system. As of 2005 the star with the lowest iron content ever measured is the dwarf HE1327-2326, with only 1/200,000th the iron content of the Sun. By contrast, the super-metal-rich star μ Leonis has nearly double the abundance of iron as the Sun, while the planet-bearing star 14 Herculis has nearly triple the iron. Chemically peculiar stars show unusual abundances of certain elements in their spectrum; especially chromium and rare earth elements. Stars with cooler outer atmospheres, including the Sun, can form various diatomic and polyatomic molecules. Due to their great distance from the Earth, all stars except the Sun appear to the unaided eye as shining points in the night sky that twinkle because of the effect of the Earth's atmosphere. The Sun is close enough to the Earth to appear as a disk instead, and to provide daylight. Other than the Sun, the star with the largest apparent size is R Doradus, with an angular diameter of only 0.057 arcseconds. The disks of most stars are much too small in angular size to be observed with"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_28",
    "chunk": "current ground-based optical telescopes, so interferometer telescopes are required to produce images of these objects. Another technique for measuring the angular size of stars is through occultation. By precisely measuring the drop in brightness of a star as it is occulted by the Moon (or the rise in brightness when it reappears), the star's angular diameter can be computed. Stars range in size from neutron stars, which vary anywhere from 20 to 40 km (25 mi) in diameter, to supergiants like Betelgeuse in the Orion constellation, which has a diameter about 640 times that of the Sun with a much lower density. The motion of a star relative to the Sun can provide useful information about the origin and age of a star, as well as the structure and evolution of the surrounding galaxy. The components of motion of a star consist of the radial velocity toward or away from the Sun, and the traverse angular movement, which is called its proper motion. Radial velocity is measured by the doppler shift of the star's spectral lines and is given in units of km/s. The proper motion of a star, its parallax, is determined by precise astrometric measurements in units of"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_29",
    "chunk": "milli-arc seconds (mas) per year. With knowledge of the star's parallax and its distance, the proper motion velocity can be calculated. Together with the radial velocity, the total velocity can be calculated. Stars with high rates of proper motion are likely to be relatively close to the Sun, making them good candidates for parallax measurements. When both rates of movement are known, the space velocity of the star relative to the Sun or the galaxy can be computed. Among nearby stars, it has been found that younger population I stars have generally lower velocities than older, population II stars. The latter have elliptical orbits that are inclined to the plane of the galaxy. A comparison of the kinematics of nearby stars has allowed astronomers to trace their origin to common points in giant molecular clouds; such groups with common points of origin are referred to as stellar associations. The magnetic field of a star is generated within regions of the interior where convective circulation occurs. This movement of conductive plasma functions like a dynamo, wherein the movement of electrical charges induce magnetic fields, as does a mechanical dynamo. Those magnetic fields have a great range that extend throughout and beyond"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_30",
    "chunk": "the star. The strength of the magnetic field varies with the mass and composition of the star, and the amount of magnetic surface activity depends upon the star's rate of rotation. This surface activity produces starspots, which are regions of strong magnetic fields and lower than normal surface temperatures. Coronal loops are arching magnetic field flux lines that rise from a star's surface into the star's outer atmosphere, its corona. The coronal loops can be seen due to the plasma they conduct along their length. Stellar flares are bursts of high-energy particles that are emitted due to the same magnetic activity. Young, rapidly rotating stars tend to have high levels of surface activity because of their magnetic field. The magnetic field can act upon a star's stellar wind, functioning as a brake to gradually slow the rate of rotation with time. Thus, older stars such as the Sun have a much slower rate of rotation and a lower level of surface activity. The activity levels of slowly rotating stars tend to vary in a cyclical manner and can shut down altogether for periods of time. During the Maunder Minimum, for example, the Sun underwent a 70-year period with almost no"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_31",
    "chunk": "sunspot activity. Stars have masses ranging from less than half the solar mass to over 200 solar masses (see List of most massive stars). One of the most massive stars known is Eta Carinae, which, with 100–150 times as much mass as the Sun, will have a lifespan of only several million years. Studies of the most massive open clusters suggests 150 M☉ as a rough upper limit for stars in the current era of the universe. This represents an empirical value for the theoretical limit on the mass of forming stars due to increasing radiation pressure on the accreting gas cloud. Several stars in the R136 cluster in the Large Magellanic Cloud have been measured with larger masses, but it has been determined that they could have been created through the collision and merger of massive stars in close binary systems, sidestepping the 150 M☉ limit on massive star formation. The first stars to form after the Big Bang may have been larger, up to 300 M☉, due to the complete absence of elements heavier than lithium in their composition. This generation of supermassive population III stars is likely to have existed in the very early universe (i.e., they"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_32",
    "chunk": "are observed to have a high redshift), and may have started the production of chemical elements heavier than hydrogen that are needed for the later formation of planets and life. In June 2015, astronomers reported evidence for Population III stars in the Cosmos Redshift 7 galaxy at z = 6.60. With a mass only 80 times that of Jupiter (MJ), 2MASS J0523-1403 is the smallest known star undergoing nuclear fusion in its core. For stars with metallicity similar to the Sun, the theoretical minimum mass the star can have and still undergo fusion at the core, is estimated to be about 75 MJ. When the metallicity is very low, the minimum star size seems to be about 8.3% of the solar mass, or about 87 MJ. Smaller bodies called brown dwarfs, occupy a poorly defined grey area between stars and gas giants. The combination of the radius and the mass of a star determines its surface gravity. Giant stars have a much lower surface gravity than do main sequence stars, while the opposite is the case for degenerate, compact stars such as white dwarfs. The surface gravity can influence the appearance of a star's spectrum, with higher gravity causing a"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_33",
    "chunk": "broadening of the absorption lines. The rotation rate of stars can be determined through spectroscopic measurement, or more exactly determined by tracking their starspots. Young stars can have a rotation greater than 100 km/s at the equator. The B-class star Achernar, for example, has an equatorial velocity of about 225 km/s or greater, causing its equator to bulge outward and giving it an equatorial diameter that is more than 50% greater than between the poles. This rate of rotation is just below the critical velocity of 300 km/s at which speed the star would break apart. By contrast, the Sun rotates once every 25–35 days depending on latitude, with an equatorial velocity of 1.93 km/s. A main sequence star's magnetic field and the stellar wind serve to slow its rotation by a significant amount as it evolves on the main sequence. Degenerate stars have contracted into a compact mass, resulting in a rapid rate of rotation. However they have relatively low rates of rotation compared to what would be expected by conservation of angular momentum—the tendency of a rotating body to compensate for a contraction in size by increasing its rate of spin. A large portion of the star's angular"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_34",
    "chunk": "momentum is dissipated as a result of mass loss through the stellar wind. In spite of this, the rate of rotation for a pulsar can be very rapid. The pulsar at the heart of the Crab nebula, for example, rotates 30 times per second. The rotation rate of the pulsar will gradually slow due to the emission of radiation. The surface temperature of a main sequence star is determined by the rate of energy production of its core and by its radius, and is often estimated from the star's color index. The temperature is normally given in terms of an effective temperature, which is the temperature of an idealized black body that radiates its energy at the same luminosity per surface area as the star. The effective temperature is only representative of the surface, as the temperature increases toward the core. The temperature in the core region of a star is several million kelvins. The stellar temperature will determine the rate of ionization of various elements, resulting in characteristic absorption lines in the spectrum. The surface temperature of a star, along with its visual absolute magnitude and absorption features, is used to classify a star (see classification below). Massive main"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_35",
    "chunk": "sequence stars can have surface temperatures of 50,000 K. Smaller stars such as the Sun have surface temperatures of a few thousand K. Red giants have relatively low surface temperatures of about 3,600 K; but they have a high luminosity due to their large exterior surface area. The energy produced by stars, a product of nuclear fusion, radiates to space as both electromagnetic radiation and particle radiation. The particle radiation emitted by a star is manifested as the stellar wind, which streams from the outer layers as electrically charged protons and alpha and beta particles. A steady stream of almost massless neutrinos emanate directly from the star's core. The production of energy at the core is the reason stars shine so brightly: every time two or more atomic nuclei fuse together to form a single atomic nucleus of a new heavier element, gamma ray photons are released from the nuclear fusion product. This energy is converted to other forms of electromagnetic energy of lower frequency, such as visible light, by the time it reaches the star's outer layers. The color of a star, as determined by the most intense frequency of the visible light, depends on the temperature of the"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_36",
    "chunk": "star's outer layers, including its photosphere. Besides visible light, stars emit forms of electromagnetic radiation that are invisible to the human eye. In fact, stellar electromagnetic radiation spans the entire electromagnetic spectrum, from the longest wavelengths of radio waves through infrared, visible light, ultraviolet, to the shortest of X-rays, and gamma rays. From the standpoint of total energy emitted by a star, not all components of stellar electromagnetic radiation are significant, but all frequencies provide insight into the star's physics. Using the stellar spectrum, astronomers can determine the surface temperature, surface gravity, metallicity and rotational velocity of a star. If the distance of the star is found, such as by measuring the parallax, then the luminosity of the star can be derived. The mass, radius, surface gravity, and rotation period can then be estimated based on stellar models. (Mass can be calculated for stars in binary systems by measuring their orbital velocities and distances. Gravitational microlensing has been used to measure the mass of a single star.) With these parameters, astronomers can estimate the age of the star. The luminosity of a star is the amount of light and other forms of radiant energy it radiates per unit of time."
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_37",
    "chunk": "It has units of power. The luminosity of a star is determined by its radius and surface temperature. Many stars do not radiate uniformly across their entire surface. The rapidly rotating star Vega, for example, has a higher energy flux (power per unit area) at its poles than along its equator. Patches of the star's surface with a lower temperature and luminosity than average are known as starspots. Small, dwarf stars such as the Sun generally have essentially featureless disks with only small starspots. Giant stars have much larger, more obvious starspots, and they exhibit strong stellar limb darkening. That is, the brightness decreases towards the edge of the stellar disk. Red dwarf flare stars such as UV Ceti may possess prominent starspot features. The apparent brightness of a star is expressed in terms of its apparent magnitude. It is a function of the star's luminosity, its distance from Earth, the extinction effect of interstellar dust and gas, and the altering of the star's light as it passes through Earth's atmosphere. Intrinsic or absolute magnitude is directly related to a star's luminosity, and is the apparent magnitude a star would be if the distance between the Earth and the star"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_38",
    "chunk": "were 10 parsecs (32.6 light-years). Both the apparent and absolute magnitude scales are logarithmic units: one whole number difference in magnitude is equal to a brightness variation of about 2.5 times (the 5th root of 100 or approximately 2.512). This means that a first magnitude star (+1.00) is about 2.5 times brighter than a second magnitude (+2.00) star, and about 100 times brighter than a sixth magnitude star (+6.00). The faintest stars visible to the naked eye under good seeing conditions are about magnitude +6. On both apparent and absolute magnitude scales, the smaller the magnitude number, the brighter the star; the larger the magnitude number, the fainter the star. The brightest stars, on either scale, have negative magnitude numbers. The variation in brightness (ΔL) between two stars is calculated by subtracting the magnitude number of the brighter star (mb) from the magnitude number of the fainter star (mf), then using the difference as an exponent for the base number 2.512; that is to say: Relative to both luminosity and distance from Earth, a star's absolute magnitude (M) and apparent magnitude (m) are not equivalent; for example, the bright star Sirius has an apparent magnitude of −1.44, but it has"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_39",
    "chunk": "an absolute magnitude of +1.41. The Sun has an apparent magnitude of −26.7, but its absolute magnitude is only +4.83. Sirius, the brightest star in the night sky as seen from Earth, is approximately 23 times more luminous than the Sun, while Canopus, the second brightest star in the night sky with an absolute magnitude of −5.53, is approximately 14,000 times more luminous than the Sun. Despite Canopus being vastly more luminous than Sirius, the latter star appears the brighter of the two. This is because Sirius is merely 8.6 light-years from the Earth, while Canopus is much farther away at a distance of 310 light-years. The most luminous known stars have absolute magnitudes of roughly −12, corresponding to 6 million times the luminosity of the Sun. Theoretically, the least luminous stars are at the lower limit of mass at which stars are capable of supporting nuclear fusion of hydrogen in the core; stars just above this limit have been located in the NGC 6397 cluster. The faintest red dwarfs in the cluster are absolute magnitude 15, while a 17th absolute magnitude white dwarf has been discovered. The current stellar classification system originated in the early 20th century, when stars"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_40",
    "chunk": "were classified from A to Q based on the strength of the hydrogen line. It was thought that the hydrogen line strength was a simple linear function of temperature. Instead, it was more complicated: it strengthened with increasing temperature, peaked near 9000 K, and then declined at greater temperatures. The classifications were since reordered by temperature, on which the modern scheme is based. Stars are given a single-letter classification according to their spectra, ranging from type O, which are very hot, to M, which are so cool that molecules may form in their atmospheres. The main classifications in order of decreasing surface temperature are: O, B, A, F, G, K, and M. A variety of rare spectral types are given special classifications. The most common of these are types L and T, which classify the coldest low-mass stars and brown dwarfs. Each letter has 10 sub-divisions, numbered from 0 to 9, in order of decreasing temperature. However, this system breaks down at extreme high temperatures as classes O0 and O1 may not exist. In addition, stars may be classified by the luminosity effects found in their spectral lines, which correspond to their spatial size and is determined by their surface"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_41",
    "chunk": "gravity. These range from 0 (hypergiants) through III (giants) to V (main sequence dwarfs); some authors add VII (white dwarfs). Main sequence stars fall along a narrow, diagonal band when graphed according to their absolute magnitude and spectral type. The Sun is a main sequence G2V yellow dwarf of intermediate temperature and ordinary size. There is additional nomenclature in the form of lower-case letters added to the end of the spectral type to indicate peculiar features of the spectrum. For example, an \"e\" can indicate the presence of emission lines; \"m\" represents unusually strong levels of metals, and \"var\" can mean variations in the spectral type. White dwarf stars have their own class that begins with the letter D. This is further sub-divided into the classes DA, DB, DC, DO, DZ, and DQ, depending on the types of prominent lines found in the spectrum. This is followed by a numerical value that indicates the temperature. Variable stars have periodic or random changes in luminosity because of intrinsic or extrinsic properties. Of the intrinsically variable stars, the primary types can be subdivided into three principal groups. During their stellar evolution, some stars pass through phases where they can become pulsating variables."
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_42",
    "chunk": "Pulsating variable stars vary in radius and luminosity over time, expanding and contracting with periods ranging from minutes to years, depending on the size of the star. This category includes Cepheid and Cepheid-like stars, and long-period variables such as Mira. Eruptive variables are stars that experience sudden increases in luminosity because of flares or mass ejection events. This group includes protostars, Wolf-Rayet stars, and flare stars, as well as giant and supergiant stars. Cataclysmic or explosive variable stars are those that undergo a dramatic change in their properties. This group includes novae and supernovae. A binary star system that includes a nearby white dwarf can produce certain types of these spectacular stellar explosions, including the nova and a Type 1a supernova. The explosion is created when the white dwarf accretes hydrogen from the companion star, building up mass until the hydrogen undergoes fusion. Some novae are recurrent, having periodic outbursts of moderate amplitude. Stars can vary in luminosity because of extrinsic factors, such as eclipsing binaries, as well as rotating stars that produce extreme starspots. A notable example of an eclipsing binary is Algol, which regularly varies in magnitude from 2.1 to 3.4 over a period of 2.87 days. The"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_43",
    "chunk": "interior of a stable star is in a state of hydrostatic equilibrium: the forces on any small volume almost exactly counterbalance each other. The balanced forces are inward gravitational force and an outward force due to the pressure gradient within the star. The pressure gradient is established by the temperature gradient of the plasma; the outer part of the star is cooler than the core. The temperature at the core of a main sequence or giant star is at least on the order of 10 K. The resulting temperature and pressure at the hydrogen-burning core of a main sequence star are sufficient for nuclear fusion to occur and for sufficient energy to be produced to prevent further collapse of the star. As atomic nuclei are fused in the core, they emit energy in the form of gamma rays. These photons interact with the surrounding plasma, adding to the thermal energy at the core. Stars on the main sequence convert hydrogen into helium, creating a slowly but steadily increasing proportion of helium in the core. Eventually the helium content becomes predominant, and energy production ceases at the core. Instead, for stars of more than 0.4 M☉, fusion occurs in a slowly"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_44",
    "chunk": "expanding shell around the degenerate helium core. In addition to hydrostatic equilibrium, the interior of a stable star will maintain an energy balance of thermal equilibrium. There is a radial temperature gradient throughout the interior that results in a flux of energy flowing toward the exterior. The outgoing flux of energy leaving any layer within the star will exactly match the incoming flux from below. The radiation zone is the region of the stellar interior where the flux of energy outward is dependent on radiative heat transfer, since convective heat transfer is inefficient in that zone. In this region the plasma will not be perturbed, and any mass motions will die out. Where this is not the case, then the plasma becomes unstable and convection will occur, forming a convection zone. This can occur, for example, in regions where very high energy fluxes occur, such as near the core or in areas with high opacity (making radiatative heat transfer inefficient) as in the outer envelope. The occurrence of convection in the outer envelope of a main sequence star depends on the star's mass. Stars with several times the mass of the Sun have a convection zone deep within the interior"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_45",
    "chunk": "and a radiative zone in the outer layers. Smaller stars such as the Sun are just the opposite, with the convective zone located in the outer layers. Red dwarf stars with less than 0.4 M☉ are convective throughout, which prevents the accumulation of a helium core. For most stars the convective zones will vary over time as the star ages and the constitution of the interior is modified. The photosphere is that portion of a star that is visible to an observer. This is the layer at which the plasma of the star becomes transparent to photons of light. From here, the energy generated at the core becomes free to propagate into space. It is within the photosphere that sun spots, regions of lower than average temperature, appear. Above the level of the photosphere is the stellar atmosphere. In a main sequence star such as the Sun, the lowest level of the atmosphere, just above the photosphere, is the thin chromosphere region, where spicules appear and stellar flares begin. Above this is the transition region, where the temperature rapidly increases within a distance of only 100 km (62 mi). Beyond this is the corona, a volume of super-heated plasma that"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_46",
    "chunk": "can extend outward to several million kilometres. The existence of a corona appears to be dependent on a convective zone in the outer layers of the star. Despite its high temperature, the corona emits very little light, due to its low gas density. The corona region of the Sun is normally only visible during a solar eclipse. From the corona, a stellar wind of plasma particles expands outward from the star, until it interacts with the interstellar medium. For the Sun, the influence of its solar wind extends throughout a bubble-shaped region called the heliosphere. When nuclei fuse, the mass of the fused product is less than the mass of the original parts. This lost mass is converted to electromagnetic energy, according to the mass–energy equivalence relationship E = m c 2 {\\displaystyle E=mc^{2}} . A variety of nuclear fusion reactions take place in the cores of stars, that depend upon their mass and composition. The hydrogen fusion process is temperature-sensitive, so a moderate increase in the core temperature will result in a significant increase in the fusion rate. As a result, the core temperature of main sequence stars only varies from 4 million kelvin for a small M-class star"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_47",
    "chunk": "to 40 million kelvin for a massive O-class star. In the Sun, with a 16-million-kelvin core, hydrogen fuses to form helium in the proton–proton chain reaction: There are a couple other paths, in which He and He combine to form Be, which eventually (with the addition of another proton) yields two He, a gain of one. where γ is a gamma ray photon, νe is a neutrino, and H and He are isotopes of hydrogen and helium, respectively. The energy released by this reaction is in millions of electron volts. Each individual reaction produces only a tiny amount of energy, but because enormous numbers of these reactions occur constantly, they produce all the energy necessary to sustain the star's radiation output. In comparison, the combustion of two hydrogen gas molecules with one oxygen gas molecule releases only 5.7 eV. In more massive stars, helium is produced in a cycle of reactions catalyzed by carbon called the carbon-nitrogen-oxygen cycle. In evolved stars with cores at 100 million kelvin and masses between 0.5 and 10 M☉, helium can be transformed into carbon in the triple-alpha process that uses the intermediate element beryllium: In massive stars, heavier elements can be burned in a"
  },
  {
    "source": "Star.txt",
    "chunk_id": "Star.txt_48",
    "chunk": "contracting core through the neon-burning process and oxygen-burning process. The final stage in the stellar nucleosynthesis process is the silicon-burning process that results in the production of the stable isotope iron-56. Any further fusion would be an endothermic process that consumes energy, and so further energy can only be produced through gravitational collapse."
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_0",
    "chunk": "# Stellar dynamics Stellar dynamics is the branch of astrophysics which describes in a statistical way the collective motions of stars subject to their mutual gravity. The essential difference from celestial mechanics is that the number of body N ≫ 10. {\\displaystyle N\\gg 10.} Typical galaxies have upwards of millions of macroscopic gravitating bodies and countless number of neutrinos and perhaps other dark microscopic bodies. Also each star contributes more or less equally to the total gravitational field, whereas in celestial mechanics the pull of a massive body dominates any satellite orbits. Stellar dynamics also has connections to the field of plasma physics. The two fields underwent significant development during a similar time period in the early 20th century, and both borrow mathematical formalism originally developed in the field of fluid mechanics. In accretion disks and stellar surfaces, the dense plasma or gas particles collide very frequently, and collisions result in equipartition and perhaps viscosity under magnetic field. We see various sizes for accretion disks and stellar atmosphere, both made of enormous number of microscopic particle mass, ( L / V , M / N ) {\\displaystyle (L/V,M/N)} The system crossing time scale is long in stellar dynamics, where it"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_1",
    "chunk": "is handy to note that 1000 pc / 1 km/s = 1000 Myr = HubbleTime / 14. {\\displaystyle 1000{\\text{pc}}/1{\\text{km/s}}=1000{\\text{Myr}}={\\text{HubbleTime}}/14.} The long timescale means that, unlike gas particles in accretion disks, stars in galaxy disks very rarely see a collision in their stellar lifetime. However, galaxies collide occasionally in galaxy clusters, and stars have close encounters occasionally in star clusters. As a rule of thumb, the typical scales concerned (see the Upper Portion of P.C.Budassi's Logarithmic Map of the Universe) are ( L / V , M / N ) {\\displaystyle (L/V,M/N)} At a superficial level, all of stellar dynamics might be formulated as an N-body problem by Newton's second law, where the equation of motion (EOM) for internal interactions of an isolated stellar system of N members can be written down as, m i d 2 r i d t 2 = ∑ i = 1 i ≠ j N G m i m j ( r j − r i ) ‖ r j − r i ‖ 3 . {\\displaystyle m_{i}{\\frac {d^{2}\\mathbf {r_{i}} }{dt^{2}}}=\\sum _{i=1 \\atop i\\neq j}^{N}{\\frac {Gm_{i}m_{j}\\left(\\mathbf {r} _{j}-\\mathbf {r} _{i}\\right)}{\\left\\|\\mathbf {r} _{j}-\\mathbf {r} _{i}\\right\\|^{3}}}.} Here in the N-body system, any individual member, m i {\\displaystyle"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_2",
    "chunk": "m_{i}} is influenced by the gravitational potentials of the remaining m j {\\displaystyle m_{j}} members. In practice, except for in the highest performance computer simulations, it is not feasible to calculate rigorously the future of a large N system this way. Also this EOM gives very little intuition. Historically, the methods utilised in stellar dynamics originated from the fields of both classical mechanics and statistical mechanics. In essence, the fundamental problem of stellar dynamics is the N-body problem, where the N members refer to the members of a given stellar system. Given the large number of objects in a stellar system, stellar dynamics can address both the global, statistical properties of many orbits as well as the specific data on the positions and velocities of individual orbits. Stellar dynamics involves determining the gravitational potential of a substantial number of stars. The stars can be modeled as point masses whose orbits are determined by the combined interactions with each other. Typically, these point masses represent stars in a variety of clusters or galaxies, such as a Galaxy cluster, or a Globular cluster. Without getting a system's gravitational potential by adding all of the point-mass potentials in the system at every second,"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_3",
    "chunk": "stellar dynamicists develop potential models that can accurately model the system while remaining computationally inexpensive. The gravitational potential, Φ {\\displaystyle \\Phi } , of a system is related to the acceleration and the gravitational field, g {\\displaystyle \\mathbf {g} } by: d 2 r i d t 2 = g → = − ∇ r i Φ ( r i ) , Φ ( r i ) = − ∑ k = 1 k ≠ i N G m k ‖ r i − r k ‖ , {\\displaystyle {\\frac {d^{2}\\mathbf {r_{i}} }{dt^{2}}}}=\\mathbf {\\vec {g}} =-\\nabla _{\\mathbf {r_{i}} }\\Phi (\\mathbf {r_{i}} ),~~\\Phi (\\mathbf {r} _{i})=-\\sum _{k=1 \\atop k\\neq i}^{N}{{\\frac {Gm_{k}}{\\left\\|\\mathbf {r} _{i}-\\mathbf {r} _{k}\\right\\|}},} whereas the potential is related to a (smoothened) mass density, ρ {\\displaystyle \\rho } , via the Poisson's equation in the integral form Φ ( r ) = − ∫ G ρ ( R ) d 3 R ‖ r − R ‖ {\\displaystyle \\Phi (\\mathbf {r} )=-\\int {G\\rho (\\mathbf {R} )d^{3}\\mathbf {R} \\over \\left\\|\\mathbf {r} -\\mathbf {R} \\right\\|}} or the more common differential form ∇ 2 Φ = 4 π G ρ . {\\displaystyle \\nabla ^{2}\\Phi =4\\pi G\\rho .} Consider an analytically smooth spherical potential"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_4",
    "chunk": "Φ ( r ) ≡ ( − V 0 2 ) + [ r 2 − r 0 2 2 r 0 2 , 1 − r 0 r ] max V 0 2 ≡ Φ ( r 0 ) − V e 2 ( r ) 2 , Φ ( r 0 ) = − V 0 2 , g = − ∇ Φ ( r ) = − Ω 2 r H ( r 0 − r ) − G M 0 r 2 H ( r − r 0 ) , Ω = V 0 r 0 , M 0 = V 0 2 r 0 G , {\\displaystyle {\\begin{aligned}\\Phi (r)&\\equiv \\left(-V_{0}^{2}\\right)+\\left[{r^{2}-r_{0}^{2} \\over 2r_{0}^{2}},~~1-{r_{0} \\over r}\\right]_{\\max }\\!\\!\\!\\!V_{0}^{2}\\equiv \\Phi (r_{0})-{V_{e}^{2}(r) \\over 2},~~\\Phi (r_{0})=-V_{0}^{2},\\\\\\mathbf {g} &=-\\mathbf {\\nabla } \\Phi (r)=-\\Omega ^{2}rH(r_{0}-r)-{GM_{0} \\over r^{2}}H(r-r_{0}),~~\\Omega ={V_{0} \\over r_{0}},~~M_{0}={V_{0}^{2}r_{0} \\over G},\\end{aligned}}} where V e ( r ) {\\displaystyle V_{e}(r)} takes the meaning of the speed to \"escape to the edge\" r 0 {\\displaystyle r_{0}} , and 2 V 0 {\\displaystyle {\\sqrt {2}}V_{0}} is the speed to \"escape from the edge to infinity\". The gravity is like the restoring force of harmonic oscillator inside the sphere, and Keplerian outside as described by the"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_5",
    "chunk": "Heaviside functions. We can fix the normalisation V 0 {\\displaystyle V_{0}} by computing the corresponding density using the spherical Poisson Equation G ρ = d 4 π r 2 d r r 2 d Φ d r = d ( G M ) 4 π r 2 d r = 3 V 0 2 4 π r 0 2 H ( r 0 − r ) , {\\displaystyle G\\rho ={d \\over 4\\pi r^{2}dr}{r^{2}d\\Phi \\over dr}={d(GM) \\over 4\\pi r^{2}dr}={3V_{0}^{2} \\over 4\\pi r_{0}^{2}}H(r_{0}-r),} where the enclosed mass M ( r ) = r 2 d Φ G d r = ∫ 0 r d r ∫ 0 π ( r d θ ) ∫ 0 2 π ( r sin ⁡ θ d φ ) ρ 0 H ( r 0 − r ) = M 0 x 3 | x = r r 0 . {\\displaystyle M(r)={r^{2}d\\Phi \\over Gdr}=\\int _{0}^{r}dr\\int _{0}^{\\pi }(rd\\theta )\\int _{0}^{2\\pi }(r\\sin \\theta d\\varphi )\\rho _{0}H(r_{0}-r)=\\left.M_{0}x^{3}\\right|_{x={r \\over r_{0}}}.} Hence the potential model corresponds to a uniform sphere of radius r 0 {\\displaystyle r_{0}} , total mass M 0 {\\displaystyle M_{0}} with V 0 r 0 ≡ 4 π G ρ 0 3 = G M 0 r 0"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_6",
    "chunk": "3 . {\\displaystyle {V_{0} \\over r_{0}}\\equiv {\\sqrt {4\\pi G\\rho _{0} \\over 3}}={\\sqrt {GM_{0} \\over r_{0}^{3}}}.} While both the equations of motion and Poisson Equation can also take on non-spherical forms, depending on the coordinate system and the symmetry of the physical system, the essence is the same: The motions of stars in a galaxy or in a globular cluster are principally determined by the average distribution of the other, distant stars. The infrequent stellar encounters involve processes such as relaxation, mass segregation, tidal forces, and dynamical friction that influence the trajectories of the system's members. There are three related approximations made in the Newtonian EOM and Poisson Equation above. Firstly above equations neglect relativistic corrections, which are of order of ( v / c ) 2 ≪ 10 − 4 {\\displaystyle (v/c)^{2}\\ll 10^{-4}} as typical stellar 3-dimensional speed, v ∼ 3 − 3000 {\\displaystyle v\\sim 3-3000} km/s, is much below the speed of light. Secondly non-gravitational force is typically negligible in stellar systems. For example, in the vicinity of a typical star the ratio of radiation-to-gravity force on a hydrogen atom or ion, Q Eddington = σ e 4 π m H c L ⊙ r 2 G M ⊙"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_7",
    "chunk": "r 2 = 1 30 , 000 , {\\displaystyle Q^{\\text{Eddington}}={{\\sigma _{e} \\over 4\\pi m_{H}c}{L\\odot \\over r^{2}} \\over {GM_{\\odot } \\over r^{2}}}={1 \\over 30,000},} hence radiation force is negligible in general, except perhaps around a luminous O-type star of mass 30 M ⊙ {\\displaystyle 30M_{\\odot }} , or around a black hole accreting gas at the Eddington limit so that its luminosity-to-mass ratio L ∙ / M ∙ {\\displaystyle L_{\\bullet }/M_{\\bullet }} is defined by Q Eddington = 1 {\\displaystyle Q^{\\text{Eddington}}=1} . Thirdly a star can be swallowed if coming within a few Schwarzschild radii of the black hole. This radius of Loss is given by s ≤ s Loss = 6 G M ∙ c 2 {\\displaystyle s\\leq s_{\\text{Loss}}={\\frac {6GM_{\\bullet }}{c^{2}}}} The loss cone can be visualised by considering infalling particles aiming to the black hole within a small solid angle (a cone in velocity). These particle with small θ ≪ 1 {\\displaystyle \\theta \\ll 1} have small angular momentum per unit mass J ≡ r v sin ⁡ θ ≤ J loss = 4 G M ∙ c . {\\displaystyle J\\equiv rv\\sin \\theta \\leq J_{\\text{loss}}={\\frac {4GM_{\\bullet }}{c}}.} Their small angular momentum (due to ) does not make a high"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_8",
    "chunk": "enough barrier near s Loss {\\displaystyle s_{\\text{Loss}}} to force the particle to turn around. The effective potential Φ eff ( r ) ≡ E − r ˙ 2 2 = J 2 2 r 2 + Φ ( r ) , {\\displaystyle \\Phi _{\\text{eff}}(r)\\equiv E-{{\\dot {r}}^{2} \\over 2}={J^{2} \\over 2r^{2}}+\\Phi (r),} is always positive infinity in Newtonian gravity. However, in GR, it nosedives to minus infinity near 6 G M ∙ c 2 {\\displaystyle {\\frac {6GM_{\\bullet }}{c^{2}}}} if J ≤ 4 G M ∙ c . {\\displaystyle J\\leq {\\frac {4GM_{\\bullet }}{c}}.} Sparing a rigorous GR treatment, one can verify this s loss , J loss {\\displaystyle s_{\\text{loss}},J_{\\text{loss}}} by computing the last stable circular orbit, where the effective potential is at an inflection point Φ eff ″ ( s loss ) = Φ eff ′ ( s loss ) = 0 {\\displaystyle \\Phi ''_{\\text{eff}}(s_{\\text{loss}})=\\Phi '_{\\text{eff}}(s_{\\text{loss}})=0} using an approximate classical potential of a Schwarzschild black hole Φ ( r ) = − ( 4 G M ∙ / c ) 2 2 r 2 [ 1 + 3 ( 6 G M ∙ / c 2 ) 2 8 r 2 ] − G M ∙ r [ 1 − ( 6"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_9",
    "chunk": "G M ∙ / c 2 ) 2 r 2 ] . {\\displaystyle \\Phi (r)=-{(4GM_{\\bullet }/c)^{2} \\over 2r^{2}}\\left[1+{3(6GM_{\\bullet }/c^{2})^{2} \\over 8r^{2}}\\right]-{\\frac {GM_{\\bullet }}{r}}\\left[1-{(6GM_{\\bullet }/c^{2})^{2} \\over r^{2}}\\right].} A star can be tidally torn by a heavier black hole when coming within the so-called Hill's radius of the black hole, inside which a star's surface gravity yields to the tidal force from the black hole, i.e., ( 1 − 1.5 ) ≥ Q tide ≡ G M ⊙ / R ⊙ 2 [ G M ∙ / s Hill 2 − G M ∙ / ( s Hill + R ⊙ ) 2 ] , s Hill → R ⊙ ( ( 2 − 3 ) G M ∙ G M ⊙ ) 1 3 , {\\displaystyle (1-1.5)\\geq Q^{\\text{tide}}\\equiv {GM_{\\odot }/R_{\\odot }^{2} \\over [GM_{\\bullet }/s_{\\text{Hill}}^{2}-GM_{\\bullet }/(s_{\\text{Hill}}+R_{\\odot })^{2}]},~~~s_{\\text{Hill}}\\rightarrow R_{\\odot }\\left({(2-3)GM_{\\bullet } \\over GM_{\\odot }}\\right)^{1 \\over 3},} For typical black holes of M ∙ = ( 10 0 − 10 8.5 ) M ⊙ {\\displaystyle M_{\\bullet }=(10^{0}-10^{8.5})M_{\\odot }} the destruction radius max [ s Hill , s Loss ] = 400 R ⊙ max [ ( M ∙ 3 × 10 7 M ⊙ ) 1 / 3 , M ∙ 3 × 10"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_10",
    "chunk": "7 M ⊙ ] = ( 1 − 4000 ) R ⊙ ≪ 0.001 p c , {\\displaystyle \\max[s_{\\text{Hill}},s_{\\text{Loss}}]=400R_{\\odot }\\max \\left[\\left({M_{\\bullet } \\over 3\\times 10^{7}M_{\\odot }}\\right)^{1/3},{M_{\\bullet } \\over 3\\times 10^{7}M_{\\odot }}\\right]=(1-4000)R_{\\odot }\\ll 0.001\\mathrm {pc} ,} where 0.001pc is the stellar spacing in the densest stellar systems (e.g., the nuclear star cluster in the Milky Way centre). Hence (main sequence) stars are generally too compact internally and too far apart spaced to be disrupted by even the strongest black hole tides in galaxy or cluster environment. A particle of mass m {\\displaystyle m} with a relative speed V will be deflected when entering the (much larger) cross section π s ∙ 2 {\\displaystyle \\pi s_{\\bullet }^{2}} of a black hole. This so-called sphere of influence is loosely defined by, up to a Q-like fudge factor ln ⁡ Λ {\\displaystyle {\\sqrt {\\ln \\Lambda }}} , 1 ∼ ln ⁡ Λ ≡ V 2 / 2 G ( M ∙ + m ) / s ∙ , {\\displaystyle 1\\sim {\\sqrt {\\ln \\Lambda }}\\equiv {\\frac {V^{2}/2}{G(M_{\\bullet }+m)/s_{\\bullet }}},} hence for a Sun-like star we have, s ∙ = G ( M ∙ + M ⊙ ) ln ⁡ Λ V 2 / 2 ≈"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_11",
    "chunk": "M ∙ M ⊙ V ⊙ 2 V 2 R ⊙ > [ s Hill , s Loss ] m a x = ( 1 − 4000 ) R ⊙ , {\\displaystyle s_{\\bullet }={G(M_{\\bullet }+M_{\\odot }){\\sqrt {\\ln \\Lambda }} \\over V^{2}/2}\\approx {M_{\\bullet } \\over M_{\\odot }}{V_{\\odot }^{2} \\over V^{2}}R_{\\odot }>[s_{\\text{Hill}},s_{\\text{Loss}}]_{max}=(1-4000)R_{\\odot },} i.e., stars will neither be tidally disrupted nor physically hit/swallowed in a typical encounter with the black hole thanks to the high surface escape speed V ⊙ = 2 G M ⊙ / R ⊙ = 615 k m / s {\\displaystyle V_{\\odot }={\\sqrt {2GM_{\\odot }/R_{\\odot }}}=615\\mathrm {km/s} } from any solar mass star, comparable to the internal speed between galaxies in the Bullet Cluster of galaxies, and greater than the typical internal speed V ∼ 2 G ( N M ⊙ ) / R ≪ 300 k m / s {\\displaystyle V\\sim {\\sqrt {2G(NM_{\\odot })/R}}\\ll \\mathrm {300km/s} } inside all star clusters and in galaxies. First consider a heavy black hole of mass M ∙ {\\displaystyle M_{\\bullet }} is moving through a dissipational gas of (rescaled) thermal sound speed ς' {\\displaystyle {\\text{ς'}}} and density ρ gas {\\displaystyle \\rho _{\\text{gas}}} , then every gas particle of mass m will"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_12",
    "chunk": "likely transfer its relative momentum m V ∙ {\\displaystyle mV_{\\bullet }} to the BH when coming within a cross-section of radius s ∙ ≡ ( G M ∙ + G m ) ln ⁡ Λ ( V ∙ 2 + ς' 2 ) / 2 , {\\displaystyle s_{\\bullet }\\equiv {(GM_{\\bullet }+Gm){\\sqrt {\\ln \\Lambda }} \\over (V_{\\bullet }^{2}+{\\text{ς'}}^{2})/2},} In a time scale t fric {\\displaystyle t_{\\text{fric}}} that the black hole loses half of its streaming velocity, its mass may double by Bondi accretion, a process of capturing most of gas particles that enter its sphere of influence s ∙ {\\displaystyle s_{\\bullet }} , dissipate kinetic energy by gas collisions and fall in the black hole. The gas capture rate is M ∙ t Bondi g a s = ς' 2 + V ∙ 2 ( π s ∙ 2 ) ρ gas = 4 π ρ gas [ ( G M ∙ ) 2 ( ς' 2 + V ∙ 2 ) 3 2 ] ln ⁡ Λ , ς' ≡ σ 1 + γ 3 2 ( 9 / 8 ) 2 / 3 ≈ [ ς , γ σ ] max , {\\displaystyle {M_{\\bullet } \\over t_{\\text{Bondi}}^{gas}}={\\sqrt {{\\text{ς'}}^{2}+V_{\\bullet }^{2}}}(\\pi"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_13",
    "chunk": "s_{\\bullet }^{2})\\rho _{\\text{gas}}=4\\pi \\rho _{\\text{gas}}\\left[{(GM_{\\bullet })^{2} \\over ({\\text{ς'}}^{2}+V_{\\bullet }^{2})^{3 \\over 2}}\\right]\\ln \\Lambda ,~~{\\text{ς'}}\\equiv \\sigma {\\sqrt {1+\\gamma ^{3} \\over 2(9/8)^{2/3}}}\\approx [{\\text{ς}},\\gamma \\sigma ]_{\\text{max}},} where the polytropic index γ {\\displaystyle \\gamma } is the sound speed in units of velocity dispersion squared, and the rescaled sound speed ς' {\\displaystyle {\\text{ς'}}} allows us to match the Bondi spherical accretion rate, M ˙ ∙ ≈ π ρ gas ς [ ( G M ∙ ) ς 2 ] 2 {\\displaystyle {\\dot {M}}_{\\bullet }\\approx \\pi \\rho _{\\text{gas}}{\\text{ς}}\\left[{(GM_{\\bullet }) \\over {\\text{ς}}^{2}}\\right]^{2}} for the adiabatic gas γ = 5 / 3 {\\displaystyle \\gamma =5/3} , compared to M ˙ ∙ ≈ 4 π ρ gas ς [ ( G M ∙ ) ς 2 ] 2 {\\displaystyle {\\dot {M}}_{\\bullet }\\approx 4\\pi \\rho _{\\text{gas}}{\\text{ς}}\\left[{(GM_{\\bullet }) \\over {\\text{ς}}^{2}}\\right]^{2}} of the isothermal case γ = 1 {\\displaystyle \\gamma =1} . Coming back to star tidal disruption and star capture by a (moving) black hole, setting ln ⁡ Λ = 1 {\\displaystyle \\ln \\Lambda =1} , we could summarise the BH's growth rate from gas and stars, M ∙ t Bondi g a s + M ∙ t loss ∗ {\\displaystyle {M_{\\bullet } \\over t_{\\text{Bondi}}^{gas}}+{M_{\\bullet } \\over t_{\\text{loss}}^{*}}} with, M ˙"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_14",
    "chunk": "∙ = ς' 2 + V ∙ 2 m n ( π s ∙ 2 , π s Hill 2 , π s Loss 2 ) max , s ∙ ≈ ( G M ∙ + G m ) ( V ∙ 2 + ς' 2 ) / 2 , {\\displaystyle {\\dot {M}}_{\\bullet }={\\sqrt {{\\text{ς'}}^{2}+V_{\\bullet }^{2}}}mn(\\pi s_{\\bullet }^{2},\\pi s_{\\text{Hill}}^{2},\\pi s_{\\text{Loss}}^{2})_{\\text{max}},~~s_{\\bullet }\\approx {(GM_{\\bullet }+Gm) \\over (V_{\\bullet }^{2}+{\\text{ς'}}^{2})/2},} because the black hole consumes a fractional/most part of star/gas particles passing its sphere of influence. Consider the case that a heavy black hole of mass M ∙ {\\displaystyle M_{\\bullet }} moves relative to a background of stars in random motion in a cluster of total mass ( N M ⊙ ) {\\displaystyle (NM_{\\odot })} with a mean number density n ∼ ( N − 1 ) / ( 4 π R 3 / 3 ) {\\displaystyle n\\sim (N-1)/(4\\pi R^{3}/3)} within a typical size R {\\displaystyle R} . Intuition says that gravity causes the light bodies to accelerate and gain momentum and kinetic energy (see slingshot effect). By conservation of energy and momentum, we may conclude that the heavier body will be slowed by an amount to compensate. Since there is a loss of"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_15",
    "chunk": "momentum and kinetic energy for the body under consideration, the effect is called dynamical friction. After certain time of relaxations the heavy black hole's kinetic energy should be in equal partition with the less-massive background objects. The slow-down of the black hole can be described as − M ∙ V ˙ ∙ = M ∙ V ∙ t fric star , {\\displaystyle -{M_{\\bullet }{\\dot {V}}_{\\bullet }}={M_{\\bullet }V_{\\bullet } \\over t_{\\text{fric}}^{\\text{star}}},} where t fric star {\\displaystyle t_{\\text{fric}}^{\\text{star}}} is called a dynamical friction time. Consider a Mach-1 BH, which travels initially at the sound speed ς = V 0 {\\displaystyle {\\text{ς}}=V_{0}} , hence its Bondi radius s ∙ {\\displaystyle s_{\\bullet }} satisfies G M ∙ ln ⁡ Λ s ∙ = V 0 2 = ς 2 = 0.4053 G M ⊙ ( N − 1 ) R , {\\displaystyle {GM_{\\bullet }{\\sqrt {\\ln \\Lambda }} \\over s_{\\bullet }}=V_{0}^{2}={\\text{ς}}^{2}={0.4053GM_{\\odot }(N-1) \\over R},} where the sound speed is ς = 4 G M ⊙ ( N − 1 ) π 2 R {\\displaystyle {\\text{ς}}={\\sqrt {4GM_{\\odot }(N-1) \\over \\pi ^{2}R}}} with the prefactor 4 π 2 ≈ 4 10 = 0.4 {\\displaystyle {4 \\over \\pi ^{2}}\\approx {4 \\over 10}=0.4} fixed by the fact that for"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_16",
    "chunk": "a uniform spherical cluster of the mass density ρ = n M ⊙ ≈ M ⊙ ( N − 1 ) 4.19 R 3 {\\displaystyle \\rho =nM_{\\odot }\\approx {M_{\\odot }(N-1) \\over 4.19R^{3}}} , half of a circular period is the time for \"sound\" to make a oneway crossing in its longest dimension, i.e., 2 t ς ≡ 2 t cross ≡ 2 R ς = π R 3 G M ⊙ ( N − 1 ) ≈ ( 0.4244 G ρ ) − 1 / 2 . {\\displaystyle 2t_{\\text{ς}}\\equiv 2t_{\\text{cross}}\\equiv {2R \\over {\\text{ς}}}=\\pi {\\sqrt {R^{3} \\over GM_{\\odot }(N-1)}}\\approx (0.4244G\\rho )^{-1/2}.} It is customary to call the \"half-diameter\" crossing time t cross {\\displaystyle t_{\\text{cross}}} the dynamical time scale. Assume the BH stops after traveling a length of l fric ≡ ς t fric {\\displaystyle l_{\\text{fric}}\\equiv {\\text{ς}}t_{\\text{fric}}} with its momentum M ∙ V 0 = M ∙ ς {\\displaystyle M_{\\bullet }V_{0}=M_{\\bullet }{\\text{ς}}} deposited to M ∙ M ⊙ {\\displaystyle {M_{\\bullet } \\over M_{\\odot }}} stars in its path over l fric / ( 2 R ) {\\displaystyle l_{\\text{fric}}/(2R)} crossings, then the number of stars deflected by the BH's Bondi cross section per \"diameter\" crossing time is N defl = ( M ∙"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_17",
    "chunk": "M ⊙ ) 2 R l fric = N π s ∙ 2 π R 2 = N ( M ∙ 0.4053 M ⊙ N ) 2 ln ⁡ Λ . {\\displaystyle N^{\\text{defl}}={({M_{\\bullet } \\over M_{\\odot }})}{2R \\over l_{\\text{fric}}}=N{\\pi s_{\\bullet }^{2} \\over \\pi R^{2}}=N\\left({M_{\\bullet } \\over 0.4053M_{\\odot }N}\\right)^{2}\\ln \\Lambda .} More generally, the Equation of Motion of the BH at a general velocity V ∙ {\\displaystyle \\mathbf {V} _{\\bullet }} in the potential Φ {\\displaystyle \\Phi } of a sea of stars can be written as − d d t ( M ∙ V ∙ ) − M ∙ ∇ Φ ≡ ( M ∙ V ∙ ) t fric = N π s ∙ 2 π R 2 ⏞ N defl ( M ⊙ V ∙ ) 2 t ς = 8 ln ⁡ Λ ′ N t ς M ∙ V ∙ , {\\displaystyle -{d \\over dt}(M_{\\bullet }V_{\\bullet })-M_{\\bullet }\\nabla \\Phi \\equiv {(M_{\\bullet }V_{\\bullet }) \\over t_{\\text{fric}}}=\\overbrace {N\\pi s_{\\bullet }^{2} \\over \\pi R^{2}} ^{N^{\\text{defl}}}{(M_{\\odot }V_{\\bullet }) \\over 2t_{\\text{ς}}}={8\\ln \\Lambda ' \\over Nt_{\\text{ς}}}M_{\\bullet }V_{\\bullet },} π 2 8 ≈ 1 {\\displaystyle {\\pi ^{2} \\over 8}\\approx 1} and the Coulomb logarithm modifying factor ln ⁡ Λ ′ ln ⁡ Λ"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_18",
    "chunk": "≡ [ π 2 8 ] 2 [ ( 1 + V ∙ 2 ς' 2 ) ] − 2 ( 1 + M ⊙ M ∙ ) ≤ [ ς' V ∙ ] 4 ≤ 1 {\\displaystyle {\\ln \\Lambda ' \\over \\ln \\Lambda }\\equiv \\left[{\\pi ^{2} \\over 8}\\right]^{2}\\left[(1+{V_{\\bullet }^{2} \\over {\\text{ς'}}^{2}})\\right]^{-2}(1+{M_{\\odot } \\over M_{\\bullet }})\\leq \\left[{{\\text{ς'}} \\over V_{\\bullet }}\\right]^{4}\\leq 1} discounts friction on a supersonic moving BH with mass M ∙ ≥ M ⊙ {\\displaystyle M_{\\bullet }\\geq M_{\\odot }} . As a rule of thumb, it takes about a sound crossing t ς' {\\displaystyle t_{\\text{ς'}}} time to \"sink\" subsonic BHs, from the edge to the centre without overshooting, if they weigh more than 1/8th of the total cluster mass. Lighter and faster holes can stay afloat much longer. The full Chandrasekhar dynamical friction formula for the change in velocity of the object involves integrating over the phase space density of the field of matter and is far from transparent. It reads as M ∙ d ( V ∙ ) d t = − M ∙ V ∙ t fric star = − m V ∙ n ( x ) d x 3 d t ln ⁡ Λ lag ,"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_19",
    "chunk": "{\\displaystyle {M_{\\bullet }d(\\mathbf {V} _{\\bullet }) \\over dt}=-{M_{\\bullet }\\mathbf {V} _{\\bullet } \\over t_{\\text{fric}}^{\\text{star}}}=-{m\\mathbf {V} _{\\bullet }~n(\\mathbf {x} )d\\mathbf {x} ^{3} \\over dt}\\ln \\Lambda _{\\text{lag}},} where n ( x ) d x 3 = d t V ∙ ( π s ∙ 2 ) n ( x ) = d t n ( x ) | V ∙ | π [ G ( m + M ∙ ) | V ∙ | 2 / 2 ] 2 {\\displaystyle ~~n(\\mathbf {x} )dx^{3}=dtV_{\\bullet }(\\pi s_{\\bullet }^{2})n(\\mathbf {x} )=dtn(\\mathbf {x} )|V_{\\bullet }|\\pi \\left[{G(m+M_{\\bullet }) \\over |V_{\\bullet }|^{2}/2}\\right]^{2}} is the number of particles in an infinitesimal cylindrical volume of length | V ∙ d t | {\\displaystyle |V_{\\bullet }dt|} and a cross-section π s ∙ 2 {\\displaystyle \\pi s_{\\bullet }^{2}} within the black hole's sphere of influence. Like the \"Couloumb logarithm\" ln ⁡ Λ {\\displaystyle \\ln \\Lambda } factors in the contribution of distant background particles, here the factor ln ⁡ ( Λ lag ) {\\displaystyle \\ln(\\Lambda _{\\text{lag}})} also factors in the probability of finding a background slower-than-BH particle to contribute to the drag. The more particles are overtaken by the BH, the more particles drag the BH, and the greater is ln ⁡"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_20",
    "chunk": "( Λ beaten ) {\\displaystyle \\ln(\\Lambda _{\\text{beaten}})} . Also the bigger the system, the greater is ln ⁡ Λ {\\displaystyle \\ln \\Lambda } . A background of elementary (gas or dark) particles can also induce dynamical friction, which scales with the mass density of the surrounding medium, m n {\\displaystyle m~n} ; the lower particle mass m is compensated by the higher number density n. The more massive the object, the more matter will be pulled into the wake. Summing up the gravitational drag of both collisional gas and collisionless stars, we have M ∙ d ( V ∙ ) M ∙ d t = − 4 π [ G M ∙ | V ∙ | ] 2 V ^ ∙ ( ρ gas ln ⁡ Λ lag g a s + m n * ln ⁡ Λ lag ∗ ) . {\\displaystyle M_{\\bullet }{d(\\mathbf {V} _{\\bullet }) \\over M_{\\bullet }dt}=-4\\pi \\left[{GM_{\\bullet } \\over |V_{\\bullet }|}\\right]^{2}\\mathbf {\\hat {V}} _{\\bullet }(\\rho _{\\text{gas}}\\ln \\Lambda _{\\text{lag}}^{gas}+mn_{\\text{*}}\\ln \\Lambda _{\\text{lag}}^{*}).~~} Here the \"lagging-behind\" fraction for gas and for stars are given by ln ⁡ Λ lag g a s ( u ) = ln ⁡ [ 1 + u λ ] 1 2 [ |"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_21",
    "chunk": "1 − u | λ ] H [ u − λ − 1 ] − H [ 1 − λ − u ] 2 exp ⁡ [ u + λ , 1 ] min 2 − [ u − λ , 1 ] min 2 4 λ , ≈ ln ⁡ [ ( u 3 − 1 ) 2 + λ 3 + u 3 − 1 1 + λ 3 − 1 ] 1 3 , u ≡ | V ∙ | t ς' t , λ ≡ ( s ∙ ς' t ) ln ⁡ Λ lag ∗ ln ⁡ Λ ≡ ∫ 0 | m V ∙ | ( 4 π p 2 d p ) e − p 2 2 ( m σ ) 2 ( 2 π m σ ) 3 | p = m | v | ≈ | V ∙ | 3 | V ∙ | 3 + 3.45 σ 3 , ln ⁡ Λ = ∫ d x 1 3 2 H e a v i s i d e [ n ( x 1 ) n ( x ) − 1 − M ∙ N M ⊙ ] ( s ∙"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_22",
    "chunk": "2 + | x 1 − x | 2 ) 3 2 ≈ ln ⁡ 1 + ( 0.123 N M ⊙ M ∙ ) 2 , {\\displaystyle {\\begin{aligned}\\ln \\Lambda _{\\text{lag}}^{gas}(u)&=\\ln ~{\\left[{1+u \\over \\lambda }\\right]^{1 \\over 2}\\left[{|1-u| \\over \\lambda }\\right]^{H[u-\\lambda -1]-H[1-\\lambda -u] \\over 2} \\over \\exp {[u+\\lambda ,1]_{\\min }^{2}-[u-\\lambda ,1]_{\\min }^{2} \\over 4\\lambda }},\\\\&\\approx \\ln \\left[{{\\sqrt {(u^{3}-1)^{2}+\\lambda ^{3}}}+u^{3}-1 \\over {\\sqrt {1+\\lambda ^{3}}}-1}\\right]^{1 \\over 3},~~u\\equiv {|V_{\\bullet }|t \\over {\\text{ς'}}t},~~\\lambda \\equiv ({s_{\\bullet } \\over {\\text{ς'}}t})\\\\{\\ln \\Lambda _{\\text{lag}}^{*} \\over \\ln \\Lambda }&\\equiv \\int _{0}^{|mV_{\\bullet }|}\\!\\!\\!\\!{(4\\pi p^{2}dp)e^{-{p^{2} \\over 2(m\\sigma )^{2}}} \\over ({\\sqrt {2\\pi }}m\\sigma )^{3}}\\left.\\right|_{p=m|v|}\\approx {|\\mathbf {V} _{\\bullet }|^{3} \\over |\\mathbf {V} _{\\bullet }|^{3}+3.45\\sigma ^{3}},\\\\\\ln \\Lambda &=\\int {d\\mathbf {x_{1}} ^{3}~2Heaviside[{n(\\mathbf {x_{1}} ) \\over n(\\mathbf {x} )}-1-{M_{\\bullet } \\over NM_{\\odot }}] \\over (s_{\\bullet }^{2}+|\\mathbf {x_{1}} -\\mathbf {x} |^{2})^{3 \\over 2}}\\approx \\ln {\\sqrt {1+\\left({0.123NM_{\\odot } \\over M_{\\bullet }}\\right)^{2}}},\\end{aligned}}} where we have further assumed that the BH starts to move from time t = 0 {\\displaystyle t=0} ; the gas is isothermal with sound speed ς {\\displaystyle {\\text{ς}}} ; the background stars have of (mass) density m n ( x ) {\\displaystyle mn(\\mathbf {x} )} in a Maxwell distribution of momentum p = m v {\\displaystyle p=mv} with a Gaussian distribution velocity spread σ {\\displaystyle \\sigma }"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_23",
    "chunk": "(called velocity dispersion, typically σ ≤ ς {\\displaystyle \\sigma \\leq {\\text{ς}}} ). Interestingly, the G 2 ( m + M ∙ ) ( m n ( x ) ) {\\displaystyle G^{2}(m+M_{\\bullet })(mn(\\mathbf {x} ))} dependence suggests that dynamical friction is from the gravitational pull of by the wake, which is induced by the gravitational focusing of the massive body in its two-body encounters with background objects. We see the force is also proportional to the inverse square of the velocity at the high end, hence the fractional rate of energy loss drops rapidly at high velocities. Dynamical friction is, therefore, unimportant for objects that move relativistically, such as photons. This can be rationalized by realizing that the faster the object moves through the media, the less time there is for a wake to build up behind it. Friction tends to be the highest at the sound barrier, where ln ⁡ Λ lag g a s | u = 1 = ln ⁡ ς' t s ∙ {\\displaystyle \\ln \\Lambda _{\\text{lag}}^{gas}\\left.\\right|_{u=1}=\\ln {{\\text{ς'}}t \\over s_{\\bullet }}} . Stars in a stellar system will influence each other's trajectories due to strong and weak gravitational encounters. An encounter between two stars is defined to"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_24",
    "chunk": "be strong/weak if their mutual potential energy at the closest passage is comparable/minuscule to their initial kinetic energy. Strong encounters are rare, and they are typically only considered important in dense stellar systems, e.g., a passing star can be sling-shot out by binary stars in the core of a globular cluster. This means that two stars need to come within a separation, s ∗ = G M ⊙ + G M ⊙ V 2 / 2 = 2 1.5 G M ⊙ ς 2 = 3.29 R N − 1 , {\\displaystyle s_{*}={GM_{\\odot }+GM_{\\odot } \\over V^{2}/2}={2 \\over 1.5}{GM_{\\odot } \\over {\\text{ς}}^{2}}={3.29R \\over N-1},} where we used the Virial Theorem, \"mutual potential energy balances twice kinetic energy on average\", i.e., \"the pairwise potential energy per star balances with twice kinetic energy associated with the sound speed in three directions\", 1 ∼ Q virial ≡ 2 K ⏞ ( N M ⊙ ) V 2 | W | = N M ⊙ ς 2 + N M ⊙ ς 2 + N M ⊙ ς 2 N ( N − 1 ) 2 G M ⊙ 2 R p a i r , {\\displaystyle 1\\sim Q^{\\text{virial}}\\equiv {\\overbrace {2K} ^{(NM_{\\odot })V^{2}} \\over"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_25",
    "chunk": "|W|}={NM_{\\odot }{\\text{ς}}^{2}+NM_{\\odot }{\\text{ς}}^{2}+NM_{\\odot }{\\text{ς}}^{2} \\over {N(N-1) \\over 2}{GM_{\\odot }^{2} \\over R_{pair}}},} where the factor N ( N − 1 ) / 2 {\\displaystyle N(N-1)/2} is the number of handshakes between a pair of stars without double-counting, the mean pair separation R pair = π 2 24 R ≈ 0.411234 R {\\displaystyle R_{\\text{pair}}={\\pi ^{2} \\over 24}R\\approx 0.411234R} is only about 40\\% of the radius of the uniform sphere. Note also the similarity of the Q virial ←→ ln ⁡ Λ . {\\displaystyle Q^{\\text{virial}}\\leftarrow \\rightarrow {\\sqrt {\\ln \\Lambda }}.} The mean free path of strong encounters in a typically ( N − 1 ) = 4.19 n R 3 ≫ 100 {\\displaystyle (N-1)=4.19nR^{3}\\gg 100} stellar system is then l strong = 1 ( π s ∗ 2 ) n ≈ ( N − 1 ) 8.117 R ≫ R , {\\displaystyle l_{\\text{strong}}={1 \\over (\\pi s_{*}^{2})n}\\approx {(N-1) \\over 8.117}R\\gg R,} i.e., it takes about 0.123 N {\\displaystyle 0.123N} radius crossings for a typical star to come within a cross-section π s ∗ 2 {\\displaystyle \\pi s_{*}^{2}} to be deflected from its path completely. Hence the mean free time of a strong encounter is much longer than the crossing time R / V {\\displaystyle"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_26",
    "chunk": "R/V} . Weak encounters have a more profound effect on the evolution of a stellar system over the course of many passages. The effects of gravitational encounters can be studied with the concept of relaxation time. A simple example illustrating relaxation is two-body relaxation, where a star's orbit is altered due to the gravitational interaction with another star. Initially, the subject star travels along an orbit with initial velocity, v {\\displaystyle \\mathbf {v} } , that is perpendicular to the impact parameter, the distance of closest approach, to the field star whose gravitational field will affect the original orbit. Using Newton's laws, the change in the subject star's velocity, δ v {\\displaystyle \\delta \\mathbf {v} } , is approximately equal to the acceleration at the impact parameter, multiplied by the time duration of the acceleration. The relaxation time can be thought as the time it takes for δ v {\\displaystyle \\delta \\mathbf {v} } to equal v {\\displaystyle \\mathbf {v} } , or the time it takes for the small deviations in velocity to equal the star's initial velocity. The number of \"half-diameter\" crossings for an average star to relax in a stellar system of N {\\displaystyle N} objects is"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_27",
    "chunk": "approximately t relax t ς = N relax ⋍ 0.123 ( N − 1 ) ln ⁡ ( N − 1 ) ≫ 1 {\\displaystyle {t_{\\text{relax}} \\over t_{\\text{ς}}}=N^{\\text{relax}}\\backsimeq {\\frac {0.123(N-1)}{\\ln(N-1)}}\\gg 1} from a more rigorous calculation than the above mean free time estimates for strong deflection. The answer makes sense because there is no relaxation for a single body or 2-body system. A better approximation of the ratio of timescales is N ′ ln ⁡ 1 + N ′ 2 | N ′ = 0.123 ( N − 2 ) {\\displaystyle \\left.{\\frac {N'}{\\ln {\\sqrt {1+N'^{2}}}}}\\right|_{N'=0.123(N-2)}} , hence the relaxation time for 3-body, 4-body, 5-body, 7-body, 10-body, ..., 42-body, 72-body, 140-body, 210-body, 550-body are about 16, 8, 6, 4, 3, ..., 3, 4, 6, 8, 16 crossings. There is no relaxation for an isolated binary, and the relaxation is the fastest for a 16-body system; it takes about 2.5 crossings for orbits to scatter each other. A system with N ∼ 10 2 − 10 10 {\\displaystyle N\\sim 10^{2}-10^{10}} have much smoother potential, typically takes ∼ ln ⁡ N ′ ≈ ( 2 − 20 ) {\\displaystyle \\sim \\ln N'\\approx (2-20)} weak encounters to build a strong deflection to change orbital"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_28",
    "chunk": "energy significantly. Clearly that the dynamical friction of a black hole is much faster than the relaxation time by roughly a factor M ⊙ / M ∙ {\\displaystyle M_{\\odot }/M_{\\bullet }} , but these two are very similar for a cluster of black holes, N fric = t fric t ς → t relax t ς = N relax ∼ ( N − 1 ) 10 − 100 , when M ∙ → m ← M ⊙ . {\\displaystyle N^{\\text{fric}}={t_{\\text{fric}} \\over t_{\\text{ς}}}\\rightarrow {t_{\\text{relax}} \\over t_{\\text{ς}}}=N^{\\text{relax}}\\sim {(N-1) \\over 10-100},~{\\text{when}}~{M_{\\bullet }\\rightarrow m\\leftarrow M_{\\odot }}.} For a star cluster or galaxy cluster with, say, N = 10 3 , R = 1 p c − 10 5 p c , V = 1 k m / s − 10 3 k m / s {\\displaystyle N=10^{3},~R=\\mathrm {1pc-10^{5}pc} ,~V=\\mathrm {1km/s-10^{3}km/s} } , we have t relax ∼ 100 t ς ≈ 100 M y r − 10 G y r {\\displaystyle t_{\\text{relax}}\\sim 100t_{\\text{ς}}\\approx 100\\mathrm {Myr} -10\\mathrm {Gyr} } . Hence encounters of members in these stellar or galaxy clusters are significant during the typical 10 Gyr lifetime. On the other hand, typical galaxy with, say, N = 10 6 − 10 11 {\\displaystyle N=10^{6}-10^{11}}"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_29",
    "chunk": "stars, would have a crossing time t ς ∼ 1 k p c − 100 k p c 1 k m / s − 100 k m / s ∼ 100 M y r {\\displaystyle t_{\\text{ς}}\\sim {1\\mathrm {kpc} -100\\mathrm {kpc} \\over 1\\mathrm {km/s} -100\\mathrm {km/s} }\\sim 100\\mathrm {Myr} } and their relaxation time is much longer than the age of the Universe. This justifies modelling galaxy potentials with mathematically smooth functions, neglecting two-body encounters throughout the lifetime of typical galaxies. And inside such a typical galaxy the dynamical friction and accretion on stellar black holes over a 10-Gyr Hubble time change the black hole's velocity and mass by only an insignificant fraction Δ ∼ M ∙ 0.1 N M ⊙ t t ς ≤ M ∙ 0.1 % N M ⊙ {\\displaystyle \\Delta \\sim {M_{\\bullet } \\over 0.1NM_{\\odot }}{t \\over t_{\\text{ς}}}\\leq {M_{\\bullet } \\over 0.1\\%NM_{\\odot }}} if the black hole makes up less than 0.1% of the total galaxy mass N M ⊙ ∼ 10 6 − 11 M ⊙ {\\displaystyle NM_{\\odot }\\sim 10^{6-11}M_{\\odot }} . Especially when M ∙ ∼ M ⊙ {\\displaystyle M_{\\bullet }\\sim M_{\\odot }} , we see that a typical star never experiences an encounter, hence"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_30",
    "chunk": "stays on its orbit in a smooth galaxy potential. The dynamical friction or relaxation time identifies collisionless vs. collisional particle systems. Dynamics on timescales much less than the relaxation time is effectively collisionless because typical star will deviate from its initial orbit size by a tiny fraction t / t relax ≪ 1 {\\displaystyle t/t_{\\text{relax}}\\ll 1} . They are also identified as systems where subject stars interact with a smooth gravitational potential as opposed to the sum of point-mass potentials. The accumulated effects of two-body relaxation in a galaxy can lead to what is known as mass segregation, where more massive stars gather near the center of clusters, while the less massive ones are pushed towards the outer parts of the cluster. Having gone through the details of the rather complex interactions of particles in a gravitational system, it is always helpful to zoom out and extract some generic theme, at an affordable price of rigour, so carry on with a lighter load. First important concept is \"gravity balancing motion\" near the perturber and for the background as a whole Perturber Virial ≈ G M ∙ s ∙ ≈ V cir 2 ≈ ⟨ V ⟩ 2 ≈ ⟨ V"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_31",
    "chunk": "2 ⟩ ¯ ≈ σ 2 ≈ ( R t ς ) 2 ≈ c ς 2 ≈ G ( N m ) R ≈ Background Virial , {\\displaystyle {\\text{Perturber Virial}}\\approx {GM_{\\bullet } \\over s_{\\bullet }}\\approx V_{\\text{cir}}^{2}\\approx \\langle V\\rangle ^{2}\\approx {\\overline {\\langle V^{2}\\rangle }}\\approx \\sigma ^{2}\\approx \\left({R \\over t_{\\text{ς}}}\\right)^{2}\\approx c_{\\text{ς}}^{2}\\approx {G(Nm) \\over R}\\approx {\\text{Background Virial}},} by consistently omitting all factors of unity 4 π {\\displaystyle 4\\pi } , π {\\displaystyle \\pi } , ln ⁡ Λ {\\displaystyle \\ln {\\text{Λ}}} etc for clarity, approximating the combined mass M ∙ + m ≈ M ∙ {\\displaystyle M_{\\bullet }+m\\approx M_{\\bullet }} and being ambiguous whether the geometry of the system is a thin/thick gas/stellar disk or a (non)-uniform stellar/dark sphere with or without a boundary, and about the subtle distinctions among the kinetic energies from the local Circular rotation speed V cir {\\displaystyle V_{\\text{cir}}} , radial infall speed ⟨ V ⟩ {\\displaystyle \\langle V\\rangle } , globally isotropic or anisotropic random motion σ {\\displaystyle \\sigma } in one or three directions, or the (non)-uniform isotropic Sound speed c ς {\\displaystyle c_{\\text{ς}}} to emphasize of the logic behind the order of magnitude of the friction time scale. Second we can recap very loosely summarise"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_32",
    "chunk": "the various processes so far of collisional and collisionless gas/star or dark matter by Spherical cow style Continuity Equation on any generic quantity Q of the system: d Q d t ≈ ± Q ( l c ς ) , Q being mass M, energy E, momentum (M V), Phase density f, size R, density N m 4 π 3 R 3 . . . , {\\displaystyle {dQ \\over dt}\\approx {\\pm Q \\over ({l \\over c_{\\text{ς}}})},~{\\text{Q being mass M, energy E, momentum (M V), Phase density f, size R, density}}{Nm \\over {4\\pi \\over 3}R^{3}}...,} where the ± {\\displaystyle \\pm } sign is generally negative except for the (accreting) mass M, and the Mean free path l = c ς t fric {\\displaystyle l=c_{\\text{ς}}t_{\\text{fric}}} or the friction time t fric {\\displaystyle t_{\\text{fric}}} can be due to direct molecular viscosity from a physical collision Cross section, or due to gravitational scattering (bending/focusing/Sling shot) of particles; generally the influenced area is the greatest of the competing processes of Bondi accretion, Tidal disruption, and Loss cone capture, s 2 ≈ max [ Bondi radius s ∙ , Tidal radius s Hill , physical size s Loss cone ] 2 . {\\displaystyle s^{2}\\approx \\max \\left[{\\text{Bondi"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_33",
    "chunk": "radius}}~s_{\\bullet },{\\text{Tidal radius}}~s_{\\text{Hill}},{\\text{physical size}}~s_{\\text{Loss cone}}\\right]^{2}.} E.g., in case Q is the perturber's mass Q = M ∙ {\\displaystyle Q=M_{\\bullet }} , then we can estimate the Dynamical friction time via the (gas/star) Accretion rate M ˙ ∙ = M ∙ t fric ≈ ∫ 0 s 2 d ( area ) ( background mean flux ) ≈ s 2 ( ρ c ς ) ≈ Perturber influenced cross section ( s 2 ) background system cross section ( R 2 ) × background mass ( N m ) crossing time t ς ≈ R c ς ≈ 1 G ( N m ) R 3 ∼ G ρ ∼ κ ≈ G M ∙ G t ς G M ∙ G ( N m ) ≈ ( ρ c ς ) ( G M ∙ c ς 2 ) 2 , if consider only gravitationally focusing, ≈ M ∙ N t ς , if for a light perturber M ∙ → m = M ⊙ → 0 , if practically collisionless N → ∞ , {\\displaystyle {\\begin{aligned}{\\dot {M}}_{\\bullet }=&{M_{\\bullet } \\over t_{\\text{fric}}}\\approx \\int _{0}^{s^{2}}d({\\text{area}})~({\\text{background mean flux}})\\approx s^{2}(\\rho c_{\\text{ς}})\\\\\\approx &{\\frac {{\\text{Perturber influenced cross section}}~(s^{2})}{{\\text{background system cross section}}~(R^{2})}}\\times {\\frac {{\\text{background mass}}~(Nm)}{{\\text{crossing time}}~t_{\\text{ς}}\\approx"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_34",
    "chunk": "{R \\over c_{\\text{ς}}}\\approx {1 \\over {\\sqrt {G(Nm) \\over R^{3}}}\\sim {\\sqrt {G\\rho }}\\sim \\kappa }}}\\\\\\approx &{GM_{\\bullet } \\over Gt_{\\text{ς}}}{GM_{\\bullet } \\over G(Nm)}\\approx (\\rho c_{\\text{ς}})\\left({GM_{\\bullet } \\over c_{\\text{ς}}^{2}}\\right)^{2},~~{\\text{if consider only gravitationally focusing,}}\\\\\\approx &{M_{\\bullet } \\over Nt_{\\text{ς}}},~~{\\text{if for a light perturber}}M_{\\bullet }\\rightarrow m=M_{\\odot }\\\\\\rightarrow &0,~~{\\text{if practically collisionless}}~~N\\rightarrow \\infty ,\\end{aligned}}} where we have applied the relations motion-balancing-gravity. In the limit the perturber is just 1 of the N background particle, M ∙ → m {\\displaystyle M_{\\bullet }\\rightarrow m} , this friction time is identified with the (gravitational) Relaxation time. Again all Coulomb logarithm etc are suppressed without changing the estimations from these qualitative equations. For the rest of Stellar dynamics, we will consistently work on precise calculations through primarily Worked Examples, by neglecting gravitational friction and relaxation of the perturber, working in the limit N → ∞ {\\displaystyle N\\rightarrow \\infty } as approximated true in most galaxies on the 14Gyrs Hubble time scale, even though this is sometimes violated for some clusters of stars or clusters of galaxies.of the cluster. A concise 1-page summary of some main equations in Stellar dynamics and Accretion disc physics are shown here, where one attempts to be more rigorous on the qualitative equations above. The statistical nature of"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_35",
    "chunk": "stellar dynamics originates from the application of the kinetic theory of gases to stellar systems by physicists such as James Jeans in the early 20th century. The Jeans equations, which describe the time evolution of a system of stars in a gravitational field, are analogous to Euler's equations for an ideal fluid, and were derived from the collisionless Boltzmann equation. This was originally developed by Ludwig Boltzmann to describe the non-equilibrium behavior of a thermodynamic system. Similarly to statistical mechanics, stellar dynamics make use of distribution functions that encapsulate the information of a stellar system in a probabilistic manner. The single particle phase-space distribution function, f ( x , v , t ) {\\displaystyle f(\\mathbf {x} ,\\mathbf {v} ,t)} , is defined in a way such that f ( x , v , t ) d x d v = d N {\\displaystyle f(\\mathbf {x} ,\\mathbf {v} ,t)\\,d\\mathbf {x} \\,d\\mathbf {v} =dN} where d N / N {\\displaystyle dN/N} represents the probability of finding a given star with position x {\\displaystyle \\mathbf {x} } around a differential volume d x {\\displaystyle d\\mathbf {x} } and velocity v {\\displaystyle {\\text{v}}} around a differential velocity space volume d v {\\displaystyle d\\mathbf {v}"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_36",
    "chunk": "} . The distribution function is normalized (sometimes) such that integrating it over all positions and velocities will equal N, the total number of bodies of the system. For collisional systems, Liouville's theorem is applied to study the microstate of a stellar system, and is also commonly used to study the different statistical ensembles of statistical mechanics. In most of stellar dynamics literature, it is convenient to adopt the convention that the particle mass is unity in solar mass unit M ⊙ {\\displaystyle M_{\\odot }} , hence a particle's momentum and velocity are identical, i.e., p = m v = v , m = 1 , N total = M total , {\\displaystyle \\mathbf {p} =m\\mathbf {v} =\\mathbf {v} ,~m=1,~N_{\\text{total}}=M_{\\text{total}},} d M d x 3 d v 3 = f ( x , v , t ) = f ( x , p , t ) ≡ d N d x 3 d p 3 {\\displaystyle {dM \\over dx^{3}dv^{3}}=f(\\mathbf {x} ,\\mathbf {v} ,t)=f(\\mathbf {x} ,\\mathbf {p} ,t)\\equiv {dN \\over dx^{3}dp^{3}}} For example, the thermal velocity distribution of air molecules (of typically 15 times the proton mass per molecule) in a room of constant temperature T 0 ∼ 300 K {\\displaystyle"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_37",
    "chunk": "T_{0}\\sim \\mathrm {300K} } would have a Maxwell distribution f Max ( x , y , z , m V x , m V y , m V z ) = 1 ( 2 π ℏ ) 3 1 exp ⁡ ( E ( x , y , z , p x , p y , p z ) − μ k T 0 ) + 1 {\\displaystyle f^{\\text{Max}}(x,y,z,mV_{x},mV_{y},mV_{z})={1 \\over (2\\pi \\hbar )^{3}}{1 \\over \\exp \\left({E(x,y,z,p_{x},p_{y},p_{z})-\\mu \\over kT_{0}}\\right)+1}} f Max ∼ 1 ( 2 π ℏ / m ) 3 e μ k T 0 e − E m σ 1 2 , {\\displaystyle f^{\\text{Max}}\\sim {1 \\over (2\\pi \\hbar /m)^{3}}e^{\\mu \\over kT_{0}}e^{-E \\over m\\sigma _{1}^{2}},} where the energy per unit mass E / m = Φ ( x , y , z ) + ( V x 2 + V y 2 + V z 2 ) / 2 , {\\displaystyle E/m=\\Phi (x,y,z)+(V_{x}^{2}+V_{y}^{2}+V_{z}^{2})/2,} where Φ ( x , y , z ) ≡ g 0 z = 0 {\\displaystyle \\Phi (x,y,z)\\equiv g_{0}z=0} and σ 1 = k T 0 / m ∼ 0.3 k m / s {\\textstyle \\sigma _{1}={\\sqrt {kT_{0}/m}}\\sim \\mathrm {0.3km/s} } is the width of the velocity"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_38",
    "chunk": "Maxwell distribution, identical in each direction and everywhere in the room, and the normalisation constant e μ k T 0 {\\displaystyle e^{\\mu \\over kT_{0}}} (assume the chemical potential μ ∼ ( m σ 1 2 ) ln ⁡ [ n 0 ( 2 π ℏ m σ 1 ) 3 ] ≪ 0 {\\textstyle \\mu \\sim (m\\sigma _{1}^{2})\\ln \\left[n_{0}\\left({{\\sqrt {2\\pi }}\\hbar \\over m\\sigma _{1}}\\right)^{3}\\right]\\ll 0} such that the Fermi-Dirac distribution reduces to a Maxwell velocity distribution) is fixed by the constant gas number density n 0 = n ( x , y , 0 ) {\\displaystyle n_{0}=n(x,y,0)} at the floor level, where n ( x , y , 0 ) = ∫ − ∞ ∞ m d V x ∫ − ∞ ∞ m d V y ∫ − ∞ ∞ m d V z f ( x , y , 0 , m V x , m V y , m V z ) {\\displaystyle n(x,y,0)=\\!\\!\\int _{-\\infty }^{\\infty }mdV_{x}\\!\\!\\int _{-\\infty }^{\\infty }mdV_{y}\\!\\!\\int _{-\\infty }^{\\infty }mdV_{z}f(x,y,0,mV_{x},mV_{y},mV_{z})} n ≈ ( 2 π ) 3 / 2 ( m σ 1 ) 3 ( 2 π ℏ ) 3 e μ m σ 1 2 . {\\displaystyle n\\approx {(2\\pi )^{3/2}(m\\sigma _{1})^{3} \\over"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_39",
    "chunk": "(2\\pi \\hbar )^{3}}e^{\\mu \\over m\\sigma _{1}^{2}}.} In plasma physics, the collisionless Boltzmann equation is referred to as the Vlasov equation, which is used to study the time evolution of a plasma's distribution function. The Boltzmann equation is often written more generally with the Liouville operator L {\\displaystyle {\\mathcal {L}}} as L f ( t , x , p ) = f fit Max − f ( t , x , p ) t relax , {\\displaystyle {\\mathcal {L}}f(t,\\mathbf {x} ,\\mathbf {p} )={f_{\\text{fit}}^{\\text{Max}}-f(t,\\mathbf {x} ,\\mathbf {p} ) \\over t_{\\text{relax}}},} L ≡ ∂ ∂ t + p m ⋅ ∇ + F ⋅ ∂ ∂ p . {\\displaystyle {\\mathcal {L}}\\equiv {\\frac {\\partial }{\\partial t}}+{\\frac {\\mathbf {p} }{m}}\\cdot \\nabla +\\mathbf {F} \\cdot {\\frac {\\partial }{\\partial \\mathbf {p} }}\\,.} where F ≡ p ˙ = − m ∇ Φ {\\displaystyle \\mathbf {F} \\equiv \\mathbf {\\dot {p}} =-m\\nabla \\Phi } is the gravitational force and f fit Max {\\displaystyle f_{\\text{fit}}^{\\text{Max}}} is the Maxwell (equipartition) distribution (to fit the same density, same mean and rms velocity as f ( t , x , p ) {\\displaystyle f(t,\\mathbf {x} ,\\mathbf {p} )} ). The equation means the non-Gaussianity will decay on a (relaxation) time scale of t"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_40",
    "chunk": "relax {\\displaystyle t_{\\text{relax}}} , and the system will ultimately relaxes to a Maxwell (equipartition) distribution. Whereas Jeans applied the collisionless Boltzmann equation, along with Poisson's equation, to a system of stars interacting via the long range force of gravity, Anatoly Vlasov applied Boltzmann's equation with Maxwell's equations to a system of particles interacting via the Coulomb Force. Both approaches separate themselves from the kinetic theory of gases by introducing long-range forces to study the long term evolution of a many particle system. In addition to the Vlasov equation, the concept of Landau damping in plasmas was applied to gravitational systems by Donald Lynden-Bell to describe the effects of damping in spherical stellar systems. A nice property of f(t,x,v) is that many other dynamical quantities can be formed by its moments, e.g., the total mass, local density, pressure, and mean velocity. Applying the collisionless Boltzmann equation, these moments are then related by various forms of continuity equations, of which most notable are the Jeans equations and Virial theorem. Jeans computed the weighted velocity of the Boltzmann Equation after integrating over velocity space 1 ρ p ∫ { v p d [ f p m p ] d t − ⟨ v"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_41",
    "chunk": "⟩ p d [ f p m p ] d t } d 3 v = 0 , {\\displaystyle {1 \\over \\rho _{p}}\\int \\!\\left\\{\\mathbf {v} _{p}{d[f_{p}m_{p}] \\over dt}-\\langle {\\mathbf {v} }\\rangle _{p}{d[f_{p}m_{p}] \\over dt}\\right\\}d^{3}\\mathbf {v} =0,} and obtain the Momentum (Jeans) Eqs. of a p {\\displaystyle ^{p}} opulation (e.g., gas, stars, dark matter): ( ∂ ∂ t + ∑ j = 1 3 ⟨ v j p ⟩ ∂ ∂ x j ) ⟨ v i p ⟩ ⏞ ⟨ v ⟩ ˙ i p = ⏟ E o M − ∂ Φ ( t , x ) ∂ x i ⏞ g i ∼ O ( − G M / R 2 ) − ⏟ balance pressure ∑ j = 1 3 ∂ ρ p ∂ x j [ ρ p ( t , x ) ⏟ ∫ ∞ m p f p d 3 v σ j i p ( t , x ) ⏟ O ( c s 2 ) ] ⏞ ∫ ∞ d v 3 ( v j − ⟨ v ⟩ j p ) ( v i − ⟨ v ⟩ i p ) m p f p − ⟨ v i p ⟩"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_42",
    "chunk": "[ m ˙ p / m p ] ⏞ 1 / t | visc m p = M gas fric ⏟ snow.plough , 0 = − ∂ Φ ( t , x ) ∂ x i − ∂ ( n σ 2 ) n ∂ x i , hydrostatic isotropic velocity, no flow and friction . {\\displaystyle {\\begin{aligned}\\overbrace {\\left({\\partial \\over \\partial t}+\\sum _{j=1}^{3}\\langle {v_{j}^{p}}\\rangle {\\partial \\over \\partial x_{j}}\\right)\\langle {v_{i}^{p}}\\rangle } ^{{\\dot {\\langle {v}\\rangle }}_{i}^{p}}&\\underbrace {=} _{EoM}\\overbrace {-\\partial \\Phi (t,\\mathbf {x} ) \\over \\partial x_{i}} ^{g_{i}\\sim O(-GM/R^{2})}~~\\underbrace {-} _{\\text{balance}}^{\\text{pressure}}~~\\sum _{j=1}^{3}{\\partial \\over \\rho ^{p}\\partial x_{j}}\\overbrace {[\\underbrace {\\rho ^{p}(t,\\mathbf {x} )} _{\\int _{\\infty }\\!\\!\\!\\!m_{p}f_{p}d^{3}\\mathbf {v} }\\underbrace {\\sigma _{ji}^{p}(t,\\mathbf {x} )} _{O(c_{s}^{2})}]} ^{\\int \\limits _{\\infty }\\!\\!d\\mathbf {v} ^{3}(\\mathbf {v} _{j}-\\langle {v}\\rangle _{j}^{p})(\\mathbf {v} _{i}-\\langle {v}\\rangle _{i}^{p})m_{p}f_{p}}-{\\underbrace {\\langle {v_{i}^{p}}\\rangle \\overbrace {[{\\dot {m}}_{p}/m_{p}]} ^{1/t|_{{\\text{visc}}~m_{p}=M_{\\text{gas}}}^{\\text{fric}}}} _{\\text{snow.plough}}},\\\\0&=-{\\partial \\Phi (t,\\mathbf {x} ) \\over \\partial x_{i}}-{\\partial (n\\sigma ^{2}) \\over n\\partial x_{i}},~~{\\text{hydrostatic isotropic velocity, no flow and friction }}.\\end{aligned}}} The general version of Jeans equation, involving (3 x 3) velocity moments is cumbersome. It only becomes useful or solvable if we could drop some of these moments, especially drop the off-diagonal cross terms for systems of high symmetry, and also drop net rotation or net inflow speed everywhere. The isotropic version"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_43",
    "chunk": "is also called Hydrostatic equilibrium equation where balancing pressure gradient with gravity; the isotropic version works for axisymmetric disks as well, after replacing the derivative dr with vertical coordinate dz. It means that we could measure the gravity (of dark matter) by observing the gradients of the velocity dispersion and the number density of stars. Stellar dynamics is primarily used to study the mass distributions within stellar systems and galaxies. Early examples of applying stellar dynamics to clusters include Albert Einstein's 1921 paper applying the virial theorem to spherical star clusters and Fritz Zwicky's 1933 paper applying the virial theorem specifically to the Coma Cluster, which was one of the original harbingers of the idea of dark matter in the universe. The Jeans equations have been used to understand different observational data of stellar motions in the Milky Way galaxy. For example, Jan Oort utilized the Jeans equations to determine the average matter density in the vicinity of the solar neighborhood, whereas the concept of asymmetric drift came from studying the Jeans equations in cylindrical coordinates. Stellar dynamics also provides insight into the structure of galaxy formation and evolution. Dynamical models and observations are used to study the triaxial structure"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_44",
    "chunk": "of elliptical galaxies and suggest that prominent spiral galaxies are created from galaxy mergers. Stellar dynamical models are also used to study the evolution of active galactic nuclei and their black holes, as well as to estimate the mass distribution of dark matter in galaxies. Consider an oblate potential in cylindrical coordinates Φ ( R , z ) = G M 0 2 z 0 [ 2 sinh − 1 Q − sinh − 1 Q + − sinh − 1 Q − ] = G M 0 2 z 0 log ⁡ ( 1 + Q 2 + Q ) 2 [ 1 + Q + 2 + Q + ] [ 1 + Q − 2 + Q − ] , Q ± ≡ R 0 + | | z | ± z 0 | R , Q ≡ R 0 + [ 0 , | z | − z 0 ] max R , {\\displaystyle {\\begin{aligned}\\Phi (R,z)&={GM_{0} \\over 2z_{0}}\\left[2\\sinh ^{-1}\\!\\!Q-\\sinh ^{-1}\\!\\!Q_{+}-\\sinh ^{-1}\\!\\!Q_{-}\\right]\\\\&={GM_{0} \\over 2z_{0}}\\log {({\\sqrt {1+Q^{2}}}+Q)^{2} \\over \\left[{\\sqrt {1+Q_{+}^{2}}}+Q_{+}\\right]\\left[{\\sqrt {1+Q_{-}^{2}}}+Q_{-}\\right]},\\\\Q_{\\pm }&\\equiv {R_{0}+\\left|~|z|\\pm z_{0}~\\right| \\over R},\\\\Q&\\equiv {R_{0}+[0,|z|-z_{0}]_{\\max } \\over R},\\\\\\end{aligned}}} where z 0 , R 0 {\\displaystyle z_{0},R_{0}} are (positive) vertical and radial length scales. Despite its complexity,"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_45",
    "chunk": "we can easily see some limiting properties of the model. First we can see the total mass of the system is M 0 {\\displaystyle M_{0}} because Φ ( R , z ) → G M 0 2 z 0 ( 2 Q − − Q − − Q + ) = − G M 0 R , {\\displaystyle \\Phi (R,z)\\rightarrow {GM_{0} \\over 2z_{0}}(2Q_{-}-Q_{-}-Q_{+})=-{GM_{0} \\over R},} when we take the large radii limit R → ∞ , | z | ≥ z 0 , {\\displaystyle R\\rightarrow \\infty ,~|z|\\geq z_{0},} , so that Q = Q − = Q + − 2 z 0 R = | z | + ( R 0 − z 0 ) R → 0. {\\displaystyle Q=Q_{-}=Q_{+}-{2z_{0} \\over R}={|z|+(R_{0}-z_{0}) \\over R}\\rightarrow 0.} We can also show that some special cases of this unified potential become the potential of the Kuzmin razor-thin disk, that of the Point mass M 0 {\\displaystyle M_{0}} , and that of a uniform-Needle mass distribution: Φ K M ( R , z ) = − G M 0 R 2 + ( | z | + R 0 ) 2 , z 0 = 0 , {\\displaystyle \\Phi _{KM}(R,z)=-{GM_{0} \\over {\\sqrt {R^{2}+(|z|+R_{0})^{2}}}},~~z_{0}=0,} Φ"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_46",
    "chunk": "P T ( R , z ) = − G M 0 R 2 + z 2 , z 0 = R 0 = 0 , {\\displaystyle \\Phi _{PT}(R,z)=-{GM_{0} \\over {\\sqrt {R^{2}+z^{2}}}},~~z_{0}=R_{0}=0,} Φ U N R 0 = 0 ( R , z ) = G M 0 2 z 0 [ 2 sinh − 1 ( 0 , | z | − z 0 ) max R − sinh − 1 z 0 + | z | R − sinh − 1 | z 0 − | z | | R ] . {\\displaystyle \\Phi _{UN}^{R_{0}=0}(R,z)={GM_{0} \\over 2z_{0}}\\left[2\\sinh ^{-1}\\!\\!{(0,|z|-z_{0})_{\\max } \\over R}-\\sinh ^{-1}\\!\\!{z_{0}+|z| \\over R}-\\sinh ^{-1}\\!\\!{\\left|~z_{0}-|z|~\\right| \\over R}\\right].} First consider the vertical gravity at the boundary, g z ( R , z ) = − ∂ z Φ ( R , z ) = − G M 0 z 2 z 0 2 [ 1 R 0 2 + R 2 − 1 ( R 0 + 2 z 0 ) 2 + R 2 ] , z = ± z 0 , {\\displaystyle g_{z}(R,z)=-\\partial _{z}\\Phi (R,z)=-{GM_{0}z \\over 2z_{0}^{2}}\\left[{1 \\over {\\sqrt {R_{0}^{2}+R^{2}}}}-{1 \\over {\\sqrt {(R_{0}+2z_{0})^{2}+R^{2}}}}\\right],~~z=\\pm z_{0},} Note that both the potential and the vertical gravity are continuous across the"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_47",
    "chunk": "boundaries, hence no razor disk at the boundaries. Thanks to the fact that at the boundary, ∂ | z | ( 2 Q ) − ∂ | z | Q − = ∂ | z | ( Q + − 2 z 0 R ) = 1 R {\\displaystyle \\partial _{|z|}(2Q)-\\partial _{|z|}Q_{-}=\\partial _{|z|}\\left(Q_{+}-{\\frac {2z_{0}}{R}}\\right)={1 \\over R}} is continuous. Apply Gauss's theorem by integrating the vertical force over the entire disk upper and lower boundaries, we have 2 ∫ 0 ∞ ( 2 π R d R ) | g z ( R , z 0 ) | = 4 π G M 0 , {\\displaystyle 2\\int _{0}^{\\infty }(2\\pi RdR)|g_{z}(R,z_{0})|=4\\pi GM_{0},} confirming that M 0 {\\displaystyle M_{0}} takes the meaning of the total disk mass. The vertical gravity drops with − g z → G M 0 z ( 1 + R 0 / z 0 ) / R 3 {\\displaystyle -g_{z}\\rightarrow GM_{0}z(1+R_{0}/z_{0})/R^{3}} at large radii, which is enhanced over the vertical gravity of a point mass G M 0 z / R 3 {\\displaystyle GM_{0}z/R^{3}} due to the self-gravity of the thick disk. Insert in the cylindrical Poisson eq. ρ ( R , z ) = ∂ z ∂ z"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_48",
    "chunk": "Φ 4 π G + ∂ R ( R ∂ R Φ ) 4 π G R = M 0 R 0 / z 0 4 π ( R 2 + R 0 2 ) 3 / 2 H ( z 0 − | z | ) , {\\displaystyle \\rho (R,z)={\\partial _{z}\\partial _{z}\\Phi \\over 4\\pi G}+{\\partial _{R}(R\\partial _{R}\\Phi ) \\over 4\\pi GR}={M_{0}R_{0}/z_{0} \\over 4\\pi (R^{2}+R_{0}^{2})^{3/2}}H(z_{0}-|z|),} which drops with radius, and is zero beyond | z | > z 0 {\\displaystyle |z|>z_{0}} and uniform along the z-direction within the boundary. Integrating over the entire thick disc of uniform thickness 2 z 0 {\\displaystyle 2z_{0}} , we find the surface density and the total mass as Σ ( R ) = ( 2 z 0 ) ρ ( R , 0 ) , M 0 = ∫ 0 ∞ ( 2 π R d R ) Σ ( R ) . {\\displaystyle \\Sigma (R)=(2z_{0})\\rho (R,0),~~M_{0}=\\int _{0}^{\\infty }(2\\pi RdR)\\Sigma (R).} This confirms that the absence of extra razor thin discs at the boundaries. In the limit, z 0 → 0 {\\displaystyle z_{0}\\rightarrow 0} , this thick disc potential reduces to that of a razor-thin Kuzmin disk, for which we can verify | g"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_49",
    "chunk": "z ( R , 0 + ) | 2 π G → Σ ( R ) → M 0 R 0 2 π ( R 2 + R 0 2 ) 3 / 2 {\\displaystyle {|g_{z}(R,0+)| \\over 2\\pi G}\\rightarrow \\Sigma (R)\\rightarrow {M_{0}R_{0} \\over 2\\pi (R^{2}+R_{0}^{2})^{3/2}}} . To find the vertical and radial oscillation frequencies, we do a Taylor expansion of potential around the midplane. Φ ( R 1 , z ) ≈ Φ ( R , 0 ) + ω 2 R ( R 1 − R ) + κ 2 2 ( R 1 − R ) 2 + ν 2 2 z 2 {\\displaystyle \\Phi (R_{1},z)\\approx \\Phi (R,0)+{\\omega ^{2}R}(R_{1}-R)+{\\kappa ^{2} \\over 2}(R_{1}-R)^{2}+{\\nu ^{2} \\over 2}z^{2}} and we find the circular speed V cir {\\displaystyle V_{\\text{cir}}} and the vertical and radial epicycle frequencies to be given by ( R ω ) 2 ≡ V cir 2 = [ ( 1 + R 0 / z 0 ) G M 0 R 2 + ( R 0 + z 0 ) 2 − ( R 0 / z 0 ) G M 0 R 2 + R 0 2 ] , {\\displaystyle (R\\omega )^{2}\\equiv V_{\\text{cir}}^{2}=\\left[{(1+R_{0}/z_{0})GM_{0} \\over {\\sqrt {R^{2}+(R_{0}+z_{0})^{2}}}}-{(R_{0}/z_{0})GM_{0} \\over {\\sqrt"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_50",
    "chunk": "{R^{2}+R_{0}^{2}}}}\\right],} ν 2 = G M 0 ( R 0 / z 0 + 1 ) ( R 2 + ( R 0 + z 0 ) 2 ) 3 / 2 , {\\displaystyle \\nu ^{2}={GM_{0}(R_{0}/z_{0}+1) \\over (R^{2}+(R_{0}+z_{0})^{2})^{3/2}},} κ 2 + ν 2 − 2 ω 2 = 4 π G ρ ( R , 0 ) = G M 0 R 0 / z 0 ( R 2 + R 0 2 ) 3 / 2 . {\\displaystyle \\kappa ^{2}+\\nu ^{2}-2\\omega ^{2}=4\\pi G\\rho (R,0)={GM_{0}R_{0}/z_{0} \\over (R^{2}+R_{0}^{2})^{3/2}}.} Interestingly the rotation curve V cir {\\displaystyle V_{\\text{cir}}} is solid-body-like near the centre R ≪ R 0 {\\displaystyle R\\ll R_{0}} , and is Keplerian far away. At large radii three frequencies satisfy [ ω , ν , κ , 4 π G ρ ] | R → ∞ → [ 1 , 1 + R 0 / z 0 , 1 , R 0 / z 0 ] 1 2 G M 0 R 3 {\\textstyle \\left.\\left[\\omega ,\\nu ,\\kappa ,{\\sqrt {4\\pi G\\rho }}\\right]\\right|_{R\\to \\infty }\\to [1,1+R_{0}/z_{0},1,R_{0}/z_{0}]^{1 \\over 2}{\\sqrt {GM_{0} \\over R^{3}}}} . E.g., in the case that R → ∞ {\\displaystyle R\\to \\infty } and R 0 / z 0 = 3 {\\displaystyle"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_51",
    "chunk": "R_{0}/z_{0}=3} , the oscillations ω : ν : κ = 1 : 2 : 1 {\\displaystyle \\omega :\\nu :\\kappa =1:2:1} forms a resonance. In the case that R 0 = 0 {\\displaystyle R_{0}=0} , the density is zero everywhere except uniform needle between | z | ≤ z 0 {\\displaystyle |z|\\leq z_{0}} along the z-axis. If we further require z 0 = 0 {\\displaystyle z_{0}=0} , then we recover a well-known property for closed ellipse orbits in point mass potential, ω : ν : κ = 1 : 1 : 1. {\\displaystyle \\omega :\\nu :\\kappa =1:1:1.} For example, the phase space distribution function of non-relativistic neutrinos of mass m anywhere will not exceed the maximum value set by f ( x , v , t ) = d N d x 3 d v 3 ≤ 6 ( 2 π ℏ / m ) 3 , {\\displaystyle f(\\mathbf {x} ,\\mathbf {v} ,t)={dN \\over dx^{3}dv^{3}}\\leq {6 \\over (2\\pi \\hbar /m)^{3}},~~~} where the Fermi-Dirac statistics says there are at most 6 flavours of neutrinos within a volume d x 3 {\\displaystyle dx^{3}} and a velocity volume d v 3 = ( d p / m ) 3 = [ ( 2 π"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_52",
    "chunk": "ℏ / d x ) / m ] 3 , {\\displaystyle dv^{3}=(dp/m)^{3}=[(2\\pi \\hbar /dx)/m]^{3},} . Let's approximate the distribution is at maximum, i.e., f ( x , y , z , V x , V y , V z ) = 6 ( 2 π ℏ / m ) 3 q α 2 , 0 ≤ q ( E ) = Φ max − E V 0 2 / 2 ≤ 1 , {\\displaystyle f(x,y,z,V_{x},V_{y},V_{z})={6 \\over (2\\pi \\hbar /m)^{3}}q^{\\alpha \\over 2},~~0\\leq q(E)={\\Phi _{\\max }-E \\over V_{0}^{2}/2}\\leq 1,} where 0 ≥ Φ max ≥ E = Φ ( x , y , z ) + V x 2 + V y 2 + V z 2 2 ≥ Φ min ≡ Φ max − V 0 2 2 {\\displaystyle 0\\geq \\Phi _{\\max }\\geq E=\\Phi (x,y,z)+{V_{x}^{2}+V_{y}^{2}+V_{z}^{2} \\over 2}\\geq \\Phi _{\\min }\\equiv \\Phi _{\\max }-{V_{0}^{2} \\over 2}} such that E min , E max {\\displaystyle E_{\\min },E_{\\max }} , respectively, is the potential energy of at the centre or the edge of the gravitational bound system. The corresponding neutrino mass density, assume spherical, would be ρ ( r ) = n ( x , y , z ) m = ∫ d V"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_53",
    "chunk": "x ∫ d V y ∫ d V z m f ( x , y , z , V x , V y , V z ) , {\\displaystyle \\rho (r)=n(x,y,z)m=\\int dV_{x}\\int dV_{y}\\int dV_{z}~m~f(x,y,z,V_{x},V_{y},V_{z}),} which reduces to ρ ( r ) = C ( Φ max − Φ ( r ) ) 3 + α 2 ( Φ max − Φ min ) α 2 , C = 6 m π 2 5 / 2 B ( 1 + α 2 , 3 2 ) ( 2 π ℏ / m ) 3 {\\displaystyle \\rho (r)={C(\\Phi _{\\max }-\\Phi (r))^{3+\\alpha \\over 2} \\over (\\Phi _{\\max }-\\Phi _{\\min })^{\\alpha \\over 2}},~~~C={6m\\pi 2^{5/2}B\\left(1+{\\alpha \\over 2},{3 \\over 2}\\right) \\over (2\\pi \\hbar /m)^{3}}} Take the simple case α → 0 {\\displaystyle \\alpha \\to 0} , and estimate the density at the centre r = 0 {\\displaystyle r=0} with an escape speed V 0 {\\displaystyle V_{0}} , we have ρ ( r ) ≤ ρ ( 0 ) → m 4 V 0 3 π 2 ℏ 3 ≈ m e V 4 V 200 3 × [Cosmic Critical Density] . {\\displaystyle \\rho (r)\\leq \\rho (0)\\rightarrow {m^{4}V_{0}^{3} \\over \\pi ^{2}\\hbar ^{3}}\\approx m_{\\mathrm {eV} }^{4}V_{200}^{3}\\times {\\text{[Cosmic Critical"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_54",
    "chunk": "Density]}}.} Clearly eV-scale neutrinos with m e V ∼ 0.1 − 1 {\\displaystyle m_{eV}\\sim 0.1-1} is too light to make up the 100–10000 over-density in galaxies with escape velocity V 200 ≡ V / ( 200 k m / s ) ∼ 0.1 − 3.4 {\\displaystyle V_{200}\\equiv V/(\\mathrm {200km/s} )\\sim 0.1-3.4} , while neutrinos in clusters with V ∼ 2000 k m / s {\\displaystyle V\\sim \\mathrm {2000km/s} } could make up 100 − 1000 {\\displaystyle 100-1000} times cosmic background density. By the way the freeze-out cosmic neutrinos in your room have a non-thermal random momentum ∼ ( 2.7 K ) k c ∼ ( 1 e V / c 2 ) ( 70 k m / s ) {\\textstyle \\sim {(\\mathrm {2.7K} )k \\over c}\\sim (1~\\mathrm {eV} /c^{2})(\\mathrm {70km/s} )} , and do not follow a Maxwell distribution, and are not in thermal equilibrium with the air molecules because of the extremely low cross-section of neutrino-baryon interactions. Consider building a steady state model of the fore-mentioned uniform sphere of density ρ 0 {\\displaystyle \\rho _{0}} and potential Φ ( r ) {\\displaystyle \\Phi (r)} ρ ( | r | ) = ρ 0 ≡ M ⊙ n 0 ,"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_55",
    "chunk": "| r | 2 = x 2 + y 2 + z 2 ≤ r 0 2 , Ω ≡ 4 π G ρ 0 3 ≡ V 0 r 0 Φ ( | r | ) = Ω 2 ( x 2 + y 2 + z 2 ) − 3 V 0 2 2 = V e ( r ) 2 2 − Φ ( r 0 ) , {\\displaystyle {\\begin{aligned}\\rho (|\\mathbf {r} |)&=\\rho _{0}\\equiv M_{\\odot }n_{0},~~|\\mathbf {r} |^{2}=x^{2}+y^{2}+z^{2}\\leq r_{0}^{2},~~\\Omega \\equiv {\\sqrt {4\\pi G\\rho _{0} \\over 3}}\\equiv {V_{0} \\over r_{0}}\\\\\\Phi (|\\mathbf {r} |)&={\\Omega ^{2}(x^{2}+y^{2}+z^{2})-3V_{0}^{2} \\over 2}={V_{e}(r)^{2} \\over 2}-\\Phi (r_{0}),\\end{aligned}}} where V e ( r ) = V 0 1 − r 2 r 0 2 = 2 Φ ( r 0 ) − 2 Φ ( r ) {\\displaystyle V_{e}(r)=V_{0}{\\sqrt {1-{r^{2} \\over r_{0}^{2}}}}={\\sqrt {2\\Phi (r_{0})-2\\Phi (r)}}} is the speed to escape to the edge r 0 {\\displaystyle r_{0}} . First a recap on motion \"inside\" the uniform sphere potential. Inside this constant density core region, individual stars go on resonant harmonic oscillations of angular frequency Ω {\\displaystyle \\Omega } with x ¨ = − Ω 2 x = − ∂ x Φ , y ¨ = − Ω"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_56",
    "chunk": "2 y , y ˙ ( t ) 2 2 + Ω 2 y ( t ) 2 2 ≡ I y ( y , y ˙ ) = y ˙ ( 0 ) 2 2 + Ω 2 y ( 0 ) 2 2 ≤ ( Ω r 0 ) 2 2 z ¨ = − Ω 2 z , → z ˙ ( t ) = z ˙ ( 0 ) cos ⁡ ( Ω t ) + Ω z ( 0 ) sin ⁡ ( Ω t ) . {\\displaystyle {\\begin{aligned}{\\ddot {x}}=&-\\Omega ^{2}x=-\\partial _{x}\\Phi ,\\\\{\\ddot {y}}=&-\\Omega ^{2}y,~~~{{\\dot {y}}(t)^{2} \\over 2}+{\\Omega ^{2}y(t)^{2} \\over 2}\\equiv I_{y}(y,{\\dot {y}})={{\\dot {y}}(0)^{2} \\over 2}+{\\Omega ^{2}y(0)^{2} \\over 2}\\leq {(\\Omega r_{0})^{2} \\over 2}\\\\{\\ddot {z}}=&-\\Omega ^{2}z,\\rightarrow {\\dot {z}}(t)={\\dot {z}}(0)\\cos(\\Omega t)+\\Omega z(0)\\sin(\\Omega t).\\end{aligned}}} Loosely speaking our goal is to put stars on a weighted distribution of orbits with various energies f ( I x ( x , x ˙ ) , I y ( y , y ˙ ) , I z ( z , z ˙ ) = D F ( r , V ) {\\displaystyle f\\left(I_{x}(x,{\\dot {x}}),I_{y}(y,{\\dot {y}}),I_{z}(z,{\\dot {z}}\\right)=DF(\\mathbf {r} ,\\mathbf {V} )} , i.e., the phase space density or distribution function, such that their"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_57",
    "chunk": "overall stellar number density reproduces the constant core, hence their collective \"steady-state\" potential. Once this is reached, we call the system is a self-consistent equilibrium. Generally for a time-independent system, Jeans theorem predicts that f ( x , v ) {\\displaystyle f(\\mathbf {x} ,\\mathbf {v} )} is an implicit function of the position and velocity through a functional dependence on \"constants of motion\". For the uniform sphere, a solution for the Boltzmann Equation, written in spherical coordinates ( r , θ , ϕ ) {\\displaystyle (r,\\theta ,\\phi )} and its velocity components ( V r , V θ , V ϕ ) {\\displaystyle (V_{r},V_{\\theta },V_{\\phi })} is f ( r , θ , φ , V r , V θ , V φ ) = C 0 V 0 3 V 0 2 2 Q , {\\displaystyle f(r,\\theta ,\\varphi ,V_{r},V_{\\theta },V_{\\varphi })={C_{0} \\over V_{0}^{3}}{\\sqrt {V_{0}^{2} \\over 2Q}},} where C 0 = 2 π − 2 ρ 0 {\\displaystyle C_{0}=2\\pi ^{-2}\\rho _{0}} is a normalisation constant, which has the dimension of (mass) density. And we define a (positive enthalpy-like dimension km 2 / s 2 {\\displaystyle {\\text{km}}^{2}/{\\text{s}}^{2}} ) Quantity Q [ x , v ] ≡ [ 0 , ( −"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_58",
    "chunk": "V 0 2 − E ) + J 2 2 r 0 2 ] max [ J z | J z | , 0 ] max . {\\displaystyle Q[\\mathbf {x} ,\\mathbf {v} ]\\equiv \\left[0,\\left(-V_{0}^{2}-E\\right)+{J^{2} \\over 2r_{0}^{2}}\\right]_{\\max }\\left[{J_{z} \\over |J_{z}|},0\\right]_{\\max }.} Clearly anti-clockwise rotating stars with J z ≤ 0 , Q = 0 {\\displaystyle J_{z}\\leq 0,~~Q=0} are excluded. J 2 = r 2 V t 2 = r 2 ( V θ 2 + V φ 2 ) , {\\displaystyle J^{2}=r^{2}V_{t}^{2}=r^{2}(V_{\\theta }^{2}+V_{\\varphi }^{2}),} E = V r 2 + V t 2 2 + Φ ( r ) , V t ≡ V θ 2 + V φ 2 {\\displaystyle E={V_{r}^{2}+V_{t}^{2} \\over 2}+\\Phi (r),~V_{t}\\equiv {\\sqrt {V_{\\theta }^{2}+V_{\\varphi }^{2}}}} Insert the potential and these definitions of the orbital energy E and angular momentum J and its z-component Jz along every stellar orbit, we have 2 Q = Heaviside ( V φ | V φ | ) × [ V 0 2 ( 1 − r 2 r 0 2 ) − V r 2 − ( 1 − r 2 r 0 2 ) ( V θ 2 + V φ 2 ) , 0 ] max , {\\displaystyle 2Q={\\text{Heaviside}}\\left({V_{\\varphi }"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_59",
    "chunk": "\\over |V_{\\varphi }|}\\right)\\times \\left[V_{0}^{2}\\left(1-{r^{2} \\over r_{0}^{2}}\\right)-V_{r}^{2}-\\left(1-{r^{2} \\over r_{0}^{2}}\\right){\\left(V_{\\theta }^{2}+V_{\\varphi }^{2}\\right)},0\\right]_{\\max },} which implies | V r | ≤ V e ( r ) {\\displaystyle |V_{r}|\\leq V_{e}(r)} , and | V θ | , V φ {\\displaystyle |V_{\\theta }|,V_{\\varphi }} between zero and V 0 {\\displaystyle V_{0}} . To verify the above E , J z {\\displaystyle E,~J_{z}} being constants of motion in our spherical potential, we note d E / d t = ∂ E ∂ t + v ∂ E ∂ x + ( − ∇ Φ ) ∂ E ∂ v {\\displaystyle dE/dt={\\partial E \\over \\partial t}+\\mathbf {v} {\\partial E \\over \\partial \\mathbf {x} }+(\\mathbf {-\\nabla \\Phi } ){\\partial E \\over \\partial \\mathbf {v} }} d E / d t = ∂ Φ ∂ t + v ∂ Φ ∂ x + ( − ∇ Φ ) v = ∂ Φ ∂ t = 0 {\\displaystyle dE/dt={\\partial \\Phi \\over \\partial t}+\\mathbf {v} {\\partial \\Phi \\over \\partial \\mathbf {x} }+(\\mathbf {-\\nabla \\Phi } )\\mathbf {v} ={\\partial \\Phi \\over \\partial t}=0} for any \"steady state\" potential. d J z / d t = ∂ J z ∂ t + ∂ J z ∂ x ⋅ v − ( ∇ Φ"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_60",
    "chunk": ") ⋅ ∂ J z ∂ v , {\\displaystyle dJ_{z}/dt={\\partial J_{z} \\over \\partial t}+{\\partial J_{z} \\over \\partial \\mathbf {x} }\\cdot \\mathbf {v} -(\\mathbf {\\nabla \\Phi } )\\cdot {\\partial J_{z} \\over \\partial \\mathbf {v} },} which reduces to d J z / d t = 0 + [ ( V y ) V x + ( − V x ) V y ] − [ ( − y ) x R ∂ Φ ( R , z ) ∂ R + ( x ) y R ∂ Φ ( R , z ) ∂ R ] = 0 {\\displaystyle dJ_{z}/dt=0+[(V_{y})V_{x}+(-V_{x})V_{y}]-\\left[(-y){x \\over R}{\\partial \\Phi (R,z) \\over \\partial R}+(x){y \\over R}{\\partial \\Phi (R,z) \\over \\partial R}\\right]=0} around the z-axis of any axisymmetric potential, where R = x 2 + y 2 {\\textstyle R={\\sqrt {x^{2}+y^{2}}}} . Likewise the x and y components of the angular momentum are also conserved for a spherical potential. Hence d J / d t = 0 {\\displaystyle dJ/dt=0} . So for any time-independent spherical potential (including our uniform sphere model), the orbital energy E and angular momentum J and its z-component Jz along every stellar orbit satisfy d E [ x , v ] / d t = d"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_61",
    "chunk": "J [ x , v ] / d t = d J z [ x , v ] / d t = 0. {\\displaystyle dE[\\mathbf {x} ,\\mathbf {v} ]/dt=dJ[\\mathbf {x} ,\\mathbf {v} ]/dt=dJ_{z}[\\mathbf {x} ,\\mathbf {v} ]/dt=0.} Hence using the chain rule, we have d d t Q ( E [ x , v ] , J [ x , v ] , J z [ x , v ] ) = ∂ Q ∂ E d E d t + ∂ Q ∂ J z d J z d t + ∂ Q ∂ J d J d t = 0 , {\\displaystyle {d \\over dt}Q(E[\\mathbf {x} ,\\mathbf {v} ],J[\\mathbf {x} ,\\mathbf {v} ],J_{z}[\\mathbf {x} ,\\mathbf {v} ])={\\partial Q \\over \\partial E}{dE \\over dt}+{\\partial Q \\over \\partial J_{z}}{dJ_{z} \\over dt}+{\\partial Q \\over \\partial J}{dJ \\over dt}=0,} i.e., d d t f = f ′ ( Q ) d Q [ x , v ] d t = 0 {\\textstyle {d \\over dt}f=f'(Q){dQ[\\mathbf {x} ,\\mathbf {v} ] \\over dt}=0} , so that CBE is satisfied, i.e., our f ( x , v ) = f ( E [ x , v ] , J [ x , v ] ,"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_62",
    "chunk": "J z [ x , v ] ) {\\displaystyle f(\\mathbf {x} ,\\mathbf {v} )=f(E[\\mathbf {x} ,\\mathbf {v} ],J[\\mathbf {x} ,\\mathbf {v} ],J_{z}[\\mathbf {x} ,\\mathbf {v} ])} is a solution to the Collisionless Boltzmann Equation for our static spherical potential. We can find out various moments of the above distribution function, reformatted as with the help of three Heaviside functions, f ( | r | , V r , V θ , V φ ) = C 0 V 0 3 H ( 1 − x ) ( 1 − x 2 ) 1 2 | x ≡ | r | r 0 H ( V φ ) H ( 1 − q ) ( 1 − q ) 1 2 , q ( r , V ) ≡ V r 2 V e ( | r | ) 2 + V θ 2 V 0 2 + V φ 2 V 0 2 , {\\displaystyle f(|\\mathbf {r} |,V_{r},V_{\\theta },V_{\\varphi })={C_{0} \\over V_{0}^{3}}\\left.{{\\text{H}}(1-x) \\over \\left(1-x^{2}\\right)^{1 \\over 2}}\\right|_{x\\equiv {|\\mathbf {r} | \\over r_{0}}}{{\\text{H}}(V_{\\varphi }){\\text{H}}(1-q) \\over (1-q)^{1 \\over 2}},~~q(\\mathbf {r} ,\\mathbf {V} )\\equiv {V_{r}^{2} \\over V_{e}(|\\mathbf {r} |)^{2}}+{V_{\\theta }^{2} \\over V_{0}^{2}}+{V_{\\varphi }^{2} \\over V_{0}^{2}},} once we input the expression for the earlier potential"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_63",
    "chunk": "Φ ( r ) {\\displaystyle \\Phi (r)} inside r ≤ r 0 {\\displaystyle r\\leq r_{0}} , or even better the speed to \"escape from r to the edge\" r 0 {\\displaystyle r_{0}} of a uniform sphere V e ( r ) = V 0 1 − r 2 r 0 2 . {\\displaystyle V_{e}(r)=V_{0}{\\sqrt {1-{r^{2} \\over r_{0}^{2}}}}.} Clearly the factor V e ( | r | ) 2 Q = max [ 0 , 1 1 − q ] {\\displaystyle {V_{e}(|\\mathbf {r} |) \\over {\\sqrt {2Q}}}={\\sqrt {\\max[0,{1 \\over 1-q}]}}} in the DF (distribution function) is well-defined only if Q ≥ 0 → q ≤ 1 {\\displaystyle Q\\geq 0\\rightarrow q\\leq 1} , which implies a narrow range on radius 0 ≤ | r | < r 0 {\\displaystyle 0\\leq |\\mathbf {r} |<r_{0}} and excludes high velocity particles, e.g., V t > V 0 > V e ( r ) {\\displaystyle V_{t}>V_{0}>V_{e}(r)} , from the distribution function (DF, i.e., phase space density). In fact, the positivity carves the ( V φ ≥ 0 {\\displaystyle V_{\\varphi }\\geq 0} ) left-half of an ellipsoid in the [ V r , V θ , V φ ] {\\displaystyle [V_{r},V_{\\theta },V_{\\varphi }]} velocity space (\"velocity ellipsoid\"),"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_64",
    "chunk": "q ( r , V ) ≡ V r 2 V 0 2 ( 1 − r 2 / r 0 2 ) + ( V θ 2 V 0 2 + V φ 2 V 0 2 ) ≡ u r 2 + u θ 2 + u φ 2 ≤ 1 , {\\displaystyle q(\\mathbf {r} ,\\mathbf {V} )\\equiv {V_{r}^{2} \\over V_{0}^{2}(1-r^{2}/r_{0}^{2})}+\\left({V_{\\theta }^{2} \\over V_{0}^{2}}+{V_{\\varphi }^{2} \\over V_{0}^{2}}\\right)\\equiv u_{r}^{2}+u_{\\theta }^{2}+u_{\\varphi }^{2}\\leq 1,} where ( u r , u θ , u φ ) {\\displaystyle (u_{r},u_{\\theta },u_{\\varphi })} is ( V r , V θ , V φ ) {\\displaystyle (V_{r},V_{\\theta },V_{\\varphi })} rescaled by the function V e ( r ) = V 0 1 − r 2 / r 0 2 {\\displaystyle V_{e}(r)=V_{0}{\\sqrt {1-r^{2}/r_{0}^{2}}}} or V 0 {\\displaystyle V_{0}} respectively. The velocity ellipsoid (in this case) has rotational symmetry around the r axis or V r {\\displaystyle V_{r}} axis. It is more squashed (in this case) away from the radial direction, hence more tangentially anisotropic because everywhere V e ( r ) < V 0 {\\displaystyle V_{e}(r)<V_{0}} , except at the origin, where the ellipsoid looks isotropic. Now we compute the moments of the phase space. E.g., the"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_65",
    "chunk": "resulting density (moment) is ρ ( r , θ , φ ) = ∫ − V e ( r ) V e ( r ) d V r ∫ − V 0 V 0 d V θ ∫ 0 V 0 d V φ C 0 V 0 3 ( 2 Q V 0 2 ) − 1 / 2 = ∫ − 1 1 ∫ − 1 1 ∫ 0 1 ( V e d u r ) ( V 0 d u θ ) ( V 0 d u φ ) C 0 V 0 3 ( 1 − r 2 / r 0 2 ) 1 / 2 ( 1 − q ) 1 / 2 | q = u r 2 + u θ 2 + u φ 2 = C 0 ∫ 0 1 ( 1 − u 2 ) − 1 / 2 ( 2 π u 2 d u ) = ρ 0 {\\displaystyle {\\begin{aligned}\\rho (r,\\theta ,\\varphi )&=\\int _{-V_{e}(r)}^{V_{e}(r)}dV_{r}\\int _{-V_{0}}^{V_{0}}dV_{\\theta }\\int _{0}^{V_{0}}dV_{\\varphi }{C_{0} \\over V_{0}^{3}}\\left({2Q \\over V_{0}^{2}}\\right)^{-1/2}\\\\&=\\int _{-1}^{1}\\int _{-1}^{1}\\int _{0}^{1}{(V_{e}du_{r})(V_{0}du_{\\theta })(V_{0}du_{\\varphi })C_{0} \\over V_{0}^{3}(1-r^{2}/r_{0}^{2})^{1/2}(1-q)^{1/2}}\\left.\\right|_{q=u_{r}^{2}+u_{\\theta }^{2}+u_{\\varphi }^{2}}\\\\&=C_{0}{\\int _{0}^{1}(1-u^{2})^{-1/2}(2\\pi u^{2}du)}=\\rho _{0}\\end{aligned}}} is indeed a spherical (angle-independent) and uniform (radius-independent) density inside the edge, where the"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_66",
    "chunk": "normalisation constant C 0 = 2 π − 2 ρ 0 {\\displaystyle C_{0}=2\\pi ^{-2}\\rho _{0}} . The streaming velocity is computed as the weighted mean of the velocity vector ⟨ V ⟩ ( x ) ≡ ∫ f d V 3 V ∫ f d V 3 = 1 ρ ∫ f d V 3 [ V r , V θ , V φ ] C 0 V 0 2 ( 2 Q ) − 1 / 2 = [ ∫ − 1 1 u r . . . d u r , ∫ − 1 1 u θ . . . d u θ , ∫ 0 1 ( 2 d u r ) ∫ 0 1 − u r 2 ( 2 d u θ ) ∫ 0 1 − u r 2 − u θ 2 d u φ u φ V 0 ( 1 − u r 2 − u θ 2 − u φ 2 ) 1 / 2 ∫ 0 1 ( 2 π U d U ) ∫ 0 1 − U 2 d u φ ( 1 − U 2 − u φ 2 ) − 1 / 2 ] = ["
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_67",
    "chunk": "0 , 0 , 4 V 0 3 π ] = V ( x ) ¯ , {\\displaystyle {\\begin{aligned}\\langle \\mathbf {V} \\rangle (\\mathbf {x} )&\\equiv {\\int fd\\mathbf {V} ^{3}\\mathbf {V} \\over \\int fd\\mathbf {V} ^{3}}\\\\&={1 \\over \\rho }\\int fd\\mathbf {V} ^{3}[V_{r},V_{\\theta },V_{\\varphi }]{C_{0}V_{0}^{2}(2Q)^{-1/2}}\\\\&=\\left[{\\int _{-1}^{1}\\!\\!u_{r}...du_{r},~~\\int _{-1}^{1}\\!\\!u_{\\theta }...du_{\\theta },~~\\int _{0}^{1}(2du_{r})\\int _{0}^{\\sqrt {1-u_{r}^{2}}}\\!\\!(2du_{\\theta })\\int _{0}^{\\sqrt {1-u_{r}^{2}-u_{\\theta }^{2}}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!{du_{\\varphi }u_{\\varphi }V_{0} \\over (1-u_{r}^{2}-u_{\\theta }^{2}-u_{\\varphi }^{2})^{1/2}} \\over \\int _{0}^{1}(2\\pi UdU)\\int _{0}^{\\sqrt {1-U^{2}}}du_{\\varphi }(1-U^{2}-u_{\\varphi }^{2})^{-1/2}}\\right]\\\\&=\\left[0,0,{4V_{0} \\over 3\\pi }\\right]={\\overline {\\mathbf {V} (\\mathbf {x} )}},\\end{aligned}}} where the global average (indicated by the overline bar) of flow implies uniform pattern of flat azimuthal rotation, but zero net streaming everywhere in the meridional ( r , θ ) {\\displaystyle (r,\\theta )} plane. Incidentally, the angular momentum global average of this flat-rotation sphere is r × ⟨ V ⟩ ¯ = ∫ 0 r 0 ( ρ 4 π r 2 d r ) M 0 [ 0 , 0 , r ⟨ V φ ⟩ ] = [ 0 , 0 , 3 r 0 4 V φ ¯ ] . {\\displaystyle {\\overline {\\mathbf {r} \\times \\langle \\mathbf {V} \\rangle }}=\\int _{0}^{r_{0}}{(\\rho 4\\pi r^{2}dr) \\over M_{0}}[0,0,r\\langle V_{\\varphi }\\rangle ]=[0,0,{3r_{0} \\over 4}{\\overline {V_{\\varphi }}}].} Note global average of centre of mass does"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_68",
    "chunk": "not change, so V i ( x ) ¯ = 0 {\\displaystyle {\\overline {\\mathbf {V} _{i}(\\mathbf {x} )}}=0} due to global momentum conservation in each rectangular direction i = x , y , z {\\displaystyle i=x,y,z} , and this does not contradict the global non-zero rotation. Likewise thanks to the symmetry of f ( r , θ , φ , V r , V θ , V φ ) = f ( r , θ , ± φ , ± V r , ± V θ , V φ ) {\\displaystyle f(r,\\theta ,\\varphi ,V_{r},V_{\\theta },V_{\\varphi })=f(r,\\theta ,\\pm \\varphi ,\\pm V_{r},\\pm V_{\\theta },V_{\\varphi })} , we have ⟨ ( ± V r ) V φ ⟩ = 0 {\\displaystyle \\langle \\mathbf {(\\pm V_{r})V_{\\varphi }} \\rangle =0} , ⟨ ( ± V θ ) V φ ⟩ = 0 {\\displaystyle ~\\langle \\mathbf {(\\pm V_{\\theta })V_{\\varphi }} \\rangle =0} , ⟨ ( ± V r ) V θ ⟩ = 0 {\\displaystyle ~\\langle \\mathbf {(\\pm V_{r})V_{\\theta }} \\rangle =0} everywhere}. Likewise the rms velocity in the rotation direction is computed by a weighted mean as follows, E.g., ⟨ V φ 2 ⟩ ( | x | ) ≡ ∫ f d V 3"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_69",
    "chunk": "V φ 2 ρ ( | r | ) = ∫ 0 1 ( 2 d u r ) ∫ 0 1 − u r 2 ( 2 d u θ ) ∫ 0 1 − u r 2 − u θ 2 d u φ ( u φ V 0 ) 2 ( 1 − q ) 1 / 2 ∫ 0 1 ( 2 π u 2 d u ) ( 1 − u 2 ) − 1 / 2 = 0.25 V 0 2 = 0.5 ⟨ V t 2 ⟩ = ∫ 0 1 ( 2 d u r ) ∫ 0 1 − u r 2 ( 2 d u φ ) ∫ 0 1 − u r 2 − u φ 2 d u θ ( u θ V 0 ) 2 ( 1 − q ) 1 / 2 ∫ 0 1 ( 2 π u 2 d u ) ( 1 − u 2 ) − 1 / 2 = ⟨ V θ 2 ⟩ ( | x | ) , {\\displaystyle {\\begin{aligned}\\langle \\mathbf {V} _{\\varphi }^{2}\\rangle (|\\mathbf {x} |)&\\equiv {\\int fd\\mathbf {V} ^{3}V_{\\varphi }^{2} \\over \\rho (|\\mathbf {r} |)}\\\\&={\\int _{0}^{1}(2du_{r})\\int _{0}^{\\sqrt"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_70",
    "chunk": "{1-u_{r}^{2}}}(2du_{\\theta })\\int _{0}^{\\sqrt {1-u_{r}^{2}-u_{\\theta }^{2}}}du_{\\varphi }{(u_{\\varphi }V_{0})^{2} \\over (1-q)^{1/2}} \\over \\int _{0}^{1}{(2\\pi u^{2}du)(1-u^{2})^{-1/2}}}\\\\&=0.25V_{0}^{2}=0.5\\langle V_{t}^{2}\\rangle \\\\&={\\!\\!\\int _{0}^{1}(2du_{r})\\!\\!\\int _{0}^{\\sqrt {1-u_{r}^{2}}}(2du_{\\varphi })\\!\\!\\int _{0}^{\\sqrt {1-u_{r}^{2}-u_{\\varphi }^{2}}}du_{\\theta }{(u_{\\theta }V_{0})^{2} \\over (1-q)^{1/2}} \\over \\int _{0}^{1}{(2\\pi u^{2}du)(1-u^{2})^{-1/2}}}\\\\&=\\langle \\mathbf {V} _{\\theta }^{2}\\rangle (|\\mathbf {x} |),\\\\\\end{aligned}}} Here ⟨ V t 2 ⟩ = ⟨ V θ 2 + V φ 2 ⟩ = 0.5 V 0 2 . {\\displaystyle \\langle V_{t}^{2}\\rangle =\\langle V_{\\theta }^{2}+V_{\\varphi }^{2}\\rangle =0.5V_{0}^{2}.} Likewise ⟨ V r 2 ⟩ ( x ) = ∫ 0 1 ( d u φ ) ∫ 0 1 − u φ 2 ( 2 d u θ ) ∫ 0 1 − u φ 2 − u θ 2 ( 2 d u r ) ( u r V e ( r ) ) 2 ( 1 − q ) 1 / 2 ∫ 0 1 ( 2 π u 2 d u ) ( 1 − u 2 ) − 1 / 2 = ( V 0 2 1 − r 2 r 0 2 ) 2 . {\\displaystyle \\langle \\mathbf {V} _{r}^{2}\\rangle (\\mathbf {x} )={\\!\\!\\int _{0}^{1}(du_{\\varphi })\\int _{0}^{\\sqrt {1-u_{\\varphi }^{2}}}\\!\\!(2du_{\\theta })\\!\\!\\int _{0}^{\\sqrt {1-u_{\\varphi }^{2}-u_{\\theta }^{2}}}\\!\\!\\!{(2du_{r})(u_{r}V_{e}(r))^{2} \\over (1-q)^{1/2}} \\over \\int _{0}^{1}{(2\\pi u^{2}du)(1-u^{2})^{-1/2}}}=\\left({V_{0} \\over 2}{\\sqrt {1-{r^{2} \\over r_{0}^{2}}}}\\right)^{2}.} So the"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_71",
    "chunk": "pressure tensor or dispersion tensor is σ i j 2 ( r ) = P i j ( r ) ρ ( r ) = ⟨ V i V j ⟩ − ⟨ V i ⟩ ⟨ V j ⟩ = [ [ 1 − ( r r 0 ) 2 ] ( V 0 2 ) 2 0 0 0 ( V 0 2 ) 2 0 0 0 [ 1 − ( 8 3 π ) 2 ] ( V 0 2 ) 2 ] {\\displaystyle {\\begin{aligned}\\sigma _{ij}^{2}(\\mathbf {r} )=&{P_{ij}(\\mathbf {r} ) \\over \\rho (\\mathbf {r} )}\\\\=&\\langle \\mathbf {V} _{i}\\mathbf {V} _{j}\\rangle -\\langle \\mathbf {V} _{i}\\rangle \\langle \\mathbf {V} _{j}\\rangle \\\\=&{\\begin{bmatrix}\\left[1-({r \\over r_{0}})^{2}\\right]\\left({V_{0} \\over 2}\\right)^{2}&0&0\\\\0&\\left({V_{0} \\over 2}\\right)^{2}&0\\\\0&0&\\left[1-({8 \\over 3\\pi })^{2}\\right]\\left({V_{0} \\over 2}\\right)^{2}\\end{bmatrix}}\\end{aligned}}} with zero off-diagonal terms because of the symmetric velocity distribution. Note while there is no Dark Matter in producing the previous flat rotation curve, the price is shown by the reduction factor 8 3 π = 0.8488 {\\displaystyle {8 \\over 3\\pi }=0.8488} in the random velocity spread in the azimuthal direction. Among the diagonal dispersion tensor moments, σ θ ≡ σ θ θ 2 = 0.5 V 0 {\\displaystyle \\sigma _{\\theta }\\equiv {\\sqrt {\\sigma _{\\theta \\theta"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_72",
    "chunk": "}^{2}}}=0.5V_{0}} is the biggest among the three at all radii, and σ φ ≡ σ φ φ 2 ≥ σ r ≡ σ r r 2 {\\displaystyle \\sigma _{\\varphi }\\equiv {\\sqrt {\\sigma _{\\varphi \\varphi }^{2}}}\\geq \\sigma _{r}\\equiv {\\sqrt {\\sigma _{rr}^{2}}}} only near the edge between 0.8488 r 0 ≤ r ≤ r 0 {\\displaystyle 0.8488r_{0}\\leq r\\leq r_{0}} . The larger tangential kinetic energy than that of radial motion seen in the diagonal dispersions is often phrased by an anisotropy parameter β ( r ) ≡ 1 − 0.5 ⟨ V t 2 ( | r | ) ⟩ ⟨ V r 2 ⟩ ( | r | ) = 1 − ⟨ V θ 2 ( | r | ) ⟩ ⟨ V r 2 ⟩ ( | r | ) = − r 2 r 0 2 − r 2 ≤ 0 ; {\\displaystyle \\beta (r)\\equiv 1-{0.5\\langle {\\mathbf {V} _{t}}^{2}(|\\mathbf {r} |)\\rangle \\over \\langle {\\mathbf {V} _{r}}^{2}\\rangle (|\\mathbf {r} |)}=1-{\\langle {\\mathbf {V} _{\\theta }}^{2}(|\\mathbf {r} |)\\rangle \\over \\langle {\\mathbf {V} _{r}}^{2}\\rangle (|\\mathbf {r} |)}=-{r^{2} \\over r_{0}^{2}-r^{2}}\\leq 0;} a positive anisotropy would have meant that radial motion dominated, and a negative anisotropy means that tangential motion dominates (as in this"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_73",
    "chunk": "uniform sphere). 2 K M 0 = ⟨ V 2 ⟩ ¯ ≡ ⟨ V 2 ¯ ⟩ = M 0 − 1 ∫ 0 M 0 ⟨ V θ 2 + V φ 2 + V r 2 ⟩ d M = M 0 − 1 ∫ 0 1 ( V 0 2 4 + V 0 2 4 + ( 1 − x 2 ) V 0 2 4 ) d ( x 3 M 0 ) = 0.6 V 0 2 , x ≡ r r 0 = ( M M 0 ) 1 3 , {\\displaystyle {\\begin{aligned}{2K \\over M_{0}}&={\\overline {\\langle V^{2}\\rangle }}\\equiv \\langle {\\overline {V^{2}}}\\rangle \\\\&=M_{0}^{-1}\\int _{0}^{M_{0}}\\langle V_{\\theta }^{2}+V_{\\varphi }^{2}+V_{r}^{2}\\rangle dM\\\\&=M_{0}^{-1}\\int _{0}^{1}\\left({V_{0}^{2} \\over 4}+{V_{0}^{2} \\over 4}+{(1-x^{2})V_{0}^{2} \\over 4}\\right)d(x^{3}M_{0})=0.6V_{0}^{2},~~x\\equiv {r \\over r_{0}}=\\left({M \\over M_{0}}\\right)^{1 \\over 3},\\end{aligned}}} which balances the potential energy per unit mass of the uniform sphere, inside which M ∝ r 3 ∝ x 3 {\\displaystyle M\\propto r^{3}\\propto x^{3}} . The average Virial per unit mass can be computed from averaging its local value r ⋅ ( − ∇ Φ ) {\\displaystyle \\mathbf {r} \\cdot (-\\mathbf {\\nabla } \\Phi )} , which yields W M 0 = r ⋅ ( − ∇ Φ ) ¯"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_74",
    "chunk": "= M 0 − 1 ∫ 0 r 0 r ⋅ − G M r | r | 3 ( ρ d r 3 ) = − M 0 − 1 ∫ 0 M 0 G M | r | d M = − M 0 − 1 ∫ 0 M 0 G M d M r 0 ( M / M 0 ) 1 3 = − 3 G M 0 5 r 0 = − 0.6 V 0 2 , {\\displaystyle {\\begin{aligned}{W \\over M_{0}}&={\\overline {\\mathbf {r} \\cdot (-\\mathbf {\\nabla } \\Phi )}}\\\\&=M_{0}^{-1}\\int _{0}^{r_{0}}\\mathbf {r} \\cdot {-GM\\mathbf {r} \\over |\\mathbf {r} |^{3}}(\\rho d\\mathbf {r} ^{3})=-M_{0}^{-1}\\int _{0}^{M_{0}}{GM \\over |\\mathbf {r} |}dM\\\\&=-M_{0}^{-1}\\int _{0}^{M_{0}}{GM~dM \\over r_{0}~(M/M_{0})^{1 \\over 3}}=-{3GM_{0} \\over 5r_{0}}=-0.6V_{0}^{2},\\end{aligned}}} as required by the Virial Theorem. For this self-gravitating sphere, we can also verify that the virial per unit mass equals the averages of half of the potential E pot M 0 = ⟨ Φ 2 ⟩ ¯ = M 0 − 1 ∫ x > 0 x < 1 Φ ( r 0 x ) 2 d ( M 0 x 3 ) = W M 0 = − 2 K M 0 . {\\displaystyle {\\begin{aligned}{E_{\\text{pot}} \\over M_{0}}&={\\overline {\\langle {\\Phi \\over"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_75",
    "chunk": "2}\\rangle }}\\\\&=M_{0}^{-1}\\int _{x>0}^{x<1}{\\Phi (r_{0}x) \\over 2}d(M_{0}x^{3})\\\\&={W \\over M_{0}}={-2K \\over M_{0}}.\\end{aligned}}} Hence we have verified the validity of Virial Theorem for a uniform sphere under self-gravity, i.e., the gravity due to the mass density of the stars is also the gravity that stars move in self-consistently; no additional dark matter halo contributes to its potential, for example. Jeans Equation is a relation on how the pressure gradient of a system should be balancing the potential gradient for an equilibrium galaxy. In our uniform sphere, the potential gradient or gravity is ∇ Φ = d Φ d r = Ω 2 r ≥ 0 , Ω = V 0 r 0 . {\\displaystyle \\nabla \\Phi ={d\\Phi \\over dr}={\\Omega ^{2}r}\\geq 0,~~\\Omega ={V_{0} \\over r_{0}}.} The radial pressure gradient − d ( ρ σ r 2 ) ρ d r = − d σ r 2 d r − σ r 2 r d log ⁡ ρ d log ⁡ r = Ω 2 r 2 + 0 ≥ 0. {\\displaystyle -{d(\\rho \\sigma _{r}^{2}) \\over \\rho dr}=-{d\\sigma _{r}^{2} \\over dr}-{\\sigma _{r}^{2} \\over r}{d\\log \\rho \\over d\\log r}={\\Omega ^{2}r \\over 2}+0\\geq 0.} The reason for the discrepancy is partly due to centrifugal force V ¯"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_76",
    "chunk": "φ 2 r = ( 0.8488 V 0 ) 2 r > 0 , {\\displaystyle {{\\bar {V}}_{\\varphi }^{2} \\over r}={(0.8488V_{0})^{2} \\over r}>0,} and partly due to anisotropic pressure ( σ θ 2 − σ r 2 ) r = 0.25 Ω 2 r ≥ 0 ( σ φ 2 − σ r 2 ) r = 0.25 Ω 2 r − 0.1801 V 0 2 r = ± , {\\displaystyle {\\begin{aligned}{(\\sigma _{\\theta }^{2}-\\sigma _{r}^{2}) \\over r}&=0.25\\Omega ^{2}r\\geq 0\\\\{(\\sigma _{\\varphi }^{2}-\\sigma _{r}^{2}) \\over r}&=0.25\\Omega ^{2}r-{0.1801V_{0}^{2} \\over r}=\\pm ,\\end{aligned}}} so 0.2643 V 0 = σ φ < σ r = 0.5 V 0 {\\displaystyle 0.2643V_{0}=\\sigma _{\\varphi }<\\sigma _{r}=0.5V_{0}} at the very centre, but the two balance at radius r = 0.8488 r 0 {\\displaystyle r=0.8488r_{0}} , and then reverse to 0.2643 V 0 = σ φ > σ r = 0 {\\displaystyle 0.2643V_{0}=\\sigma _{\\varphi }>\\sigma _{r}=0} at the very edge. Now we can verify that ∂ ⟨ V r ⟩ ∂ t = ( − ∑ i = x , y , z V i ∂ i ⟨ V r ⟩ ) − ⟨ V r ⟩ t fric − ∇ r Φ + ∑ i = x , y , z"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_77",
    "chunk": "− ∂ i ( n σ i r 2 ) n = V ¯ θ 2 + V ¯ φ 2 − 2 V ¯ r 2 r − 0 − ∂ Φ ∂ r + [ − d ( ρ σ r 2 ) ρ d r + σ θ 2 + σ φ 2 − 2 σ r 2 r ] = 0 + ( 0.4244 V 0 ) 2 − 2 × 0 r − ( Ω 2 r ) + [ Ω 2 r 2 + ( 0.5 V 0 ) 2 + ( 0.2643 V 0 ) 2 − 2 × 0.25 Ω 2 ( r 0 2 − r 2 ) r ] = 0. {\\displaystyle {\\begin{aligned}{\\partial \\langle V_{r}\\rangle \\over \\partial t}&=(-\\sum _{i=x,y,z}V_{i}\\partial _{i}\\langle V_{r}\\rangle )-{\\cancel {\\langle V_{r}\\rangle \\over t_{\\text{fric}}}}-\\nabla _{r}\\Phi +\\sum _{i=x,y,z}{-\\partial _{i}(n\\sigma _{ir}^{2}) \\over n}\\\\&={{\\bar {V}}_{\\theta }^{2}+{\\bar {V}}_{\\varphi }^{2}-2{\\bar {V}}_{r}^{2} \\over r}-0-{\\partial \\Phi \\over \\partial r}+\\left[-{d(\\rho \\sigma _{r}^{2}) \\over \\rho dr}+{\\sigma _{\\theta }^{2}+\\sigma _{\\varphi }^{2}-2\\sigma _{r}^{2} \\over r}\\right]\\\\&={0+(0.4244V_{0})^{2}-2\\times 0 \\over r}-(\\Omega ^{2}r)+\\\\&\\left[{\\Omega ^{2}r \\over 2}+{(0.5V_{0})^{2}+(0.2643V_{0})^{2}-2\\times 0.25\\Omega ^{2}(r_{0}^{2}-r^{2}) \\over r}\\right]\\\\&=0.\\end{aligned}}} Here the 1st line above is essentially the Jeans equation in the r-direction, which reduces to the 2nd line, the Jeans equation in"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_78",
    "chunk": "an anisotropic (aka β ≠ 0 {\\displaystyle \\beta \\neq 0} ) rotational (aka ⟨ V φ ⟩ ≠ 0 {\\displaystyle \\langle V_{\\varphi }\\rangle \\neq 0} ) axisymmetric ( ∂ φ Φ ( x , t ) = 0 {\\displaystyle \\partial _{\\varphi }\\Phi (\\mathbf {x} ,t)=0} ) sphere (aka ∂ θ n ( x , t ) = 0 {\\displaystyle \\partial _{\\theta }n(\\mathbf {x} ,t)=0} ) after much coordinate manipulations of the dispersion tensor; similar equation of motion can be obtained for the two tangential direction, e.g., ∂ ⟨ V φ ⟩ ∂ t {\\displaystyle {\\partial \\langle V_{\\varphi }\\rangle \\over \\partial t}} , which are useful in modelling ocean currents on the rotating earth surface or angular momentum transfer in accretion disks, where the frictional term − ⟨ V φ ⟩ t fric {\\displaystyle -{\\langle V_{\\varphi }\\rangle \\over t_{\\text{fric}}}} is important. The fact that the l.h.s. ∂ V r ∂ t = 0 {\\displaystyle {\\partial V_{r} \\over \\partial t}=0} means that the force is balanced on the r.h.s. for this uniform (aka ∇ x m n ( x , t ) = 0 {\\displaystyle \\nabla _{\\mathbf {x} }mn(\\mathbf {x} ,t)=0} ) spherical model of a galaxy (cluster) to stay in a"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_79",
    "chunk": "steady state (aka time-independent equilibrium ∂ n ( x , t ) ∂ t = 0 {\\displaystyle {\\partial n(\\mathbf {x} ,t) \\over \\partial t}=0} everywhere) statically (aka with zero flow ⟨ V ( x , t ) ⟩ = 0 {\\displaystyle \\langle \\mathbf {V} (\\mathbf {x} ,t)\\rangle =0} everywhere). Note systems like accretion disk can have a steady net radial inflow ⟨ V ( x ) ⟩ < 0 {\\displaystyle \\langle \\mathbf {V} (\\mathbf {x} )\\rangle <0} everywhere at all time. Consider again the thick disk potential in the above example. If the density is that of a gas fluid, then the pressure would be zero at the boundary z = ± z 0 {\\displaystyle z=\\pm z_{0}} . To find the peak of the pressure, we note that P ( R , z ) = ∫ z z 0 ∂ z Φ ρ ( R ) d z = ρ ( R ) [ Φ ( R , z 0 ) − Φ ( R , z ) ] . {\\displaystyle P(R,z)=\\int _{z}^{z_{0}}\\partial _{z}\\Phi \\rho (R)dz=\\rho (R)[\\Phi (R,z_{0})-\\Phi (R,z)].} So the fluid temperature per unit mass, i.e., the 1-dimensional velocity dispersion squared would be σ 2 ( R , z"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_80",
    "chunk": ") = P ( R , z ) ρ ( R ) , | z | ≤ z 0 {\\displaystyle \\sigma ^{2}(R,z)={P(R,z) \\over \\rho (R)},~~|z|\\leq z_{0}} σ 2 = G M 0 2 z 0 log ⁡ Q ( z ) Q ( − z ) Q ( z 0 ) Q ( − z 0 ) , Q ( z ) ≡ R 0 + z 0 + z + R 2 + ( R 0 + z 0 + z ) 2 . {\\displaystyle \\sigma ^{2}={GM_{0} \\over 2z_{0}}\\log {Q(z)Q(-z) \\over Q(z_{0})Q(-z_{0})},~~Q(z)\\equiv R_{0}+z_{0}+z+{\\sqrt {R^{2}+(R_{0}+z_{0}+z)^{2}}}.} Along the rotational z-axis, σ 2 ( 0 , z ) = G M 0 2 z 0 log ⁡ 4 ( R 0 + z 0 + z ) ( R 0 + z 0 − z ) 4 R 0 ( R 0 + 2 z 0 ) {\\displaystyle \\sigma ^{2}(0,z)={GM_{0} \\over 2z_{0}}\\log {4(R_{0}+z_{0}+z)(R_{0}+z_{0}-z) \\over 4R_{0}(R_{0}+2z_{0})}} σ ( 0 , z ) = G M 0 2 z 0 log ⁡ ( R 0 + z 0 ) 2 − z 2 ( R 0 + z 0 ) 2 − z 0 2 , {\\displaystyle \\sigma (0,z)={\\sqrt {GM_{0} \\over 2z_{0}}}{\\sqrt {\\log {(R_{0}+z_{0})^{2}-z^{2}"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_81",
    "chunk": "\\over (R_{0}+z_{0})^{2}-z_{0}^{2}}}},} which is clearly the highest at the centre and zero at the boundaries z = ± z 0 {\\displaystyle z=\\pm z_{0}} . Both the pressure and the dispersion peak at the midplane z = 0 {\\displaystyle z=0} . In fact the hottest and densest point is the centre, where P ( 0 , 0 ) = M 0 4 π R 0 2 z 0 − G M 0 log ⁡ [ 1 − ( 1 + R 0 / z 0 ) − 2 ] 2 z 0 . {\\displaystyle P(0,0)={M_{0} \\over 4\\pi R_{0}^{2}z_{0}}{-GM_{0}\\log[1-(1+R_{0}/z_{0})^{-2}] \\over 2z_{0}}.} Having looking at the a few applications of Poisson Eq. and Phase space density and especially the Jeans equation, we can extract a general theme, again using the Spherical cow approach. Jeans equation links gravity with pressure gradient, it is a generalisation of the Eq. of Motion for single particles. While Jeans equation can be solved in disk systems, the most user-friendly version of the Jeans eq. is the spherical anisotropic version for a static ⟨ v j ⟩ = 0 {\\displaystyle \\langle {v_{j}}\\rangle =0} frictionless system t fric → ∞ {\\displaystyle t_{\\text{fric}}\\rightarrow \\infty } , hence the local velocity speed"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_82",
    "chunk": "σ j 2 ( r ) = ⟨ v j 2 ⟩ ( r ) − ⟨ v j ⟩ 2 ( r ) ⏟ = 0 = ∫ ∞ d v r d v θ d v φ ( v j − ⟨ v ⟩ j p ⏞ = 0 ) 2 f p ∫ ∞ d v r d v θ d v φ f p , {\\displaystyle \\sigma _{j}^{2}(r)=\\langle {v_{j}^{2}}\\rangle (r)-\\underbrace {\\langle {v_{j}}\\rangle ^{2}(r)} _{=0}={\\int \\limits _{\\infty }\\!\\!dv_{r}dv_{\\theta }dv_{\\varphi }({v}_{j}-\\overbrace {\\langle {v}\\rangle _{j}^{p}} ^{=0})^{2}f_{p} \\over \\int \\limits _{\\infty }\\!\\!dv_{r}dv_{\\theta }dv_{\\varphi }f_{p}},} everywhere for each of the three directions j = r , θ , φ {\\displaystyle ~_{j}=~_{r},~_{\\theta },~_{\\varphi }} . One can project the phase space into these moments, which is easily if in a highly spherical system, which admits conservations of energy E = {\\displaystyle E=} and angular momentum J. The boundary of the system sets the integration range of the velocity bound in the system. In summary, in the spherical Jeans eq., d Φ d r = G M ( r ) r 2 = − d ( n ⟨ v r 2 ⟩ ) n ( r ) d r + ⟨ v θ"
  },
  {
    "source": "Stellar dynamics.txt",
    "chunk_id": "Stellar dynamics.txt_83",
    "chunk": "2 ⟩ + ⟨ v ϕ 2 ⟩ − 2 ⟨ v r 2 ⟩ r , = − d ( n ⟨ v r 2 ⟩ ) n ( r ) d r , hydrostatic equilibrium if isotropic velocity = ⟨ v t 2 ⟩ r , if purely centrifugal balancing of gravity with no radial motion , ⟨ v t 2 ⟩ ≡ ⟨ v θ 2 ⟩ + ⟨ v ϕ 2 ⟩ {\\displaystyle {\\begin{aligned}{d\\Phi \\over dr}=&{GM(r) \\over r^{2}}\\\\=&-{d(n\\langle {v_{r}^{2}}\\rangle ) \\over n(r)dr}+{\\langle {v_{\\theta }^{2}}\\rangle +\\langle {v_{\\phi }^{2}}\\rangle -2\\langle {v_{r}^{2}}\\rangle \\over r},\\\\=&-{d(n\\langle {v_{r}^{2}}\\rangle ) \\over n(r)dr},~~{\\text{hydrostatic equilibrium if isotropic velocity }}\\\\=&{\\langle v_{t}^{2}\\rangle \\over r},~~{\\text{if purely centrifugal balancing of gravity with no radial motion}},\\langle v_{t}^{2}\\rangle \\equiv \\langle {v_{\\theta }^{2}}\\rangle +\\langle {v_{\\phi }^{2}}\\rangle \\end{aligned}}} which matches the expectation from the Virial theorem r ∂ r Φ ¯ = v cir 2 ¯ = G M r ¯ = ⟨ v t 2 ⟩ ¯ {\\displaystyle {\\overline {r\\partial _{r}\\Phi }}={\\overline {v_{\\text{cir}}^{2}}}={\\overline {GM \\over r}}={\\overline {\\langle v_{t}^{2}\\rangle }}} , or in other words, the global average ¯ {\\displaystyle {\\overline {\\text{global average}}}} kinetic energy of an equilibrium equals the average kinetic energy on circular orbits with purely transverse motion."
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_0",
    "chunk": "# Stellar evolution Stellar evolution is the process by which a star changes over the course of time. Depending on the mass of the star, its lifetime can range from a few million years for the most massive to trillions of years for the least massive, which is considerably longer than the current age of the universe. The table shows the lifetimes of stars as a function of their masses. All stars are formed from collapsing clouds of gas and dust, often called nebulae or molecular clouds. Over the course of millions of years, these protostars settle down into a state of equilibrium, becoming what is known as a main-sequence star. Nuclear fusion powers a star for most of its existence. Initially the energy is generated by the fusion of hydrogen atoms at the core of the main-sequence star. Later, as the preponderance of atoms at the core becomes helium, stars like the Sun begin to fuse hydrogen along a spherical shell surrounding the core. This process causes the star to gradually grow in size, passing through the subgiant stage until it reaches the red-giant phase. Stars with at least half the mass of the Sun can also begin to"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_1",
    "chunk": "generate energy through the fusion of helium at their core, whereas more-massive stars can fuse heavier elements along a series of concentric shells. Once a star like the Sun has exhausted its nuclear fuel, its core collapses into a dense white dwarf and the outer layers are expelled as a planetary nebula. Stars with around ten or more times the mass of the Sun can explode in a supernova as their inert iron cores collapse into an extremely dense neutron star or black hole. Although the universe is not old enough for any of the smallest red dwarfs to have reached the end of their existence, stellar models suggest they will slowly become brighter and hotter before running out of hydrogen fuel and becoming low-mass white dwarfs. Stellar evolution is not studied by observing the life of a single star, as most stellar changes occur too slowly to be detected, even over many centuries. Instead, astrophysicists come to understand how stars evolve by observing numerous stars at various points in their lifetime, and by simulating stellar structure using computer models. Stellar evolution starts with the gravitational collapse of a giant molecular cloud. Typical giant molecular clouds are roughly 100 light-years"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_2",
    "chunk": "(9.5×10 km) across and contain up to 6,000,000 solar masses (1.2×10 kg). As it collapses, a giant molecular cloud breaks into smaller and smaller pieces. In each of these fragments, the collapsing gas releases gravitational potential energy as heat. As its temperature and pressure increase, a fragment condenses into a rotating ball of superhot gas known as a protostar. Filamentary structures are truly ubiquitous in the molecular cloud. Dense molecular filaments will fragment into gravitationally bound cores, which are the precursors of stars. Continuous accretion of gas, geometrical bending, and magnetic fields may control the detailed fragmentation manner of the filaments. In supercritical filaments, observations have revealed quasi-periodic chains of dense cores with spacing comparable to the filament inner width, and embedded two protostars with gas outflows. A protostar continues to grow by accretion of gas and dust from the molecular cloud, becoming a pre-main-sequence star as it reaches its final mass. Further development is determined by its mass. Mass is typically compared to the mass of the Sun: 1.0 M☉ (2.0×10 kg) means 1 solar mass. Protostars are encompassed in dust, and are thus more readily visible at infrared wavelengths. Observations from the Wide-field Infrared Survey Explorer (WISE) have"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_3",
    "chunk": "been especially important for unveiling numerous galactic protostars and their parent star clusters. Protostars with masses less than roughly 0.08 M☉ (1.6×10 kg) never reach temperatures high enough for nuclear fusion of hydrogen to begin. These are known as brown dwarfs. The International Astronomical Union defines brown dwarfs as stars massive enough to fuse deuterium at some point in their lives (13 Jupiter masses (MJ), 2.5 × 10 kg, or 0.0125 M☉). Objects smaller than 13 MJ are classified as sub-brown dwarfs (but if they orbit around another stellar object they are classified as planets). Both types, deuterium-burning and not, shine dimly and fade away slowly, cooling gradually over hundreds of millions of years. For a more-massive protostar, the core temperature will eventually reach 10 million kelvin, initiating the proton–proton chain reaction and allowing hydrogen to fuse, first to deuterium and then to helium. In stars of slightly over 1 M☉ (2.0×10 kg), the carbon–nitrogen–oxygen fusion reaction (CNO cycle) contributes a large portion of the energy generation. The onset of nuclear fusion leads relatively quickly to a hydrostatic equilibrium in which energy released by the core maintains a high gas pressure, balancing the weight of the star's matter and preventing"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_4",
    "chunk": "further gravitational collapse. The star thus evolves rapidly to a stable state, beginning the main-sequence phase of its evolution. A new star will sit at a specific point on the main sequence of the Hertzsprung–Russell diagram, with the main-sequence spectral type depending upon the mass of the star. Small, relatively cold, low-mass red dwarfs fuse hydrogen slowly and will remain on the main sequence for hundreds of billions of years or longer, whereas massive, hot O-type stars will leave the main sequence after just a few million years. A mid-sized yellow dwarf star, like the Sun, will remain on the main sequence for about 10 billion years. The Sun is thought to be in the middle of its main sequence lifespan. A star may gain a protoplanetary disk, which furthermore can develop into a planetary system. Eventually the star's core exhausts its supply of hydrogen and the star begins to evolve off the main sequence. Without the outward radiation pressure generated by the fusion of hydrogen to counteract the force of gravity, the core contracts until either electron degeneracy pressure becomes sufficient to oppose gravity or the core becomes hot enough (around 100 MK) for helium fusion to begin. Which"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_5",
    "chunk": "of these happens first depends upon the star's mass. What happens after a low-mass star ceases to produce energy through fusion has not been directly observed; the universe is around 13.8 billion years old, which is less time (by several orders of magnitude, in some cases) than it takes for fusion to cease in such stars. Recent astrophysical models suggest that red dwarfs of 0.1 M☉ may stay on the main sequence for some six to twelve trillion years, gradually increasing in both temperature and luminosity, and take several hundred billion years more to collapse, slowly, into a white dwarf. Such stars will not become red giants as the whole star is a convection zone and it will not develop a degenerate helium core with a shell burning hydrogen. Instead, hydrogen fusion will proceed until almost the whole star is helium. Slightly more massive stars do expand into red giants, but their helium cores are not massive enough to reach the temperatures required for helium fusion so they never reach the tip of the red-giant branch. When hydrogen shell burning finishes, these stars move directly off the red-giant branch like a post-asymptotic-giant-branch (AGB) star, but at lower luminosity, to become"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_6",
    "chunk": "a white dwarf. A star with an initial mass about 0.6 M☉ will be able to reach temperatures high enough to fuse helium, and these \"mid-sized\" stars go on to further stages of evolution beyond the red-giant branch. Stars of roughly 0.6–10 M☉ become red giants, which are large non-main-sequence stars of stellar classification K or M. Red giants lie along the right edge of the Hertzsprung–Russell diagram due to their red color and large luminosity. Examples include Aldebaran in the constellation Taurus and Arcturus in the constellation of Boötes. Mid-sized stars are red giants during two different phases of their post-main-sequence evolution: red-giant-branch stars, with inert cores made of helium and hydrogen-burning shells, and asymptotic-giant-branch stars, with inert cores made of carbon and helium-burning shells inside the hydrogen-burning shells. Between these two phases, stars spend a period on the horizontal branch with a helium-fusing core. Many of these helium-fusing stars cluster towards the cool end of the horizontal branch as K-type giants and are referred to as red clump giants. When a star exhausts the hydrogen in its core, it leaves the main sequence and begins to fuse hydrogen in a shell outside the core. The core increases in"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_7",
    "chunk": "mass as the shell produces more helium. Depending on the mass of the helium core, this continues for several million to one or two billion years, with the star expanding and cooling at a similar or slightly lower luminosity to its main sequence state. Eventually either the core becomes degenerate, in stars around the mass of the sun, or the outer layers cool sufficiently to become opaque, in more massive stars. Either of these changes cause the hydrogen shell to increase in temperature and the luminosity of the star to increase, at which point the star expands onto the red-giant branch. The expanding outer layers of the star are convective, with the material being mixed by turbulence from near the fusing regions up to the surface of the star. For all but the lowest-mass stars, the fused material has remained deep in the stellar interior prior to this point, so the convecting envelope makes fusion products visible at the star's surface for the first time. At this stage of evolution, the results are subtle, with the largest effects, alterations to the isotopes of hydrogen and helium, being unobservable. The effects of the CNO cycle appear at the surface during the"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_8",
    "chunk": "first dredge-up, with lower C/C ratios and altered proportions of carbon and nitrogen. These are detectable with spectroscopy and have been measured for many evolved stars. The helium core continues to grow on the red-giant branch. It is no longer in thermal equilibrium, either degenerate or above the Schönberg–Chandrasekhar limit, so it increases in temperature which causes the rate of fusion in the hydrogen shell to increase. The star increases in luminosity towards the tip of the red-giant branch. Red-giant-branch stars with a degenerate helium core all reach the tip with very similar core masses and very similar luminosities, although the more massive of the red giants become hot enough to ignite helium fusion before that point. In the helium cores of stars in the 0.6 to 2.0 solar mass range, which are largely supported by electron degeneracy pressure, helium fusion will ignite on a timescale of days in a helium flash. In the nondegenerate cores of more massive stars, the ignition of helium fusion occurs relatively slowly with no flash. The nuclear power released during the helium flash is very large, on the order of 10 times the luminosity of the Sun for a few days and 10 times"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_9",
    "chunk": "the luminosity of the Sun (roughly the luminosity of the Milky Way Galaxy) for a few seconds. However, the energy is consumed by the thermal expansion of the initially degenerate core and thus cannot be seen from outside the star. Due to the expansion of the core, the hydrogen fusion in the overlying layers slows and total energy generation decreases. The star contracts, although not all the way to the main sequence, and it migrates to the horizontal branch on the Hertzsprung–Russell diagram, gradually shrinking in radius and increasing its surface temperature. Core helium flash stars evolve to the red end of the horizontal branch but do not migrate to higher temperatures before they gain a degenerate carbon-oxygen core and start helium shell burning. These stars are often observed as a red clump of stars in the colour-magnitude diagram of a cluster, hotter and less luminous than the red giants. Higher-mass stars with larger helium cores move along the horizontal branch to higher temperatures, some becoming unstable pulsating stars in the yellow instability strip (RR Lyrae variables), whereas some become even hotter and can form a blue tail or blue hook to the horizontal branch. The morphology of the horizontal"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_10",
    "chunk": "branch depends on parameters such as metallicity, age, and helium content, but the exact details are still being modelled. After a star has consumed the helium at the core, hydrogen and helium fusion continues in shells around a hot core of carbon and oxygen. The star follows the asymptotic giant branch on the Hertzsprung–Russell diagram, paralleling the original red-giant evolution, but with even faster energy generation (which lasts for a shorter time). Although helium is being burnt in a shell, the majority of the energy is produced by hydrogen burning in a shell further from the core of the star. Helium from these hydrogen burning shells drops towards the center of the star and periodically the energy output from the helium shell increases dramatically. This is known as a thermal pulse and they occur towards the end of the asymptotic-giant-branch phase, sometimes even into the post-asymptotic-giant-branch phase. Depending on mass and composition, there may be several to hundreds of thermal pulses. There is a phase on the ascent of the asymptotic-giant-branch where a deep convective zone forms and can bring carbon from the core to the surface. This is known as the second dredge up, and in some stars there"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_11",
    "chunk": "may even be a third dredge up. In this way a carbon star is formed, very cool and strongly reddened stars showing strong carbon lines in their spectra. A process known as hot bottom burning may convert carbon into oxygen and nitrogen before it can be dredged to the surface, and the interaction between these processes determines the observed luminosities and spectra of carbon stars in particular clusters. Another well known class of asymptotic-giant-branch stars is the Mira variables, which pulsate with well-defined periods of tens to hundreds of days and large amplitudes up to about 10 magnitudes (in the visual, total luminosity changes by a much smaller amount). In more-massive stars the stars become more luminous and the pulsation period is longer, leading to enhanced mass loss, and the stars become heavily obscured at visual wavelengths. These stars can be observed as OH/IR stars, pulsating in the infrared and showing OH maser activity. These stars are clearly oxygen rich, in contrast to the carbon stars, but both must be produced by dredge ups. These mid-range stars ultimately reach the tip of the asymptotic-giant-branch and run out of fuel for shell burning. They are not sufficiently massive to start full-scale"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_12",
    "chunk": "carbon fusion, so they contract again, going through a period of post-asymptotic-giant-branch superwind to produce a planetary nebula with an extremely hot central star. The central star then cools to a white dwarf. The expelled gas is relatively rich in heavy elements created within the star and may be particularly oxygen or carbon enriched, depending on the type of the star. The gas builds up in an expanding shell called a circumstellar envelope and cools as it moves away from the star, allowing dust particles and molecules to form. With the high infrared energy input from the central star, ideal conditions are formed in these circumstellar envelopes for maser excitation. It is possible for thermal pulses to be produced once post-asymptotic-giant-branch evolution has begun, producing a variety of unusual and poorly understood stars known as born-again asymptotic-giant-branch stars. These may result in extreme horizontal-branch stars (subdwarf B stars), hydrogen deficient post-asymptotic-giant-branch stars, variable planetary nebula central stars, and R Coronae Borealis variables. In massive stars, the core is already large enough at the onset of the hydrogen burning shell that helium ignition will occur before electron degeneracy pressure has a chance to become prevalent. Thus, when these stars expand and"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_13",
    "chunk": "cool, they do not brighten as dramatically as lower-mass stars; however, they were more luminous on the main sequence and they evolve to highly luminous supergiants. Their cores become massive enough that they cannot support themselves by electron degeneracy and will eventually collapse to produce a neutron star or black hole. Extremely massive stars (more than approximately 40 M☉), which are very luminous and thus have very rapid stellar winds, lose mass so rapidly due to radiation pressure that they tend to strip off their own envelopes before they can expand to become red supergiants, and thus retain extremely high surface temperatures (and blue-white color) from their main-sequence time onwards. The largest stars of the current generation are about 100-150 M☉ because the outer layers would be expelled by the extreme radiation. Although lower-mass stars normally do not burn off their outer layers so rapidly, they can likewise avoid becoming red giants or red supergiants if they are in binary systems close enough so that the companion star strips off the envelope as it expands, or if they rotate rapidly enough so that convection extends all the way from the core to the surface, resulting in the absence of a"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_14",
    "chunk": "separate core and envelope due to thorough mixing. The core of a massive star, defined as the region depleted of hydrogen, grows hotter and denser as it accretes material from the fusion of hydrogen outside the core. In sufficiently massive stars, the core reaches temperatures and densities high enough to fuse carbon and heavier elements via the alpha process. At the end of helium fusion, the core of a star consists primarily of carbon and oxygen. In stars heavier than about 8 M☉, the carbon ignites and fuses to form neon, sodium, and magnesium. Stars somewhat less massive may partially ignite carbon, but they are unable to fully fuse the carbon before electron degeneracy sets in, and these stars will eventually leave an oxygen-neon-magnesium white dwarf. The exact mass limit for full carbon burning depends on several factors such as metallicity and the detailed mass lost on the asymptotic giant branch, but is approximately 8-9 M☉. After carbon burning is complete, the core of these stars reaches about 2.5 M☉ and becomes hot enough for heavier elements to fuse. Before oxygen starts to fuse, neon begins to capture electrons which triggers neon burning. For a range of stars of approximately"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_15",
    "chunk": "8-12 M☉, this process is unstable and creates runaway fusion resulting in an electron capture supernova. In more massive stars, the fusion of neon proceeds without a runaway deflagration. This is followed in turn by complete oxygen burning and silicon burning, producing a core consisting largely of iron-peak elements. Surrounding the core are shells of lighter elements still undergoing fusion. The timescale for complete fusion of a carbon core to an iron core is so short, just a few hundred years, that the outer layers of the star are unable to react and the appearance of the star is largely unchanged. The iron core grows until it reaches an effective Chandrasekhar mass, higher than the formal Chandrasekhar mass due to various corrections for the relativistic effects, entropy, charge, and the surrounding envelope. The effective Chandrasekhar mass for an iron core varies from about 1.34 M☉ in the least massive red supergiants to more than 1.8 M☉ in more massive stars. Once this mass is reached, electrons begin to be captured into the iron-peak nuclei and the core becomes unable to support itself. The core collapses and the star is destroyed, either in a supernova or direct collapse to a black"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_16",
    "chunk": "hole. When the core of a massive star collapses, it will form a neutron star, or in the case of cores that exceed the Tolman–Oppenheimer–Volkoff limit, a black hole. Through a process that is not completely understood, some of the gravitational potential energy released by this core collapse is converted into a Type Ib, Type Ic, or Type II supernova. It is known that the core collapse produces a massive surge of neutrinos, as observed with supernova SN 1987A. The extremely energetic neutrinos fragment some nuclei; some of their energy is consumed in releasing nucleons, including neutrons, and some of their energy is transformed into heat and kinetic energy, thus augmenting the shock wave started by rebound of some of the infalling material from the collapse of the core. Electron capture in very dense parts of the infalling matter may produce additional neutrons. Because some of the rebounding matter is bombarded by the neutrons, some of its nuclei capture them, creating a spectrum of heavier-than-iron material including the radioactive elements up to (and likely beyond) uranium. Although non-exploding red giants can produce significant quantities of elements heavier than iron using neutrons released in side reactions of earlier nuclear reactions, the"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_17",
    "chunk": "abundance of elements heavier than iron (and in particular, of certain isotopes of elements that have multiple stable or long-lived isotopes) produced in such reactions is quite different from that produced in a supernova. Neither abundance alone matches that found in the Solar System, so both supernovae, neutron star mergers and ejection of elements from red giants are required to explain the observed abundance of heavy elements and isotopes thereof. The energy transferred from collapse of the core to rebounding material not only generates heavy elements, but provides for their acceleration well beyond escape velocity, thus causing a Type Ib, Type Ic, or Type II supernova. Current understanding of this energy transfer is still not satisfactory; although current computer models of Type Ib, Type Ic, and Type II supernovae account for part of the energy transfer, they are not able to account for enough energy transfer to produce the observed ejection of material. However, neutrino oscillations may play an important role in the energy transfer problem as they not only affect the energy available in a particular flavour of neutrinos but also through other general-relativistic effects on neutrinos. Some evidence gained from analysis of the mass and orbital parameters of"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_18",
    "chunk": "binary neutron stars (which require two such supernovae) hints that the collapse of an oxygen-neon-magnesium core may produce a supernova that differs observably (in ways other than size) from a supernova produced by the collapse of an iron core. The most massive stars that exist today may be completely destroyed by a supernova with an energy greatly exceeding its gravitational binding energy. This rare event, caused by pair-instability, leaves behind no black hole remnant. In the past history of the universe, some stars were even larger than the largest that exists today, and they would immediately collapse into a black hole at the end of their lives, due to photodisintegration. After a star has burned out its fuel supply, its remnants can take one of three forms, depending on the mass during its lifetime. For a star of 1 M☉, the resulting white dwarf is of about 0.6 M☉, compressed into approximately the volume of the Earth. White dwarfs are stable because the inward pull of gravity is balanced by the degeneracy pressure of the star's electrons, a consequence of the Pauli exclusion principle. Electron degeneracy pressure provides a rather soft limit against further compression; therefore, for a given chemical"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_19",
    "chunk": "composition, white dwarfs of higher mass have a smaller volume. With no fuel left to burn, the star radiates its remaining heat into space for billions of years. A white dwarf is very hot when it first forms, more than 100,000 K at the surface and even hotter in its interior. It is so hot that a lot of its energy is lost in the form of neutrinos for the first 10 million years of its existence and will have lost most of its energy after a billion years. The chemical composition of the white dwarf depends upon its mass. A star that has a mass of about 8-12 solar masses will ignite carbon fusion to form magnesium, neon, and smaller amounts of other elements, resulting in a white dwarf composed chiefly of oxygen, neon, and magnesium, provided that it can lose enough mass to get below the Chandrasekhar limit (see below), and provided that the ignition of carbon is not so violent as to blow the star apart in a supernova. A star of mass on the order of magnitude of the Sun will be unable to ignite carbon fusion, and will produce a white dwarf composed chiefly of"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_20",
    "chunk": "carbon and oxygen, and of mass too low to collapse unless matter is added to it later (see below). A star of less than about half the mass of the Sun will be unable to ignite helium fusion (as noted earlier), and will produce a white dwarf composed chiefly of helium. In the end, all that remains is a cold dark mass sometimes called a black dwarf. However, the universe is not old enough for any black dwarfs to exist yet. If the white dwarf's mass increases above the Chandrasekhar limit, which is 1.4 M☉ for a white dwarf composed chiefly of carbon, oxygen, neon, and/or magnesium, then electron degeneracy pressure fails due to electron capture and the star collapses. Depending upon the chemical composition and pre-collapse temperature in the center, this will lead either to collapse into a neutron star or runaway ignition of carbon and oxygen. Heavier elements favor continued core collapse, because they require a higher temperature to ignite, because electron capture onto these elements and their fusion products is easier; higher core temperatures favor runaway nuclear reaction, which halts core collapse and leads to a Type Ia supernova. These supernovae may be many times brighter than"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_21",
    "chunk": "the Type II supernova marking the death of a massive star, even though the latter has the greater total energy release. This instability to collapse means that no white dwarf more massive than approximately 1.4 M☉ can exist (with a possible minor exception for very rapidly spinning white dwarfs, whose centrifugal force due to rotation partially counteracts the weight of their matter). Mass transfer in a binary system may cause an initially stable white dwarf to surpass the Chandrasekhar limit. If a white dwarf forms a close binary system with another star, hydrogen from the larger companion may accrete around and onto a white dwarf until it gets hot enough to fuse in a runaway reaction at its surface, although the white dwarf remains below the Chandrasekhar limit. Such an explosion is termed a nova. Ordinarily, atoms are mostly electron clouds by volume, with very compact nuclei at the center (proportionally, if atoms were the size of a football stadium, their nuclei would be the size of dust mites). When a stellar core collapses, the pressure causes electrons and protons to fuse by electron capture. Without electrons, which keep nuclei apart, the neutrons collapse into a dense ball (in some"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_22",
    "chunk": "ways like a giant atomic nucleus), with a thin overlying layer of degenerate matter (chiefly iron unless matter of different composition is added later). The neutrons resist further compression by the Pauli exclusion principle, in a way analogous to electron degeneracy pressure, but stronger. These stars, known as neutron stars, are extremely small—on the order of radius 10 km, no bigger than the size of a large city—and are phenomenally dense. Their period of rotation shortens dramatically as the stars shrink (due to conservation of angular momentum); observed rotational periods of neutron stars range from about 1.5 milliseconds (over 600 revolutions per second) to several seconds. When these rapidly rotating stars' magnetic poles are aligned with the Earth, we detect a pulse of radiation each revolution. Such neutron stars are called pulsars, and were the first neutron stars to be discovered. Though electromagnetic radiation detected from pulsars is most often in the form of radio waves, pulsars have also been detected at visible, X-ray, and gamma ray wavelengths. If the mass of the stellar remnant is high enough, the neutron degeneracy pressure will be insufficient to prevent collapse below the Schwarzschild radius. The stellar remnant thus becomes a black hole."
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_23",
    "chunk": "The mass at which this occurs is not known with certainty, but is currently estimated at between 2 and 3 M☉. Black holes are predicted by the theory of general relativity. According to classical general relativity, no matter or information can flow from the interior of a black hole to an outside observer, although quantum effects may allow deviations from this strict rule. The existence of black holes in the universe is well supported, both theoretically and by astronomical observation. Because the core-collapse mechanism of a supernova is, at present, only partially understood, it is still not known whether it is possible for a star to collapse directly to a black hole without producing a visible supernova, or whether some supernovae initially form unstable neutron stars which then collapse into black holes; the exact relation between the initial mass of the star and the final remnant is also not completely certain. Resolution of these uncertainties requires the analysis of more supernovae and supernova remnants. A stellar evolutionary model is a mathematical model that can be used to compute the evolutionary phases of a star from its formation until it becomes a remnant. The mass and chemical composition of the star"
  },
  {
    "source": "Stellar evolution.txt",
    "chunk_id": "Stellar evolution.txt_24",
    "chunk": "are used as the inputs, and the luminosity and surface temperature are the only constraints. The model formulae are based upon the physical understanding of the star, usually under the assumption of hydrostatic equilibrium. Extensive computer calculations are then run to determine the changing state of the star over time, yielding a table of data that can be used to determine the evolutionary track of the star across the Hertzsprung–Russell diagram, along with other evolving properties. Accurate models can be used to estimate the current age of a star by comparing its physical properties with those of stars along a matching evolutionary track."
  },
  {
    "source": "Stellar population.txt",
    "chunk_id": "Stellar population.txt_0",
    "chunk": "# Stellar population In 1944, Walter Baade categorized groups of stars within the Milky Way into stellar populations. In the abstract of the article by Baade, he recognizes that Jan Oort originally conceived this type of classification in 1926. Baade observed that bluer stars were strongly associated with the spiral arms, and yellow stars dominated near the central galactic bulge and within globular star clusters. Two main divisions were deemed population I and population II stars, with another newer, hypothetical division called population III added in 1978. Among the population types, significant differences were found with their individual observed stellar spectra. These were later shown to be very important and were possibly related to star formation, observed kinematics, stellar age, and even galaxy evolution in both spiral and elliptical galaxies. These three simple population classes usefully divided stars by their chemical composition, or metallicity. In astrophysics nomenclature metal refers to all elements heavier than helium, including chemical non-metals such as oxygen. By definition, each population group shows the trend where lower metal content indicates higher age of stars. Hence, the first stars in the universe (very low metal content) were deemed population III, old stars (low metallicity) as population II,"
  },
  {
    "source": "Stellar population.txt",
    "chunk_id": "Stellar population.txt_1",
    "chunk": "and recent stars (high metallicity) as population I. The Sun is considered population I, a recent star with a relatively high 1.4% metallicity. Observation of stellar spectra has revealed that stars older than the Sun have fewer heavy elements compared with the Sun. This immediately suggests that metallicity has evolved through the generations of stars by the process of stellar nucleosynthesis. Under current cosmological models, all matter created in the Big Bang was mostly hydrogen (75%) and helium (25%), with only a very tiny fraction consisting of other light elements such as lithium and beryllium. When the universe had cooled sufficiently, the first stars were born as population III stars, without any contaminating heavier metals. This is postulated to have affected their structure so that their stellar masses became hundreds of times more than that of the Sun. In turn, these massive stars also evolved very quickly, and their nucleosynthetic processes created the first 26 elements (up to iron in the periodic table). Many theoretical stellar models show that most high-mass population III stars rapidly exhausted their fuel and likely exploded in extremely energetic pair-instability supernovae. Those explosions would have thoroughly dispersed their material, ejecting metals into the interstellar medium"
  },
  {
    "source": "Stellar population.txt",
    "chunk_id": "Stellar population.txt_2",
    "chunk": "(ISM), to be incorporated into the later generations of stars. Their destruction suggests that no galactic high-mass population III stars should be observable. However, some population III stars might be seen in high-redshift galaxies whose light originated during the earlier history of the universe. Scientists have found evidence of an extremely small ultra metal-poor star, slightly smaller than the Sun, found in a binary system of the spiral arms in the Milky Way. The discovery opens up the possibility of observing even older stars. Stars too massive to produce pair-instability supernovae would have likely collapsed into black holes through a process known as photodisintegration. Here some matter may have escaped during this process in the form of relativistic jets, and this could have distributed the first metals into the universe. The oldest stars observed thus far, known as population II, have very low metallicities; as subsequent generations of stars were born, they became more metal-enriched, as the gaseous clouds from which they formed received the metal-rich dust manufactured by previous generations of stars from population III. As those population II stars died, they returned metal-enriched material to the interstellar medium via planetary nebulae and supernovae, enriching further the nebulae, out"
  },
  {
    "source": "Stellar population.txt",
    "chunk_id": "Stellar population.txt_3",
    "chunk": "of which the newer stars formed. These youngest stars, including the Sun, therefore have the highest metal content, and are known as population I stars. Population I stars are young stars with the highest metallicity out of all three populations and are more commonly found in the spiral arms of the Milky Way galaxy. The Sun is considered as an intermediate population I star, while the sun-like μ Arae is much richer in metals. (The term metal-rich is used to describe stars with a significantly higher metallicity than the Sun; higher than can be explained by measurement error.) Population I stars usually have regular elliptical orbits of the Galactic Center, with a low relative velocity. It was earlier hypothesized that the high metallicity of population I stars makes them more likely to possess planetary systems than the other two populations, because planets, particularly terrestrial planets, are thought to be formed by the accretion of metals. However, observations of the Kepler Space Telescope data have found smaller planets around stars with a range of metallicities, while only larger, potential gas giant planets are concentrated around stars with relatively higher metallicity – a finding that has implications for theories of gas-giant formation."
  },
  {
    "source": "Stellar population.txt",
    "chunk_id": "Stellar population.txt_4",
    "chunk": "Between the intermediate population I and the population II stars comes the intermediate disc population. Population II, or metal-poor, stars are those with relatively little of the elements heavier than helium. These objects were formed during an earlier time of the universe. Intermediate population II stars are common in the bulge near the centre of the Milky Way, whereas population II stars found in the galactic halo are older and thus more metal-deficient. Globular clusters also contain high numbers of population II stars. A characteristic of population II stars is that despite their lower overall metallicity, they often have a higher ratio of alpha elements (elements produced by the alpha process, like oxygen and neon) relative to iron (Fe) as compared with population I stars; current theory suggests that this is the result of type II supernovas being more important contributors to the interstellar medium at the time of their formation, whereas type Ia supernova metal-enrichment came at a later stage in the universe's development. Scientists have targeted these oldest stars in several different surveys, including the HK objective-prism survey of Timothy C. Beers et al. and the Hamburg-ESO survey of Norbert Christlieb et al., originally started for faint quasars."
  },
  {
    "source": "Stellar population.txt",
    "chunk_id": "Stellar population.txt_5",
    "chunk": "Thus far, they have uncovered and studied in detail about ten ultra-metal-poor (UMP) stars (such as Sneden's Star, Cayrel's Star, BD +17° 3248) and three of the oldest stars known to date: HE 0107-5240, HE 1327-2326 and HE 1523-0901. Caffau's star was identified as the most metal-poor star yet when it was found in 2012 using Sloan Digital Sky Survey data. However, in February 2014 the discovery of an even lower-metallicity star was announced, SMSS J031300.36-670839.3 located with the aid of SkyMapper astronomical survey data. Less extreme in their metal deficiency, but nearer and brighter and hence longer known, are HD 122563 (a red giant) and HD 140283 (a subgiant). Population III stars are a hypothetical population of extremely massive, luminous and hot stars with virtually no \"metals\", except possibly for intermixing ejecta from other nearby, early population III supernovae. The term was first introduced by Neville J. Woolf in 1965. Such stars are likely to have existed in the very early universe (i.e., at high redshift) and may have started the production of chemical elements heavier than hydrogen, which are needed for the later formation of planets and life as we know it. The existence of population III stars"
  },
  {
    "source": "Stellar population.txt",
    "chunk_id": "Stellar population.txt_6",
    "chunk": "is inferred from physical cosmology, but they have not yet been observed directly. Indirect evidence for their existence has been found in a gravitationally lensed galaxy in a very distant part of the universe. Their existence may account for the fact that heavy elements – which could not have been created in the Big Bang – are observed in quasar emission spectra. They are also thought to be components of faint blue galaxies. These stars likely triggered the universe's period of reionization, a major phase transition of the hydrogen gas composing most of the interstellar medium. Observations of the galaxy UDFy-38135539 suggest that it may have played a role in this reionization process. The European Southern Observatory discovered a bright pocket of early population stars in the very bright galaxy Cosmos Redshift 7 from the reionization period around 800 million years after the Big Bang, at z = 6.60. The rest of the galaxy has some later redder population II stars. Some theories hold that there were two generations of population III stars. Current theory is divided on whether the first stars were very massive or not. One possibility is that these stars were much larger than current stars: several"
  },
  {
    "source": "Stellar population.txt",
    "chunk_id": "Stellar population.txt_7",
    "chunk": "hundred solar masses, and possibly up to 1,000 solar masses. Such stars would be very short-lived and last only 2–5 million years. Such large stars may have been possible due to the lack of heavy elements and a much warmer interstellar medium from the Big Bang. Conversely, theories proposed in 2009 and 2011 suggest that the first star groups might have consisted of a massive star surrounded by several smaller stars. The smaller stars, if they remained in the birth cluster, would accumulate more gas and could not survive to the present day, but a 2017 study concluded that if a star of 0.8 solar masses (M☉) or less was ejected from its birth cluster before it accumulated more mass, it could survive to the present day, possibly even in our Milky Way galaxy. Analysis of data of extremely low-metallicity population II stars such as HE 0107-5240, which are thought to contain the metals produced by population III stars, suggest that these metal-free stars had masses of 20~130 solar masses. On the other hand, analysis of globular clusters associated with elliptical galaxies suggests pair-instability supernovae, which are typically associated with very massive stars, were responsible for their metallic composition. This"
  },
  {
    "source": "Stellar population.txt",
    "chunk_id": "Stellar population.txt_8",
    "chunk": "also explains why there have been no low-mass stars with zero metallicity observed, despite models constructed for smaller population III stars. Clusters containing zero-metallicity red dwarfs or brown dwarfs (possibly created by pair-instability supernovae) have been proposed as dark matter candidates, but searches for these types of MACHOs through gravitational microlensing have produced negative results. Population III stars are considered seeds of black holes in the early universe. Unlike high-mass black hole seeds, such as direct collapse black holes, they would have produced light ones. If they could have grown to larger than expected masses, then they could have been quasi-stars, other hypothetical seeds of heavy black holes which would have existed in the early development of the Universe before hydrogen and helium were contaminated by heavier elements. Detection of population III stars is a goal of NASA's James Webb Space Telescope. On 8 December 2022, astronomers reported the possible detection of Population III stars, in a high-redshift galaxy called RX J2129–z8He II."
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_0",
    "chunk": "# Stephen Hawking Stephen William Hawking (8 January 1942 – 14 March 2018) was an English theoretical physicist, cosmologist, and author who was director of research at the Centre for Theoretical Cosmology at the University of Cambridge. Between 1979 and 2009, he was the Lucasian Professor of Mathematics at Cambridge, widely viewed as one of the most prestigious academic posts in the world. Hawking was born in Oxford into a family of physicians. In October 1959, at the age of 17, he began his university education at University College, Oxford, where he received a first-class BA degree in physics. In October 1962, he began his graduate work at Trinity Hall, Cambridge, where, in March 1966, he obtained his PhD in applied mathematics and theoretical physics, specialising in general relativity and cosmology. In 1963, at age 21, Hawking was diagnosed with an early-onset slow-progressing form of motor neurone disease that gradually, over decades, paralysed him. After the loss of his speech, he communicated through a speech-generating device, initially through use of a handheld switch, and eventually by using a single cheek muscle. Hawking's scientific works included a collaboration with Roger Penrose on gravitational singularity theorems in the framework of general relativity,"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_1",
    "chunk": "and the theoretical prediction that black holes emit radiation, often called Hawking radiation. Initially, Hawking radiation was controversial. By the late 1970s, and following the publication of further research, the discovery was widely accepted as a major breakthrough in theoretical physics. Hawking was the first to set out a theory of cosmology explained by a union of the general theory of relativity and quantum mechanics. Hawking was a vigorous supporter of the many-worlds interpretation of quantum mechanics. He also introduced the notion of a micro black hole. Hawking achieved commercial success with several works of popular science in which he discussed his theories and cosmology in general. His book A Brief History of Time appeared on the Sunday Times bestseller list for a record-breaking 237 weeks. Hawking was a Fellow of the Royal Society, a lifetime member of the Pontifical Academy of Sciences, and a recipient of the Presidential Medal of Freedom, the highest civilian award in the United States. In 2002, Hawking was ranked number 25 in the BBC's poll of the 100 Greatest Britons. He died in 2018 at the age of 76, having lived more than 50 years following his diagnosis of motor neurone disease. Hawking was"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_2",
    "chunk": "born on 8 January 1942 in Oxford to Frank and Isobel Eileen Hawking (née Walker). Hawking's mother was born into a family of doctors in Glasgow, Scotland. His wealthy paternal great-grandfather, from Yorkshire, over-extended himself buying farm land and then went bankrupt in the great agricultural depression during the early 20th century. His paternal great-grandmother saved the family from financial ruin by opening a school in their home. Despite their families' financial constraints, both parents attended the University of Oxford, where Frank read medicine and Isobel read Philosophy, Politics and Economics. Isobel worked as a secretary for a medical research institute, and Frank was a medical researcher specializing in tropical diseases. Hawking said of his father, \"I modeled myself on him. Because he was a scientific researcher, I felt that scientific research was the natural thing to do when one grew up. The only difference was that I was not attracted to medicine or biology because they seemed too inexact and descriptive. I wanted something more fundamental, and I found it in physics.\" In 1950, when Hawking's father became head of the division of parasitology at the National Institute for Medical Research, the family moved to St Albans, Hertfordshire. In"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_3",
    "chunk": "St Albans, the family was considered highly intelligent and somewhat eccentric; meals were often spent with each person silently reading a book. They lived a frugal existence in a large, cluttered, and poorly maintained house and travelled in a converted London taxicab. During one of Hawking's father's frequent absences working in Africa, the rest of the family spent four months in Mallorca visiting his mother's friend Beryl and her husband, the poet Robert Graves. Hawking began his schooling at the Byron House School in Highgate, London. He later blamed its \"progressive methods\" for his failure to learn to read while at the school. In St Albans, the eight-year-old Hawking attended St Albans High School for Girls for a few months. At that time, younger boys could attend one of the houses. Hawking attended two private (i.e. fee-paying) schools, first Radlett School and from September 1952, St Albans School, Hertfordshire, after passing the eleven-plus a year early. The family placed a high value on education. Hawking's father wanted his son to attend Westminster School, but the 13-year-old Hawking was ill on the day of the scholarship examination. His family could not afford the school fees without the financial aid of a"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_4",
    "chunk": "scholarship, so Hawking remained at St Albans. A positive consequence was that Hawking remained close to a group of friends with whom he enjoyed board games, the manufacture of fireworks, model aeroplanes and boats, and long discussions about Christianity and extrasensory perception. From 1958 on, with the help of the mathematics teacher Dikran Tahta, they built a computer from clock parts, an old telephone switchboard and other recycled components. In 1959, he built a record player from spare parts: \"My father would have regarded it as recklessly self-indulgent, to buy a record player, but I persuaded him that I could assemble one from parts that I could buy cheap. That appealed to him as a Yorkshireman.\" Although known at school as \"Einstein\", Hawking was not initially successful academically. With time, he began to show considerable aptitude for scientific subjects and, inspired by Tahta, decided to study mathematics at university. Hawking's father advised him to study medicine, concerned that there were few jobs for mathematics graduates. He also wanted his son to attend University College, Oxford, his own alma mater. As it was not possible to read mathematics there at the time, Hawking decided to study physics and chemistry. Despite his"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_5",
    "chunk": "headmaster's advice to wait until the next year, Hawking was awarded a scholarship after taking the examinations in March 1959. Hawking began his university education at University College, Oxford, in October 1959 at the age of 17. For the first eighteen months, he was bored and lonely – he found the academic work \"ridiculously easy\". His physics tutor, Robert Berman, later said, \"It was only necessary for him to know that something could be done, and he could do it without looking to see how other people did it.\" A change occurred during his second and third years when, according to Berman, Hawking made more of an effort \"to be one of the boys\". He developed into a popular, lively and witty college member, interested in classical music and science fiction. Part of the transformation resulted from his decision to join the college boat club, the University College Boat Club, where he coxed a rowing crew. The rowing coach at the time noted that Hawking cultivated a daredevil image, steering his crew on risky courses that led to damaged boats. Hawking estimated that he studied about 1,000 hours during his three years at Oxford. These unimpressive study habits made sitting"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_6",
    "chunk": "his finals a challenge, and he decided to answer only theoretical physics questions rather than those requiring factual knowledge. A first-class degree was a condition of acceptance for his planned graduate study in cosmology at the University of Cambridge. Anxious, he slept poorly the night before the examinations, and the result was on the borderline between first- and second-class honours, making a viva (oral examination) with the Oxford examiners necessary. Hawking was concerned that he was viewed as a lazy and difficult student. So, when asked at the viva to describe his plans, he said, \"If you award me a First, I will go to Cambridge. If I receive a Second, I shall stay in Oxford, so I expect you will give me a First.\" He was held in higher regard than he believed; as Berman commented, the examiners \"were intelligent enough to realise they were talking to someone far cleverer than most of themselves\". After receiving a first-class BA degree in physics and completing a trip to Iran with a friend, he began his graduate work at Trinity Hall, Cambridge, in October 1962. Hawking's first year as a doctoral student was difficult. He was initially disappointed to find that"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_7",
    "chunk": "he had been assigned Dennis William Sciama, one of the founders of modern cosmology, as a supervisor rather than the noted astronomer Fred Hoyle, and he found his training in mathematics inadequate for work in general relativity and cosmology. After being diagnosed with motor neurone disease, Hawking fell into a depression – though his doctors advised that he continue with his studies, he felt there was little point. His disease progressed more slowly than doctors had predicted. Although Hawking had difficulty walking unsupported, and his speech was almost unintelligible, an initial diagnosis that he had only two years to live proved unfounded. With Sciama's encouragement, he returned to his work. Hawking started developing a reputation for brilliance and brashness when he publicly challenged the work of Hoyle and his student Jayant Narlikar at a lecture in June 1964. Hawking reflected, \"Before I got motor neuron disease, I was bored with life. But the prospect of an early death made me realize life was really worth living.\" When Hawking began his doctoral studies, there was much debate in the physics community about the prevailing theories of the creation of the universe: the Big Bang and Steady State theories. Inspired by Roger"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_8",
    "chunk": "Penrose's theorem of a spacetime singularity in the centre of black holes, Hawking applied the same thinking to the entire universe; and, during 1965, he wrote his thesis on this topic. Hawking's thesis was approved in 1966. There were other positive developments: Hawking received a research fellowship at Gonville and Caius College at Cambridge; he obtained his PhD degree in applied mathematics and theoretical physics, specialising in general relativity and cosmology, in March 1966; and his essay \"Singularities and the Geometry of Space–Time\" shared top honours with one by Penrose to win that year's prestigious Adams Prize. In his work, and in collaboration with Penrose, Hawking extended the singularity theorem concepts first explored in his doctoral thesis. This included not only the existence of singularities but also the theory that the universe might have started as a singularity. Their joint essay was the runner-up in the 1968 Gravity Research Foundation competition. In 1970, they published a proof that if the universe obeys the general theory of relativity and fits any of the models of physical cosmology developed by Alexander Friedmann, then it must have begun as a singularity. In 1969, Hawking accepted a specially created Fellowship for Distinction in Science"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_9",
    "chunk": "to remain at Caius. In 1970, Hawking postulated what became known as the second law of black hole dynamics, that the event horizon of a black hole can never get smaller. With James M. Bardeen and Brandon Carter, he proposed the four laws of black hole mechanics, drawing an analogy with thermodynamics. To Hawking's irritation, Jacob Bekenstein, a graduate student of John Wheeler, went further—and ultimately correctly—to apply thermodynamic concepts literally. In the early 1970s, Hawking's work with Carter, Werner Israel, and David C. Robinson strongly supported Wheeler's no-hair theorem, one that states that no matter what the original material from which a black hole is created, it can be completely described by the properties of mass, electrical charge and rotation. His essay titled \"Black Holes\" won the Gravity Research Foundation Award in January 1971. Hawking's first book, The Large Scale Structure of Space-Time, written with George Ellis, was published in 1973. Beginning in 1973, Hawking moved into the study of quantum gravity and quantum mechanics. His work in this area was spurred by a visit to Moscow and discussions with Yakov Borisovich Zel'dovich and Alexei Starobinsky, whose work showed that according to the uncertainty principle, rotating black holes emit"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_10",
    "chunk": "particles. To Hawking's annoyance, his much-checked calculations produced findings that contradicted his second law, which claimed black holes could never get smaller, and supported Bekenstein's reasoning about their entropy. His results, which Hawking presented from 1974, showed that black holes emit radiation, known today as Hawking radiation, which may continue until they exhaust their energy and evaporate. Initially, Hawking radiation was controversial. By the late 1970s and following the publication of further research, the discovery was widely accepted as a significant breakthrough in theoretical physics. Hawking was elected a Fellow of the Royal Society (FRS) in 1974, a few weeks after the announcement of Hawking radiation. At the time, he was one of the youngest scientists to become a Fellow. Hawking was appointed to the Sherman Fairchild Distinguished Visiting Professorship at the California Institute of Technology (Caltech) in 1974. He worked with a friend on the faculty, Kip Thorne, and engaged him in a scientific wager about whether the X-ray source Cygnus X-1 was a black hole. The wager was an \"insurance policy\" against the proposition that black holes did not exist. Hawking acknowledged that he had lost the bet in 1990, a bet that was the first of several"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_11",
    "chunk": "he was to make with Thorne and others. Hawking had maintained ties to Caltech, spending a month there almost every year since this first visit. Hawking returned to Cambridge in 1975 to a more academically senior post, as reader in gravitational physics. The mid-to-late 1970s were a period of growing public interest in black holes and the physicists who were studying them. Hawking was regularly interviewed for print and television. He also received increasing academic recognition of his work. In 1975, he was awarded both the Eddington Medal and the Pius XI Gold Medal, and in 1976 the Dannie Heineman Prize, the Maxwell Medal and Prize and the Hughes Medal. He was appointed a professor with a chair in gravitational physics in 1977. The following year he received the Albert Einstein Medal and an honorary doctorate from the University of Oxford. In 1979, Hawking was elected Lucasian Professor of Mathematics at the University of Cambridge. His inaugural lecture in this role was titled: \"Is the End in Sight for Theoretical Physics?\" and proposed N = 8 supergravity as the leading theory to solve many of the outstanding problems physicists were studying. His promotion coincided with a health-crisis which led to"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_12",
    "chunk": "his accepting, albeit reluctantly, some nursing services at home. At the same time, he was also making a transition in his approach to physics, becoming more intuitive and speculative rather than insisting on mathematical proofs. \"I would rather be right than rigorous\", he told Kip Thorne. In 1981, he proposed that information in a black hole is irretrievably lost when a black hole evaporates. This information paradox violates the fundamental tenet of quantum mechanics, and led to years of debate, including \"the Black Hole War\" with Leonard Susskind and Gerard 't Hooft. Cosmological inflation – a theory proposing that following the Big Bang, the universe initially expanded incredibly rapidly before settling down to a slower expansion – was proposed by Alan Guth and also developed by Andrei Linde. Following a conference in Moscow in October 1981, Hawking and Gary Gibbons organised a three-week Nuffield Workshop in the summer of 1982 on \"The Very Early Universe\" at Cambridge University, a workshop that focused mainly on inflation theory. Hawking also began a new line of quantum-theory research into the origin of the universe. In 1981 at a Vatican conference, he presented work suggesting that there might be no boundary – or beginning"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_13",
    "chunk": "or ending – to the universe. Hawking subsequently developed the research in collaboration with Jim Hartle, and in 1983 they published a model, known as the Hartle–Hawking state. It proposed that prior to the Planck epoch, the universe had no boundary in space-time; before the Big Bang, time did not exist and the concept of the beginning of the universe is meaningless. The initial singularity of the classical Big Bang models was replaced with a region akin to the North Pole. One cannot travel north of the North Pole, but there is no boundary there – it is simply the point where all north-running lines meet and end. Initially, the no-boundary proposal predicted a closed universe, which had implications about the existence of God. As Hawking explained, \"If the universe has no boundaries but is self-contained... then God would not have had any freedom to choose how the universe began.\" Further work by Hawking in the area of arrows of time led to the 1985 publication of a paper theorising that if the no-boundary proposition were correct, then when the universe stopped expanding and eventually collapsed, time would run backwards. A paper by Don Page and independent calculations by Raymond"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_14",
    "chunk": "Laflamme led Hawking to withdraw this concept. Honours continued to be awarded: in 1981 he was awarded the American Franklin Medal, and in the 1982 New Year Honours appointed a Commander of the Order of the British Empire (CBE). These awards did not significantly change Hawking's financial status, and motivated by the need to finance his children's education and home-expenses, he decided in 1982 to write a popular book about the universe that would be accessible to the general public. Instead of publishing with an academic press, he signed a contract with Bantam Books, a mass-market publisher, and received a large advance for his book. A first draft of the book, called A Brief History of Time, was completed in 1984. One of the first messages Hawking produced with his speech-generating device was a request for his assistant to help him finish writing A Brief History of Time. Peter Guzzardi, his editor at Bantam, pushed him to explain his ideas clearly in non-technical language, a process that required many revisions from an increasingly irritated Hawking. The book was published in April 1988 in the US and in June in the UK, and it proved to be an extraordinary success, rising"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_15",
    "chunk": "quickly to the top of best-seller lists in both countries and remaining there for months. The book was translated into many languages, and as of 2009, has sold an estimated 9 million copies. Media attention was intense, and a Newsweek magazine-cover and a television special both described him as \"Master of the Universe\". Success led to significant financial rewards, but also the challenges of celebrity status. Hawking travelled extensively to promote his work, and enjoyed partying into the late hours. A difficulty refusing the invitations and visitors left him limited time for work and his students. Some colleagues were resentful of the attention Hawking received, feeling it was due to his disability. He received further academic recognition, including five more honorary degrees, the Gold Medal of the Royal Astronomical Society (1985), the Paul Dirac Medal (1987) and, jointly with Penrose, the prestigious Wolf Prize (1988). In the 1989 Birthday Honours, he was appointed a Member of the Order of the Companions of Honour (CH). He reportedly declined a knighthood in the late 1990s in objection to the UK's science funding policy. Hawking pursued his work in physics: in 1993 he co-edited a book on Euclidean quantum gravity with Gary Gibbons"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_16",
    "chunk": "and published a collected edition of his own articles on black holes and the Big Bang. In 1994, at Cambridge's Newton Institute, Hawking and Penrose delivered a series of six lectures that were published in 1996 as \"The Nature of Space and Time\". In 1997, he conceded a 1991 public scientific wager made with Kip Thorne and John Preskill of Caltech. Hawking had bet that Penrose's proposal of a \"cosmic censorship conjecture\" – that there could be no \"naked singularities\" unclothed within a horizon – was correct. After discovering his concession might have been premature, a new and more refined wager was made. This one specified that such singularities would occur without extra conditions. The same year, Thorne, Hawking and Preskill made another bet, this time concerning the black hole information paradox. Thorne and Hawking argued that since general relativity made it impossible for black holes to radiate and lose information, the mass-energy and information carried by Hawking radiation must be \"new\", and not from inside the black hole event horizon. Since this contradicted the quantum mechanics of microcausality, quantum mechanics theory would need to be rewritten. Preskill argued the opposite, that since quantum mechanics suggests that the information emitted"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_17",
    "chunk": "by a black hole relates to information that fell in at an earlier time, the concept of black holes given by general relativity must be modified in some way. Hawking also maintained his public profile, including bringing science to a wider audience. The documentary A Brief History of Time, directed by Errol Morris and produced by Steven Spielberg, premiered in 1991; the film contains material from the book as well as interviews with Hawking, his friends, family and colleagues. Hawking had wanted the film to be scientific rather than biographical, but he was persuaded otherwise. The film, while a critical success, was not widely released. A popular-level collection of essays, interviews, and talks titled Black Holes and Baby Universes and Other Essays was published in 1993, and a six-part television series Stephen Hawking's Universe and a companion book appeared in 1997. As Hawking insisted, this time the focus was entirely on science. Hawking continued his writings for a popular audience, publishing The Universe in a Nutshell in 2001, and A Briefer History of Time, which he wrote in 2005 with Leonard Mlodinow to update his earlier works with the aim of making them accessible to a wider audience, and God"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_18",
    "chunk": "Created the Integers, which appeared in 2006. Along with Thomas Hertog at CERN and Jim Hartle, from 2006 on Hawking developed a theory of top-down cosmology, which says that the universe had not one unique initial state but many different ones, and therefore that it is inappropriate to formulate a theory that predicts the universe's current configuration from one particular initial state. Top-down cosmology posits that the present \"selects\" the past from a superposition of many possible histories. In doing so, the theory suggests a possible resolution of the fine-tuning question. Hawking continued to travel widely, including trips to Chile, Easter Island, South Africa, Spain (to receive the Fonseca Prize in 2008), Canada, and numerous trips to the United States. For practical reasons related to his disability, Hawking increasingly travelled by private jet, and by 2011 that had become his only mode of international travel. By 2003, consensus among physicists was growing that Hawking was wrong about the loss of information in a black hole. In a 2004 lecture in Dublin, he conceded his 1997 bet with Preskill, but described his own, somewhat controversial solution to the information paradox problem, involving the possibility that black holes have more than one"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_19",
    "chunk": "topology. In the 2005 paper he published on the subject, he argued that the information paradox was explained by examining all the alternative histories of universes, with the information loss in those with black holes being cancelled out by those without such loss. In January 2014, he called the alleged loss of information in black holes his \"biggest blunder\". As part of another longstanding scientific dispute, Hawking had emphatically argued, and bet, that the Higgs boson would never be found. The particle was proposed to exist as part of the Higgs field theory by Peter Higgs in 1964. Hawking and Higgs engaged in a heated and public debate over the matter in 2002 and again in 2008, with Higgs criticising Hawking's work and complaining that Hawking's \"celebrity status gives him instant credibility that others do not have\". The particle was discovered in July 2012 at CERN following construction of the Large Hadron Collider. Hawking quickly conceded that he had lost his bet and said that Higgs should win the Nobel Prize for Physics, which he did in 2013. In 2007, Hawking and his daughter Lucy published George's Secret Key to the Universe, a children's book designed to explain theoretical physics"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_20",
    "chunk": "in an accessible fashion and featuring characters similar to those in the Hawking family. The book was followed by sequels in 2009, 2011, 2014 and 2016. In 2002, following a UK-wide vote, the BBC included Hawking in their list of the 100 Greatest Britons. He was awarded the Copley Medal from the Royal Society (2006), the Presidential Medal of Freedom, which is America's highest civilian honour (2009), and the Russian Special Fundamental Physics Prize (2013). Several buildings have been named after him, including the Stephen W. Hawking Science Museum in San Salvador, El Salvador, the Stephen Hawking Building in Cambridge, and the Stephen Hawking Centre at the Perimeter Institute in Canada. Appropriately, given Hawking's association with time, he unveiled the mechanical \"Chronophage\" (or time-eating) Corpus Clock at Corpus Christi College, Cambridge in September 2008. During his career, Hawking supervised 39 successful PhD students. One doctoral student did not successfully complete the PhD. As required by Cambridge University policy, Hawking retired as Lucasian Professor of Mathematics in 2009. Despite suggestions that he might leave the United Kingdom as a protest against public funding cuts to basic scientific research, Hawking worked as director of research at the Cambridge University Department of Applied"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_21",
    "chunk": "Mathematics and Theoretical Physics. On 28 June 2009, as a tongue-in-cheek test of his 1992 conjecture that travel into the past is effectively impossible, Hawking held a party open to all, complete with hors d'oeuvres and iced champagne, but publicised the party only after it was over so that only time-travellers would know to attend; as expected, nobody showed up to the party. On 20 July 2015, Hawking helped launch Breakthrough Initiatives, an effort to search for extraterrestrial life. Hawking created Stephen Hawking: Expedition New Earth, a documentary on space colonisation, as a 2017 episode of Tomorrow's World. In August 2015, Hawking said that not all information is lost when something enters a black hole and there might be a possibility to retrieve information from a black hole according to his theory. In July 2017, Hawking was awarded an Honorary Doctorate from Imperial College London. Hawking's final paper – A smooth exit from eternal inflation? – was posthumously published in the Journal of High Energy Physics on 27 April 2018. Hawking met his future wife, Jane Wilde, at a party in 1962. The following year, Hawking was diagnosed with motor neurone disease. In October 1964, the couple became engaged to"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_22",
    "chunk": "marry, aware of the potential challenges that lay ahead due to Hawking's shortened life expectancy and physical limitations. Hawking later said that the engagement gave him \"something to live for\". The two were married on 14 July 1965 in their shared hometown of St Albans. The couple resided in Cambridge, within Hawking's walking distance to the Department of Applied Mathematics and Theoretical Physics (DAMTP). During their first years of marriage, Jane lived in London during the week as she completed her degree at Westfield College. They travelled to the United States several times for conferences and physics-related visits. Jane began a PhD programme through Westfield College in medieval Spanish poetry (completed in 1981). The couple had three children: Robert, born May 1967, Lucy, born November 1970, and Timothy, born April 1979. Hawking rarely discussed his illness and physical challenges—even, in a precedent set during their courtship, with Jane. His disabilities meant that the responsibilities of home and family rested firmly on his wife's shoulders, leaving him more time to think about physics. Upon his appointment in 1974 to a year-long position at the California Institute of Technology in Pasadena, California, Jane proposed that a graduate or post-doctoral student live with"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_23",
    "chunk": "them and help with his care. Hawking accepted, and Bernard Carr travelled with them as the first of many students who fulfilled this role. The family spent a generally happy and stimulating year in Pasadena. Hawking returned to Cambridge in 1975 to a new home and a new job, as reader. Don Page, with whom Hawking had begun a close friendship at Caltech, arrived to work as the live-in graduate student assistant. With Page's help and that of a secretary, Jane's responsibilities were reduced so she could return to her doctoral thesis and her new interest in singing. Around December 1977, Jane met organist Jonathan Hellyer Jones when singing in a church choir. Hellyer Jones became close to the Hawking family and, by the mid-1980s, he and Jane had developed romantic feelings for each other. According to Jane, her husband was accepting of the situation, stating \"he would not object so long as I continued to love him\". Jane and Hellyer Jones were determined not to break up the family, and their relationship remained platonic for a long period. By the 1980s, Hawking's marriage had been strained for many years. Jane felt overwhelmed by the intrusion into their family life"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_24",
    "chunk": "of the required nurses and assistants. The impact of his celebrity status was challenging for colleagues and family members, while the prospect of living up to a worldwide fairytale image was daunting for the couple. Hawking's views of religion also contrasted with her strong Christian faith and resulted in tension. After a tracheotomy in 1985, Hawking required a full-time nurse and nursing care was split across three shifts daily. In the late 1980s, Hawking grew close to one of his nurses, Elaine Mason, to the dismay of some colleagues, caregivers, and family members, who were disturbed by her strength of personality and protectiveness. In February 1990, Hawking told Jane that he was leaving her for Mason and departed the family home. After his divorce from Jane in 1995, Hawking married Mason in September, declaring, \"It's wonderful – I have married the woman I love.\" In 1999, Jane Hawking published a memoir, Music to Move the Stars, describing her marriage to Hawking and its breakdown. Its revelations caused a sensation in the media but, as was his usual practice regarding his personal life, Hawking made no public comment except to say that he did not read biographies about himself. After his"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_25",
    "chunk": "second marriage, Hawking's family felt excluded and marginalised from his life. For a period of about five years in the early 2000s, his family and staff became increasingly worried that he was being physically abused. Police investigations took place, but were closed as Hawking refused to make a complaint. In 2006, Hawking and Mason quietly divorced, and Hawking resumed closer relationships with Jane, his children, and his grandchildren. Reflecting on this happier period, a revised version of Jane's book, re-titled Travelling to Infinity: My Life with Stephen, appeared in 2007, and was made into a film, The Theory of Everything, in 2014. Hawking had a rare early-onset, slow-progressing form of motor neurone disease (MND; also known as amyotrophic lateral sclerosis (ALS) or Lou Gehrig's disease), which gradually paralysed him over decades. Hawking had experienced increasing clumsiness during his final year at Oxford, including a fall on some stairs and difficulties when rowing. The problems worsened, and his speech became slightly slurred. His family noticed the changes when he returned home for Christmas, and medical investigations were begun. The MND diagnosis came when Hawking was 21, in 1963. At the time, doctors gave him a life expectancy of two years. In"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_26",
    "chunk": "the late 1960s, Hawking's physical abilities declined: he began to use crutches and could no longer give lectures regularly. As he slowly lost the ability to write, he developed compensatory visual methods, including seeing equations in terms of geometry. The physicist Werner Israel later compared the achievements to Mozart composing an entire symphony in his head. Hawking was fiercely independent and unwilling to accept help or make concessions for his disabilities. He preferred to be regarded as \"a scientist first, popular science writer second, and, in all the ways that matter, a normal human being with the same desires, drives, dreams, and ambitions as the next person\". His wife Jane later noted: \"Some people would call it determination, some obstinacy. I've called it both at one time or another.\" He required much persuasion to accept the use of a wheelchair at the end of the 1960s, but ultimately became notorious for the wildness of his wheelchair driving. Hawking was a popular and witty colleague, but his illness, as well as his reputation for brashness, distanced him from some. When Hawking first began using a wheelchair he was using standard motorised models. The earliest surviving example of these chairs was made"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_27",
    "chunk": "by BEC Mobility and sold by Christie's in November 2018 for £296,750. Hawking continued to use this type of chair until the early 1990s, at which time his ability to use his hands to drive a wheelchair deteriorated. Hawking used a variety of different chairs from that time, including a DragonMobility Dragon elevating powerchair from 2007, as shown in the April 2008 photo of Hawking attending NASA's 50th anniversary; a Permobil C350 from 2014; and then a Permobil F3 from 2016. Hawking's speech deteriorated, and by the late 1970s he could be understood by only his family and closest friends. To communicate with others, someone who knew him well would interpret his speech into intelligible speech. Spurred by a dispute with the university over who would pay for the ramp needed for him to enter his workplace, Hawking and his wife campaigned for improved access and support for those with disabilities in Cambridge, including adapted student housing at the university. In general, Hawking had ambivalent feelings about his role as a disability rights champion: while wanting to help others, he also sought to detach himself from his illness and its challenges. His lack of engagement in this area led to"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_28",
    "chunk": "some criticism. During a visit to CERN on the border of France and Switzerland in mid-1985, Hawking contracted pneumonia, which in his condition was life-threatening; he was so ill that Jane was asked if life support should be terminated. She refused, but the consequence was a tracheotomy, which required round-the-clock nursing care and caused the loss of what remained of his speech. The National Health Service was ready to pay for a nursing home, but Jane was determined that he would live at home. The cost of the care was funded by an American foundation. Nurses were hired for the three shifts required to provide the round-the-clock support he required. One of those employed was Elaine Mason, who was to become Hawking's second wife. For his communication, Hawking initially raised his eyebrows to choose letters on a spelling card, but in 1986 he received a computer program called the \"Equalizer\" from Walter Woltosz, CEO of Words Plus, who had developed an earlier version of the software to help his mother-in-law, who also had ALS and had lost her ability to speak and write. In a method he used for the rest of his life, Hawking could now simply press a"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_29",
    "chunk": "switch to select phrases, words or letters from a bank of about 2,500–3,000 that were scanned. The program was originally run on a desktop computer. Elaine Mason's husband, David, a computer engineer, adapted a small computer and attached it to his wheelchair. Released from the need to use somebody to interpret his speech, Hawking commented that \"I can communicate better now than before I lost my voice.\" The voice he used had an American accent and is no longer produced. Despite the later availability of other voices, Hawking retained this original voice, saying that he preferred it and identified with it. Originally, Hawking activated a switch using his hand and could produce up to 15 words per minute. Lectures were prepared in advance and were sent to the speech synthesiser in short sections to be delivered. Hawking gradually lost the use of his hand, and in 2005 he began to control his communication device with movements of his cheek muscles, with a rate of about one word per minute. With this decline there was a risk of him developing locked-in syndrome, so Hawking collaborated with Intel Corporation researchers on systems that could translate his brain patterns or facial expressions into"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_30",
    "chunk": "switch activations. After several prototypes that did not perform as planned, they settled on an adaptive word predictor made by the London-based startup SwiftKey, which used a system similar to his original technology. Hawking had an easier time adapting to the new system, which was further developed after inputting large amounts of Hawking's papers and other written materials and uses predictive software similar to other smartphone keyboards. By 2009, he could no longer drive his wheelchair independently, but the same people who created his new typing mechanics were working on a method to drive his chair using movements made by his chin. This proved difficult, since Hawking could not move his neck, and trials showed that while he could indeed drive the chair, the movement was sporadic and jumpy. Near the end of his life, Hawking experienced increased breathing difficulties, often resulting in his requiring the usage of a ventilator, and being regularly hospitalised. Starting in the 1990s, Hawking accepted the mantle of role model for disabled people, lecturing and participating in fundraising activities. At the turn of the century, he and eleven other humanitarians signed the Charter for the Third Millennium on Disability, which called on governments to prevent"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_31",
    "chunk": "disability and protect the rights of disabled people. In 1999, Hawking was awarded the Julius Edgar Lilienfeld Prize of the American Physical Society. In August 2012, Hawking narrated the \"Enlightenment\" segment of the 2012 Summer Paralympics opening ceremony in London. In 2013, the biographical documentary film Hawking, in which Hawking himself is featured, was released. In September 2013, he expressed support for the legalisation of assisted suicide for the terminally ill. In August 2014, Hawking accepted the Ice Bucket Challenge to promote ALS/MND awareness and raise contributions for research. As he had pneumonia in 2013, he was advised not to have ice poured over him, but his children volunteered to accept the challenge on his behalf. In late 2006, Hawking revealed in a BBC interview that one of his greatest unfulfilled desires was to travel to space. On hearing this, Richard Branson offered a free flight into space with Virgin Galactic, which Hawking immediately accepted. Besides personal ambition, he was motivated by the desire to increase public interest in spaceflight and to show the potential of people with disabilities. On 26 April 2007, Hawking flew aboard a specially-modified Boeing 727–200 jet operated by Zero-G Corp off the coast of Florida"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_32",
    "chunk": "to experience weightlessness. Fears that the manoeuvres would cause him undue discomfort proved incorrect, and the flight was extended to eight parabolic arcs. It was described as a successful test to see if he could withstand the g-forces involved in space flight. At the time, the date of Hawking's trip to space was projected to be as early as 2009, but commercial flights to space did not commence before his death. Hawking died at his home in Cambridge on 14 March 2018, at the age of 76. His family stated that he \"died peacefully\". He was eulogised by figures in science, entertainment, politics, and other areas. The Gonville and Caius College flag flew at half-mast and a book of condolences was signed by students and visitors. A tribute was made to Hawking in the closing speech by IPC President Andrew Parsons at the closing ceremony of the 2018 Paralympic Winter Games in Pyeongchang, South Korea. His private funeral took place on 31 March 2018, at Great St Mary's Church, Cambridge. Guests at the funeral included The Theory of Everything actors Eddie Redmayne and Felicity Jones, Queen guitarist and astrophysicist Brian May, and model Lily Cole. In addition, actor Benedict Cumberbatch,"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_33",
    "chunk": "who played Stephen Hawking in Hawking, astronaut Tim Peake, Astronomer Royal Martin Rees and physicist Kip Thorne provided readings at the service. Although Hawking was an atheist, the funeral took place with a traditional Anglican service. Following the cremation, a service of thanksgiving was held at Westminster Abbey on 15 June 2018, after which his ashes were interred in the Abbey's nave, between the graves of Sir Isaac Newton and Charles Darwin. Inscribed on his memorial stone are the words \"Here lies what was mortal of Stephen Hawking 1942–2018\" and his most famed equation. He directed, at least fifteen years before his death, that the Bekenstein–Hawking entropy equation be his epitaph. In June 2018, it was announced that Hawking's words, set to music by Greek composer Vangelis, would be beamed into space from a European space agency satellite dish in Spain with the aim of reaching the nearest black hole, 1A 0620-00. Hawking's final broadcast interview, about the detection of gravitational waves resulting from the collision of two neutron stars, occurred in October 2017. His final words to the world appeared posthumously, in April 2018, in the form of a Smithsonian TV Channel documentary entitled, Leaving Earth: Or How to"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_34",
    "chunk": "Colonize a Planet. One of his final research studies, entitled A smooth exit from eternal inflation?, about the origin of the universe, was published in the Journal of High Energy Physics in May 2018. Later, in October 2018, another of his final research studies, entitled Black Hole Entropy and Soft Hair, was published, and dealt with the \"mystery of what happens to the information held by objects once they disappear into a black hole\". Also in October 2018, Hawking's last book, Brief Answers to the Big Questions, a popular science book presenting his final comments on the most important questions facing humankind, was published. On 8 November 2018, an auction of 22 personal possessions of Hawking, including his doctoral thesis (Properties of Expanding Universes, PhD thesis, Cambridge University, 1965) and wheelchair, took place, and fetched about £1.8 million. Proceeds from the auction sale of the wheelchair went to two charities, the Motor Neurone Disease Association and the Stephen Hawking Foundation; proceeds from the other items went to his estate. In March 2019, it was announced that the Royal Mint would issue a commemorative 50p coin, only available as a commemorative edition, in honour of Hawking. The same month, Hawking's nurse,"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_35",
    "chunk": "Patricia Dowdy, was struck off the nursing register for \"failures over his care and financial misconduct\". In May 2021 it was announced that an Acceptance-in-Lieu agreement between HMRC, the Department for Culture, Media and Sport, Cambridge University Library, Science Museum Group, and the Hawking Estate, would see around 10,000 pages of Hawking's scientific and other papers remain in Cambridge, while objects including his wheelchairs, speech synthesisers, and personal memorabilia from his former Cambridge office would be housed at the Science Museum. In February 2022 the \"Stephen Hawking at Work\" display opened at the Science Museum, London as the start of a two-year nationwide tour. At Google's Zeitgeist Conference in 2011, Stephen Hawking said that \"philosophy is dead\". He believed that philosophers \"have not kept up with modern developments in science\", \"have not taken science sufficiently seriously and so Philosophy is no longer relevant to knowledge claims\", \"their art is dead\" and that scientists \"have become the bearers of the torch of discovery in our quest for knowledge\". He said that philosophical problems can be answered by science, particularly new scientific theories which \"lead us to a new and very different picture of the universe and our place in it\". His"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_36",
    "chunk": "view was both praised and criticised. He said, \"Love, faith and morality belong to a different category to physics. You cannot deduce how one should behave from the laws of physics. But one could hope that the logical thought that physics and mathematics involves would guide one also in one's moral behavior.\" In 2006, Hawking posed an open question on the Internet: \"In a world that is in chaos politically, socially and environmentally, how can the human race sustain another 100 years?\", later clarifying: \"I don't know the answer. That is why I asked the question, to get people to think about it, and to be aware of the dangers we now face.\" Hawking expressed concern that life on Earth is at risk from a sudden nuclear war, a genetically engineered virus, global warming, an asteroid collision, or other dangers humans have not yet thought of. Hawking stated: \"I regard it as almost inevitable that either a nuclear confrontation or environmental catastrophe will cripple the Earth at some point in the next 1,000 years\". Such a planet-wide disaster need not result in human extinction if the human race were to be able to colonise additional planets before the disaster. Hawking"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_37",
    "chunk": "viewed spaceflight and the colonisation of space as necessary for the future of humanity. Hawking stated that, given the vastness of the universe, aliens likely exist, but that contact with them should be avoided. He warned that aliens might pillage Earth for resources. In 2010 he said, \"If aliens visit us, the outcome would be much as when Columbus landed in America, which didn't turn out well for the Native Americans.\" Hawking warned that superintelligent artificial intelligence could be pivotal in steering humanity's fate, stating that \"the potential benefits are huge... Success in creating AI would be the biggest event in human history. It might also be the last, unless we learn how to avoid the risks.\" He feared that \"an extremely intelligent future AI will probably develop a drive to survive and acquire more resources as a step toward accomplishing whatever goal it has\", and that \"The real risk with AI isn't malice but competence. A super-intelligent AI will be extremely good at accomplishing its goals, and if those goals aren't aligned with ours, we're in trouble\". He also considered that the enormous wealth generated by machines needs to be redistributed to prevent exacerbated economic inequality. Hawking was concerned"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_38",
    "chunk": "about the future emergence of a race of \"superhumans\" that would be able to design their own evolution and, as well, argued that computer viruses in today's world should be considered a new form of life, stating that \"maybe it says something about human nature, that the only form of life we have created so far is purely destructive. Talk about creating life in our own image.\" Hawking did not rule out the existence of a Creator, asking in A Brief History of Time \"Is the unified theory so compelling that it brings about its own existence?\", also stating \"If we discover a complete theory, it would be the ultimate triumph of human reason – for then we should know the mind of God\"; in his early work, Hawking spoke of God in a metaphorical sense. In the same book he suggested that the existence of God was not necessary to explain the origin of the universe. Later discussions with Neil Turok led to the realisation that the existence of God was also compatible with an open universe. He said \"All my work has shown is that you don't have to say that the way the Universe began was the"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_39",
    "chunk": "personal whim of God. But you still have the question, 'Why does the Universe bother to exist?' If you like, you can define God as the answer to that question.\" Hawking was an atheist. In an interview published in The Guardian, Hawking regarded \"the brain as a computer which will stop working when its components fail\", and the concept of an afterlife as a \"fairy story for people afraid of the dark\". In 2011, narrating the first episode of the American television series Curiosity on the Discovery Channel, Hawking declared: We are each free to believe what we want and it is my view that the simplest explanation is there is no God. No one created the universe and no one directs our fate. This leads me to a profound realisation. There is probably no heaven, and no afterlife either. We have this one life to appreciate the grand design of the universe, and for that, I am extremely grateful. Hawking's association with atheism and freethinking was in evidence from his university years onwards, when he had been a member of Oxford University's humanist group. He was later scheduled to appear as the keynote speaker at a 2017 Humanists UK"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_40",
    "chunk": "conference. In an interview with El Mundo, he said: Before we understand science, it is natural to believe that God created the universe. But now science offers a more convincing explanation. What I meant by 'we would know the mind of God' is, we would know everything that God would know, if there were a God, which there isn't. I'm an atheist. If you like, you can call the laws of science 'God', but it wouldn't be a personal God that you would meet and put questions to. Hawking was a longstanding Labour Party supporter. He recorded a tribute for the 2000 Democratic presidential candidate Al Gore, called the 2003 invasion of Iraq a \"war crime\", campaigned for nuclear disarmament, and supported stem cell research, universal health care, and action to prevent climate change. In August 2014, Hawking was one of 200 public figures who were signatories to a letter to The Guardian expressing their hope that Scotland would vote to remain part of the United Kingdom in September's referendum on that issue. Hawking believed a United Kingdom withdrawal from the European Union (Brexit) would damage the UK's contribution to science as modern research needs international collaboration, and that free"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_41",
    "chunk": "movement of people in Europe encourages the spread of ideas. Hawking said to Theresa May, \"I deal with tough mathematical questions every day, but please don't ask me to help with Brexit.\" Hawking was disappointed by Brexit and warned against envy and isolationism. Hawking was greatly concerned over health care, and maintained that without the UK National Health Service, he could not have survived into his 70s. Hawking especially feared privatisation. He stated, \"The more profit is extracted from the system, the more private monopolies grow and the more expensive healthcare becomes. The NHS must be preserved from commercial interests and protected from those who want to privatise it.\" Hawking blamed the Conservatives for cutting funding to the NHS, weakening it by privatisation, lowering staff morale through holding pay back and reducing social care. Hawking accused Jeremy Hunt of cherry picking evidence which Hawking maintained debased science. Hawking also stated, \"There is overwhelming evidence that NHS funding and the numbers of doctors and nurses are inadequate, and it is getting worse.\" In June 2017, Hawking endorsed the Labour Party in the 2017 UK general election, citing the Conservatives' proposed cuts to the NHS. But he was also critical of Labour"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_42",
    "chunk": "leader Jeremy Corbyn, expressing scepticism over whether the party could win a general election under him. Hawking feared Donald Trump's policies on global warming could endanger the planet and make global warming irreversible. He said, \"Climate change is one of the great dangers we face, and it's one we can prevent if we act now. By denying the evidence for climate change, and pulling out of the Paris Agreement, Donald Trump will cause avoidable environmental damage to our beautiful planet, endangering the natural world, for us and our children.\" Hawking further stated that this could lead Earth \"to become like Venus, with a temperature of two hundred and fifty degrees, and raining sulphuric acid\". Hawking was also a supporter of a universal basic income. He was critical of the Israeli government's position on the Israeli–Palestinian conflict, stating that their policy \"is likely to lead to disaster\". In 1988, Hawking, Arthur C. Clarke and Carl Sagan were interviewed in God, the Universe and Everything Else. They discussed the Big Bang theory, God and the possibility of extraterrestrial life. At the release party for the home video version of the A Brief History of Time, Leonard Nimoy, who had played Spock on"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_43",
    "chunk": "Star Trek, learned that Hawking was interested in appearing on the show. Nimoy made the necessary contact, and Hawking played a holographic simulation of himself in an episode of Star Trek: The Next Generation in 1993. The same year, his synthesiser voice was recorded for the Pink Floyd song \"Keep Talking\", and in 1999 for an appearance on The Simpsons. Hawking appeared in documentaries titled The Real Stephen Hawking (2001), Stephen Hawking: Profile (2002) and Hawking (2013), and the documentary series Stephen Hawking, Master of the Universe (2008). Hawking also guest-starred in Futurama and had a recurring role in The Big Bang Theory. Hawking allowed the use of his copyrighted voice in the biographical 2014 film The Theory of Everything, in which he was portrayed by Eddie Redmayne in an Academy Award-winning role. Hawking was featured at the Monty Python Live (Mostly) show in 2014. He was shown to sing an extended version of the \"Galaxy Song\", after running down Brian Cox with his wheelchair, in a pre-recorded video. Hawking used his fame to advertise products, including a wheelchair, National Savings, British Telecom, Specsavers, Egg Banking, and Go Compare. In 2015, he applied to trademark his name. Broadcast in March"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_44",
    "chunk": "2018 just a week or two before his death, Hawking was the voice of The Book Mark II on The Hitchhiker's Guide to the Galaxy radio series, and he was the guest of Neil deGrasse Tyson on StarTalk. The 2021 animated sitcom The Freak Brothers features a recurring character, Mayor Pimco, who is apparently modelled after Stephen Hawking. On 8 January 2022, Google featured Hawking in a Google Doodle on the occasion of his 80th birthday. On Desert Island Discs, Hawking chose Francis Poulenc's Gloria, Brahms's Violin Concerto, Beethoven's String Quartet No. 15, Wagner's Die Walküre, Act 1, The Beatles's \"Please Please Me\", Mozart's Requiem, Puccini's \"O Principe, che a lunghe carovane\" and Edith Piaf's \"Non, je ne regrette rien\" (\"That just about sums up my life.\") For his book, he chose George Eliot's Middlemarch: \"I think someone, maybe it was Virginia Woolf, said it was a book for adults. I'm not sure I'm grownup yet, but I will give it a try.\" Hawking received numerous awards and honours. Already early in the list, in 1974 he was elected a Fellow of the Royal Society (FRS). At that time, his nomination read: Hawking has made major contributions to the field"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_45",
    "chunk": "of general relativity. These derive from a deep understanding of what is relevant to physics and astronomy, and especially from a mastery of wholly new mathematical techniques. Following the pioneering work of Penrose he established, partly alone and partly in collaboration with Penrose, a series of successively stronger theorems establishing the fundamental result that all realistic cosmological models must possess singularities. Using similar techniques, Hawking has proved the basic theorems on the laws governing black holes: that stationary solutions of Einstein's equations with smooth event horizons must necessarily be axisymmetric; and that in the evolution and interaction of black holes, the total surface area of the event horizons must increase. In collaboration with G. Ellis, Hawking is the author of an impressive and original treatise on \"Space-time in the Large\". The citation continues, \"Other important work by Hawking relates to the interpretation of cosmological observations and to the design of gravitational wave detectors.\" Hawking was also a member of the American Academy of Arts and Sciences (1984), the American Philosophical Society (1984), and the United States National Academy of Sciences (1992). Hawking received the 2015 BBVA Foundation Frontiers of Knowledge Award in Basic Sciences shared with Viatcheslav Mukhanov for discovering"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_46",
    "chunk": "that the galaxies were formed from quantum fluctuations in the early Universe. At the 2016 Pride of Britain Awards, Hawking received the lifetime achievement award \"for his contribution to science and British culture\". After receiving the award from Prime Minister Theresa May, Hawking humorously requested that she not seek his help with Brexit. In 2017, the Cambridge Union Society, in conjunction with Hawking, established the Professor Stephen Hawking Fellowship. The fellowship is awarded annually to an individual who has made an exceptional contribution to the STEM fields and social discourse, with a particular focus on impacts affecting the younger generations. Each fellow delivers a lecture on a topic of their choosing, known as the 'Hawking Lecture'. Hawking himself accepted the inaugural fellowship, and he delivered the first Hawking Lecture in his last public appearance before his death. Hawking was a member of the advisory board of the Starmus Festival, and had a major role in acknowledging and promoting science communication. The Stephen Hawking Medal for Science Communication is an annual award initiated in 2016 to honour members of the arts community for contributions that help build awareness of science. Recipients receive a medal bearing a portrait of Hawking by Alexei"
  },
  {
    "source": "Stephen Hawking.txt",
    "chunk_id": "Stephen Hawking.txt_47",
    "chunk": "Leonov, and the other side represents an image of Leonov himself performing the first spacewalk along with an image of the \"Red Special\", the guitar of Queen musician and astrophysicist Brian May (with music being another major component of the Starmus Festival). The Starmus III Festival in 2016 was a tribute to Stephen Hawking and the book of all Starmus III lectures, \"Beyond the Horizon\", was also dedicated to him. The first recipients of the medals, which were awarded at the festival, were chosen by Hawking himself. They were composer Hans Zimmer, physicist Jim Al-Khalili, and the science documentary Particle Fever."
  },
  {
    "source": "Subaru Telescope.txt",
    "chunk_id": "Subaru Telescope.txt_0",
    "chunk": "# Subaru Telescope Subaru Telescope (すばる望遠鏡, Subaru Bōenkyō) is the 8.2-metre (320 in) telescope of the National Astronomical Observatory of Japan, located at the Mauna Kea Observatory on Hawaii. It is named after the open star cluster known in English as the Pleiades. It had the largest monolithic primary mirror in the world from its commissioning until the Large Binocular Telescope opened in 2005. The Subaru Telescope is a Ritchey-Chretien reflecting telescope. Instruments can be mounted at a Cassegrain focus below the primary mirror; at either of two Nasmyth focal points in enclosures on the sides of the telescope mount, to which light can be directed with a tertiary mirror; or at the prime focus in lieu of a secondary mirror, an arrangement rare on large telescopes, to provide a wide field of view suited to deep wide-field surveys. In 1984, the University of Tokyo formed an engineering working group to develop and study the concept of a 7.5-metre (300 in) telescope. In 1985, the astronomy committee of Japan's science council gave top priority to the development of a \"Japan National Large Telescope\" (JNLT), and in 1986, the University of Tokyo signed an agreement with the University of Hawaii to"
  },
  {
    "source": "Subaru Telescope.txt",
    "chunk_id": "Subaru Telescope.txt_1",
    "chunk": "build the telescope in Hawaii. In 1988, the National Astronomical Observatory of Japan was formed through a reorganization of the University's Tokyo Astronomical Observatory, to oversee the JNLT and other large national astronomy projects. Construction of the Subaru Telescope began in April 1991, and later that year, a public contest gave the telescope its official name, \"Subaru Telescope\". Construction was completed in 1998, and the first scientific images were taken in January 1999. In September 1999, Princess Sayako of Japan dedicated the telescope. A number of state-of-the-art technologies were worked into the telescope design. For example, 261 computer-controlled actuators press the main mirror from underneath, which corrects for primary mirror distortion caused by changes in the telescope orientation. The telescope enclosure building is also shaped to improve the quality of astronomical images by minimizing the effects caused by atmospheric turbulence. Subaru is one of the few state-of-the-art telescopes to have been used with the naked eye. For the dedication, an eyepiece was constructed so that Princess Sayako could look through it directly. It was enjoyed by the staff for a few nights until it was replaced with the much more sensitive working instruments. Subaru is the primary tool in the"
  },
  {
    "source": "Subaru Telescope.txt",
    "chunk_id": "Subaru Telescope.txt_2",
    "chunk": "search for Planet Nine. Its large field of view, 75 times that of the Keck telescopes, and strong light-gathering power are suited for deep wide-field sky surveys. The search, split between a research group led by Konstantin Batygin and Michael Brown and another led by Scott Sheppard and Chad Trujillo, is expected to take up to five years. Two separate incidents claimed the lives of four workers during the construction of the telescope. On October 13, 1993, 42-year-old Paul F. Lawrence was fatally injured when a forklift tipped over onto him. On January 16, 1996, sparks from a welder ignited insulation which smoldered, generating noxious smoke that killed Marvin Arruda, 52, Ricky Del Rosario, 38, and Warren K. \"Kip\" Kaleo, 36, and sent twenty-six other workers to the hospital in Hilo. All four workers are memorialized by a plaque outside the base of the telescope dome and a sign posted temporarily each January along the Mauna Kea access road. On July 2, 2011, the telescope operator in Hilo noted an anomaly from the top unit of the telescope. Upon further examination, coolant from the top unit was found to have leaked over the primary mirror and other parts of the"
  },
  {
    "source": "Subaru Telescope.txt",
    "chunk_id": "Subaru Telescope.txt_3",
    "chunk": "telescope. Observation using Nasmyth foci resumed on July 22, and Cassegrain focus resumed on August 26. On September 15, 2023, an abnormal load sensor value of the primary mirror fixed point was observed during a maintenance operational test. Later, a part fell onto the primary mirror during repair work of the mirror cover. Science observation was suspended. After the replacement of sensor and the repair work of the primary mirror damage, it returned to observation on 3 March 2024. Several cameras and spectrographs can be mounted at Subaru Telescope's four focal points for observations in visible and infrared wavelengths. In 2024, in collaboration with the TESS space telescope, the Subaru telescope discovered planet Gliese 12b."
  },
  {
    "source": "Sublimation (phase transition).txt",
    "chunk_id": "Sublimation (phase transition).txt_0",
    "chunk": "# Sublimation (phase transition) Sublimation is the transition of a substance directly from the solid to the gas state, without passing through the liquid state. The verb form of sublimation is sublime, or less preferably, sublimate. Sublimate also refers to the product obtained by sublimation. The point at which sublimation occurs rapidly (for further details, see below) is called critical sublimation point, or simply sublimation point. Notable examples include sublimation of dry ice at room temperature and atmospheric pressure, and that of solid iodine with heating. The reverse process of sublimation is deposition (also called desublimation), in which a substance passes directly from a gas to a solid phase, without passing through the liquid state. Technically, all solids may sublime, though most sublime at extremely low rates that are hardly detectable under usual conditions. At normal pressures, most chemical compounds and elements possess three different states at different temperatures. In these cases, the transition from the solid to the gas state requires an intermediate liquid state. The pressure referred to is the partial pressure of the substance, not the total (e.g. atmospheric) pressure of the entire system. Thus, any solid can sublime if its vapour pressure is higher than the"
  },
  {
    "source": "Sublimation (phase transition).txt",
    "chunk_id": "Sublimation (phase transition).txt_1",
    "chunk": "surrounding partial pressure of the same substance, and in some cases, sublimation occurs at an appreciable rate (e.g. water ice just below 0 °C). For some substances, such as carbon and arsenic, sublimation from solid state is much more achievable than evaporation from liquid state and it is difficult to obtain them as liquids. This is because the pressure of their triple point in its phase diagram (which corresponds to the lowest pressure at which the substance can exist as a liquid) is very high. Sublimation is caused by the absorption of heat which provides enough energy for some molecules to overcome the attractive forces of their neighbors and escape into the vapor phase. Since the process requires additional energy, sublimation is an endothermic change. The enthalpy of sublimation (also called heat of sublimation) can be calculated by adding the enthalpy of fusion and the enthalpy of vaporization. While the definition of sublimation is simple, there is often confusion as to what counts as a sublimation. Vaporization (from liquid to gas) is divided into two types: vaporization on the surface of the liquid is called evaporation, and vaporization at the boiling point with formation of bubbles in the interior of"
  },
  {
    "source": "Sublimation (phase transition).txt",
    "chunk_id": "Sublimation (phase transition).txt_2",
    "chunk": "the liquid is called boiling. However there is no such distinction for the solid-to-gas transition, which is always called sublimation in both corresponding cases. For clarification, a distinction between the two corresponding cases is needed. With reference to a phase diagram, the sublimation that occurs left of the solid-gas boundary, the triple point or the solid-liquid boundary (corresponding to evaporation in vaporization) may be called gradual sublimation; and the substance sublimes gradually, regardless of rate. The sublimation that occurs at the solid-gas boundary (critical sublimation point) (corresponding to boiling in vaporization) may be called rapid sublimation, and the substance sublimes rapidly. The words \"gradual\" and \"rapid\" have acquired special meanings in this context and no longer describe the rate of sublimation. The term sublimation refers specifically to a physical change of state and is not used to describe the transformation of a solid to a gas in a chemical reaction. For example, the dissociation on heating of solid ammonium chloride into hydrogen chloride and ammonia is not sublimation but a chemical reaction. Similarly the combustion of candles, containing paraffin wax, to carbon dioxide and water vapor is not sublimation but a chemical reaction with oxygen. Sublimation is historically used as"
  },
  {
    "source": "Sublimation (phase transition).txt",
    "chunk_id": "Sublimation (phase transition).txt_3",
    "chunk": "a generic term to describe a two-step phase transition ― a solid-to-gas transition (sublimation in a more precise definition) followed by a gas-to-solid transition (deposition). (See below) The examples shown are substances that noticeably sublime under certain conditions. Solid carbon dioxide (dry ice) sublimes rapidly along the solid-gas boundary (sublimation point) below the triple point (e.g., at the temperature of −78.5 °C, at atmospheric pressure), whereas its melting into liquid CO2 can occur along the solid-liquid boundary (melting point) at pressures and temperatures above the triple point (i.e., 5.1 atm, −56.6 °C). Snow and ice sublime gradually at temperatures below the solid-liquid boundary (melting point) (generally 0 °C), and at partial pressures below the triple point pressure of 612 Pa (0.00604 atm), at a low rate. In freeze-drying, the material to be dehydrated is frozen and its water is allowed to sublime under reduced pressure or vacuum. The loss of snow from a snowfield during a cold spell is often caused by sunshine acting directly on the upper layers of the snow. Sublimation of ice is a factor to the erosive wear of glacier ice, known as ablation in glaciology. Naphthalene, an organic compound commonly found in pesticides such as"
  },
  {
    "source": "Sublimation (phase transition).txt",
    "chunk_id": "Sublimation (phase transition).txt_4",
    "chunk": "mothballs, sublimes easily because it is made of non-polar molecules that are held together only by van der Waals intermolecular forces. Naphthalene is a solid that sublimes gradually at standard temperature and pressure, at a high rate, with the critical sublimation point at around 80 °C (176 °F). At low temperature, its vapour pressure is high enough, 1 mmHg at 53 °C, to make the solid form of naphthalene evaporate into gas. On cool surfaces, the naphthalene vapours will solidify to form needle-like crystals. Iodine sublimes gradually and produces visible fumes on gentle heating at standard atmospheric temperature. It is possible to obtain liquid iodine at atmospheric pressure by controlling the temperature at just between the melting point and the boiling point of iodine. In forensic science, iodine vapor can reveal latent fingerprints on paper. At atmospheric pressure, arsenic sublimes gradually upon heating, and sublimes rapidly at 887 K (614 °C). Cadmium and zinc sublime much more than other common materials, so they are not suitable materials for use in vacuum. Sublimation is a technique used by chemists to purify compounds. A solid is typically placed in a sublimation apparatus and heated under vacuum. Under this reduced pressure, the solid"
  },
  {
    "source": "Sublimation (phase transition).txt",
    "chunk_id": "Sublimation (phase transition).txt_5",
    "chunk": "volatilizes and condenses as a purified compound on a cooled surface (cold finger), leaving a non-volatile residue of impurities behind. Once heating ceases and the vacuum is removed, the purified compound may be collected from the cooling surface. For even higher purification efficiencies, a temperature gradient is applied, which also allows for the separation of different fractions. Typical setups use an evacuated glass tube that is heated gradually in a controlled manner. The material flow is from the hot end, where the initial material is placed, to the cold end that is connected to a pump stand. By controlling temperatures along the length of the tube, the operator can control the zones of re-condensation, with very volatile compounds being pumped out of the system completely (or caught by a separate cold trap), moderately volatile compounds re-condensing along the tube according to their different volatilities, and non-volatile compounds remaining in the hot end. Vacuum sublimation of this type is also the method of choice for purification of organic compounds for use in the organic electronics industry, where very high purities (often > 99.99%) are needed to satisfy the standards for consumer electronics and other applications. In ancient alchemy, a protoscience that"
  },
  {
    "source": "Sublimation (phase transition).txt",
    "chunk_id": "Sublimation (phase transition).txt_6",
    "chunk": "contributed to the development of modern chemistry and medicine, alchemists developed a structure of basic laboratory techniques, theory, terminology, and experimental methods. Sublimation was used to refer to the process in which a substance is heated to a vapor, then immediately collects as sediment on the upper portion and neck of the heating medium (typically a retort or alembic), but can also be used to describe other similar non-laboratory transitions. It was mentioned by alchemical authors such as Basil Valentine and George Ripley, and in the Rosarium philosophorum, as a process necessary for the completion of the magnum opus. Here, the word sublimation was used to describe an exchange of \"bodies\" and \"spirits\" similar to laboratory phase transition between solids and gases. Valentine, in his Le char triomphal de l'antimoine (Triumphal Chariot of Antimony, published 1646) made a comparison to spagyrics in which a vegetable sublimation can be used to separate the spirits in wine and beer. Ripley used language more indicative of the mystical implications of sublimation, indicating that the process has a double aspect in the spiritualization of the body and the corporalizing of the spirit. He writes: And Sublimations we make for three causes, The first cause"
  },
  {
    "source": "Sublimation (phase transition).txt",
    "chunk_id": "Sublimation (phase transition).txt_7",
    "chunk": "is to make the body spiritual. The second is that the spirit may be corporeal, And become fixed with it and consubstantial. The third cause is that from its filthy original. It may be cleansed, and its saltiness sulphurious May be diminished in it, which is infectious. The enthalpy of sublimation has commonly been predicted using the equipartition theorem. If the lattice energy is assumed to be approximately half the packing energy, then the following thermodynamic corrections can be applied to predict the enthalpy of sublimation. Assuming a 1 molar ideal gas gives a correction for the thermodynamic environment (pressure and volume) in which pV = RT, hence a correction of 1RT. Additional corrections for the vibrations, rotations and translation then need to be applied. From the equipartition theorem gaseous rotation and translation contribute 1.5RT each to the final state, therefore a +3RT correction. Crystalline vibrations and rotations contribute 3RT each to the initial state, hence −6RT. Summing the RT corrections; −6RT + 3RT + RT = −2RT. This leads to the following approximate sublimation enthalpy. A similar approximation can be found for the entropy term if rigid bodies are assumed. Δ H sublimation = − U lattice energy −"
  },
  {
    "source": "Sublimation (phase transition).txt",
    "chunk_id": "Sublimation (phase transition).txt_8",
    "chunk": "2 R T {\\displaystyle \\Delta H_{\\text{sublimation}}=-U_{\\text{lattice energy}}-2RT} Dye-sub printing is a digital printing technology using full color artwork that works with polyester and polymer-coated substrates. Also referred to as digital sublimation, the process is commonly used for decorating apparel, signs and banners, as well as novelty items such as cell phone covers, plaques, coffee mugs, and other items with sublimation-friendly surfaces. The process uses the science of sublimation, in which heat and pressure are applied to a solid, turning it into a gas through an endothermic reaction without passing through the liquid phase. In sublimation printing, unique sublimation dyes are transferred to sheets of “transfer” paper via liquid gel ink through a piezoelectric print head. The ink is deposited on these high-release inkjet papers, which are used for the next step of the sublimation printing process. After the digital design is printed onto sublimation transfer sheets, it is placed on a heat press along with the substrate to be sublimated. In order to transfer the image from the paper to the substrate, it requires a heat press process that is a combination of time, temperature and pressure. The heat press applies this special combination, which can change depending on the"
  },
  {
    "source": "Sublimation (phase transition).txt",
    "chunk_id": "Sublimation (phase transition).txt_9",
    "chunk": "substrate, to “transfer” the sublimation dyes at the molecular level into the substrate. The most common dyes used for sublimation activate at 350 degrees Fahrenheit. However, a range of 380 to 420 degrees Fahrenheit is normally recommended for optimal color. The result of the sublimation process is a nearly permanent, high resolution, full color print. Because the dyes are infused into the substrate at the molecular level, rather than applied at a topical level (such as with screen printing and direct to garment printing), the prints will not crack, fade or peel from the substrate under normal conditions."
  },
  {
    "source": "Surface gravity.txt",
    "chunk_id": "Surface gravity.txt_0",
    "chunk": "# Surface gravity The surface gravity, g, of an astronomical object is the gravitational acceleration experienced at its surface at the equator, including the effects of rotation. The surface gravity may be thought of as the acceleration due to gravity experienced by a hypothetical test particle which is very close to the object's surface and which, in order not to disturb the system, has negligible mass. For objects where the surface is deep in the atmosphere and the radius not known, the surface gravity is given at the 1 bar pressure level in the atmosphere. Surface gravity is measured in units of acceleration, which, in the SI system, are meters per second squared. It may also be expressed as a multiple of the Earth's standard surface gravity, which is equal to In astrophysics, the surface gravity may be expressed as log g, which is obtained by first expressing the gravity in cgs units, where the unit of acceleration and surface gravity is centimeters per second squared (cm/s), and then taking the base-10 logarithm of the cgs value of the surface gravity. Therefore, the surface gravity of Earth could be expressed in cgs units as 980.665 cm/s, and then taking the"
  },
  {
    "source": "Surface gravity.txt",
    "chunk_id": "Surface gravity.txt_1",
    "chunk": "base-10 logarithm (\"log g\") of 980.665, giving 2.992 as \"log g\". The surface gravity of a white dwarf is very high, and of a neutron star even higher. A white dwarf's surface gravity is around 100,000 g (10 m/s) whilst the neutron star's compactness gives it a surface gravity of up to 7×10 m/s with typical values of order 10 m/s (that is more than 10 times that of Earth). One measure of such immense gravity is that neutron stars have an escape velocity of around 100,000 km/s, about a third of the speed of light. Since black holes do not have a surface, the surface gravity is not defined. In the Newtonian theory of gravity, the gravitational force exerted by an object is proportional to its mass: an object with twice the mass-produces twice as much force. Newtonian gravity also follows an inverse square law, so that moving an object twice as far away divides its gravitational force by four, and moving it ten times as far away divides it by 100. This is similar to the intensity of light, which also follows an inverse square law: with relation to distance, light becomes less visible. Generally speaking, this can"
  },
  {
    "source": "Surface gravity.txt",
    "chunk_id": "Surface gravity.txt_2",
    "chunk": "be understood as geometric dilution corresponding to point-source radiation into three-dimensional space. A large object, such as a planet or star, will usually be approximately round, approaching hydrostatic equilibrium (where all points on the surface have the same amount of gravitational potential energy). On a small scale, higher parts of the terrain are eroded, with eroded material deposited in lower parts of the terrain. On a large scale, the planet or star itself deforms until equilibrium is reached. For most celestial objects, the result is that the planet or star in question can be treated as a near-perfect sphere when the rotation rate is low. However, for young, massive stars, the equatorial azimuthal velocity can be quite high—up to 200 km/s or more—causing a significant amount of equatorial bulge. Examples of such rapidly rotating stars include Achernar, Altair, Regulus A and Vega. The fact that many large celestial objects are approximately spheres makes it easier to calculate their surface gravity. According to the shell theorem, the gravitational force outside a spherically symmetric body is the same as if its entire mass were concentrated in the center, as was established by Sir Isaac Newton. Therefore, the surface gravity of a planet"
  },
  {
    "source": "Surface gravity.txt",
    "chunk_id": "Surface gravity.txt_3",
    "chunk": "or star with a given mass will be approximately inversely proportional to the square of its radius, and the surface gravity of a planet or star with a given average density will be approximately proportional to its radius. For example, the recently discovered planet, Gliese 581 c, has at least 5 times the mass of Earth, but is unlikely to have 5 times its surface gravity. If its mass is no more than 5 times that of the Earth, as is expected, and if it is a rocky planet with a large iron core, it should have a radius approximately 50% larger than that of Earth. Gravity on such a planet's surface would be approximately 2.2 times as strong as on Earth. If it is an icy or watery planet, its radius might be as large as twice the Earth's, in which case its surface gravity might be no more than 1.25 times as strong as the Earth's. These proportionalities may be expressed by the formula: g ∝ m r 2 {\\displaystyle g\\propto {\\frac {m}{r^{2}}}} where g is the surface gravity of an object, expressed as a multiple of the Earth's, m is its mass, expressed as a multiple of"
  },
  {
    "source": "Surface gravity.txt",
    "chunk_id": "Surface gravity.txt_4",
    "chunk": "the Earth's mass (5.976×10 kg) and r its radius, expressed as a multiple of the Earth's (mean) radius (6,371 km). For instance, Mars has a mass of 6.4185×10 kg = 0.107 Earth masses and a mean radius of 3,390 km = 0.532 Earth radii. The surface gravity of Mars is therefore approximately 0.107 0.532 2 = 0.38 {\\displaystyle {\\frac {0.107}{0.532^{2}}}=0.38} times that of Earth. Without using the Earth as a reference body, the surface gravity may also be calculated directly from Newton's law of universal gravitation, which gives the formula g = G M r 2 {\\displaystyle g={\\frac {GM}{r^{2}}}} where M is the mass of the object, r is its radius, and G is the gravitational constant. If ρ = M/V denote the mean density of the object, this can also be written as g = 4 π 3 G ρ r {\\displaystyle g={\\frac {4\\pi }{3}}G\\rho r} so that, for fixed mean density, the surface gravity g is proportional to the radius r. Solving for mass, this equation can be written as g = G ( 4 π ρ 3 ) 2 / 3 M 1 / 3 {\\displaystyle g=G\\left({\\frac {4\\pi \\rho }{3}}\\right)^{2/3}M^{1/3}} But density is not constant, but increases"
  },
  {
    "source": "Surface gravity.txt",
    "chunk_id": "Surface gravity.txt_5",
    "chunk": "as the planet grows in size, as they are not incompressible bodies. That is why the experimental relationship between surface gravity and mass does not grow as 1/3 but as 1/2: g = M 1 / 2 {\\displaystyle g=M^{1/2}} here with g in times Earth's surface gravity and M in times Earth's mass. In fact, the exoplanets found fulfilling the former relationship have been found to be rocky planets. Thus, for rocky planets, density grows with mass as ρ ∝ M 1 / 4 {\\displaystyle \\rho \\propto M^{1/4}} . For gas giant planets such as Jupiter, Saturn, Uranus, and Neptune, the surface gravity is given at the 1 bar pressure level in the atmosphere. It has been found that for giant planets with masses in the range up to 100 times Earth's mass, their gravity surface is nevertheless very similar and close to 1g, a region named the gravity plateau. Most real astronomical objects are not perfectly spherically symmetric. One reason for this is that they are often rotating, which means that they are affected by the combined effects of gravitational force and centrifugal force. This causes stars and planets to be oblate, which means that their surface gravity is"
  },
  {
    "source": "Surface gravity.txt",
    "chunk_id": "Surface gravity.txt_6",
    "chunk": "smaller at the equator than at the poles. This effect was exploited by Hal Clement in his SF novel Mission of Gravity, dealing with a massive, fast-spinning planet where gravity was much higher at the poles than at the equator. To the extent that an object's internal distribution of mass differs from a symmetric model, the measured surface gravity may be used to deduce things about the object's internal structure. This fact has been put to practical use since 1915–1916, when Roland Eötvös's torsion balance was used to prospect for oil near the city of Egbell (now Gbely, Slovakia.) In 1924, the torsion balance was used to locate the Nash Dome oil fields in Texas. It is sometimes useful to calculate the surface gravity of simple hypothetical objects which are not found in nature. The surface gravity of infinite planes, tubes, lines, hollow shells, cones, and even more unrealistic structures may be used to provide insights into the behavior of real structures. In relativity, the Newtonian concept of acceleration turns out not to be clear cut. For a black hole, which must be treated relativistically, one cannot define a surface gravity as the acceleration experienced by a test body at"
  },
  {
    "source": "Surface gravity.txt",
    "chunk_id": "Surface gravity.txt_7",
    "chunk": "the object's surface because there is no surface, although the event horizon is a natural alternative candidate, but this still presents a problem because the acceleration of a test body at the event horizon of a black hole turns out to be infinite in relativity. Because of this, a renormalized value is used that corresponds to the Newtonian value in the non-relativistic limit. The value used is generally the local proper acceleration (which diverges at the event horizon) multiplied by the gravitational time dilation factor (which goes to zero at the event horizon). For the Schwarzschild case, this value is mathematically well behaved for all non-zero values of r and M. When one talks about the surface gravity of a black hole, one is defining a notion that behaves analogously to the Newtonian surface gravity, but is not the same thing. In fact, the surface gravity of a general black hole is not well defined. However, one can define the surface gravity for a black hole whose event horizon is a Killing horizon. The surface gravity κ {\\displaystyle \\kappa } of a static Killing horizon is the acceleration, as exerted at infinity, needed to keep an object at the horizon."
  },
  {
    "source": "Surface gravity.txt",
    "chunk_id": "Surface gravity.txt_8",
    "chunk": "Mathematically, if k a {\\displaystyle k^{a}} is a suitably normalized Killing vector, then the surface gravity is defined by k a ∇ a k b = κ k b , {\\displaystyle k^{a}\\,\\nabla _{a}k^{b}=\\kappa k^{b},} where the equation is evaluated at the horizon. For a static and asymptotically flat spacetime, the normalization should be chosen so that k a k a → − 1 {\\displaystyle k^{a}k_{a}\\to -1} as r → ∞ {\\displaystyle r\\to \\infty } , and so that κ ≥ 0 {\\displaystyle \\kappa \\geq 0} . For the Schwarzschild solution, take k a {\\displaystyle k^{a}} to be the time translation Killing vector k a ∂ a = ∂ ∂ t {\\textstyle k^{a}\\partial _{a}={\\frac {\\partial }{\\partial t}}} , and more generally for the Kerr–Newman solution take k a ∂ a = ∂ ∂ t + Ω ∂ ∂ φ {\\textstyle k^{a}\\partial _{a}={\\frac {\\partial }{\\partial t}}+\\Omega {\\frac {\\partial }{\\partial \\varphi }}} , the linear combination of the time translation and axisymmetry Killing vectors which is null at the horizon, where Ω {\\displaystyle \\Omega } is the angular velocity. Since k a {\\displaystyle k^{a}} is a Killing vector k a ∇ a k b = κ k b {\\displaystyle k^{a}\\,\\nabla _{a}k^{b}=\\kappa k^{b}} implies"
  },
  {
    "source": "Surface gravity.txt",
    "chunk_id": "Surface gravity.txt_9",
    "chunk": "− k a ∇ b k a = κ k b {\\displaystyle -k^{a}\\,\\nabla ^{b}k_{a}=\\kappa k^{b}} . In ( t , r , θ , φ ) {\\displaystyle (t,r,\\theta ,\\varphi )} coordinates k a = ( 1 , 0 , 0 , 0 ) {\\displaystyle k^{a}=(1,0,0,0)} . Performing a coordinate change to the advanced Eddington–Finklestein coordinates v = t + r + 2 M ln ⁡ | r − 2 M | {\\textstyle v=t+r+2M\\ln |r-2M|} causes the metric to take the form d s 2 = − ( 1 − 2 M r ) d v 2 + ( d v d r + d r d v ) + r 2 ( d θ 2 + sin 2 ⁡ θ d φ 2 ) . {\\displaystyle ds^{2}=-\\left(1-{\\frac {2M}{r}}\\right)\\,dv^{2}+\\left(dv\\,dr+\\,dr\\,dv\\right)+r^{2}\\left(d\\theta ^{2}+\\sin ^{2}\\theta \\,d\\varphi ^{2}\\right).} Under a general change of coordinates the Killing vector transforms as k v = A t v k t {\\displaystyle k^{v}=A_{t}^{v}k^{t}} giving the vectors k a ′ = δ v a ′ = ( 1 , 0 , 0 , 0 ) {\\displaystyle k^{a'}=\\delta _{v}^{a'}=(1,0,0,0)} and k a ′ = g a ′ v = ( − 1 + 2 M r , 1 , 0 , 0"
  },
  {
    "source": "Surface gravity.txt",
    "chunk_id": "Surface gravity.txt_10",
    "chunk": ") . {\\textstyle k_{a'}=g_{a'v}=\\left(-1+{\\frac {2M}{r}},1,0,0\\right).} Considering the b = v {\\displaystyle v} entry for k a ∇ a k b = κ k b {\\displaystyle k^{a}\\,\\nabla _{a}k^{b}=\\kappa k^{b}} gives the differential equation − 1 2 ∂ ∂ r ( − 1 + 2 M r ) = κ . {\\textstyle -{\\frac {1}{2}}{\\frac {\\partial }{\\partial r}}\\left(-1+{\\frac {2M}{r}}\\right)=\\kappa .} Therefore, the surface gravity for the Schwarzschild solution with mass M {\\displaystyle M} is κ = 1 4 M {\\displaystyle \\kappa ={\\frac {1}{4M}}} ( κ = c 4 / 4 G M {\\displaystyle \\kappa ={c^{4}}/{4GM}} in SI units). The surface gravity for the uncharged, rotating black hole is, simply κ = g − k , {\\displaystyle \\kappa =g-k,} where g = 1 4 M {\\textstyle g={\\frac {1}{4M}}} is the Schwarzschild surface gravity, and k := M Ω + 2 {\\displaystyle k:=M\\Omega _{+}^{2}} is the spring constant of the rotating black hole. Ω + {\\displaystyle \\Omega _{+}} is the angular velocity at the event horizon. This expression gives a simple Hawking temperature of 2 π T = g − k {\\displaystyle 2\\pi T=g-k} . The surface gravity for the Kerr–Newman solution is κ = r + − r − 2 ( r + 2"
  },
  {
    "source": "Surface gravity.txt",
    "chunk_id": "Surface gravity.txt_11",
    "chunk": "+ a 2 ) = M 2 − Q 2 − J 2 / M 2 2 M 2 − Q 2 + 2 M M 2 − Q 2 − J 2 / M 2 , {\\displaystyle \\kappa ={\\frac {r_{+}-r_{-}}{2\\left(r_{+}^{2}+a^{2}\\right)}}={\\frac {\\sqrt {M^{2}-Q^{2}-J^{2}/M^{2}}}{2M^{2}-Q^{2}+2M{\\sqrt {M^{2}-Q^{2}-J^{2}/M^{2}}}}},} where Q {\\displaystyle Q} is the electric charge, J {\\displaystyle J} is the angular momentum, define r ± := M ± M 2 − Q 2 − J 2 / M 2 {\\textstyle r_{\\pm }:=M\\pm {\\sqrt {M^{2}-Q^{2}-J^{2}/M^{2}}}} to be the locations of the two horizons and a := J / M {\\displaystyle a:=J/M} . Surface gravity for stationary black holes is well defined. This is because all stationary black holes have a horizon that is Killing. Recently there has been a shift towards defining the surface gravity of dynamical black holes whose spacetime does not admit a timelike Killing vector (field). Several definitions have been proposed over the years by various authors, such as peeling surface gravity and Kodama surface gravity. As of current, there is no consensus or agreement on which definition, if any, is correct. Semiclassical results indicate that the peeling surface gravity is ill-defined for transient objects formed in finite time of a"
  },
  {
    "source": "Surface gravity.txt",
    "chunk_id": "Surface gravity.txt_12",
    "chunk": "distant observer."
  },
  {
    "source": "Taurids.txt",
    "chunk_id": "Taurids.txt_0",
    "chunk": "# Taurids The Taurids are an annual meteor shower, associated with the comet Encke. The Taurids are actually two separate showers, with a Southern and a Northern component. The Southern Taurids originated from Comet Encke, while the Northern Taurids originated from the asteroid 2004 TG10, possibly a large fragment of Encke due to its similar orbital parameters. They are named after their radiant point in the constellation Taurus, where they are seen to come from in the sky. Because of their occurrence in late October and early November, they are also called Halloween fireballs. Since 2P/Encke is such a short period comet, the meteors have the slowest impact speed of the annual well-known meteor showers. Comet Encke and the Taurid complex are believed to be remnants of a disrupted 40-km-class comet from about 10,000 years ago, breaking into several pieces and releasing material by normal cometary activity, mass loss via YORP spin-up, or occasionally by close encounters with the tidal force of Earth or other planets (Whipple, 1940; Klačka, 1999). In total, this meteoroid stream is the largest in the inner Solar System. Since the stream is rather spread out in space, Earth takes several weeks to pass through it,"
  },
  {
    "source": "Taurids.txt",
    "chunk_id": "Taurids.txt_1",
    "chunk": "causing an extended period of meteor activity, compared with the much smaller periods of activity in other showers. The Taurids are also made up of weightier material, pebbles instead of dust grains. The daytime showers are active from May to July (Beta Taurids and Zeta Perseids), while the nighttime showers are active from September to December. Typically, Taurids appear at a rate of about 5 per hour, moving slowly across the sky at about 28 kilometers per second (17 mi/s), or 100,800 km/h (65,000 mph). If larger than a pebble, these meteors may become bolides as bright as the Moon and leave behind smoke trails. Due to the gravitational perturbations of planets, especially Jupiter, the Taurids have spread out over time, allowing separate segments labeled the Northern Taurids (NTA) and Southern Taurids (STA) to become observable. The Southern Taurids are active from about September 23 to December 8, while the Northern Taurids are active from about October 13 to December 2. Essentially these are two cross sections of a single, broad, continuous stream in space. The Beta Taurids and Zeta Perseids, encountered by the Earth in June/July, are also cross sections of the stream that approach from the Earth's daytime"
  },
  {
    "source": "Taurids.txt",
    "chunk_id": "Taurids.txt_2",
    "chunk": "side and, as such, cannot be observed visually in the way the (night-time) Northern and Southern Taurids of October/November can. Astronomers Duncan Steel and Bill Napier even suggest the Beta Taurids could be the cause of the Tunguska event of June 30, 1908. In 1962 and 1963, the Mars 1 probe recorded one micrometeorite strike every two minutes at altitudes ranging from 6,000 to 40,000 km (3,700 to 24,900 mi) from Earth's surface due to the Taurids meteor shower, and also recorded similar densities at distances from 20 to 40 million km (12 to 25 million mi) from Earth. The Taurid stream has a cycle of activity that peaks roughly every 2,500 to 3,000 years, when the core of the stream passes nearer to Earth and produces more intense showers. In fact, because of the separate \"branches\" (night-time in one part of the year and daytime in another; and Northern/Southern in each case) there are two (possibly overlapping) peaks separated by a few centuries, every 3000 years. The next peak is expected around 3000 AD. The Taurids also have more frequent peaks which may result from a heavier concentration of material in the stream, which only encounter Earth during some"
  },
  {
    "source": "Taurids.txt",
    "chunk_id": "Taurids.txt_3",
    "chunk": "passes. Over Poland in 1995, all-sky cameras imaged an absolute magnitude –17 Taurid bolide that was estimated to be 900 kg and perhaps a meter in diameter. In 1993, it was predicted that there would be a swarm of activity in 2005. Around Halloween in 2005, many fireballs were witnessed that affected people's night vision. Astronomers have taken to calling these the \"Halloween fireballs.\" During the Southern Taurid meteor shower in 2013, fireball sightings were spotted over southern California, Arizona, Nevada, and Utah. A 2021 study by Ignacio Ferrín and Vincenzo Orofino catalogued 88 probable members of the swarm and showed that many such as the 2212 Hephaistos group and the 169P/NEAT group exhibit cometary activity. A brief flash of light from a lunar impact event was recorded by NASA scientist Rob Suggs and astronomer Bill Cooke on November 7, 2005, while testing a new 250 mm (10 in) telescope and video camera they had built to monitor the Moon for meteor strikes. After consulting star charts, they concluded that the impact body was likely part of the Taurid meteor shower. This may be the first photographic record of such a strike, which some witnesses claim to have visually observed"
  },
  {
    "source": "Taurids.txt",
    "chunk_id": "Taurids.txt_4",
    "chunk": "on rare occasions."
  },
  {
    "source": "Telescope.txt",
    "chunk_id": "Telescope.txt_0",
    "chunk": "# Telescope A telescope is a device used to observe distant objects by their emission, absorption, or reflection of electromagnetic radiation. Originally, it was an optical instrument using lenses, curved mirrors, or a combination of both to observe distant objects – an optical telescope. Nowadays, the word \"telescope\" is defined as a wide range of instruments capable of detecting different regions of the electromagnetic spectrum, and in some cases other types of detectors. The first known practical telescopes were refracting telescopes with glass lenses and were invented in the Netherlands at the beginning of the 17th century. They were used for both terrestrial applications and astronomy. The reflecting telescope, which uses mirrors to collect and focus light, was invented within a few decades of the first refracting telescope. In the 20th century, many new types of telescopes were invented, including radio telescopes in the 1930s and infrared telescopes in the 1960s. The word telescope was coined in 1611 by the Greek mathematician Giovanni Demisiani for one of Galileo Galilei's instruments presented at a banquet at the Accademia dei Lincei. In the Starry Messenger, Galileo had used the Latin term perspicillum. The root of the word is from the Ancient Greek"
  },
  {
    "source": "Telescope.txt",
    "chunk_id": "Telescope.txt_1",
    "chunk": "τῆλε, tele 'far' and σκοπεῖν, skopein 'to look or see'; τηλεσκόπος, teleskopos 'far-seeing'. The earliest existing record of a telescope was a 1608 patent submitted to the government in the Netherlands by Middelburg spectacle maker Hans Lipperhey for a refracting telescope. The actual inventor is unknown but word of it spread through Europe. Galileo heard about it and, in 1609, built his own version, and made his telescopic observations of celestial objects. The idea that the objective, or light-gathering element, could be a mirror instead of a lens was being investigated soon after the invention of the refracting telescope. The potential advantages of using parabolic mirrors—reduction of spherical aberration and no chromatic aberration—led to many proposed designs and several attempts to build reflecting telescopes. In 1668, Isaac Newton built the first practical reflecting telescope, of a design which now bears his name, the Newtonian reflector. The invention of the achromatic lens in 1733 partially corrected color aberrations present in the simple lens and enabled the construction of shorter, more functional refracting telescopes. Reflecting telescopes, though not limited by the color problems seen in refractors, were hampered by the use of fast tarnishing speculum metal mirrors employed during the 18th and"
  },
  {
    "source": "Telescope.txt",
    "chunk_id": "Telescope.txt_2",
    "chunk": "early 19th century—a problem alleviated by the introduction of silver coated glass mirrors in 1857, and aluminized mirrors in 1932. The maximum physical size limit for refracting telescopes is about 1 meter (39 inches), dictating that the vast majority of large optical researching telescopes built since the turn of the 20th century have been reflectors. The largest reflecting telescopes currently have objectives larger than 10 meters (33 feet), and work is underway on several 30–40m designs. The 20th century also saw the development of telescopes that worked in a wide range of wavelengths from radio to gamma-rays. The first purpose-built radio telescope went into operation in 1937. Since then, a large variety of complex astronomical instruments have been developed. Since the atmosphere is opaque for most of the electromagnetic spectrum, only a few bands can be observed from the Earth's surface. These bands are visible – near-infrared and a portion of the radio-wave part of the spectrum. For this reason there are no X-ray or far-infrared ground-based telescopes as these have to be observed from orbit. Even if a wavelength is observable from the ground, it might still be advantageous to place a telescope on a satellite due to issues"
  },
  {
    "source": "Telescope.txt",
    "chunk_id": "Telescope.txt_3",
    "chunk": "such as clouds, astronomical seeing and light pollution. The disadvantages of launching a space telescope include cost, size, maintainability and upgradability. Some examples of space telescopes from NASA are the Hubble Space Telescope that detects visible light, ultraviolet, and near-infrared wavelengths, the Spitzer Space Telescope that detects infrared radiation, and the Kepler Space Telescope that discovered thousands of exoplanets. The latest telescope that was launched was the James Webb Space Telescope on December 25, 2021, in Kourou, French Guiana. The Webb telescope detects infrared light. The name \"telescope\" covers a wide range of instruments. Most detect electromagnetic radiation, but there are major differences in how astronomers must go about collecting light (electromagnetic radiation) in different frequency bands. As wavelengths become longer, it becomes easier to use antenna technology to interact with electromagnetic radiation (although it is possible to make very tiny antenna). The near-infrared can be collected much like visible light; however, in the far-infrared and submillimetre range, telescopes can operate more like a radio telescope. For example, the James Clerk Maxwell Telescope observes from wavelengths from 3 μm (0.003 mm) to 2000 μm (2 mm), but uses a parabolic aluminum antenna. On the other hand, the Spitzer Space Telescope,"
  },
  {
    "source": "Telescope.txt",
    "chunk_id": "Telescope.txt_4",
    "chunk": "observing from about 3 μm (0.003 mm) to 180 μm (0.18 mm) uses a mirror (reflecting optics). Also using reflecting optics, the Hubble Space Telescope with Wide Field Camera 3 can observe in the frequency range from about 0.2 μm (0.0002 mm) to 1.7 μm (0.0017 mm) (from ultra-violet to infrared light). With photons of the shorter wavelengths, with the higher frequencies, glancing-incident optics, rather than fully reflecting optics are used. Telescopes such as TRACE and SOHO use special mirrors to reflect extreme ultraviolet, producing higher resolution and brighter images than are otherwise possible. A larger aperture does not just mean that more light is collected, it also enables a finer angular resolution. Telescopes may also be classified by location: ground telescope, space telescope, or flying telescope. They may also be classified by whether they are operated by professional astronomers or amateur astronomers. A vehicle or permanent campus containing one or more telescopes or other instruments is called an observatory. Radio telescopes are directional radio antennas that typically employ a large dish to collect radio waves. The dishes are sometimes constructed of a conductive wire mesh whose openings are smaller than the wavelength being observed. Unlike an optical telescope, which"
  },
  {
    "source": "Telescope.txt",
    "chunk_id": "Telescope.txt_5",
    "chunk": "produces a magnified image of the patch of sky being observed, a traditional radio telescope dish contains a single receiver and records a single time-varying signal characteristic of the observed region; this signal may be sampled at various frequencies. In some newer radio telescope designs, a single dish contains an array of several receivers; this is known as a focal-plane array. By collecting and correlating signals simultaneously received by several dishes, high-resolution images can be computed. Such multi-dish arrays are known as astronomical interferometers and the technique is called aperture synthesis. The 'virtual' apertures of these arrays are similar in size to the distance between the telescopes. As of 2005, the record array size is many times the diameter of the Earth – using space-based very-long-baseline interferometry (VLBI) telescopes such as the Japanese HALCA (Highly Advanced Laboratory for Communications and Astronomy) VSOP (VLBI Space Observatory Program) satellite. Aperture synthesis is now also being applied to optical telescopes using optical interferometers (arrays of optical telescopes) and aperture masking interferometry at single reflecting telescopes. Radio telescopes are also used to collect microwave radiation, which has the advantage of being able to pass through the atmosphere and interstellar gas and dust clouds. Some"
  },
  {
    "source": "Telescope.txt",
    "chunk_id": "Telescope.txt_6",
    "chunk": "radio telescopes such as the Allen Telescope Array are used by programs such as SETI and the Arecibo Observatory to search for extraterrestrial life. An optical telescope gathers and focuses light mainly from the visible part of the electromagnetic spectrum. Optical telescopes increase the apparent angular size of distant objects as well as their apparent brightness. For the image to be observed, photographed, studied, and sent to a computer, telescopes work by employing one or more curved optical elements, usually made from glass lenses and/or mirrors, to gather light and other electromagnetic radiation to bring that light or radiation to a focal point. Optical telescopes are used for astronomy and in many non-astronomical instruments, including: theodolites (including transits), spotting scopes, monoculars, binoculars, camera lenses, and spyglasses. There are three main optical types: A Fresnel imager is a proposed ultra-lightweight design for a space telescope that uses a Fresnel lens to focus light. Beyond these basic optical types there are many sub-types of varying optical design classified by the task they perform such as astrographs, comet seekers and solar telescopes. Most ultraviolet light is absorbed by the Earth's atmosphere, so observations at these wavelengths must be performed from the upper atmosphere"
  },
  {
    "source": "Telescope.txt",
    "chunk_id": "Telescope.txt_7",
    "chunk": "or from space. X-rays are much harder to collect and focus than electromagnetic radiation of longer wavelengths. X-ray telescopes can use X-ray optics, such as Wolter telescopes composed of ring-shaped 'glancing' mirrors made of heavy metals that are able to reflect the rays just a few degrees. The mirrors are usually a section of a rotated parabola and a hyperbola, or ellipse. In 1952, Hans Wolter outlined 3 ways a telescope could be built using only this kind of mirror. Examples of space observatories using this type of telescope are the Einstein Observatory, ROSAT, and the Chandra X-ray Observatory. In 2012 the NuSTAR X-ray Telescope was launched which uses Wolter telescope design optics at the end of a long deployable mast to enable photon energies of 79 keV. Higher energy X-ray and gamma ray telescopes refrain from focusing completely and use coded aperture masks: the patterns of the shadow the mask creates can be reconstructed to form an image. X-ray and Gamma-ray telescopes are usually installed on high-flying balloons or Earth-orbiting satellites since the Earth's atmosphere is opaque to this part of the electromagnetic spectrum. An example of this type of telescope is the Fermi Gamma-ray Space Telescope which was"
  },
  {
    "source": "Telescope.txt",
    "chunk_id": "Telescope.txt_8",
    "chunk": "launched in June 2008. The detection of very high energy gamma rays, with shorter wavelength and higher frequency than regular gamma rays, requires further specialization. Such detections can be made either with the Imaging Atmospheric Cherenkov Telescopes (IACTs) or with Water Cherenkov Detectors (WCDs). Examples of IACTs are H.E.S.S. and VERITAS with the next-generation gamma-ray telescope, the Cherenkov Telescope Array (CTA), currently under construction. HAWC and LHAASO are examples of gamma-ray detectors based on the Water Cherenkov Detectors. A discovery in 2012 may allow focusing gamma-ray telescopes. At photon energies greater than 700 keV, the index of refraction starts to increase again."
  },
  {
    "source": "Telesto (moon).txt",
    "chunk_id": "Telesto (moon).txt_0",
    "chunk": "# Telesto (moon) Telesto /təˈlɛstoʊ/ is a moon of Saturn. It was discovered by Smith, Reitsema, Larson and Fountain in 1980 from ground-based observations, and was provisionally designated S/1980 S 13. In the following months, several other apparitions were observed: S/1980 S 24, S/1980 S 33, and S/1981 S 1. In 1983 it was officially named after Telesto of Greek mythology. It is also designated as Saturn XIII or Tethys B. Telesto is co-orbital with Tethys, residing in Tethys's leading Lagrangian point (L4). This relationship was first identified by Seidelmann et al. in 1981. Another moon, Calypso, resides in the other (trailing) Lagrangian point of Tethys, 60 degrees in the other direction from Tethys. The Saturnian system has two additional trojan moons. The Cassini probe performed a distant flyby of Telesto on October 11, 2005. The resulting images show that its surface is surprisingly smooth, devoid of small impact craters."
  },
  {
    "source": "Temporal paradox.txt",
    "chunk_id": "Temporal paradox.txt_0",
    "chunk": "# Temporal paradox A temporal paradox, time paradox, or time travel paradox, is a paradox, an apparent contradiction, or logical contradiction associated with the idea of time travel or other foreknowledge of the future. While the notion of time travel to the future complies with the current understanding of physics via relativistic time dilation, temporal paradoxes arise from circumstances involving hypothetical time travel to the past – and are often used to demonstrate its impossibility. Temporal paradoxes fall into three broad groups: bootstrap paradoxes, consistency paradoxes, and Newcomb's paradox. Bootstrap paradoxes violate causality by allowing future events to influence the past and cause themselves, or \"bootstrapping\", which derives from the idiom \"pull oneself up by one's bootstraps.\" Consistency paradoxes, on the other hand, are those where future events influence the past to cause an apparent contradiction, exemplified by the grandfather paradox, where a person travels to the past to prevent the conception of one of their ancestors, thus eliminating all the ancestor's descendants. Newcomb's paradox stems from the apparent contradictions that stem from the assumptions of both free will and foreknowledge of future events. All of these are sometimes referred to individually as \"causal loops.\" The term \"time loop\" is"
  },
  {
    "source": "Temporal paradox.txt",
    "chunk_id": "Temporal paradox.txt_1",
    "chunk": "sometimes referred to as a causal loop, but although they appear similar, causal loops are unchanging and self-originating, whereas time loops are constantly resetting. A bootstrap paradox, also known as an information loop, an information paradox, an ontological paradox, or a \"predestination paradox\" is a paradox of time travel that occurs when any event, such as an action, information, an object, or a person, ultimately causes itself, as a consequence of either retrocausality or time travel. Backward time travel would allow information, people, or objects whose histories seem to \"come from nowhere\". Such causally looped events then exist in spacetime, but their origin cannot be determined. The notion of objects or information that are \"self-existing\" in this way is often viewed as paradoxical. A notable example occurs in the 1958 science fiction short story \"—All You Zombies—\", by Robert A. Heinlein, wherein the main character, an intersex individual, becomes both their own mother and father; the 2014 film Predestination is based on the story. Allen Everett gives the movie Somewhere in Time as an example involving an object with no origin: an old woman gives a watch to a playwright who later travels back in time and meets the same"
  },
  {
    "source": "Temporal paradox.txt",
    "chunk_id": "Temporal paradox.txt_2",
    "chunk": "woman when she was young, and shows her the watch that she will later give to him. An example of information which \"came from nowhere\" is in the movie Star Trek IV: The Voyage Home, in which a 23rd-century engineer travels back in time, and gives the formula for transparent aluminum to the 20th-century engineer who supposedly invented it. Smeenk uses the term \"predestination paradox\" to refer specifically to situations in which a time traveler goes back in time to try to prevent some event in the past. The \"predestination paradox\" is a concept in time travel and temporal mechanics, often explored in science fiction. It occurs when a future event is the cause of a past event, which in turn becomes the cause of the future event, forming a self-sustaining loop in time. This paradox challenges conventional understandings of cause and effect, as the events involved are both the origin and the result of each other. A notable example is found in the TV series Doctor Who, where a character saves her father in the past, fulfilling a memory he had shared with her as a child about a strange woman having saved his life. The predestination paradox raises"
  },
  {
    "source": "Temporal paradox.txt",
    "chunk_id": "Temporal paradox.txt_3",
    "chunk": "philosophical questions about free will, determinism, and the nature of time itself. It is commonly used as a narrative device in fiction to highlight the interconnectedness of events and the inevitability of certain outcomes. The consistency paradox or grandfather paradox occurs when the past is changed in any way that directly negates the conditions required for the time travel to occur in the first place, thus creating a contradiction. A common example given is traveling to the past and preventing the conception of one's ancestors (such as causing the death of the ancestor's parent beforehand), thus preventing the conception of oneself. If the traveler were not born, then it would not be possible to undertake such an act in the first place; therefore, the ancestor proceeds to beget the traveler's next-generation ancestor and secure the line to the traveler. There is no predicted outcome to this scenario. Consistency paradoxes occur whenever changing the past is possible. A possible resolution is that a time traveller can do anything that did happen, but cannot do anything that did not happen. Doing something that did not happen results in a contradiction. This is referred to as the Novikov self-consistency principle. The grandfather paradox"
  },
  {
    "source": "Temporal paradox.txt",
    "chunk_id": "Temporal paradox.txt_4",
    "chunk": "encompasses any change to the past, and it is presented in many variations, including killing one's past self. Both the \"retro-suicide paradox\" and the \"grandfather paradox\" appeared in letters written into Amazing Stories in the 1920s. Another variant of the grandfather paradox is the \"Hitler paradox\" or \"Hitler's murder paradox\", in which the protagonist travels back in time to murder Adolf Hitler before he can rise to power in Germany, thus preventing World War II and the Holocaust. Rather than necessarily physically preventing time travel, the action removes any reason for the travel, along with any knowledge that the reason ever existed. Physicist John Garrison et al. give a variation of the paradox of an electronic circuit that sends a signal through a time machine to shut itself off, and receives the signal before it sends it. Newcomb's paradox is a thought experiment showing an apparent contradiction between the expected utility principle and the strategic dominance principle. The thought experiment is often extended to explore causality and free will by allowing for \"perfect predictors\": if perfect predictors of the future exist, for example if time travel exists as a mechanism for making perfect predictions, then perfect predictions appear to contradict"
  },
  {
    "source": "Temporal paradox.txt",
    "chunk_id": "Temporal paradox.txt_5",
    "chunk": "free will because decisions apparently made with free will are already known to the perfect predictor. Predestination does not necessarily involve a supernatural power, and could be the result of other \"infallible foreknowledge\" mechanisms. Problems arising from infallibility and influencing the future are explored in Newcomb's paradox. Even without knowing whether time travel to the past is physically possible, it is possible to show using modal logic that changing the past results in a logical contradiction. If it is necessarily true that the past happened in a certain way, then it is false and impossible for the past to have occurred in any other way. A time traveler would not be able to change the past from the way it is, but would only act in a way that is already consistent with what necessarily happened. Consideration of the grandfather paradox has led some to the idea that time travel is by its very nature paradoxical and therefore logically impossible. For example, the philosopher Bradley Dowden made this sort of argument in the textbook Logical Reasoning, arguing that the possibility of creating a contradiction rules out time travel to the past entirely. However, some philosophers and scientists believe that time"
  },
  {
    "source": "Temporal paradox.txt",
    "chunk_id": "Temporal paradox.txt_6",
    "chunk": "travel into the past need not be logically impossible provided that there is no possibility of changing the past, as suggested, for example, by the Novikov self-consistency principle. Dowden revised his view after being convinced of this in an exchange with the philosopher Norman Swartz. A recent proposed resolution argues that if time is not an inherent property of the universe but is instead emergent from the laws of entropy, as some modern theories suggest, then it presents a natural solution to the Grandfather Paradox. In this framework, \"time travel\" is reinterpreted not as movement along a linear continuum but as a reconfiguration of the present state of the universe to match a prior entropic configuration. Because the original chronological sequence—including events like the time traveler’s birth—remains preserved in the universe’s irreversible entropic progression, actions within the reconfigured state cannot alter the causal history that produced the traveler. This avoids paradoxes by treating time as a thermodynamic artifact rather than a mutable dimension. Consideration of the possibility of backward time travel in a hypothetical universe described by a Gödel metric led famed logician Kurt Gödel to assert that time might itself be a sort of illusion. He suggests something along"
  },
  {
    "source": "Temporal paradox.txt",
    "chunk_id": "Temporal paradox.txt_7",
    "chunk": "the lines of the block time view, in which time is just another dimension like space, with all events at all times being fixed within this four-dimensional \"block\". Sergey Krasnikov writes that these bootstrap paradoxes – information or an object looping through time – are the same; the primary apparent paradox is a physical system evolving into a state in a way that is not governed by its laws. He does not find these paradoxical and attributes problems regarding the validity of time travel to other factors in the interpretation of general relativity. A 1992 paper by physicists Andrei Lossev and Igor Novikov labeled such items without origin as Jinn, with the singular term Jinnee. This terminology was inspired by the Jinn of the Quran, which are described as leaving no trace when they disappear. Lossev and Novikov allowed the term \"Jinn\" to cover both objects and information with the reflexive origin; they called the former \"Jinn of the first kind\", and the latter \"Jinn of the second kind\". They point out that an object making circular passage through time must be identical whenever it is brought back to the past, otherwise it would create an inconsistency; the second law"
  },
  {
    "source": "Temporal paradox.txt",
    "chunk_id": "Temporal paradox.txt_8",
    "chunk": "of thermodynamics seems to require that the object tends to a lower energy state throughout its history, and such objects that are identical in repeating points in their history seem to contradict this, but Lossev and Novikov argued that since the second law only requires entropy to increase in closed systems, a Jinnee could interact with its environment in such a way as to regain \"lost\" entropy. They emphasize that there is no \"strict difference\" between Jinn of the first and second kind. Krasnikov equivocates between \"Jinn\", \"self-sufficient loops\", and \"self-existing objects\", calling them \"lions\" or \"looping or intruding objects\", and asserts that they are no less physical than conventional objects, \"which, after all, also could appear only from either infinity or a singularity.\" The self-consistency principle developed by Igor Dmitriyevich Novikov expresses one view as to how backward time travel would be possible without the generation of paradoxes. According to this hypothesis, even though general relativity permits some exact solutions that allow for time travel that contain closed timelike curves that lead back to the same point in spacetime, physics in or near closed timelike curves (time machines) can only be consistent with the universal laws of physics, and"
  },
  {
    "source": "Temporal paradox.txt",
    "chunk_id": "Temporal paradox.txt_9",
    "chunk": "thus only self-consistent events can occur. Anything a time traveler does in the past must have been part of history all along, and the time traveler can never do anything to prevent the trip back in time from happening, since this would represent an inconsistency. The authors concluded that time travel need not lead to unresolvable paradoxes, regardless of what type of object was sent to the past. Physicist Joseph Polchinski considered a potentially paradoxical situation involving a billiard ball that is fired into a wormhole at just the right angle such that it will be sent back in time and collides with its earlier self, knocking it off course, which would stop it from entering the wormhole in the first place. Kip Thorne referred to this problem as \"Polchinski's paradox\". Thorne and two of his students at Caltech, Fernando Echeverria and Gunnar Klinkhammer, went on to find a solution that avoided any inconsistencies, and found that there was more than one self-consistent solution, with slightly different angles for the glancing blow in each case. Later analysis by Thorne and Robert Forward showed that for certain initial trajectories of the billiard ball, there could be an infinite number of self-consistent"
  },
  {
    "source": "Temporal paradox.txt",
    "chunk_id": "Temporal paradox.txt_10",
    "chunk": "solutions. It is plausible that there exist self-consistent extensions for every possible initial trajectory, although this has not been proven. The lack of constraints on initial conditions only applies to spacetime outside of the chronology-violating region of spacetime; the constraints on the chronology-violating region might prove to be paradoxical, but this is not yet known. Novikov's views are not widely accepted. Visser views causal loops and Novikov's self-consistency principle as an ad hoc solution, and supposes that there are far more damaging implications of time travel. Krasnikov similarly finds no inherent fault in causal loops but finds other problems with time travel in general relativity. Another conjecture, the cosmic censorship hypothesis, suggests that every closed timelike curve passes through an event horizon, which prevents such causal loops from being observed. The interacting-multiple-universes approach is a variation of the many-worlds interpretation of quantum mechanics that involves time travelers arriving in a different universe than the one from which they came; it has been argued that, since travelers arrive in a different universe's history and not their history, this is not \"genuine\" time travel. Stephen Hawking has argued for the chronology protection conjecture, that even if the MWI is correct, we should"
  },
  {
    "source": "Temporal paradox.txt",
    "chunk_id": "Temporal paradox.txt_11",
    "chunk": "expect each time traveler to experience a single self-consistent history so that time travelers remain within their world rather than traveling to a different one. David Deutsch has proposed that quantum computation with a negative delay—backward time travel—produces only self-consistent solutions, and the chronology-violating region imposes constraints that are not apparent through classical reasoning. However Deutsch's self-consistency condition has been demonstrated as capable of being fulfilled to arbitrary precision by any system subject to the laws of classical statistical mechanics, even if it is not built up by quantum systems. Allen Everett has also argued that even if Deutsch's approach is correct, it would imply that any macroscopic object composed of multiple particles would be split apart when traveling back in time, with different particles emerging in different worlds."
  },
  {
    "source": "Terence Dickinson.txt",
    "chunk_id": "Terence Dickinson.txt_0",
    "chunk": "# Terence Dickinson Terence Dickinson CM (10 November 1943 – 1 February 2023) was a Canadian amateur astronomer and astrophotographer who lived near Yarker, Ontario, Canada. He was the author of 14 astronomy books for both adults and children. He was the founder and former editor of SkyNews magazine. Dickinson had been an astronomy commentator for Discovery Channel Canada and taught at St. Lawrence College. He made appearances at such places as the Ontario Science Centre. In 1994, the International Astronomical Union committee on Minor Planet Nomenclature named asteroid 5272 Dickinson in honour of his \"ability to explain the universe in everyday language\". Dickinson was born in Toronto, Ontario, on 10 November 1943. He became interested in astronomy at age five after seeing a bright meteor from just outside his family's home. When he was 14 he received a 60 mm telescope as a Christmas present, the first of nearly 20 telescopes he owned. Past occupations include editor of Astronomy magazine (1974-75) and planetarium instructor. He became a full-time science writer in 1976. He received the 1993 Industry Canada's Michael Smith Award for Public Promotion of Science, the 1993 Canadian Science Writers' Association Award First Place for Science and Technology"
  },
  {
    "source": "Terence Dickinson.txt",
    "chunk_id": "Terence Dickinson.txt_1",
    "chunk": "writing, and the Royal Canadian Institute's Sandford Fleming Medal in 1992. In 1995 Dickinson was made a Member of the Order of Canada, which is the nation's highest civilian achievement award. The Astronomical Society of the Pacific awarded him the Klumpke-Roberts Award in 1996. He received an honorary Doctor of Science degree from Queen's University in 2019. In 1983, Dickinson published NightWatch: A Practical Guide to Viewing the Universe. The book includes star charts, tables of future solar and lunar eclipses, planetary conjunctions, planet locations, and other illustrations. The Journal of the Royal Astronomical Society described NightWatch as the essential star-watching guide for amateur astronomers of all levels of experience. The book has become the world's best-selling manual for amateur stargazing. Dickinson internationally published twelve titles, primarily through Firefly Books."
  },
  {
    "source": "Terrestrial planet.txt",
    "chunk_id": "Terrestrial planet.txt_0",
    "chunk": "# Terrestrial planet A terrestrial planet, tellurian planet, telluric planet, or rocky planet, is a planet that is composed primarily of silicate, rocks or metals. Within the Solar System, the terrestrial planets accepted by the IAU are the inner planets closest to the Sun: Mercury, Venus, Earth and Mars. Among astronomers who use the geophysical definition of a planet, two or three planetary-mass satellites – Earth's Moon, Io, and sometimes Europa – may also be considered terrestrial planets. The large rocky asteroids Pallas and Vesta are sometimes included as well, albeit rarely. The terms \"terrestrial planet\" and \"telluric planet\" are derived from Latin words for Earth (Terra and Tellus), as these planets are, in terms of structure, Earth-like. Terrestrial planets are generally studied by geologists, astronomers, and geophysicists. Terrestrial planets have a solid planetary surface, making them substantially different from larger gaseous planets, which are composed mostly of some combination of hydrogen, helium, and water existing in various physical states. All terrestrial planets in the Solar System have the same basic structure, such as a central metallic core (mostly iron) with a surrounding silicate mantle. The large rocky asteroid 4 Vesta has a similar structure; possibly so does the smaller"
  },
  {
    "source": "Terrestrial planet.txt",
    "chunk_id": "Terrestrial planet.txt_1",
    "chunk": "one 21 Lutetia. Another rocky asteroid 2 Pallas is about the same size as Vesta, but is significantly less dense; it appears to have never differentiated a core and a mantle. The Earth's Moon and Jupiter's moon Io have similar structures to terrestrial planets, but Earth's Moon has a much smaller iron core. Another Jovian moon Europa has a similar density but has a significant ice layer on the surface: for this reason, it is sometimes considered an icy planet instead. Terrestrial planets can have surface structures such as canyons, craters, mountains, volcanoes, and others, depending on the presence at any time of an erosive liquid or tectonic activity or both. Terrestrial planets have secondary atmospheres, generated by volcanic out-gassing or from comet impact debris. This contrasts with the outer, giant planets, whose atmospheres are primary; primary atmospheres were captured directly from the original solar nebula. The Solar System has four terrestrial planets under the dynamical definition: Mercury, Venus, Earth and Mars. The Earth's Moon as well as Jupiter's moons Io and Europa would also count geophysically, as well as perhaps the large protoplanet-asteroids Pallas and Vesta (though those are borderline cases). Among these bodies, only the Earth has an"
  },
  {
    "source": "Terrestrial planet.txt",
    "chunk_id": "Terrestrial planet.txt_2",
    "chunk": "active surface hydrosphere. Europa is believed to have an active hydrosphere under its ice layer. During the formation of the Solar System, there were many terrestrial planetesimals and proto-planets, but most merged with or were ejected by the four terrestrial planets, leaving only Pallas and Vesta to survive more or less intact. These two were likely both dwarf planets in the past, but have been battered out of equilibrium shapes by impacts. Some other protoplanets began to accrete and differentiate but suffered catastrophic collisions that left only a metallic or rocky core, like 16 Psyche or 8 Flora respectively. Many S-type and M-type asteroids may be such fragments. The other round bodies from the asteroid belt outward are geophysically icy planets. They are similar to terrestrial planets in that they have a solid surface, but are composed of ice and rock rather than of rock and metal. These include the dwarf planets, such as Ceres, Pluto and Eris, which are found today only in the regions beyond the formation snow line where water ice was stable under direct sunlight in the early Solar System. It also includes the other round moons, which are ice-rock (e.g. Ganymede, Callisto, Titan, and Triton)"
  },
  {
    "source": "Terrestrial planet.txt",
    "chunk_id": "Terrestrial planet.txt_3",
    "chunk": "or even almost pure (at least 99%) ice (Tethys and Iapetus). Some of these bodies are known to have subsurface hydrospheres (Ganymede, Callisto, Enceladus, and Titan), like Europa, and it is also possible for some others (e.g. Ceres, Mimas, Dione, Miranda, Ariel, Triton, and Pluto). Titan even has surface bodies of liquid, albeit liquid methane rather than water. Jupiter's Ganymede, though icy, does have a metallic core like the Moon, Io, Europa, and the terrestrial planets. The name Terran world has been suggested to define all solid worlds (bodies assuming a rounded shape), without regard to their composition. It would thus include both terrestrial and icy planets. The uncompressed density of a terrestrial planet is the average density its materials would have at zero pressure. A greater uncompressed density indicates a greater metal content. Uncompressed density differs from the true average density (also often called \"bulk\" density) because compression within planet cores increases their density; the average density depends on planet size, temperature distribution, and material stiffness as well as composition. Calculations to estimate uncompressed density inherently require a model of the planet's structure. Where there have been landers or multiple orbiting spacecraft, these models are constrained by seismological data"
  },
  {
    "source": "Terrestrial planet.txt",
    "chunk_id": "Terrestrial planet.txt_4",
    "chunk": "and also moment of inertia data derived from the spacecraft's orbits. Where such data is not available, uncertainties are inevitably higher. The uncompressed densities of the rounded terrestrial bodies directly orbiting the Sun trend towards lower values as the distance from the Sun increases, consistent with the temperature gradient that would have existed within the primordial solar nebula. The Galilean satellites show a similar trend going outwards from Jupiter; however, no such trend is observable for the icy satellites of Saturn or Uranus. The icy worlds typically have densities less than 2 g·cm. Eris is significantly denser (2.43±0.05 g·cm), and may be mostly rocky with some surface ice, like Europa. It is unknown whether extrasolar terrestrial planets in general will follow such a trend. The data in the tables below are mostly taken from a list of gravitationally rounded objects of the Solar System and planetary-mass moon. All distances from the Sun are averages. Most of the planets discovered outside the Solar System are giant planets, because they are more easily detectable. But since 2005, hundreds of potentially terrestrial extrasolar planets have also been found, with several being confirmed as terrestrial. Most of these are super-Earths, i.e. planets with masses"
  },
  {
    "source": "Terrestrial planet.txt",
    "chunk_id": "Terrestrial planet.txt_5",
    "chunk": "between Earth's and Neptune's; super-Earths may be gas planets or terrestrial, depending on their mass and other parameters. During the early 1990s, the first extrasolar planets were discovered orbiting the pulsar PSR B1257+12, with masses of 0.02, 4.3, and 3.9 times that of Earth, by pulsar timing. When 51 Pegasi b, the first planet found around a star still undergoing fusion, was discovered, many astronomers assumed it to be a gigantic terrestrial, because it was assumed no gas giant could exist as close to its star (0.052 AU) as 51 Pegasi b did. It was later found to be a gas giant. In 2005, the first planets orbiting a main-sequence star and which showed signs of being terrestrial planets were found: Gliese 876 d and OGLE-2005-BLG-390Lb. Gliese 876 d orbits the red dwarf Gliese 876, 15 light years from Earth, and has a mass seven to nine times that of Earth and an orbital period of just two Earth days. OGLE-2005-BLG-390Lb has about 5.5 times the mass of Earth and orbits a star about 21,000 light-years away in the constellation Scorpius. From 2007 to 2010, three (possibly four) potential terrestrial planets were found orbiting within the Gliese 581 planetary system."
  },
  {
    "source": "Terrestrial planet.txt",
    "chunk_id": "Terrestrial planet.txt_6",
    "chunk": "The smallest, Gliese 581e, is only about 1.9 Earth masses, but orbits very close to the star. Two others, Gliese 581c and the disputed Gliese 581d, are more-massive super-Earths orbiting in or close to the habitable zone of the star, so they could potentially be habitable, with Earth-like temperatures. Another possibly terrestrial planet, HD 85512 b, was discovered in 2011; it has at least 3.6 times the mass of Earth. The radius and composition of all these planets are unknown. The first confirmed terrestrial exoplanet, Kepler-10b, was found in 2011 by the Kepler space telescope, specifically designed to discover Earth-size planets around other stars using the transit method. In the same year, the Kepler space telescope mission team released a list of 1235 extrasolar planet candidates, including six that are \"Earth-size\" or \"super-Earth-size\" (i.e. they have a radius less than twice that of the Earth) and in the habitable zone of their star. Since then, Kepler has discovered hundreds of planets ranging from Moon-sized to super-Earths, with many more candidates in this size range (see image). In 2016, statistical modeling of the relationship between a planet's mass and radius using a broken power law appeared to suggest that the transition"
  },
  {
    "source": "Terrestrial planet.txt",
    "chunk_id": "Terrestrial planet.txt_7",
    "chunk": "point between rocky, terrestrial worlds and mini-Neptunes without a defined surface was in fact very close to Earth and Venus's, suggesting that rocky worlds much larger than our own are in fact quite rare. This resulted in some advocating for the retirement of the term \"super-earth\" as being scientifically misleading. Since 2016 the catalog of known exoplanets has increased significantly, and there have been several published refinements of the mass-radius model. As of 2024, the expected transition point between rocky and intermediate-mass planets sits at roughly 4.4 earth masses, and roughly 1.6 earth radii. In September 2020, astronomers using microlensing techniques reported the detection, for the first time, of an Earth-mass rogue planet (named OGLE-2016-BLG-1928) unbounded by any star, and free-floating in the Milky Way galaxy. The following exoplanets have a density of at least 5 g/cm and a mass below Neptune's and are thus very likely terrestrial: Kepler-10b, Kepler-20b, Kepler-36b, Kepler-48d, Kepler 68c, Kepler-78b, Kepler-89b, Kepler-93b, Kepler-97b, Kepler-99b, Kepler-100b, Kepler-101c, Kepler-102b, Kepler-102d, Kepler-113b, Kepler-131b, Kepler-131c, Kepler-138c, Kepler-406b, Kepler-406c, Kepler-409b. In 2013, astronomers reported, based on Kepler space mission data, that there could be as many as 40 billion Earth- and super-Earth-sized planets orbiting in the habitable zones of Sun-like"
  },
  {
    "source": "Terrestrial planet.txt",
    "chunk_id": "Terrestrial planet.txt_8",
    "chunk": "stars and red dwarfs within the Milky Way. Eleven billion of these estimated planets may be orbiting Sun-like stars. The nearest such planet may be 12 light-years away, according to the scientists. However, this does not give estimates for the number of extrasolar terrestrial planets, because there are planets as small as Earth that have been shown to be gas planets (see Kepler-138d). Estimates show that about 80% of potentially habitable worlds are covered by land, and about 20% are ocean planets. Planets with rations more like those of Earth, which was 30% land and 70% ocean, only make up 1% of these worlds."
  },
  {
    "source": "The Astrophysical Journal.txt",
    "chunk_id": "The Astrophysical Journal.txt_0",
    "chunk": "# The Astrophysical Journal The Astrophysical Journal (ApJ) is a peer-reviewed scientific journal of astrophysics and astronomy, established in 1895 by American astronomers George Ellery Hale and James Edward Keeler. The journal discontinued its print edition and became an electronic-only journal in 2015. Since 1953, The Astrophysical Journal Supplement Series (ApJS) has been published in conjunction with The Astrophysical Journal, with generally longer articles to supplement the material in the journal. It publishes six volumes per year, with two 280-page issues per volume. The Astrophysical Journal Letters (ApJL), established in 1967 by Subrahmanyan Chandrasekhar as Part 2 of The Astrophysical Journal, is now a separate journal focusing on the rapid publication of high-impact astronomical research. The three journals were published by the University of Chicago Press for the American Astronomical Society until, in January 2009, publication was transferred to IOP Publishing, following the move of the society's Astronomical Journal in 2008. The reason for the changes were given by the society as the increasing financial demands of the University of Chicago Press. Compared to journals in other scientific disciplines, The Astrophysical Journal has a larger (> 85%) acceptance rate, which, however, is similar to other journals covering astronomy and astrophysics."
  },
  {
    "source": "The Astrophysical Journal.txt",
    "chunk_id": "The Astrophysical Journal.txt_1",
    "chunk": "On January 1, 2022, the AAS Journals, including ApJ, changed to an open access model, with access restrictions and subscription charges removed from previously published papers. Articles accepted after October 11, 2022, will be published under the Creative Commons license CC-BY-SA 4.0. Non-open access articles accepted before that date will be free to access but will still need permission to reuse. The journal was founded in 1895 by George Ellery Hale and James E. Keeler as The Astrophysical Journal: An International Review of Spectroscopy and Astronomical Physics. In addition to the two founding editors, there was an international board of associate editors: M. A. Cornu, Paris; N. C. Dunér, Upsala; William Huggins, London; P. Tacchini, Rome; H. C. Vogel, Potsdam, C. S. Hastings, Yale; A. A. Michelson, Chicago; E. C. Pickering, Harvard; H. A. Rowland, Johns Hopkins; and C. A. Young, Princeton. It was intended that the journal would fill the gap between journals in astronomy and physics, providing a venue for publication of articles on astronomical applications of the spectroscope; on laboratory research closely allied to astronomical physics, including wavelength determinations of metallic and gaseous spectra and experiments on radiation and absorption; on theories of the Sun, Moon, planets,"
  },
  {
    "source": "The Astrophysical Journal.txt",
    "chunk_id": "The Astrophysical Journal.txt_2",
    "chunk": "comets, meteors, and nebulae; and on instrumentation for telescopes and laboratories. The further development of ApJ up to 1995 was outlined by Helmut Abt in an article entitled \"Some Statistical Highlights of the Astrophysical Journal\" in 1995."
  },
  {
    "source": "The Mechanical Universe.txt",
    "chunk_id": "The Mechanical Universe.txt_0",
    "chunk": "# The Mechanical Universe The Mechanical Universe...And Beyond is a 52-part telecourse, filmed at the California Institute of Technology, that introduces university level physics, covering topics from Copernicus to quantum mechanics. The 1985-86 series was produced by Caltech and INTELECOM, a nonprofit consortium of California community colleges now known as Intelecom Learning, with financial support from Annenberg/CPB. The series, which aired on PBS affiliate stations before being distributed on LaserDisc and eventually YouTube, is known for its use of computer animation. Produced starting in 1982, the videos make heavy use of historical dramatizations and visual aids to explain physics concepts. The latter were state of the art at the time, incorporating almost eight hours of computer animation created by computer graphics pioneer Jim Blinn along with assistants Sylvie Rueff and Tom Brown at the Jet Propulsion Laboratory. Each episode opens and closes with bookend segments in which Caltech professor David Goodstein, speaking in a lecture hall, delivers explanations \"that can't quite be put into the mouth of our affable, faceless narrator\". After more than a quarter century, the series is still often used as a supplemental teaching aid, for its clear explanation of fundamental concepts such as special relativity. The"
  },
  {
    "source": "The Mechanical Universe.txt",
    "chunk_id": "The Mechanical Universe.txt_1",
    "chunk": "bookend segments featuring Goodstein were specially staged versions of actual freshman physics lectures from Caltech's courses Physics 1a and 1b. The organization and the choice of topics to emphasize in the television show reflect a then-recent revision of Caltech's introductory physics curriculum, the first total overhaul since the one represented by The Feynman Lectures on Physics almost two decades earlier. While Feynman generally sought contemporary examples of topics, the later revision of the curriculum brought a more historical focus: In essence, the earlier Feynman course had sought to make physics exciting by relating each subject, wherever possible, to contemporary scientific problems. The new course took the opposite tack, of trying to recreate the historical excitement of the original discovery. For example, classical mechanics—a notoriously difficult and uninspiring subject for students—is treated as the discovery of \"our place in the universe\". Accordingly, celestial mechanics is the backbone of the subject and its climax is Newton's solution of the Kepler problem. Episode 22 solved the Kepler problem — that is, demonstrating that an inverse-square law of gravity implies that orbits are conic sections — using a variant of the Laplace–Runge–Lenz vector, though not by that name. The room seen in the bookend"
  },
  {
    "source": "The Mechanical Universe.txt",
    "chunk_id": "The Mechanical Universe.txt_2",
    "chunk": "segments is the Bridge lecture hall at Caltech. Many of the extras were students from other schools, and the front rows of the lecture hall were deliberately filled with more women than would have typically been seen at Caltech lectures. The TV production team added fake wood paneling to the lecture hall so that it would more closely resemble that seen in the show The Paper Chase. Later, the Caltech physics department was sufficiently impressed by the result that panels were installed permanently. Many seats in the lecture hall had to be removed in order to make room for the camera track and studio lights. To cover this, additional reaction shots of a full lecture hall were filmed later, so that the illusion of a complete audience could be created in editing. For most of the footage of Goodstein himself, only two rows of students were present. Many other video segments were shot on location, for example at a Linde industrial plant that produced liquid air. Historical scenes were often made to be generic, in order to facilitate their reuse across multiple episodes: \"Young Newton strolls through an apple orchard, old Newton testily refuses a cup of tea from a"
  },
  {
    "source": "The Mechanical Universe.txt",
    "chunk_id": "The Mechanical Universe.txt_3",
    "chunk": "servant, and so on\". Footage featuring historical reenactment of Johannes Kepler was purchased from Carl Sagan's 1980 television series Cosmos: A Personal Voyage. The series was originally planned to consist of 26 episodes. This was later expanded to 60 episodes, a number then cut back to the eventual total of 52 for budget and production-schedule reasons. The show was intended not to require previous experience with calculus. Instead, the basics of differential and integral calculus would both be taught early in the series itself. Caltech mathematician Tom M. Apostol joined the Mechanical Universe production staff in order to ensure that the series did not compromise on the quality of the mathematics it presented. Seeing an example of Blinn's computer animation for the first time convinced Apostol that the series could bring mathematics \"to life in a way that cannot be done in a textbook or at the chalkboard\". When test screenings to humanities students revealed that their greatest difficulty learning calculus was a weak background in trigonometry, Apostol wrote a primer on the subject to be distributed with the telecourse. After advising the production of The Mechanical Universe, Apostol decided that a similar series, geared to high-school mathematics, would be"
  },
  {
    "source": "The Mechanical Universe.txt",
    "chunk_id": "The Mechanical Universe.txt_4",
    "chunk": "beneficial. This became the later Caltech series Project Mathematics!, which also featured computer animation by Blinn. Some of Blinn's animations for The Mechanical Universe were reused in the new series, in order to illustrate applications of algebra, geometry, and trigonometry. The 1990 science-fiction action film Total Recall used portions of the Mechanical Universe title sequence, in a scene where the protagonist (Douglas Quaid, played by Arnold Schwarzenegger) is offered virtual vacations in locales around the Solar System. The animation was used without licensing, and consequently, Caltech and Intelecom sued Carolco Pictures for $3 million. In order to present detailed mathematical equation derivations, the show employed a technique its creators called the \"algebraic ballet\". Computer animation presented derivations in step-by-step detail, but rapidly and with touches of whimsy, such as algebraic terms being canceled by a Monty Python-esque stomping foot or the hand of God from Michelangelo's The Creation of Adam. Blinn felt that Cosmos had taken itself \"too seriously\", and so he aimed to include more humor in the Mechanical Universe animations. The goal was to avoid putting the viewers' \"brains into a 60-cycle hum\", without sacrificing rigor; the creators intended that students could learn the overall gist of each"
  },
  {
    "source": "The Mechanical Universe.txt",
    "chunk_id": "The Mechanical Universe.txt_5",
    "chunk": "derivation from the animation, and then study the details using the accompanying textbook. Computer animation was also used to portray idealizations of physical systems, like simulated billiard balls illustrating Newton's laws of motion. Blinn had used some of the same software earlier to visualize the interaction of DNA and DNA polymerase for Cosmos. One commenter deemed these animations \"particularly useful in providing students with subjective insights into dynamic three-dimensional phenomena such as magnetic fields\". Creating the computer graphics necessary to visualize physics concepts led Blinn to invent new techniques for simulating clouds, as well as the virtual \"blobby objects\" known as metaballs. Blinn used the vertex coordinates of regular icosahedra and dodecahedra to determine the placement of electric field lines radiating away from point charges. Most of the narration was voiced by actor Aaron Fletcher, who also played Galileo Galilei in the historical segments. Some portions, such as explanations of particular technical details, were narrated by Sally Beaty, the show's executive producer. Shorter versions of Mechanical Universe episodes, 10 to 20 minutes in length, were created for use in high schools. This adaptation, for which a dozen high-school teachers and administrators were consultants, was supported by a $650,000 grant from"
  },
  {
    "source": "The Mechanical Universe.txt",
    "chunk_id": "The Mechanical Universe.txt_6",
    "chunk": "the National Science Foundation. These videos were distributed alongside supplemental written material for teachers' benefit, and were intended to be employed in conjunction with existing textbooks. Yorkshire Television later produced a version repackaged for the United Kingdom audience, which was released in April 1991. Annenberg/CPB provided the funding for the production of The Mechanical Universe. The show was one of the first twelve projects funded by the initial $90 million pledge the Annenberg Foundation gave to the Corporation for Public Broadcasting in the early 1980s. The total cost of the project was roughly $10 million. PBS and The Learning Channel began broadcasting The Mechanical Universe in September 1985. During the fall of 1986, roughly 100 PBS stations carried The Mechanical Universe, and by the fall of 1987, over 600 higher-education institutions had purchased it or licensed the episodes for use. In 1992, Goodstein noted that the series had been broadcast, via PBS, by over 100 stations, \"usually at peculiar hours when innocent people were unlikely to tune in accidentally on a differential equation in the act of being solved\". He observed that detailed viewership figures were difficult to obtain, but when the show had been broadcast in Miami during Saturday"
  },
  {
    "source": "The Mechanical Universe.txt",
    "chunk_id": "The Mechanical Universe.txt_7",
    "chunk": "mornings, the producers were able to obtain Nielsen ratings. In fact, it came in second in its time slot, beating the kiddie cartoons on two network stations. There were 18,000 faithful core households in Dade County alone, the median age of the viewers was 18, and half were female. However, we seldom get that kind of detailed information. Anecdotal information in the form of letters and phone calls indicates very considerable enthusiasm among users at all levels from casual viewers to high-school students to research university professors, but there have also been a number of sharp disappointments, particularly when Instructional Television administrators have tried to handle TMU like a conventional telecourse. Similarly, a 1988 review in Physics Today suggested that the programs would not function well on their own as a telecourse, but would work much better as a supplement to a traditional classroom or a more standard distance-learning course such as Open University. The reviewers also found the \"algebraic ballet\" of computer-animated equations too fast to follow: \"After a short time, one yearns for a live professor filling the blackboard with equations\". Similarly, a review in the American Journal of Physics, while praising the \"technical proficiency of the films\","
  },
  {
    "source": "The Mechanical Universe.txt",
    "chunk_id": "The Mechanical Universe.txt_8",
    "chunk": "wrote of the animated equation manipulations: \"As the MIT students say, this is like trying to take a drink of water out of a fire hose\". A considerably more enthusiastic evaluation came from physicist Charles H. Holbrow, who told Olenick: \"These materials will constitute the principal visual image of physics for decades\". A reviewer writing for Educational Technology found the animations \"fascinating to watch\" and opined that they were at least as effective as what many instructors could manage at a traditional blackboard. An editorial in the Los Angeles Times called the show \"extraordinary\" and the animations \"splendid\", quipping that \"if differential calculus is not television's Supreme Test, it would certainly make the semifinals in any competition\". Goodstein and Olenick reported that younger viewers tended to enjoy the \"algebraic ballet\" style \"much more than older viewers, who are made uncomfortable by the algebraic manipulations they cannot quite follow\". In 1986, The Mechanical Universe was used as part of a summer program for gifted children, to overall success. A 1987 study at Indiana University Bloomington used 14 Mechanical Universe episodes as part of an introductory course on Newtonian mechanics, with generally positive results: [T]hese tapes were particularly effective in placing Newtonian"
  },
  {
    "source": "The Mechanical Universe.txt",
    "chunk_id": "The Mechanical Universe.txt_9",
    "chunk": "mechanics in a historical perspective; dramatizing the historical overthrow of Aristotelian and medieval ideas; illustrating the diverse nature of scientists and the scientific endeavor; stimulating student interest and enthusiasm; and, through excellent animation, illustrating the time dimension of certain mechanics concepts. The companion text [...] was placed on library reserve for the course but was not extensively utilized by students. A follow-up study found that the videos could also be helpful explaining physics to professors in other fields. Negative reactions generally had less to do with the intrinsic perceived quality of the episodes than with the time the science-history material took away from content seen as \"critical exam-preparing instruction\". The investigator recalled: [S]ome students, thinking that the videotape material would not be covered on the tests, headed for the doors when the lights dimmed! To counter this tendency I started to use a few test questions based on historical or literary details discussed in the videotapes. Some students were outraged: \"What is this, a poetry class?\" Classroom use continued into the 1990s. A minority education program at the University of California, Berkeley employed Mechanical Universe episode segments (on LaserDisc) as part of group discussions. In a 1993 review of the"
  },
  {
    "source": "The Mechanical Universe.txt",
    "chunk_id": "The Mechanical Universe.txt_10",
    "chunk": "series, a science historian stated that he had used episodes in his classes for several years, naming \"Kepler's Three Laws\" and \"The Michelson–Morley Experiment\" as his personal favorites. The highlight of the Kepler film is a segment in which we are shown an exquisite graphical realization of the way in which Kepler actually figured out that the orbits of the planets are elliptical rather than circular. The sheer difficulty of the problem he faced and the elegance of the method he applied to solve it are abundantly clear. I cannot imagine a better way to present this magnificent discovery, which can easily appear so trivial. A 2005 column in The Physics Teacher suggested The Mechanical Universe as preparatory viewing for instructors attempting to teach physics for the first time. The Physics Teacher has also recommended the series \"as enrichment or a makeup assignment for high-ability students\". Writing for Wired magazine's web site, Rhett Allain cited the series as an example of videos that could replace some functions of traditional lectures. In 1987, \"The Lorentz Transformation\" (episode 42) was awarded the sixteenth annual Japan Prize for educational television. Other awards received by The Mechanical Universe include the 1986 Gold Award from"
  },
  {
    "source": "The Mechanical Universe.txt",
    "chunk_id": "The Mechanical Universe.txt_11",
    "chunk": "the Birmingham International Film Festival, two \"Cindy\" awards from the International Association of Audio Visual Communicators (1987 and 1988), a Gold Award (1985) and a Silver Award (1987) from the International Film and TV Festival of New York, Silver (1986) and Gold Apple (1987) awards from the National Educational Film and Video Festival, and a Gold Plaque (1985) from the Chicago International Film Festival. Goodstein received the 1999 Oersted Medal for his work in physics education, including The Mechanical Universe. For his contributions to the field of computer graphics, including his animations for Cosmos, The Mechanical Universe and Project Mathematics!, Blinn received a MacArthur fellowship in 1991, as well as the 1999 Steven A. Coons Award. Like many introductory physics texts, The Mechanical Universe cites the spectacular 1940 collapse of the Tacoma Narrows Bridge as an example of resonance, using footage of the disaster in the \"Resonance\" episode. However, as more-recent expositions have emphasized, the catastrophic oscillations that destroyed the bridge were not due to simple mechanical resonance, but to a more complicated interaction between the bridge and the winds passing through it—a phenomenon known as aeroelastic flutter. This phenomenon is a kind of \"self-sustaining vibration\" that lies beyond the"
  },
  {
    "source": "The Mechanical Universe.txt",
    "chunk_id": "The Mechanical Universe.txt_12",
    "chunk": "regime of applicability of the linear theory of the externally-driven simple harmonic oscillator. The opening sequence used for the first 26 episodes lists the show's title as The Mechanical Universe, whereas the latter 26 episodes are titled The Mechanical Universe ...and Beyond. The reason for the addition is explained by Goodstein in the closing lecture segment of the final episode: In the first scientific revolution, disputation over the interpretation of human or divine authority was replaced by observation, by measurement, by the testing of hypotheses, all of it with the powerful help of quantitative mathematical reasoning. And the result of all that was the mechanical universe, a universe that inexorably worked out its destiny according to precise, predictable, mechanical laws. Today, we no longer believe in that universe. If I know the precise position of some particle at some instant of time, I cannot have any idea of where it's going or how fast. And it doesn't make any difference at all if you say, \"All right, you don't know where it's going, but where is it really going?\" That is precisely the kind of question that is scientifically meaningless. That is the nature of the world we live in."
  },
  {
    "source": "The Mechanical Universe.txt",
    "chunk_id": "The Mechanical Universe.txt_13",
    "chunk": "That is the quantum mechanical universe. The series can be purchased from Caltech or streamed from online video sources, including Caltech's official YouTube channel. Caltech also posted on YouTube a series of short videos made by Blinn to demonstrate the show's computer animation at SIGGRAPH conferences."
  },
  {
    "source": "Theory of relativity.txt",
    "chunk_id": "Theory of relativity.txt_0",
    "chunk": "# Theory of relativity The theory of relativity usually encompasses two interrelated physics theories by Albert Einstein: special relativity and general relativity, proposed and published in 1905 and 1915, respectively. Special relativity applies to all physical phenomena in the absence of gravity. General relativity explains the law of gravitation and its relation to the forces of nature. It applies to the cosmological and astrophysical realm, including astronomy. The theory transformed theoretical physics and astronomy during the 20th century, superseding a 200-year-old theory of mechanics created primarily by Isaac Newton. It introduced concepts including 4-dimensional spacetime as a unified entity of space and time, relativity of simultaneity, kinematic and gravitational time dilation, and length contraction. In the field of physics, relativity improved the science of elementary particles and their fundamental interactions, along with ushering in the nuclear age. With relativity, cosmology and astrophysics predicted extraordinary astronomical phenomena such as neutron stars, black holes, and gravitational waves. Albert Einstein published the theory of special relativity in 1905, building on many theoretical results and empirical findings obtained by Albert A. Michelson, Hendrik Lorentz, Henri Poincaré and others. Max Planck, Hermann Minkowski and others did subsequent work. Einstein developed general relativity between 1907 and"
  },
  {
    "source": "Theory of relativity.txt",
    "chunk_id": "Theory of relativity.txt_1",
    "chunk": "1915, with contributions by many others after 1915. The final form of general relativity was published in 1916. The term \"theory of relativity\" was based on the expression \"relative theory\" (German: Relativtheorie) used in 1906 by Planck, who emphasized how the theory uses the principle of relativity. In the discussion section of the same paper, Alfred Bucherer used for the first time the expression \"theory of relativity\" (German: Relativitätstheorie). By the 1920s, the physics community understood and accepted special relativity. It rapidly became a significant and necessary tool for theorists and experimentalists in the new fields of atomic physics, nuclear physics, and quantum mechanics. By comparison, general relativity did not appear to be as useful, beyond making minor corrections to predictions of Newtonian gravitation theory. It seemed to offer little potential for experimental test, as most of its assertions were on an astronomical scale. Its mathematics seemed difficult and fully understandable only by a small number of people. Around 1960, general relativity became central to physics and astronomy. New mathematical techniques to apply to general relativity streamlined calculations and made its concepts more easily visualized. As astronomical phenomena were discovered, such as quasars (1963), the 3-kelvin microwave background radiation (1965),"
  },
  {
    "source": "Theory of relativity.txt",
    "chunk_id": "Theory of relativity.txt_2",
    "chunk": "pulsars (1967), and the first black hole candidates (1981), the theory explained their attributes, and measurement of them further confirmed the theory. Special relativity is a theory of the structure of spacetime. It was introduced in Einstein's 1905 paper \"On the Electrodynamics of Moving Bodies\" (for the contributions of many other physicists and mathematicians, see History of special relativity). Special relativity is based on two postulates which are contradictory in classical mechanics: The resultant theory copes with experiment better than classical mechanics. For instance, postulate 2 explains the results of the Michelson–Morley experiment. Moreover, the theory has many surprising and counterintuitive consequences. Some of these are: The defining feature of special relativity is the replacement of the Galilean transformations of classical mechanics by the Lorentz transformations. (See Maxwell's equations of electromagnetism.) General relativity is a theory of gravitation developed by Einstein in the years 1907–1915. The development of general relativity began with the equivalence principle, under which the states of accelerated motion and being at rest in a gravitational field (for example, when standing on the surface of the Earth) are physically identical. The upshot of this is that free fall is inertial motion: an object in free fall is"
  },
  {
    "source": "Theory of relativity.txt",
    "chunk_id": "Theory of relativity.txt_3",
    "chunk": "falling because that is how objects move when there is no force being exerted on them, instead of this being due to the force of gravity as is the case in classical mechanics. This is incompatible with classical mechanics and special relativity because in those theories inertially moving objects cannot accelerate with respect to each other, but objects in free fall do so. To resolve this difficulty Einstein first proposed that spacetime is curved. Einstein discussed his idea with mathematician Marcel Grossmann and they concluded that general relativity could be formulated in the context of Riemannian geometry which had been developed in the 1800s. In 1915, he devised the Einstein field equations which relate the curvature of spacetime with the mass, energy, and any momentum within it. Technically, general relativity is a theory of gravitation whose defining feature is its use of the Einstein field equations. The solutions of the field equations are metric tensors which define the topology of the spacetime and how objects move inertially. Einstein stated that the theory of relativity belongs to a class of \"principle-theories\". As such, it employs an analytic method, which means that the elements of this theory are not based on hypothesis"
  },
  {
    "source": "Theory of relativity.txt",
    "chunk_id": "Theory of relativity.txt_4",
    "chunk": "but on empirical discovery. By observing natural processes, we understand their general characteristics, devise mathematical models to describe what we observed, and by analytical means we deduce the necessary conditions that have to be satisfied. Measurement of separate events must satisfy these conditions and match the theory's conclusions. Relativity is a falsifiable theory: It makes predictions that can be tested by experiment. In the case of special relativity, these include the principle of relativity, the constancy of the speed of light, and time dilation. The predictions of special relativity have been confirmed in numerous tests since Einstein published his paper in 1905, but three experiments conducted between 1881 and 1938 were critical to its validation. These are the Michelson–Morley experiment, the Kennedy–Thorndike experiment, and the Ives–Stilwell experiment. Einstein derived the Lorentz transformations from first principles in 1905, but these three experiments allow the transformations to be induced from experimental evidence. Maxwell's equations—the foundation of classical electromagnetism—describe light as a wave that moves with a characteristic velocity. The modern view is that light needs no medium of transmission, but Maxwell and his contemporaries were convinced that light waves were propagated in a medium, analogous to sound propagating in air, and ripples"
  },
  {
    "source": "Theory of relativity.txt",
    "chunk_id": "Theory of relativity.txt_5",
    "chunk": "propagating on the surface of a pond. This hypothetical medium was called the luminiferous aether, at rest relative to the \"fixed stars\" and through which the Earth moves. Fresnel's partial ether dragging hypothesis ruled out the measurement of first-order (v/c) effects, and although observations of second-order effects (v/c) were possible in principle, Maxwell thought they were too small to be detected with then-current technology. The Michelson–Morley experiment was designed to detect second-order effects of the \"aether wind\"—the motion of the aether relative to the Earth. Michelson designed an instrument called the Michelson interferometer to accomplish this. The apparatus was sufficiently accurate to detect the expected effects, but he obtained a null result when the first experiment was conducted in 1881, and again in 1887. Although the failure to detect an aether wind was a disappointment, the results were accepted by the scientific community. In an attempt to salvage the aether paradigm, FitzGerald and Lorentz independently created an ad hoc hypothesis in which the length of material bodies changes according to their motion through the aether. This was the origin of FitzGerald–Lorentz contraction, and their hypothesis had no theoretical basis. The interpretation of the null result of the Michelson–Morley experiment is"
  },
  {
    "source": "Theory of relativity.txt",
    "chunk_id": "Theory of relativity.txt_6",
    "chunk": "that the round-trip travel time for light is isotropic (independent of direction), but the result alone is not enough to discount the theory of the aether or validate the predictions of special relativity. While the Michelson–Morley experiment showed that the velocity of light is isotropic, it said nothing about how the magnitude of the velocity changed (if at all) in different inertial frames. The Kennedy–Thorndike experiment was designed to do that, and was first performed in 1932 by Roy Kennedy and Edward Thorndike. They obtained a null result, and concluded that \"there is no effect ... unless the velocity of the solar system in space is no more than about half that of the earth in its orbit\". That possibility was thought to be too coincidental to provide an acceptable explanation, so from the null result of their experiment it was concluded that the round-trip time for light is the same in all inertial reference frames. The Ives–Stilwell experiment was carried out by Herbert Ives and G.R. Stilwell first in 1938 and with better accuracy in 1941. It was designed to test the transverse Doppler effect – the redshift of light from a moving source in a direction perpendicular to"
  },
  {
    "source": "Theory of relativity.txt",
    "chunk_id": "Theory of relativity.txt_7",
    "chunk": "its velocity—which had been predicted by Einstein in 1905. The strategy was to compare observed Doppler shifts with what was predicted by classical theory, and look for a Lorentz factor correction. Such a correction was observed, from which was concluded that the frequency of a moving atomic clock is altered according to special relativity. Those classic experiments have been repeated many times with increased precision. Other experiments include, for instance, relativistic energy and momentum increase at high velocities, experimental testing of time dilation, and modern searches for Lorentz violations. General relativity has also been confirmed many times, the classic experiments being the perihelion precession of Mercury's orbit, the deflection of light by the Sun, and the gravitational redshift of light. Other tests confirmed the equivalence principle and frame dragging. Far from being simply of theoretical interest, relativistic effects are important practical engineering concerns. Satellite-based measurement needs to take into account relativistic effects, as each satellite is in motion relative to an Earth-bound user, and is thus in a different frame of reference under the theory of relativity. Global positioning systems such as GPS, GLONASS, and Galileo, must account for all of the relativistic effects in order to work with precision,"
  },
  {
    "source": "Theory of relativity.txt",
    "chunk_id": "Theory of relativity.txt_8",
    "chunk": "such as the consequences of the Earth's gravitational field. This is also the case in the high-precision measurement of time. Instruments ranging from electron microscopes to particle accelerators would not work if relativistic considerations were omitted."
  },
  {
    "source": "Time-domain astronomy.txt",
    "chunk_id": "Time-domain astronomy.txt_0",
    "chunk": "# Time-domain astronomy Time-domain astronomy is the study of how astronomical objects change with time. Said to have begun with Galileo's Letters on Sunspots, the field has now naturally expanded to encompass variable objects beyond the Solar System. Temporal variation may originate from movement of the source, or changes in the object itself. Common targets include novae, supernovae, pulsating stars, flare stars, blazars and active galactic nuclei. Optical time domain surveys include OGLE, HAT-South, PanSTARRS, SkyMapper, ASAS, WASP, CRTS, GOTO, and the forthcoming LSST at the Vera C. Rubin Observatory. Time-domain astronomy studies transient astronomical events (\"transients\"), which include various types of variable stars, including periodic, quasi-periodic, high proper motion stars, and lifecycle events (supernovae, kilonovae) or other changes in behavior or type. Non-stellar transients include asteroids, planetary transits and comets. Transients characterize astronomical objects or phenomena whose duration of presentation may be from milliseconds to days, weeks, or even several years. This is in contrast to the timescale of the millions or billions of years during which the galaxies and their component stars in the universe have evolved. The term is used for violent deep-sky events, such as supernovae, novae, dwarf nova outbursts, gamma-ray bursts, and tidal disruption events,"
  },
  {
    "source": "Time-domain astronomy.txt",
    "chunk_id": "Time-domain astronomy.txt_1",
    "chunk": "as well as gravitational microlensing. Time-domain astronomy also involves long-term studies of variable stars and their changes on the timescale of minutes to decades. Variability studied can be intrinsic, including periodic or semi-regular pulsating stars, young stellar objects, stars with outbursts, asteroseismology studies; or extrinsic, which results from eclipses (in binary stars, planetary transits), stellar rotation (in pulsars, spotted stars), or gravitational microlensing events. Modern time-domain astronomy surveys often uses robotic telescopes, automatic classification of transient events, and rapid notification of interested people. Blink comparators have long been used to detect differences between two photographic plates, and image subtraction became more used when digital photography eased the normalization of pairs of images. Due to large fields of view required, the time-domain work involves storing and transferring a huge amount of data. This includes data mining techniques, classification, and the handling of heterogeneous data. The importance of time-domain astronomy was recognized in 2018 by German Astronomical Society by awarding a Karl Schwarzschild Medal to Andrzej Udalski for \"pioneering contribution to the growth of a new field of astrophysics research, time-domain astronomy, which studies the variability of brightness and other parameters of objects in the universe in different time scales.\" Also the"
  },
  {
    "source": "Time-domain astronomy.txt",
    "chunk_id": "Time-domain astronomy.txt_2",
    "chunk": "2017 Dan David Prize was awarded to the three leading researchers in the field of time-domain astronomy: Neil Gehrels (Swift Gamma-Ray Burst Mission), Shrinivas Kulkarni (Palomar Transient Factory), Andrzej Udalski (Optical Gravitational Lensing Experiment). Before the invention of telescopes, transient events that were visible to the naked eye, from within or near the Milky Way Galaxy, were very rare, and sometimes hundreds of years apart. However, such events were recorded in antiquity, such as the supernova in 1054 observed by Chinese, Japanese and Arab astronomers, and the event in 1572 known as \"Tycho's Supernova\" after Tycho Brahe, who studied it until it faded after two years. Even though telescopes made it possible to see more distant events, their small fields of view – typically less than 1 square degree – meant that the chances of looking in the right place at the right time were low. Schmidt cameras and other astrographs with wide field were invented in the 20th century, but mostly used to survey the unchanging heavens. Historically time domain astronomy has come to include appearance of comets and variable brightness of Cepheid-type variable stars. Old astronomical plates exposed from the 1880s through the early 1990s held by the"
  },
  {
    "source": "Time-domain astronomy.txt",
    "chunk_id": "Time-domain astronomy.txt_3",
    "chunk": "Harvard College Observatory are being digitized by the DASCH project. The interest in transients has intensified when large CCD detectors started to be available to the astronomical community. As telescopes with larger fields of view and larger detectors come into use in the 1990s, first massive and regular survey observations were initiated - pioneered by the gravitational microlensing surveys such as Optical Gravitational Lensing Experiment and the MACHO Project. These efforts, beside the discovery of the microlensing events itself, resulted in the orders of magnitude more variable stars known to mankind. Subsequent, dedicated sky surveys such as the Palomar Transient Factory, the spacecraft Gaia and the LSST, focused on expanding the coverage of the sky monitoring to fainter objects, more optical filters and better positional and proper motions measurement capabilities. In 2022, the Gravitational-wave Optical Transient Observer (GOTO) began looking for collisions between neutron stars. The ability of modern instruments to observe in wavelengths invisible to the human eye (radio waves, infrared, ultraviolet, X-ray) increases the amount of information that may be obtained when a transient is studied. In radio astronomy the LOFAR is looking for radio transients. Radio time domain studies have long included pulsars and scintillation. Projects to"
  },
  {
    "source": "Time-domain astronomy.txt",
    "chunk_id": "Time-domain astronomy.txt_4",
    "chunk": "look for transients in X-ray and gamma rays include Cherenkov Telescope Array, eROSITA, AGILE, Fermi, HAWC, INTEGRAL, MAXI, Swift Gamma-Ray Burst Mission and Space Variable Objects Monitor. Gamma ray bursts are a well known high energy electromagnetic transient. The proposed ULTRASAT satellite will observe a field of more than 200 square degrees continuously in an ultraviolet wavelength that is particularly important for detecting supernovae within minutes of their occurrence."
  },
  {
    "source": "Timeline of Solar System exploration.txt",
    "chunk_id": "Timeline of Solar System exploration.txt_0",
    "chunk": "# Timeline of Solar System exploration This is a timeline of Solar System exploration ordering events in the exploration of the Solar System by date of spacecraft launch. It includes: The dates listed are launch dates, but the achievements noted may have occurred some time later—in some cases, a considerable time later (for example, Voyager 2, launched 20 August 1977, did not reach Neptune until 1989)."
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_0",
    "chunk": "# Tongva The Tongva (/ˈtɒŋvə/ TONG-və) are an Indigenous people of California from the Los Angeles Basin and the Southern Channel Islands, an area covering approximately 4,000 square miles (10,000 km). In the precolonial era, the people lived in as many as 100 villages and primarily identified by their village rather than by a pan-tribal name. During colonization, the Spanish referred to these people as Gabrieleño and Fernandeño, names derived from the Spanish missions built on their land: Mission San Gabriel Arcángel and Mission San Fernando Rey de España. Tongva is the most widely circulated endonym among the people, used by Narcisa Higuera in 1905 to refer to inhabitants in the vicinity of Mission San Gabriel. Some people who identify as direct lineal descendants of the people advocate the use of their ancestral name Kizh as an endonym. The Tongva, along with neighboring groups such as the Chumash, played an important role in the cultural and economic dynamics of the region at the time of European encounter. They had developed an extensive trade network through te'aats (plank-built boats). Their food and material culture was based on an Indigenous worldview that positioned humans as one strand in a web of life"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_1",
    "chunk": "(as expressed in their creation stories). Over time, different communities came to speak distinct dialects of the Tongva language, part of the Takic subgroup of the Uto-Aztecan language family. There may have been five or more such languages (three on the southernmost Channel Islands and at least two on the mainland). European contact was first made in 1542 by Spanish explorer Juan Rodriguez Cabrillo, who was greeted at Santa Catalina by people in a canoe. The following day, Cabrillo and his men entered a large bay on the mainland, which they named Baya de los Fumos (\"Bay of Smokes\") because of the many smoke fires they saw there. The Indigenous people smoked their fish for preservation. This is commonly believed to be San Pedro Bay, near present-day San Pedro. The Gaspar de Portolá land expedition in 1769 resulted in the founding of Mission San Gabriel by Catholic missionary Junipero Serra in 1771. Under the mission system, the Spanish initiated an era of forced relocation and virtual enslavement of the peoples to secure their labor. In addition, the Native Americans were exposed to the Old World diseases endemic among the colonists. As they lacked any acquired immunity, the Native Americans suffered"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_2",
    "chunk": "epidemics with high mortality, leading to the rapid collapse of Tongva society and lifeways. They retaliated by way of resistance and rebellions, including an unsuccessful rebellion in 1785 by Nicolás José and female chief Toypurina. In 1821, Mexico gained its independence from Spain and secularized the missions. They sold the mission lands, known as ranchos, to elite ranchers and forced the Tongva to assimilate. Most became landless refugees during this time. In 1848, California was ceded to the United States following the Mexican-American War. The US government signed 18 treaties between 1851 and 1852 promising 8.5 million acres (3.4 million ha) of land for reservations. However, these treaties were never ratified by the Senate. The US had negotiated with people who did not represent the Tongva and had no authority to cede their land. During the following occupation by Americans, many of the Tongva and other Indigenous peoples were targeted with arrest. Unable to pay fines, they were used as convict laborers in a system of legalized slavery to expand the city of Los Angeles for Anglo-American settlers, who became the new majority in the area by 1880. In the early 20th century, an extinction myth was purported about the"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_3",
    "chunk": "Gabrieleño, who largely identified publicly as Mexican-American by this time. However, a close-knit community of the people remained in contact with one another between Tejon Pass and San Gabriel township into the 20th century. Since 2006, four organizations have claimed to represent the people: Two of the groups, the hyphen and the slash group, were founded after a hostile split over the question of building an Indian casino. In 1994, the state of California recognized the Gabrielino \"as the aboriginal tribe of the Los Angeles Basin.\" No organized group representing the Tongva has attained recognition as a tribe by the federal government. The lack of federal recognition has prevented self-identified Tongva descendants from having control over Tongva ancestral remains, artifacts, and has left them without a land base in the Tongva traditional homeland. In 2008, more than 1,700 people identified as Tongva or claimed partial ancestry. In 2013, it was reported that the four Tongva groups that have applied for federal recognition had more than 3,900 members in total. The Tongva Taraxat Paxaavxa Conservancy was established to campaign for the rematriation of Tongva homelands. In 2022, a 1-acre site was returned to the conservancy in Altadena, which marked the first"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_4",
    "chunk": "time the Tongva had land in Los Angeles County in 200 years. Tongva territories border those of numerous other tribes in the region. The historical Tongva lands made up what is now called \"the coastal region of Los Angeles County, the northwest portion of Orange County and off-lying islands.\" In 1962 Curator Bernice Johnson, of Southwest Museum, asserted that the northern boundary was somewhere between Topanga and Malibu (perhaps the vicinity of Malibu Creek) and the southern boundary was Orange County's Aliso Creek. The word Tongva was coined by C. Hart Merriam in 1905 from numerous informants. These included Mrs. James Rosemyre (née Narcisa Higuera) (Gabrileño), who lived around Fort Tejon, near Bakersfield. Merriam's orthography makes it clear that the endonym would be pronounced /ˈtɒŋveɪ/, TONG-vay. Some descendants prefer the endonym Kizh, which they argue is an earlier and more historically accurate name that was well documented by records of the Smithsonian Institution, Congress, the Catholic Church, the San Gabriel Mission, and other historical scholars. The Spanish referred to the Indigenous peoples surrounding Mission San Gabriel as the Gabrieleño. This was not their autonym, or their name for themselves. Because of historical uses, the term is part of every official"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_5",
    "chunk": "tribe's name in this area, spelled either as \"Gabrieleño\" or \"Gabrielino.\" Because tribal groups have disagreed about appropriate use of the term Tongva, they have adopted Gabrieleño as a mediating term. For example, when Debra Martin, a city council member from Pomona, led a project in 2017 to dedicate wooden statues in local Ganesha Park to the Indigenous people of the area, they disagreed over which name, Tongva or Kizh, should be used on the dedication plaque. Tribal officials tentatively agreed to use the term Gabrieleño. The Act of September 21, 1968, introduced this concept of the affiliation of an applicant's ancestors in order to exclude certain individuals from receiving a share of the award to the “Indians of California” who chose to receive a share of any awards to certain tribes in California that had splintered off from the generic group. The members or ancestors of the petitioning group were not affected by the exclusion in the Act. Individuals with lineal or collateral descent from an Indian tribe who resided in California in 1852, would, if not excluded by the provisions of the Act of 1968, remain on the list of the “Indians of California.” To comply with the"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_6",
    "chunk": "Act, the Secretary of Interior would have to collect information about the group affiliation of an applicant's Indian ancestors. That information would be used to identify applicants who could share in another award. The group affiliation of an applicant's ancestors was thus a basis for exclusion from, but not a requirement for inclusion on, the judgment roll. The act of 1968 stated that the Secretary of the Interior would distribute an equal share of the award to the individuals on the judgment roll “regardless of group affiliation.” Many lines of evidence suggest that the Tongva are descended from Uto-Aztecan-speaking peoples who originated in what is now Nevada, and moved southwest into coastal Southern California 3,500 years ago. According to a model proposed by archaeologist Mark Q. Sutton, these migrants either absorbed or pushed out the earlier Hokan-speaking inhabitants. By 500 AD, one source estimates the Tongva may have come to occupy all the lands now associated with them, although this is unclear and contested among scholars. In 1811, the priests of Mission San Gabriel recorded at least four languages; Kokomcar, Guiguitamcar, Corbonamga, and Sibanga. During the same time, three languages were recorded in Mission San Fernando. Prior to Russian and"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_7",
    "chunk": "Spanish colonization in what is now referred to California, the Tongva were primarily identified by their associated villages (Topanga, Cahuenga, Tujunga, Cucamonga, etc.) For example, individuals from Yaanga were known as Yaangavit among the people (in mission records, they were recorded as Yabit). The Tongva lived in as many as one hundred villages. One or two clans would usually constitute a village, which was the center of Tongva life. The Tongva spoke a language of the Uto-Aztecan family (the remote ancestors of the Tongva probably coalesced as a people in the Sonoran Desert, between perhaps 3,000 and 5,000 years ago). The diversity within the Takic group is \"moderately deep\"; rough estimates by comparative linguists place the breakup of common Takic into the Luiseño-Juaneño on one hand, and the Tongva-Serrano on the other, at about 2,000 years ago. (This is comparable to the differentiation of the Romance languages of Europe). The division of the Tongva/Serrano group into the separate Tongva and Serrano peoples is more recent, and may have been influenced by Spanish missionary activity. The majority of Tongva territory was located in what has been referred to as the Sonoran life zone, with rich ecological resources of acorn, pine nut,"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_8",
    "chunk": "small game, and deer. On the coast, shellfish, sea mammals, and fish were available. Prior to Christianization, the prevailing Tongva worldview was that humans were not the apex of creation, but were rather one strand in the web of life. Humans, along with plants, animals, and the land were in a reciprocal relationship of mutual respect and care, which is evident in their creation stories. The Tongva understand time as nonlinear and there is constant communication with ancestors. On October 7, 1542, an exploratory expedition led by Spanish explorer Juan Cabrillo reached Santa Catalina in the Channel Islands, where his ships were greeted by Tongva in a canoe. The following day, Cabrillo and his men, the first Europeans known to have interacted with the Gabrieleño people, entered a large bay on the mainland, which they named \"Baya de los Fumos\" (\"Bay of Smokes\") on account of the many smoke fires they saw there. This is commonly believed to be San Pedro Bay, near present-day San Pedro. The Gaspar de Portola expedition in 1769 was the first contact by land to reach Tongva territory, marking the beginning of Spanish colonization. Franciscan padre Junipero Serra accompanied Portola. Within two years of the"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_9",
    "chunk": "expedition, Serra had founded four missions, including Mission San Gabriel, founded in 1771 and rebuilt in 1774, and Mission San Fernando, founded in 1797. The people enslaved at San Gabriel were referred to as Gabrieleños, while those enslaved at San Fernando were referred to as Fernandeños. Although their language idioms were distinguishable, they did not diverge greatly, and it is possible there were as many as half a dozen dialects rather than the two which the existence of the missions has lent the appearance of being standard. The demarcation of the Fernandeño and the Gabrieleño territories is mostly conjectural and there is no known point in which the two groups differed markedly in customs. The wider Gabrieleño group occupied what is now Los Angeles County south of the Sierra Madre and half of Orange County, as well as the islands of Santa Catalina and San Clemente. The Spanish oversaw the construction of Mission San Gabriel in 1771. The Spanish colonizers used slave labor from local villages to construct the Missions. Following the destruction of the original mission, probably due to El Niño flooding, the Spanish ordered the mission relocated five miles north in 1774 and began referring to the Tongva"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_10",
    "chunk": "as \"Gabrieleno.\" At the Gabrieleño settlement of Yaanga along the Los Angeles River, missionaries and Indian neophytes, or baptized converts, built the first town of Los Angeles in 1781. It was called El Pueblo de Nuestra Señora la Reina de los Ángeles de Porciúncula (The Village of Our Lady, the Queen of the Angels of Porziuncola). In 1784, a sister mission, the Nuestra Señora Reina de los Angeles Asistencia, was founded at Yaanga as well. Entire villages were baptized and indoctrinated into the mission system with devastating results. For example, from 1788 to 1815, natives of the village of Guaspet were baptized at San Gabriel. Proximity to the missions created mass tension for Native Californians, which initiated \"forced transformations in all aspects of daily life, including manners of speaking, eating, working, and connecting with the supernatural.\" As stated by scholars John Dietler, Heather Gibson, and Benjamin Vargas, \"Catholic enterprises of proselytization, acceptance into a mission as a convert, in theory, required abandoning most, if not all, traditional lifeways.\" Various strategies of control were implemented to retain control, such as the use of violence, segregation by age and gender, and using new converts as instruments of control over others. For example,"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_11",
    "chunk": "Mission San Gabriel's Father Zalvidea punished suspected shamans \"with frequent flogging and by chaining traditional religious practitioners together in pairs and sentencing them to hard labor in the sawmill.\" A missionary during this period reported that three out of four children died at Mission San Gabriel before reaching the age of 2. Nearly 6,000 Tongva lie buried in the grounds of the San Gabriel Mission. Carey McWilliams characterized it as follows: \"the Franciscan padres eliminated Indians with the effectiveness of Nazis operating concentration camps....\" There is much evidence of Tongva's resistance to the mission system. Many individuals returned to their village at the time of death. Many converts retained their traditional practices in both domestic and spiritual contexts, despite the attempts by the padres and missionaries to control them. Traditional foods were incorporated into the mission diet and lithic and shell bead production and use persisted. More overt strategies of resistance such as refusal to enter the system, work slowdowns, abortion and infanticide of children resulting from rape, and fugiti were also prevalent. Five major uprisings were recorded at Mission San Gabriel alone. Two late-eighteenth-century rebellions against the mission system were led by Nicolás José, who was an early convert"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_12",
    "chunk": "who had two social identities: \"publicly participating in Catholic sacraments at the mission but privately committed to traditional dances, celebrations, and rituals.\" He participated in a failed attempt to kill the mission's priests in 1779 and organized eight foothill villages in a revolt in October 1785 with Toypurina, who further organized the villages, which \"demonstrated a previously undocumented level of regional political unification both within and well beyond the mission.\" However, divided loyalties among the natives contributed to the failure of the 1785 attempt as well as mission soldiers being alerted of the attempt by converts or neophytes. Toypurina, José and two other leaders of the rebellion, Chief Tomasajaquichi of Juvit village and a man named Alijivit, from the nearby village of Jajamovit, were put on trial for the 1785 rebellion. At his trial, José stated that he participated because the ban at the mission on dances and ceremonies instituted by the missionaries, and enforced by the governor of California in 1782, was intolerable as they prevented their mourning ceremonies. When questioned about the attack, Toypurina is famously quoted as saying that she participated in the instigation because “[she hated] the padres and all of you, for living here on"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_13",
    "chunk": "my native soil, for trespassing upon the land of my forefathers and despoiling our tribal domains. … I came [to the mission] to inspire the dirty cowards to fight, and not to quail at the sight of Spanish sticks that spit fire and death, nor [to] retch at the evil smell of gunsmoke – and be done with you white invaders!’ Scholars have debated the accuracy of this quote, from Thomas Workman Temple II's article “Toypurina the Witch and the Indian Uprising at San Gabriel” suggesting it may differ significantly from the testimony recorded by Spanish authorities at the time. According to the soldier who recorded her words, she stated simply that she ‘‘was angry with the Padres and the others of the Mission, because they had come to live and establish themselves in her land.’’ In June 1788, nearly three years later, their sentences arrived from Mexico City: Nicolás José was banned from San Gabriel and sentenced to six years of hard labor in irons at the most distant penitentiary in the region. Toypurina was banished from Mission San Gabriel and sent to the most distant Spanish mission. Resistance to Spanish rule demonstrated how the Spanish Crown's claims to"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_14",
    "chunk": "California were both insecure and contested. By the 1800s, San Gabriel was the richest in the entire colonial mission system, supplying cattle, sheep, goats, hogs, horses, mules, and other supplies for settlers and settlements throughout Alta California. The mission system has been described by some historians as a form of coerced labor that deeply disrupted Indigenous lifeways and autonomy. Latter-day ethnologist Hugo Reid reported, “Indian children were taken from their parents to be raised behind bars at the mission. They were allowed outside the locked dormitories only to attend to church business and their assigned chores. When they were old enough, boys and girls were put to work in the vast vineyards and orchards owned by the missions. Soldiers watched, ready to hunt down any who tried to escape.” Writing in 1852, Reid said he knew of Tongva who “had an ear lopped off or were branded on the lip for trying to get away.” In 1810, the \"Gabrieleño\" labor population at the mission was recorded to be 1,201. It jumped to 1,636 in 1820 and then declined to 1,320 in 1830. Resistance to this system of forced labor continued into the early 19th century. In 1817, the San Gabriel"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_15",
    "chunk": "Mission recorded that there were \"473 Indian fugitives.\" In 1828, a German immigrant purchased the land on which the village of Yang-Na stood and evicted the entire community with the help of Mexican officials. The mission period ended in 1834 with secularization under Mexican rule. Some \"Gabrieleño\" were absorbed into Mexican society as a result of secularization, which emancipated the neophytes. Tongva and other California Natives largely became workers while former Spanish elites were granted huge land grants. Land was systemically denied to California Natives by Californio land-owning men. In the Los Angeles basin area, only 20 former neophytes from San Gabriel Mission received any land from secularization. What they received were relatively small plots of land. A \"Gabrieleño\" by the name of Prospero Elias Dominguez was granted a 22-acre plot near the mission while Mexican authorities granted the remainder of the mission land, approximately 1.5 million acres, to a few colonist families. In 1846, it was noted by researcher Kelly Lytle Hernández that 140 Gabrieleños signed a petition demanding access to mission lands and that Californio authorities rejected their petition. Emancipated from enslavement in the missions yet barred from their own land, most Tongva became landless refugees during this"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_16",
    "chunk": "period. Entire villages fled inland to escape the invaders and continued devastation. Others moved to Los Angeles, a city which saw an increase in the Native population from 200 in 1820 to 553 in 1836 (out of a total population of 1,088). As stated by scholar Ralph Armbruster-Sandoval, \"while they should have been owners, the Tongva became workers, performing strenuous, back-breaking labor just as they had done ever since settler colonialism emerged in Southern California.\" As described by researcher Heather Valdez Singleton, Los Angeles was heavily dependent on Native labor and \"grew slowly on the back of the Gabrieleño laborers.\" Some of the people became vaqueros on the ranches, highly skilled horsemen or cowboys, herding and caring for the cattle. There was little land available to the Tongva to use for food outside of the ranches. Some crops such as corn and beans were planted on ranchos to sustain the workers. Several Gabrieleño families stayed within the San Gabriel township, which became \"the cultural and geographic center of the Gabrieleño community.\" Yaanga also diversified and increased in size, with peoples of various Native backgrounds coming to live together shortly following secularization. However, the government had instituted a system dependent on"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_17",
    "chunk": "Native labor and servitude and increasingly eliminated any alternatives within the Los Angeles area. As explained by Kelly Lytle Hernández, \"there was no place for Natives living but not working in Mexican Los Angeles. In turn, the ayuntaminto (city council) passed new laws to compel Natives to work or be arrested.\" In January 1836, the council directed Californios to sweep across Los Angeles to arrest \"all drunken Indians.\" As recorded by Hernández, \"Tongva men and women, along with an increasingly diverse set of their Native neighbors, filled the jail and convict labor crews in Mexican Los Angeles.\" By 1844, most Natives in Los Angeles worked as servants in a perpetual system of servitude, tending to the land and serving settlers, invaders, and colonizers. The ayuntamiunto forced the Native settlement of Yaanga to move farther away from town. By the mid-1840s, the settlement was forcibly moved eastward across the Los Angeles River, placing a divide between Mexican Los Angeles and the nearest Native community. However, \"Native men, women, and children continued to live (not just work) in the city. On Saturday Nights, they even held parties, danced, and gambled at the removed Yaanga village and also at the plaza at the"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_18",
    "chunk": "center of town.\" In response, the Californios continued to attempt to control Native lives, issuing Alta California governor Pio Pico a petition in 1846 stating: \"We ask that the Indians be placed under strict police surveillance or the persons for whom the Indians work give [the Indians] quarter at the employer's rancho.\" In 1847, a law was passed that prohibited Gabrielenos from entering the city without proof of employment. A part of the proclamation read: Indians who have no masters but are self-sustaining, shall be lodged outside of the City limits in localities widely separated... All vagrant Indians of either sex who have not tried to secure a situation within four days and are found unemployed, shall be put to work on public works or sent to the house of correction. In 1848, Los Angeles formally became a town in the United States following the Mexican-American War. Landless and unrecognized, the people faced systemic discrimination, exploitation through convict labor, and loss of autonomy under American occupation. Some of the people were displaced to small Mexican and Native communities in the Eagle Rock and Highland Park districts of Los Angeles as well as Pauma, Pala, Temecula, Pechanga, and San Jacinto. The"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_19",
    "chunk": "imprisonment of Natives in Los Angeles was a symbol of establishing the new \"rule of law.\" The city's vigilante community would routinely \"invade\" the jail and hang the accused in the streets. Once Congress granted statehood to California in 1850, many of the first laws passed targeted Natives for arrest, imprisonment, and convict labor. The 1850 Act for the Government and Protection of Indians \"targeted Native peoples for easy arrest by stipulating that they could be arrested on vagrancy charges based 'on the complaint of any reasonable citizen'\" and Gabrieleños faced the brunt of this policy. Section 14 of the act stated: When an Indian is convicted of any offence before a Justice of the Peace punishable by fine, any white person may, by consent of the Justice, give bond for said Indian, conditioned for the payment of said fine and costs, and in such case the Indian shall be compelled to work for the person so bailing, until he has discharged or cancelled the fine assessed against him. Native men were disproportionately criminalized and swept into this legalized system of indentured servitude. As was recorded by Anglo-American settlers, \"'White men, whom the Marshal is too discreet to arrest' ..."
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_20",
    "chunk": "spilled out of the town's many saloons, streets, and brothels, but the aggressive and targeted enforcement of state and local vagrancy and drunk codes filled the Los Angeles County Jail with Natives, most of whom were men.\" Most spent their days working on the county chain gang, which was largely involved with keeping the city streets clean in the 1850s and 1860s but increasingly included road construction projects as well. Although federal officials reported that there were an estimated 16,930 California Indians and 1,050 at Mission San Gabriel, \"the federal agents ignored them and those living in Los Angeles\" because they were viewed as \"friendly to the whites,\" as revealed in the personal diaries of Commissioner George W. Barbour. In 1852, superintendent of Indian Affairs Edward Fitzgerald Beale echoed this sentiment, reporting that \"because these Indians were Christians, with many holding ranch jobs and having interacted with whites,\" that \"they are not much to be dreaded.\" Although a California Senate Bill of 2008 asserted that the US government signed treaties with the Gabrieleño, promising 8.5 million acres (3,400,000 ha) of land for reservations, and that these treaties were never ratified, a paper published in 1972 by Robert Heizer of the"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_21",
    "chunk": "University of California at Berkeley, shows that the eighteen treaties made between April 29, 1851, and August 22, 1852, were negotiated with persons who did not represent the Tongva people and that none of these persons had authority to cede lands that belonged to the people. An 1852 editorial in the Los Angeles Star revealed the public's anger towards any possibility of the Gabrieleño receiving recognition and exercising sovereignty: To place upon our most fertile soil the most degraded race of aborigines upon the North American Continent, to invest them with the rights of sovereignty, and to teach them that they are to be treated as powerful and independent nations, is planting the seeds of future disaster and ruin... We hope that the general government will let us alone – that it will neither undertake to feed, settle or remove the Indians amongst whom we in the South reside, and that they leave everything just as it now exists, except affording us the protection which two or three cavalry companies would give. In 1852, Hugo Reid wrote a series of letters for the Los Angeles Star from the center of the Gabrieleño community in San Gabriel township, describing Gabrieleño life"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_22",
    "chunk": "and culture. Reid himself was married to a Gabrieleño woman by the name of Bartolomea Cumicrabit, who he renamed \"Victoria.\" Reid wrote the following: \"Their chiefs still exist. In San Gabriel remain only four, and those young... They have no jurisdiction more than to appoint times for holding of Feasts and regulating affairs connected with the church [traditional structure made of brush].\" There is some speculation that Reid was campaigning for the position of Indian agent in Southern California, but died before he could be appointed. Instead, in 1852, Benjamin D. Wilson was appointed, who maintained the status quo. The letters of Hugo Reid revealed the names of 28 Gabrielino villages. In 1855, the Gabrieleño were reported by the superintendent of Indian affairs Thomas J. Henley to be in \"a miserable and degraded condition.\" However, Henley admitted that moving them to a reservation, potentially at Sebastian Reserve in Tejon Pass, would be opposed by the citizens because \"in the vineyards, especially during the grape season, their labor is made useful and is obtained at a cheap rate.\" A few Gabrieleño were in fact at Sebastian Reserve and maintained contact with the people living in San Gabriel during this time. In"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_23",
    "chunk": "1859, amidst increasing criminalization and absorption into the city's burgeoning convict labor system, the county grand jury declared \"stringent vagrant laws should be enacted and enforced compelling such persons ['Indians'] to obtain an honest livelihood or seek their old homes in the mountains.\" This declaration ignored Reid's research, which stated that most Tongva villages, including Yaanga, \"were located in the basin, along its rivers and on its shoreline, stretching from the deserts and to the sea.\" Only a few villages led by tomyaars (chiefs) were \"in the mountains, where Chengiichngech's avengers, serpents, and bears lived,\" as described by historian Kelly Lytle Hernández. However, \"the grand jury dismissed the depths of Indigenous claims to life, land, and sovereignty in the region and, instead, chose to frame Indigenous peoples as drunks and vagrants loitering in Los Angeles... disavowing a long history of Indigenous belonging in the basin.\" While in 1848, Los Angeles had been a small town largely of Mexicans and Natives, by 1880 it was home to an Anglo-American majority following waves of white migration in the 1870s from the completion of the transcontinental railroad. As stated by research Heather Valdez Singleton, newcomers \"took advantage of the fact that many Gabrieleño"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_24",
    "chunk": "families, who had cultivated and lived on the same land for generations, did not hold legal title to the land, and used the law to evict Indian families.\" The Gabrieleño became vocal about this and notified former Indian agent J. Q. Stanley, who referred to them as \"half-civilized\" yet lobbied to protect the Gabrieleño \"against the lawless whites living amongst them,\" arguing that they would become \"vagabonds\" otherwise. However, active Indian agent Augustus P. Greene's recommendation took precedent, arguing that \"Mission Indians in southern California were slowing the settlement of this portion of the country for non-Indians and suggested that the Indians be completely assimilated,\" as summarized by Singleton. In 1882, Helen Hunt Jackson was sent by the federal government to document the condition of the Mission Indians in southern California. She reported that there were a considerable number of people \"in the colonies in the San Gabriel Valley, where they live like gypsies in brush huts, here today, gone tomorrow, eking out a miserable existence by days' work.\" However, even though Jackson's report would become the impetus for the Mission Indian Relief Act of 1891, the Gabrieleño were \"overlooked by the commission charged with setting aside lands for Mission"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_25",
    "chunk": "Indians.\" It is speculated that this may have been attributed to what was perceived as their compliance with the government, which caused them to be neglected, as noted earlier by Indian agent J. Q. Stanley. By the early twentieth century, Gabrieleño identity had suffered greatly under American occupation. Most Gabrieleño publicly identified as Mexican, learned Spanish, and adopted Catholicism while keeping their identity a secret. In schools, students were punished for mentioning that they were \"Indian\" and many of the people assimilated into Mexican-American or Chicano culture. Further attempts to establish a reservation for the Gabrieleño in 1907 failed. Soon it began to be perpetuated in the local press that the Gabrieleño were extinct. In February 1921, the Los Angeles Times declared that the death of Jose de los Santos Juncos, an Indigenous man who lived at Mission San Gabriel and was 106 years old at his time of passing, \"marked the passing of a vanished race.\" In 1925, Alfred Kroeber declared that the Gabrieleño culture was extinct, stating \"they have melted away so completely that we know more of the finer facts of the culture of ruder tribes.\" Scholars have noted that this extinction myth has proven to be"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_26",
    "chunk": "\"remarkably resilient,\" yet is untrue. Despite being declared extinct, Gabrieleño children were still being assimilated by federal agents who encouraged enrollment at Sherman Indian School in Riverside, California. Between 1890 and 1920, at least 50 Gabrieleño children were recorded at the school. Between 1910 and 1920, the establishment of the Mission Indian Federation, of which the Gabrieleño joined, led to the 1928 California Indians Jurisdictional Act, which created official enrollment records for those who could prove ancestry from a California Indian living in the state in 1852. Over 150 people self-identified as Gabrieleño on this roll. A Gabrieleño woman at Tejon Reservation provided the names and addresses of several Gabrieleño living in San Gabriel, showing that contact between the group at Tejon Reservation and the group at San Gabriel township, which are more than 70 miles apart, was being maintained into the 1920s and 1930s. In 1971, Bernice Johnston, former curator of the Southwest Museum and author of California’s Gabrieleno Indians (1962), spoke to the Los Angeles Times: “After spending much of her life trying to trace the Indians, she believes she almost came in contact with some Gabrielenos a few years ago…She relates that on a Sunday, while giving"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_27",
    "chunk": "a tour of the museum, ‘I saw these shy, dark people looking around. They were asking questions about the Gabrieleno Indians. I asked why they wanted to know, and nearly fell over when they told me they were Gabrielenos and wanted to know something about themselves. I was busy with the tour, we were crowded. I rushed back to them as soon as I could but they were gone. I didn’t even get their names.” Historians and contemporary Tongva advocates have argued that Anglo-American institutions, including schools and museums, have historically challenged the preservation of tribal identity throughout the 20th and 21st centuries. Contemporary members have cited being denied the legitimacy of their identity. Tribal identity is also hindered by a lack of federal recognition and having no land base, which has meant that the tribe has access to almost none of their traditional homelands. The Tongva have also struggled to protect their sacred sites, ancestral remains, and artefacts from destruction in the 21st century. In 2001, a 9,000-year-old Bolsa Chica village site was heavily damaged. The company that performed the initial archaeological survey was fined $600,000 for its poor assessment that clearly favored the developer. Burials near the site"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_28",
    "chunk": "of Genga were unearthed and moved, despite opposition from the tribes, in favor of commercial development. In 2019, CSU, Long Beach dumped trash and dirt on top of Puvunga in its construction of new student housing, which reawakened a decades-long dispute between the university and the tribe over the treatment of the sacred site. In 2022, it was announced that part of the village site of Genga may be transformed into a green space. Leaders of the project have claimed that \"tribal descendants of the area’s earliest residents will also have a voice\" in how the park is developed. The Tongva Taraxat Paxaavxa Conservancy has been established as part of the Land Back movement and for the rematriation of Tongva homelands. The kuuyam nahwá’a (\"guest exchange\") has been developed by the conservancy as a way for people living in the homelands of the Tongva to pay a form of contribution for living on the land. In October 2022, a 1-acre site was returned to the conservancy by a private resident in Altadena, which marked the first time the Tongva had land in Los Angeles County in 200 years. The Tongva lived in the main part of the most fertile lowland"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_29",
    "chunk": "of southern California, including a stretch of sheltered coast with a pleasant climate and abundant food resources, as well as Santa Catalina, San Clemente, and San Nicolas Islands. The Tongva were a prominent cultural group south of the Tehachapi and among the Uto-Aztecan speakers in California, influencing other Indigenous groups through trade and interaction. Many of the cultural developments of the surrounding southern peoples had their origin with the Gabrieleño. The Tongva territory was the center of a flourishing trade network that extended from the Channel Islands in the west to the Colorado River in the east, allowing the people to maintain trade relations with the Cahuilla, Serrano, Luiseño, Chumash, and Mohave. Like all Indigenous peoples, they utilized and existed in an interconnected relationship with the flora and fauna of their familial territory. Villages were located throughout four major ecological zones, as noted by biologist Matthew Teutimez: interior mountains and foothills, grassland/oak woodland, sheltered coastal canyons, and the exposed coast. Therefore, resources such as plants, animals, and earth minerals were diverse and used for various purposes, including for food and materials. Prominent flora included oak (Quercus agrifolia) and willow (Salix spp.) trees, chia (Salvia columbariae), cattail (Typha spp.), datura or"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_30",
    "chunk": "jimsonweed (Datura innoxia), white sage (Salvia apiana), Juncus spp., Mexican Elderberry (Sambucus), wild tobacco (Nicotiana spp.), and yucca (Hesperoyucca whipplei). Prominent fauna included mule deer, pronghorn, black bear, grizzly bear, black-tailed jackrabbit, cottontail, bald eagle, red-tailed hawk, dolphin, and gray whale. The Tongva had a concentrated population along the coast. They fished and hunted in the estuary of the Los Angeles River, and like the Chumash, their neighbors to the north and west along the Pacific coast, the Gabrieleño built seaworthy plank canoes, called te'aat, from driftwood. To build them, they used planks of driftwood pine that were sewn together with vegetable fiber cord, edge to edge, and then glued with the tar that was available either from the La Brea Tar Pits, or as asphalt that had washed up on shore from offshore oil seeps. The finished vessel was caulked with plant fibers and tar, stained with red ochre, and sealed with pine pitch. The te'aat, as noted by the Sebastián Vizcaíno expedition, could hold up to 20 people as well as their gear and trade goods. These canoes allowed the development of trade between the mainland villages and the offshore islands, and were important to the region's economy"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_31",
    "chunk": "and social organization, with trade in food and manufactured goods being carried on between the people on the mainland coast and people in the interior as well. The Gabrieleño regularly paddled their canoes to Catalina Island, where they gathered abalone, which they pried off the rocks with implements made of fragments of whale ribs or other strong bones. In the Tongva economic system, food resources were managed by the village chief, who was given a portion of the yield of each day's hunting, fishing, or gathering to add to the communal food reserves. Individual families stored some food to be used in times of scarcity. Villages were located in places with accessible drinking water, protection from the elements, and productive areas where different ecological niches on the land intersected. Situating their villages at these resource islands enabled the Tongva to gather the plant products of two or more zones in close proximity. Households consisted of a main house (kiiy) and temporary camp shelters used during food-gathering excursions. In the summer, families who lived near grasslands collected roots, seeds, flowers, fruit, and leafy greens, and in the winter families who lived near chaparral shrubland collected nuts and acorns, yucca, and hunted"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_32",
    "chunk": "deer. The group used “wooden tongs” to collect prickly pear fruits. Some prairie communities moved to the coast in the winter to fish, hunt whales and elephant seals, and harvest shellfish. Those villages located on the coast during the summer went on food collecting trips inland during the winter rainy season to gather roots, tubers, corms, and bulbs of plants including cattails, lilies, and wild onions. The Tongva did not practice horticulture or agriculture, as their well-developed hunter-gatherer and trade economy provided adequate food resources. The bread was made from the yellow pollen of cattail heads, and the underground rhizomes were dried and ground into a starchy meal. The young shoots were eaten raw. The seeds of chia, a herbaceous plant of the sage family, were gathered in large quantities when they were ripe. The flower heads were beaten with a paddle over a tightly woven basket to collect the seeds. These were dried or roasted and ground into a flour called \"pinole,\" which was often mixed with the flour of other ground seeds or grains. Water was added to make a cooling drink; mixing with less water yielded a kind of porridge that could be baked into cakes. Acorn"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_33",
    "chunk": "mush was a staple food as it was of all the Indigenous peoples who were forcibly relocated to missions in Southern California. Acorns were gathered in October; this was a communal effort with the men climbing the trees and shaking them while the women and children collected the nuts. The acorns were stored in large wicker granaries supported by wooden stakes well above the ground. Preparing them for food took about a week. Acorns were placed, one at a time, on end in the slight hollow of a rock and their shells broken by a light blow from a small hammerstone; then the membrane, or skin, covering the acorn meat was removed. Following this process the acorn meats were dried for days, after which the kernels were pounded into a meal with a pestle. This was done in a stone mortar or in a mortar hole in a boulder. Large bedrock outcroppings near oak stands often display evidence of the community mills where the women labored. The pounded acorn meal was put into baskets and the bitter tannic acid it contained was leached out to make the meal more palatable and digestible. The prepared meal was cooked by boiling in"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_34",
    "chunk": "water in a watertight grass-woven basket or in a soapstone bowl into which heated stones were dropped. Soapstone casseroles were used directly over the fire. Various foods of meat, seeds, or roots were cooked by the same method. The mush thus prepared was eaten cold or nearly so, as was all their food. Another favored Tongva food was the seed kernel of a species of plum (prunus ilicifolia (common name: holly-leaf cherry) they called islay, which was ground into meal and made into gruel. Men performed most of the heavy, short-duration labor; they hunted, fished, helped with some food-gathering, and carried on trade with other cultural groups. Large game animals were hunted with bows and arrows, and small game was taken with deadfall traps, snares, and bows made of buckeye wood. John P. Harrington recorded that rattlesnake venom was used as an arrow poison. Burrowing animals were driven from their burrows with smoke and clubbed; communal rabbit drives were made during the seasonal controlled burning of chaparral on the prairie, the rabbits being killed with nets, bow and arrows, and throwing sticks. Harpoons, spear-throwers, and clubs were used to hunt marine mammals and te'aat used to access them. Fishing was"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_35",
    "chunk": "done from shorelines or along rivers, streams, and creeks with hook and line, nets, basketry traps, spears, bow and arrows, and poisons made from plants. Reciprocity and sharing of resources were important values in Tongva culture. Hugo Reid reported that the hoarding of food supplies was so stigmatized by the Tongva moral code that hunters would give away large portions of coveted foods such as fresh meat, and under some circumstances, were prohibited from eating their own kill or fishermen from eating their own catch. Women collected and prepared plant and some animal food resources and made baskets, pots, and clothing. In their old age, they and the old men cared for the young and taught them Tongva lifeways. Tongva material culture and technology reflected a sophisticated knowledge of the working properties of natural materials and a highly developed artisanship, shown in many articles of everyday utility decorated with shell inlay, carving, and painting. Most of these items, including baskets, shell tools, and wooden weapons, were extremely perishable. Soapstone from quarries on Catalina Island was used to make cooking implements, animal carvings, pipes, ritual objects, and ornaments. Using the stems of rushes (Juncus sp .), grass (Muhlenbergia rigens), and squawbush"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_36",
    "chunk": "(Rhus trilobata), women fabricated coiled and twined basketry in a three-color pattern for household use, seed collecting, and ceremonial containers to hold grave offerings. They sealed some baskets, such as water bottles, with asphalt to make watertight containers for holding liquids. The Tongva used the leaves of tule reeds as well as those of cattails to weave mats and thatch their shelters. Living in the mild climate of southern California, the men and children usually went nude, and women wore only a two-piece skirt, the back part being made from the flexible inner bark of cottonwood or willow, or occasionally deerskin. The front apron was made of cords of twisted dogbane or milkweed. People went barefoot except in rough areas where they wore crude sandals made of yucca fiber. In cold weather, they wore robes or capes made from twisted strips of rabbit fur, deer skins, or bird skins with the feathers still attached. Also used as blankets at night, these were made of sea otter skins along the coast and on the islands. “Women were tattooed from cheek to shoulder blade, from elbow to shoulder,” with cactus thorns used as needles and charcoal dust rubbed into the wounds as"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_37",
    "chunk": "“ink,” leaving a blue-gray mark under the skin after the wounds healed. There were three capital crimes in the community: murder, incest and disrespect for elders. According to Father Gerónimo Boscana, relations between the Chumash, Gabrieleños, Luiseños, and Diegueños, as he called them, were generally peaceful but “when there was a war it was ferocious…no quarter was given, and no prisoners were taken except the wounded.” The earliest ethnological surveys of the Christianized population of the San Gabriel area, who were then known by the Spanish as Gabrielino, were conducted in the mid-19th century. By this time, their pre-Christian religious beliefs and mythology were already fading. The Gabrieleño language was on the brink of extinction by 1900, so only fragmentary records of the Indigenous language and culture of the Gabrieleño have been preserved. Gabrieleño was one of the Cupan languages in the Takic language group, which is part of the Uto-Aztecan family of languages. It may be considered a dialect of Fernandeño, but it has not been a language of everyday conversation since the 1940s. The Gabrieleño people now speak English but a few are attempting to revive their language by using it in everyday conversation and ceremonial contexts. Presently,"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_38",
    "chunk": "Gabrieleño is also being used in language revitalization classes and in some public discussions regarding religious and environmental issues. The library of Loyola Marymount University, located in Los Angeles (Westchester), has an extensive collection of archival materials related to the Tongva and their history. In the 21st century, an estimated 1,700 people self-identify as members of the Tongva or Gabrieleño tribe. In 1994, the state of California recognized the Gabrielino-Tongva Tribe (Spanish: Tribu de Gabrieleño-Tongva) and the Fernandino-Tongva Tribe (Spanish: Tribu de Fernandeño-Tongva), but neither has gained federal recognition. In 2013, it was reported that the four Tongva groups that have applied for federal recognition had over 3,900 members collectively. The Gabrieleño/Tongva people do not accept one organization or government as representing them. They have had strong internal disagreements about governance and their future, largely related to plans supported by some members to open a gaming casino on land that would be considered part of the Gabrieleño/Tongva's homeland. Gaming casinos have generated great revenues for many Native American tribes, but not all Tongva people believe the benefits outweigh the negative aspects. The Gabrielino/Tongva Tribe (sometimes called the \"slash\" group) and Gabrielino-Tongva Tribe (sometimes called the \"hyphen\" group) are the two"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_39",
    "chunk": "primary factions advocating a casino for the Tongva nation, with sharing of revenues by all the people. The Gabrielino Tribal Council of San Gabriel, now known as the Kizh Nation (Gabrieleño Band of Mission Indians), claims that it does not support gaming. The Gabrieleno Tongva San Gabriel Band of Mission Indians also does not support gambling and has been operating and meeting in the city of San Gabriel for over a hundred years. None of these organizations is recognized as a tribe by the federal government. In 1990, the Gabrielino/Tongva of San Gabriel filed for federal recognition. Other Gabrieleño groups have done the same. The Gabrielino/Tongva of California Tribal Council and the Coastal Gabrielino-Diegueno Band of Mission Indians filed federal petitions in 1997. These applications for federal recognition remain pending. The San Gabriel group gained acknowledgement of its nonprofit status by the state of California in 1994. In 2001, the San Gabriel council divided over concessions given to the developers of Playa Vista and a proposal to build an Indian casino in Compton, California. A Santa Monica faction formed that advocated gaming for the tribe, which the San Gabriel faction opposed. The San Gabriel council and Santa Monica faction sued"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_40",
    "chunk": "each other over allegations that the San Gabriel faction expelled some members in order to increase gaming shares for other members. There were allegations that the Santa Monica faction stole tribal records in order to support its case for federal recognition. In September 2006, the Santa Monica faction divided into the \"slash\" and \"hyphen\" groups: the Gabrielino/Tongva Tribe and Gabrielino-Tongva Tribe. Tribal secretary Sam Dunlap and tribal attorney Jonathan Stein confronted each other over various alleged fiscal improprieties and derogatory comments made to each other. Since that time, the slash group has hired former state senator Richard Polanco as its chief executive officer. The hyphen group has allied with Stein and issued warrants for the arrest of Polanco and members of the slash group. Stein's group (hyphen), the Gabrielino-Tongva Tribe, is based in Santa Monica. It has proposed a casino to be built in Garden Grove, California, approximately two miles south of Disneyland. In September 2007, the city council of Garden Grove unanimously rejected the casino proposal, instead choosing to build a water park on the land. Controversies have arisen in contemporary California related to land-use issues and Native American rights, including those of the Tongva. Since the late twentieth"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_41",
    "chunk": "century, both the state and the United States governments have improved respect of Indigenous rights and tribal sovereignty. The Tongva have challenged local development plans in the courts in order to protect and preserve some of their sacred grounds. Given the long Indigenous history in the area, not all archaeological sites have been identified. Sometimes land developers have inadvertently disturbed Tongva burial grounds. The tribe denounced archaeologists breaking bones of ancestral remains found during an excavation of a site at Playa Vista. In the 1990s, the Gabrielino/Tongva Springs Foundation revived use of the Tongva Sacred Springs, also known as Kuruvungna Springs, for sacred ceremonies. The natural springs are located on the site of a former Tongva village, now developed as the campus of University High School in West Los Angeles. The Tongva consider the springs to be one of their last remaining sacred sites and they regularly make them the centerpiece of ceremonial events. The Tongva have another sacred area known as Puvungna. They believe it is the birthplace of the Tongva prophet Chingishnish, and many believe it to be the place of creation. The site contains an active spring and the area was formerly inhabited by a Tongva village."
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_42",
    "chunk": "It has been developed as part of the grounds of California State University, Long Beach. A portion of Puvungna, a Tongva burial ground on the western edge of the campus, is listed on the National Register of Historic Places. In October 2019, following the dumping of soil, along with concrete, rebar and other debris, on \"land that holds archaeological artefacts actively used by local Tribal groups for ceremonies\" from a nearby construction site, the Juaneño Band of Mission Indians, Acjachemen Nation–Belardes (an organization that self-identifies as a Native American tribe), and the California Cultural Resource Preservation Alliance (CCRPA) filed a lawsuit against the university. In November 2019, the university agreed to stop dumping materials onto the site, and as of 2020 the lawsuit between these parties is still ongoing. Tongva/Gabrieleño/Fernandeño oral literature is relatively little known, due to their early Christianization in the 1770s by Spanish missions in California. The available evidence suggests strong cultural links with the group's linguistic kin and neighbors to the south and east, the Luiseño and the Cahuilla. According to Kroeber (1925), the pre-Christian Tongva had a \"mythic-ritual-social six-god pantheon\". The principal deity was Chinigchinix, also known as Quaoar. Another important figure is Weywot, the"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_43",
    "chunk": "god of the sky, who was created by Quaoar. Weywot ruled over the Tongva, but he was very cruel, and he was finally killed by his own sons. When the Tongva assembled to decide what to do next, they had a vision of a ghostly being who called himself Quaoar, who said he had come to restore order and to give laws to the people. After he had given instructions as to which groups would have political and spiritual leadership, he began to dance and slowly ascended into heaven. After consulting with the Tongva, astronomers Michael E. Brown and Chad Trujillo used the name of Quaoar to name a large object in the Kuiper belt that they had discovered, 50000 Quaoar (2002). When Brown later found a satellite of Quaoar, he left the choice of name up to the Tongva, who selected Weywot (2009). From the Spanish colonial period, Tongva place names have been absorbed into general use in Southern California. Examples include Pacoima, Tujunga, Topanga, Rancho Cucamonga, Azusa (Azucsagna), and Cahuenga Pass. Sacred sites that have not been totally demolished, destroyed, or built over include Puvunga, Kuruvungna Springs, and Eagle Rock. In other cases, toponyms or places have been"
  },
  {
    "source": "Tongva.txt",
    "chunk_id": "Tongva.txt_44",
    "chunk": "recently named to honor the Indigenous peoples. The Gabrielino Trail is a 28-mile (45 km) path through the Angeles National Forest, created and named in 1970. A 2,656-foot (810 m) summit in the Verdugo Mountains, in Glendale, was named Tongva Peak in 2002, following a proposal by Richard Toyon. Tongva Park is a 6.2-acre (2.5 ha) park in Santa Monica, California. The park is located just south of Colorado Avenue, between Ocean Avenue and Main Street. The park includes an amphitheater, playground, garden, fountains, picnic areas, and restrooms. The park was dedicated on October 13, 2013."
  },
  {
    "source": "Trojan (celestial body).txt",
    "chunk_id": "Trojan (celestial body).txt_0",
    "chunk": "# Trojan (celestial body) In astronomy, a trojan is a small celestial body (mostly asteroids) that shares the orbit of a larger body, remaining in a stable orbit approximately 60° ahead of or behind the main body near one of its Lagrangian points L4 and L5. Trojans can share the orbits of planets or of large moons. Trojans are one type of co-orbital object. In this arrangement, a star and a planet orbit about their common barycenter, which is close to the center of the star because it is usually much more massive than the orbiting planet. In turn, a much smaller mass than both the star and the planet, located at one of the Lagrangian points of the star–planet system, is subject to a combined gravitational force that acts through this barycenter. Hence the smallest object orbits around the barycenter with the same orbital period as the planet, and the arrangement can remain stable over time. In the Solar System, most known trojans share the orbit of Jupiter. They are divided into the Greek camp at L4 (ahead of Jupiter) and the Trojan camp at L5 (trailing Jupiter). More than a million Jupiter trojans larger than one kilometer are"
  },
  {
    "source": "Trojan (celestial body).txt",
    "chunk_id": "Trojan (celestial body).txt_1",
    "chunk": "thought to exist, of which more than 7,000 are currently catalogued. In other planetary orbits only nine Mars trojans, 31 Neptune trojans, two Uranus trojans, two Earth trojans, and one Saturn trojan have been found to date. A temporary Venus trojan is also known. Numerical orbital dynamics stability simulations indicate that Saturn probably does not have any primordial trojans. The same arrangement can appear when the primary object is a planet and the secondary is one of its moons, whereby much smaller trojan moons can share its orbit. All known trojan moons are part of the Saturn system. Telesto and Calypso are trojans of Tethys, and Helene and Polydeuces of Dione. In 1772, the Italian–French mathematician and astronomer Joseph-Louis Lagrange obtained two constant-pattern solutions (collinear and equilateral) of the general three-body problem. In the restricted three-body problem, with one mass negligible (which Lagrange did not consider), the five possible positions of that mass are now termed Lagrange points. The term \"trojan\" originally referred to the \"trojan asteroids\" (Jovian trojans) that orbit close to the Lagrangian points of Jupiter. These have long been named for figures from the Trojan War of Greek mythology. By convention, the asteroids orbiting near the L4"
  },
  {
    "source": "Trojan (celestial body).txt",
    "chunk_id": "Trojan (celestial body).txt_2",
    "chunk": "point of Jupiter are named for the characters from the Greek side of the war, whereas those orbiting near the L5 of Jupiter are from the Trojan side. There are two exceptions, named before the convention was adopted: 624 Hektor in the L4 group, and 617 Patroclus in the L5 group. Astronomers estimate that the Jovian trojans are about as numerous as the asteroids of the asteroid belt. Later on, objects were found orbiting near the Lagrangian points of Neptune, Mars, Earth, Uranus, and Venus. Minor planets at the Lagrangian points of planets other than Jupiter may be called Lagrangian minor planets. Whether or not a system of star, planet, and trojan is stable depends on how large the perturbations are to which it is subject. If, for example, the planet is the mass of Earth, and there is also a Jupiter-mass object orbiting that star, the trojan's orbit would be much less stable than if the second planet had the mass of Pluto. As a rule of thumb, the system is likely to be long-lived if m1 > 100m2 > 10,000m3 (in which m1, m2, and m3 are the masses of the star, planet, and trojan). More formally, in"
  },
  {
    "source": "Trojan (celestial body).txt",
    "chunk_id": "Trojan (celestial body).txt_3",
    "chunk": "a three-body system with circular orbits, the stability condition is 27(m1m2 + m2m3 + m3m1) < (m1 + m2 + m3). So the trojan being a mote of dust, m3→0, imposes a lower bound on ⁠m1/m2⁠ of ⁠25+√621/2⁠ ≈ 24.9599. And if the star were hyper-massive, m1→+∞, then under Newtonian gravity, the system is stable whatever the planet and trojan masses. And if ⁠m1/m2⁠ = ⁠m2/m3⁠, then both must exceed 13+√168 ≈ 25.9615. However, this all assumes a three-body system; once other bodies are introduced, even if distant and small, stability of the system requires even larger ratios."
  },
  {
    "source": "Tweet (social media).txt",
    "chunk_id": "Tweet (social media).txt_0",
    "chunk": "# Tweet (social media) A tweet (officially known as a post since 2023) is a short status update on the social networking site Twitter (officially known as X since 2023) which can include images, videos, GIFs, straw polls, hashtags, mentions, and hyperlinks. Around 80% of all tweets are made by 10% of users, averaging 138 tweets per month, with the median user making only two tweets per month. Following the acquisition of Twitter by Elon Musk in October 2022, and rebranding of the site as \"X\" in July 2023, all references to the word \"tweet\" were removed from the service, changed to \"post\", and \"retweet\" changed to \"repost\". The terms \"tweet\" and \"retweet\" are still more popular when referring to posts on X. The service has experimented with changing how tweets work over the years to attract more users and to keep them on the site. The character limit was originally 140 characters when the service started, had media attachments no longer count in the mid-2010s, and doubled altogether in 2017. Now, a tweet can contain up to 280 characters and include media. Users subscribed to X Premium (formerly Twitter Blue) can post up to 25,000 characters and can include"
  },
  {
    "source": "Tweet (social media).txt",
    "chunk_id": "Tweet (social media).txt_1",
    "chunk": "bold and italic styling. Tweets originally were limited to 140 characters when the service launched, in 2006. Twitter was originally designed to be used on SMS text messages, which are limited to 160 characters. Twitter reserved 20 characters for the username, leaving 140 characters for the post. The original limit was seen as an iconic fixture of the platform, encouraging \"speed and brevity\". Increasing the limit had been a topic of discussion inside the company for years, and had been resurfaced in 2015 for ways to grow the userbase. At the time, internal discussion also involved excluding links and mentions from the character limit. By January 2016, an internal product named \"Beyond 140\" was in development, targeting Q1 of the same year for expanding tweet limits. By the end of 2015, the company was moving close to introducing a 5,000 or 10,000 character limit. An unfinalized version had tweets that went over the old 140 character threshold only showing the first 140 characters, with a call-to-action that there was more in the tweet. Clicking on the tweet would reveal the rest, which was done to retain the same feel of the timeline. The change was controversial internally and met with"
  },
  {
    "source": "Tweet (social media).txt",
    "chunk_id": "Tweet (social media).txt_2",
    "chunk": "backlash by users. Dorsey confirmed that the 140 character limit would remain, but had told employees upon his return as CEO that the once-sacred aspects of Twitter were no longer untouchable. In May 2016, a week after being leaked, Twitter announced that media attachments (images, GIFs, videos, polls, quote tweets) nor mentions in replies would no longer increase the character limit to be rolled out later in the year to ready developers. The changes rolled out in September, except for the @replies, which were tested in October and then rolled out in March 2017, a year after the original announcement. These changes were a compromise to internal resistance to a 10,000 character limit from the year before. On September 26, 2017, Twitter announced the company was testing doubling the character limit—from 140 to 280. It was an effort for users to be more expressive with their tweets, as users were otherwise cramming ideas into a single tweet by rewriting and removing vowels, or not tweeting at all. It began testing to a small group of users in all languages, excluding Japanese, Chinese, and Korean, because the three languages can say double the amount of information in one character. According to"
  },
  {
    "source": "Tweet (social media).txt",
    "chunk_id": "Tweet (social media).txt_3",
    "chunk": "the company's statistics, 0.4% of tweets in Japanese hit the 140 character ceiling, while 9% of tweets in English hit the ceiling. Users not in the test group were able to see and interact with them normally. The change was similarly controversial internally as the 10,000 character limit proposal. The immediate reaction by Twitter users was largely negative. URLs can be linked on Twitter. A tweet's links are converted to the t.co link shortener, and use up 23 characters out of the limit. The shortener was introduced in June 2011 to allow users to save space on their links, without needing a third-party service like Bitly or TinyURL. Some users use screenshots of text and uploaded them as images to increase the amount of words they could include in a tweet. Beginning in 2012, tweets linking to partnered websites would show, below the content of the tweet, expanded media: an excerpt of a linked news article or an embedded video. Twitter already had a way to see Instagram posts and YouTube videos, called \"expanded tweets\". Twitter then began allowing websites apply to test offering cards for Twitter users. Later in 2012, notably after Facebook purchased it, Instagram started cropping images"
  },
  {
    "source": "Tweet (social media).txt",
    "chunk_id": "Tweet (social media).txt_4",
    "chunk": "displayed in cards, with the plan to end them all together. Between July 2022 and January 2023, Twitter tested a feature where two users could be the author of a tweet, which would be posted on both of their accounts. Both users' profile pictures, names, and handles are shown. One user drafts a tweet in the Composer field, then invite a user that is both following them and has their account published. Edits could not be made once the invite was sent, with the alternative being deleting the invitation and making a second one. The second author could accept the invitation, at which the tweet would then be posted to both accounts. Once published, the second user could revoke them being a co-author, and the tweet would change to being written by the first author and being removed from the second author's tweets. Until the second author accepts the invitation, the tweet would be unlisted, not appearing on the authors' timelines or in searches, but available via a direct link. It was tested with some accounts in the US, Canada, and South Korea. The company noted during the test that the feature may be turned off and all CoTweets deleted."
  },
  {
    "source": "Tweet (social media).txt",
    "chunk_id": "Tweet (social media).txt_5",
    "chunk": "The feature was spotted in code in December 2021. On January 31, Twitter suddenly and quietly decided to stop new CoTweets from being made, though noted that it could return in the future. CoTweets were able to be seen for another month, before being converted to a normal tweet for the first author, and a retweet for the second author. Though Twitter's support page offered a generic reasoning for discontinuing the feature, Elon Musk said that it was to focus on allowing users to add text attachments. Twitter briefly tested a feature in 2022 that allowed users to set the current status—codenamed \"vibe\"— for a tweet or account, from a small set of emoji-phrase combinations. It would allow the user to either tag per-tweet, or on the profile level with it showing on tweets and the profile. Testing began on vibes in June 2022 with a wider selection that could be put above tweets, but disappeared after some time. Phrases included \"✔️ Current status\" and \"💤 Case of the Mondays\". Twitter removed the ability to add vibes to tweets. Users can interact with tweets by 'retweeting' (reblogging), liking, quoting the tweet, or replying to it. In November 2009, Twitter began"
  },
  {
    "source": "Tweet (social media).txt",
    "chunk_id": "Tweet (social media).txt_6",
    "chunk": "rolling out the ability to 'retweet' a tweet. Prior to this, people would write \"RT @username\" before quoting the original tweet. Some people limited their 140 character limit down further, so that other people could always fit their entire tweet in a proto-retweet. In 2023, with the rebranding of Twitter to X, \"retweets\" were quietly renamed to \"reposts\"; however, \"retweets\" remained the most commonly used term on the site. Tweets can be liked by users, adding them to a list that other users used to be able to view, prior to likes becoming private for all users. The feature was available when Twitter launched in 2006. Until 2015, 'likes' were called 'favorites' (or 'favs'). The service renamed them because people \"often misunderstood\" the feature, and people reacted more positively in user tests. Users had the option of hiding their likes from the public, though their like would not be hidden from the list of users who likes a given tweet. Jack Dorsey said in 2019 that, if he had to create Twitter over again, he would deemphasize the like, or not include it altogether because it did not positively contribute to healthy conversations. Likes used to be public and they"
  },
  {
    "source": "Tweet (social media).txt",
    "chunk_id": "Tweet (social media).txt_7",
    "chunk": "are not broadcast to the user's tweets timeline. When likes were public, users would often forget their likes were public or liked more revealing tweets. High-profile users and politicians' accounts have liked pornographic, hateful, and racist tweets. For instance, in 2017, Ted Cruz's account liked a tweet with a two-minute porn video about a day after it was posted. Cruz said that many people had access to his account and one of his staff members pressed the like button in \"an honest mistake\". Likes would later be privatized for all users profiles, with Elon Musk stating \"Public likes are incentivizing the wrong behavior\", and encouraging users to like more Tweets without fear of being noticed. Likes are now anonymous, except to the author of the tweet, and to the person who liked it. Verified users could choose to private their likes prior to the update. When not logged in, users' tweets are sorted by how many likes they received, opposed to reverse-chronological. In 2014, Twitter began testing a new feature that allows users to embed a tweet inside their tweet to add additional commentary. Prior to this, users could include a snippet of another tweet in a new tweet, but"
  },
  {
    "source": "Tweet (social media).txt",
    "chunk_id": "Tweet (social media).txt_8",
    "chunk": "were limited to quoting the—at the time—140 character limit. It was originally called \"retweet with comment\", and was later named \"quote tweet\". Following the rebranding of Twitter to X, quote tweets were renamed, simply dropping the \"tweet\" to become \"quotes\". The common name still largely remains \"quote tweets\". Multiple tweets in reply to each other are grouped together in 'threads'. The 140 character limit prevented users from posting as complete thoughts as they desired, and resorted to making upwards of dozens of tweets, which all showed in a disjointed manner, dubbed a \"tweetstorm\". It was popularized by Marc Andreessen. Users are able to add a bookmark to individual tweets via the bookmark button, or within share icon menu, saving them to revisit them later. The bookmarks are private, but tweets display the number of times it has been bookmarked, if at all. The development was revealed to in October 2017. The feature, highly requested by Japanese users, started from an annual hack week at the company and called \"#ShareForLater\". Previously, users would resort to liking the tweet or by sending it to themselves. Liking tweets is often seen as an endorsement or positive endorsement, and the likes are public and"
  },
  {
    "source": "Tweet (social media).txt",
    "chunk_id": "Tweet (social media).txt_9",
    "chunk": "are notified to the user who made the tweet. The feature was tested in November for some users, and rolled out in February 2018 on mobile alongside a new share menu. The web version of Twitter did not test the bookmark feature until November 2018 When released, the user who made the tweet would have been unaware that a tweet was bookmarked. In March 2020, Twitter added a label to a manipulated video of then-candidate Joe Biden that Donald Trump retweeted. Two months later, as a result of the COVID-19 pandemic, Twitter introduced a policy that would label or warn users on tweets with COVID-19 misinformation. The company said at the time that other areas would have labels covered, and shortly afterwards, misleading information on elections were included. On March 26, then-US president Donald Trump made two false statements about mail-in ballots, claiming they were \"substantially fraudulent\". Within 24 hours of the tweet, Twitter's general counsel and the acting head of policy jointly decided to label Trump's tweets, with several hours of internal debate from company leaders, and then-CEO Jack Dorsey signed off on the decision shortly before the label was applied. The labels, which told readers to \"Get the"
  },
  {
    "source": "Tweet (social media).txt",
    "chunk_id": "Tweet (social media).txt_10",
    "chunk": "facts about mail-in ballots\", was the first time they were applied to Trump's tweets. A spokesperson for Twitter said that the tweets contained \"potentially misleading information about voting processes and have been labeled to provide additional context around mail-in ballots\". The label linked to articles by CNN, The Washington Post, and The Hill, as well as summaries of claims of fraud. Three days later, a tweet about the George Floyd protests in Minneapolis–Saint Paul was hidden from view. In the weeks after the January 6 United States Capitol attack, Twitter rolled out a new program that allowed users to add notes underneath tweets that would benefit from additional context. Prior to the transfer of Twitter to Elon Musk, Community Notes were officially called Birdwatch. The first tweet, made by Jack Dorsey, was made on March 21, 2006. It has the Snowflake ID of 20. The Iconfactory was developing a Twitter application in 2006 called \"Twitterrific\" and developer Craig Hockenberry began a search for a shorter way to refer to \"Post a Twitter Update.\" In 2007 they began using \"twit\" before Twitter developer Blaine Cook suggested that \"tweet\" be used instead. \"Tweet\" was added to the Merriam-Webster dictionary in 2011 and"
  },
  {
    "source": "Tweet (social media).txt",
    "chunk_id": "Tweet (social media).txt_11",
    "chunk": "to the Oxford English Dictionary in 2012. Both its use as a verb and noun were added. This was notable as the Oxford English Dictionary normally waits ten years after the coining of a word to add it to the dictionary. In 2023, the terms \"tweet\" and \"retweet\" were quietly retired in favor of the terms \"post\" and \"repost\", as a part of Twitter's rebrand to X, but many users continue to use the former terms on the platform. The median Twitter user tweets twice a month. Around 80% of tweets made are from 10% of the users, who tweet 138 times per month. 65% of the prolific users are women, compared to 48% of the bottom 90%. Most of the prolific users tweet about political issues. There is no difference in political views between the two groups. 25% of the prolific users use automated tools to make tweets, compared to 15% of the others."
  },
  {
    "source": "Type Ia supernova.txt",
    "chunk_id": "Type Ia supernova.txt_0",
    "chunk": "# Type Ia supernova A Type Ia supernova (read: \"type one-A\") is a type of supernova that occurs in binary systems (two stars orbiting one another) in which one of the stars is a white dwarf. The other star can be anything from a giant star to an even smaller white dwarf. Physically, carbon–oxygen white dwarfs with a low rate of rotation are limited to below 1.44 solar masses (M☉). Beyond this \"critical mass\", they reignite and in some cases trigger a supernova explosion; this critical mass is often referred to as the Chandrasekhar mass, but is marginally different from the absolute Chandrasekhar limit, where electron degeneracy pressure is unable to prevent catastrophic collapse. If a white dwarf gradually accretes mass from a binary companion, or merges with a second white dwarf, the general hypothesis is that a white dwarf's core will reach the ignition temperature for carbon fusion as it approaches the Chandrasekhar mass. Within a few seconds of initiation of nuclear fusion, a substantial fraction of the matter in the white dwarf undergoes a runaway reaction, releasing enough energy (1×10 J) to unbind the star in a supernova explosion. The Type Ia category of supernova produces a fairly"
  },
  {
    "source": "Type Ia supernova.txt",
    "chunk_id": "Type Ia supernova.txt_1",
    "chunk": "consistent peak luminosity because of the fixed critical mass at which a white dwarf will explode. Their consistent peak luminosity allows these explosions to be used as standard candles to measure the distance to their host galaxies: the visual magnitude of a type Ia supernova, as observed from Earth, indicates its distance from Earth. The Type Ia supernova is a subcategory in the Minkowski–Zwicky supernova classification scheme, which was devised by German-American astronomer Rudolph Minkowski and Swiss astronomer Fritz Zwicky. There are several means by which a supernova of this type can form, but they share a common underlying mechanism. Theoretical astronomers long believed the progenitor star for this type of supernova is a white dwarf, and empirical evidence for this was found in 2014 when a Type Ia supernova was observed in the galaxy Messier 82. When a slowly-rotating carbon–oxygen white dwarf accretes matter from a companion, it can exceed the Chandrasekhar limit of about 1.44 M☉, beyond which it can no longer support its weight with electron degeneracy pressure. In the absence of a countervailing process, the white dwarf would collapse to form a neutron star, in an accretion-induced non-ejective process, as normally occurs in the case of"
  },
  {
    "source": "Type Ia supernova.txt",
    "chunk_id": "Type Ia supernova.txt_2",
    "chunk": "a white dwarf that is primarily composed of magnesium, neon, and oxygen. The current view among astronomers who model Type Ia supernova explosions, however, is that this limit is never actually attained and collapse is never initiated. Instead, the increase in pressure and density due to the increasing weight raises the temperature of the core, and as the white dwarf approaches about 99% of the limit, a period of convection ensues, lasting approximately 1,000 years. At some point in this simmering phase, a deflagration flame front is born, powered by carbon fusion. The details of the ignition are still unknown, including the location and number of points where the flame begins. Oxygen fusion is initiated shortly thereafter, but this fuel is not consumed as completely as carbon. Once fusion begins, the temperature of the white dwarf increases. A main sequence star supported by thermal pressure can expand and cool which automatically regulates the increase in thermal energy. However, degeneracy pressure is independent of temperature; white dwarfs are unable to regulate temperature in the manner of normal stars, so they are vulnerable to runaway fusion reactions. The flare accelerates dramatically, in part due to the Rayleigh–Taylor instability and interactions with turbulence."
  },
  {
    "source": "Type Ia supernova.txt",
    "chunk_id": "Type Ia supernova.txt_3",
    "chunk": "It is still a matter of considerable debate whether this flare transforms into a supersonic detonation from a subsonic deflagration. Regardless of the exact details of how the supernova ignites, it is generally accepted that a substantial fraction of the carbon and oxygen in the white dwarf fuses into heavier elements within a period of only a few seconds, with the accompanying release of energy increasing the internal temperature to billions of degrees. The energy released (1–2×10 J) is more than sufficient to unbind the star; that is, the individual particles making up the white dwarf gain enough kinetic energy to fly apart from each other. The star explodes violently and releases a shock wave in which matter is typically ejected at speeds on the order of 5,000–20,000 km/s, roughly 6% of the speed of light. The energy released in the explosion also causes an extreme increase in luminosity. The typical visual absolute magnitude of Type Ia supernovae is Mv = −19.3 (about 5 billion times brighter than the Sun), with little variation. The Type Ia supernova leaves no compact remnant, but the whole mass of the former white dwarf dissipates through space. The theory of this type of supernova"
  },
  {
    "source": "Type Ia supernova.txt",
    "chunk_id": "Type Ia supernova.txt_4",
    "chunk": "is similar to that of novae, in which a white dwarf accretes matter more slowly and does not approach the Chandrasekhar limit. In the case of a nova, the infalling matter causes a hydrogen fusion surface explosion that does not disrupt the star. Type Ia supernovae differ from Type II supernovae, which are caused by the cataclysmic explosion of the outer layers of a massive star as its core collapses, powered by release of gravitational potential energy via neutrino emission. One model for the formation of this category of supernova is a close binary star system. The progenitor binary system consists of main sequence stars, with the primary possessing more mass than the secondary. Being greater in mass, the primary is the first of the pair to evolve onto the asymptotic giant branch, where the star's envelope expands considerably. If the two stars share a common envelope then the system can lose significant amounts of mass, reducing the angular momentum, orbital radius and period. After the primary has degenerated into a white dwarf, the secondary star later evolves into a red giant and the stage is set for mass accretion onto the primary. During this final shared-envelope phase, the two"
  },
  {
    "source": "Type Ia supernova.txt",
    "chunk_id": "Type Ia supernova.txt_5",
    "chunk": "stars spiral in closer together as angular momentum is lost. The resulting orbit can have a period as brief as a few hours. If the accretion continues long enough, the white dwarf may eventually approach the Chandrasekhar limit. The white dwarf companion could also accrete matter from other types of companions, including a subgiant or (if the orbit is sufficiently close) even a main sequence star. The actual evolutionary process during this accretion stage remains uncertain, as it can depend both on the rate of accretion and the transfer of angular momentum to the white dwarf companion. It has been estimated that single degenerate progenitors account for no more than 20% of all Type Ia supernovae. A second possible mechanism for triggering a Type Ia supernova is the merger of two white dwarfs whose combined mass exceeds the Chandrasekhar limit. The resulting merger is called a super-Chandrasekhar mass white dwarf. In such a case, the total mass would not be constrained by the Chandrasekhar limit. Collisions of solitary stars within the Milky Way occur only once every 10 to 10 years; far less frequently than the appearance of novae. Collisions occur with greater frequency in the dense core regions of"
  },
  {
    "source": "Type Ia supernova.txt",
    "chunk_id": "Type Ia supernova.txt_6",
    "chunk": "globular clusters (cf. blue stragglers). A likely scenario is a collision with a binary star system, or between two binary systems containing white dwarfs. This collision can leave behind a close binary system of two white dwarfs. Their orbit decays and they merge through their shared envelope. A study based on SDSS spectra found 15 double systems of the 4,000 white dwarfs tested, implying a double white dwarf merger every 100 years in the Milky Way: this rate matches the number of Type Ia supernovae detected in our neighborhood. A double degenerate scenario is one of several explanations proposed for the anomalously massive (2 M☉) progenitor of SN 2003fg. It is the only possible explanation for SNR 0509-67.5, as all possible models with only one white dwarf have been ruled out. It has also been strongly suggested for SN 1006, given that no companion star remnant has been found there. Observations made with NASA's Swift space telescope ruled out existing supergiant or giant companion stars of every Type Ia supernova studied. The supergiant companion's blown out outer shell should emit X-rays, but this glow was not detected by Swift's XRT (X-ray telescope) in the 53 closest supernova remnants. For 12"
  },
  {
    "source": "Type Ia supernova.txt",
    "chunk_id": "Type Ia supernova.txt_7",
    "chunk": "Type Ia supernovae observed within 10 days of the explosion, the satellite's UVOT (ultraviolet/optical telescope) showed no ultraviolet radiation originating from the heated companion star's surface hit by the supernova shock wave, meaning there were no red giants or larger stars orbiting those supernova progenitors. In the case of SN 2011fe, the companion star must have been smaller than the Sun, if it existed. The Chandra X-ray Observatory revealed that the X-ray radiation of five elliptical galaxies and the bulge of the Andromeda Galaxy is 30–50 times fainter than expected. X-ray radiation should be emitted by the accretion discs of Type Ia supernova progenitors. The missing radiation indicates that few white dwarfs possess accretion discs, ruling out the common, accretion-based model of Ia supernovae. Inward spiraling white dwarf pairs are strongly-inferred candidate sources of gravitational waves, although they have not been directly observed. It has been proposed that a group of sub-luminous supernovae should be classified as Type Iax. This type of supernova may not always completely destroy the white dwarf progenitor, but instead leave behind a zombie star. Known examples of type Iax supernovae include: the historical supernova SN 1181, SN 1991T, SN 1991bg, SN 2002cx, and SN 2012Z."
  },
  {
    "source": "Type Ia supernova.txt",
    "chunk_id": "Type Ia supernova.txt_8",
    "chunk": "The supernova SN 1181 is believed to be associated with the supernova remnant Pa 30 and its central star IRAS 00500+6713, which is the result of a merger of a CO white dwarf and an ONe white dwarf. This makes Pa 30 and IRAS 00500+6713 the only SN Iax remnant in the Milky Way. Unlike the other types of supernovae, Type Ia supernovae generally occur in all types of galaxies, including ellipticals. They show no preference for regions of current stellar formation. As white dwarf stars form at the end of a star's main sequence evolutionary period, such a long-lived star system may have wandered far from the region where it originally formed. Thereafter a close binary system may spend another million years in the mass transfer stage (possibly forming persistent nova outbursts) before the conditions are ripe for a Type Ia supernova to occur. A long-standing problem in astronomy has been the identification of supernova progenitors. Direct observation of a progenitor would provide useful constraints on supernova models. As of 2006, the search for such a progenitor had been ongoing for longer than a century. Observation of the supernova SN 2011fe has provided useful constraints. Previous observations with the"
  },
  {
    "source": "Type Ia supernova.txt",
    "chunk_id": "Type Ia supernova.txt_9",
    "chunk": "Hubble Space Telescope did not show a star at the position of the event, thereby excluding a red giant as the source. The expanding plasma from the explosion was found to contain carbon and oxygen, making it likely the progenitor was a white dwarf primarily composed of these elements. Similarly, observations of the nearby SN PTF 11kx, discovered January 16, 2011 (UT) by the Palomar Transient Factory (PTF), lead to the conclusion that this explosion arises from single-degenerate progenitor, with a red giant companion, thus suggesting there is no single progenitor path to SN Ia. Direct observations of the progenitor of PTF 11kx were reported in the August 24 edition of Science and support this conclusion, and also show that the progenitor star experienced periodic nova eruptions before the supernova – another surprising discovery. However, later analysis revealed that the circumstellar material is too massive for the single-degenerate scenario, and fits better the core-degenerate scenario. In May 2015, NASA reported that the Kepler space observatory observed KSN 2011b, a Type Ia supernova in the process of exploding. Details of the pre-nova moments may help scientists better judge the quality of Type Ia supernovae as standard candles, which is an important"
  },
  {
    "source": "Type Ia supernova.txt",
    "chunk_id": "Type Ia supernova.txt_10",
    "chunk": "link in the argument for dark energy. In July 2019, the Hubble Space Telescope took three images of a Type Ia supernova through a gravitational lens. This supernova appeared at three different times in the evolution of its brightness due to the differing path length of the light in the three images; at −24, 92, and 107 days from peak luminosity. A fourth image will appear in 2037 allowing observation of the entire luminosity cycle of the supernova. Type Ia supernovae have a characteristic light curve, their graph of luminosity as a function of time after the explosion. Near the time of maximal luminosity, the spectrum contains lines of intermediate-mass elements from oxygen to calcium; these are the main constituents of the outer layers of the star. Months after the explosion, when the outer layers have expanded to the point of transparency, the spectrum is dominated by light emitted by material near the core of the star, heavy elements synthesized during the explosion; most prominently isotopes close to the mass of iron (iron-peak elements). The radioactive decay of nickel-56 through cobalt-56 to iron-56 produces high-energy photons, which dominate the energy output of the ejecta at intermediate to late times. The"
  },
  {
    "source": "Type Ia supernova.txt",
    "chunk_id": "Type Ia supernova.txt_11",
    "chunk": "use of Type Ia supernovae to measure precise distances was pioneered by a collaboration of Chilean and US astronomers, the Calán/Tololo Supernova Survey. In a series of papers in the 1990s the survey showed that while Type Ia supernovae do not all reach the same peak luminosity, a single parameter measured from the light curve can be used to correct unreddened Type Ia supernovae to standard candle values. The original correction to standard candle value is known as the Phillips relationship and was shown by this group to be able to measure relative distances to 7% accuracy. The cause of this uniformity in peak brightness is related to the amount of nickel-56 produced in white dwarfs presumably exploding near the Chandrasekhar limit. The similarity in the absolute luminosity profiles of nearly all known Type Ia supernovae has led to their use as a secondary standard candle in extragalactic astronomy. Improved calibrations of the Cepheid variable distance scale and direct geometric distance measurements to NGC 4258 from the dynamics of maser emission when combined with the Hubble diagram of the Type Ia supernova distances have led to an improved value of the Hubble constant. In 1998, observations of distant Type Ia"
  },
  {
    "source": "Type Ia supernova.txt",
    "chunk_id": "Type Ia supernova.txt_12",
    "chunk": "supernovae indicated the unexpected result that the universe seems to undergo an accelerating expansion. Three members from two teams were subsequently awarded Nobel Prizes for this discovery. There is significant diversity within the class of Type Ia supernovae. Reflecting this, a plethora of sub-classes have been identified. Two prominent and well-studied examples include 1991T-likes, an overluminous ( M V ≲ − 19.5 ) {\\displaystyle (M_{V}\\lesssim -19.5)} subclass that exhibits particularly strong iron absorption lines and abnormally small silicon features, and 1991bg-likes, an exceptionally dim ( M V ≳ − 18 ) {\\displaystyle (M_{V}\\gtrsim -18)} subclass characterized by strong early titanium absorption features and rapid photometric and spectral evolution. Despite their abnormal luminosities, members of both peculiar groups can be standardized by use of the Phillips relation, defined at blue wavelengths, to determine distance."
  },
  {
    "source": "Ultra-short period planet.txt",
    "chunk_id": "Ultra-short period planet.txt_0",
    "chunk": "# Ultra-short period planet An ultra-short period (USP) planet is a type of exoplanet with an orbital period of less than one Earth day. At this short distance, tidal interactions lead to relatively rapid orbital and spin evolution. Therefore when there is a USP planet around a mature main-sequence star it is most likely that the planet has a circular orbit and is tidally locked. There are not many USP planets with sizes exceeding 2 Earth radii. About one out of 200 Sun-like stars (G dwarfs) has an ultra-short-period planet. There is a strong dependence of the occurrence rate on the mass of the host star. The occurrence rate falls from (1.1 ± 0.4)% for M dwarfs to (0.15 ± 0.05)% for F dwarfs. Mostly the USP planets seem consistent with an Earth-like composition of 70% rock and 30% iron, but K2-229b has a higher density suggesting a more massive iron core. WASP-47e and 55 Cnc e have a lower density and are compatible with pure rock, or a rocky-iron body surrounded by a layer of water (or other volatiles). A difference between hot Jupiters and terrestrial USP planets is the proximity of planetary companions. Hot Jupiters are rarely found"
  },
  {
    "source": "Ultra-short period planet.txt",
    "chunk_id": "Ultra-short period planet.txt_1",
    "chunk": "with other planets within a factor of 2–3 in orbital period or distance. In contrast, terrestrial USP planets almost always have longer-period planetary companions. The period ratio between adjacent planets tends to be larger if one of them is a USP planet suggesting the USP planet has undergone tidal orbital decay which may still be ongoing. USP planets also tend to have higher mutual inclinations with adjacent planets than for pairs of planets in wider orbits, suggesting that USP planets have experienced inclination excitation in addition to orbital decay. There are several known giant planets with a period shorter than one day. Their occurrence must be lower by at least an order of magnitude than that of terrestrial USP planets. It had been proposed that USP planets were the rocky cores of evaporated hot Jupiters, however the metallicity of the host stars of USP planets is lower than that of hot Jupiters' stars so it seems more likely that USP planets are the cores of evaporated gas dwarfs. A study by the TESS-Keck Survey using 17 USP planets found that USP planets predominantly have an Earth-like compositions with iron core mass of about 32% and have masses below runaway accretion."
  },
  {
    "source": "Ultra-short period planet.txt",
    "chunk_id": "Ultra-short period planet.txt_2",
    "chunk": "USP are also almost always found in multiple-planet systems around stars with solar metallicity. Studies of TOI-561b found that it is an USP planet with the lowest density (3.8 ± 0.5 g cm) as of April 2022. The low density of this planet is explained with a massive water layer, no H/He envelope, as well as a predicted water steam atmosphere. The steam atmosphere could be detected with JWST in the future. More complex models might be needed to fully explain the unusual properties of TOI-561b."
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_0",
    "chunk": "# Universe The universe is all of space and time and their contents. It comprises all of existence, any fundamental interaction, physical process and physical constant, and therefore all forms of matter and energy, and the structures they form, from sub-atomic particles to entire galactic filaments. Since the early 20th century, the field of cosmology establishes that space and time emerged together at the Big Bang 13.787±0.020 billion years ago and that the universe has been expanding since then. The portion of the universe that can be seen by humans is approximately 93 billion light-years in diameter at present, but the total size of the universe is not known. Some of the earliest cosmological models of the universe were developed by ancient Greek and Indian philosophers and were geocentric, placing Earth at the center. Over the centuries, more precise astronomical observations led Nicolaus Copernicus to develop the heliocentric model with the Sun at the center of the Solar System. In developing the law of universal gravitation, Isaac Newton built upon Copernicus's work as well as Johannes Kepler's laws of planetary motion and observations by Tycho Brahe. Further observational improvements led to the realization that the Sun is one of a"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_1",
    "chunk": "few hundred billion stars in the Milky Way, which is one of a few hundred billion galaxies in the observable universe. Many of the stars in a galaxy have planets. At the largest scale, galaxies are distributed uniformly and the same in all directions, meaning that the universe has neither an edge nor a center. At smaller scales, galaxies are distributed in clusters and superclusters which form immense filaments and voids in space, creating a vast foam-like structure. Discoveries in the early 20th century have suggested that the universe had a beginning and has been expanding since then. According to the Big Bang theory, the energy and matter initially present have become less dense as the universe expanded. After an initial accelerated expansion called the inflationary epoch at around 10 seconds, and the separation of the four known fundamental forces, the universe gradually cooled and continued to expand, allowing the first subatomic particles and simple atoms to form. Giant clouds of hydrogen and helium were gradually drawn to the places where matter was most dense, forming the first galaxies, stars, and everything else seen today. From studying the effects of gravity on both matter and light, it has been discovered"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_2",
    "chunk": "that the universe contains much more matter than is accounted for by visible objects; stars, galaxies, nebulas and interstellar gas. This unseen matter is known as dark matter. In the widely accepted ΛCDM cosmological model, dark matter accounts for about 25.8%±1.1% of the mass and energy in the universe while about 69.2%±1.2% is dark energy, a mysterious form of energy responsible for the acceleration of the expansion of the universe. Ordinary ('baryonic') matter therefore composes only 4.84%±0.1% of the universe. Stars, planets, and visible gas clouds only form about 6% of this ordinary matter. There are many competing hypotheses about the ultimate fate of the universe and about what, if anything, preceded the Big Bang, while other physicists and philosophers refuse to speculate, doubting that information about prior states will ever be accessible. Some physicists have suggested various multiverse hypotheses, in which the universe might be one among many. The physical universe is defined as all of space and time (collectively referred to as spacetime) and their contents. Such contents comprise all of energy in its various forms, including electromagnetic radiation and matter, and therefore planets, moons, stars, galaxies, and the contents of intergalactic space. The universe also includes the"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_3",
    "chunk": "physical laws that influence energy and matter, such as conservation laws, classical mechanics, and relativity. The universe is often defined as \"the totality of existence\", or everything that exists, everything that has existed, and everything that will exist. In fact, some philosophers and scientists support the inclusion of ideas and abstract concepts—such as mathematics and logic—in the definition of the universe. The word universe may also refer to concepts such as the cosmos, the world, and nature. The word universe derives from the Old French word univers, which in turn derives from the Latin word universus, meaning 'combined into one'. The Latin word 'universum' was used by Cicero and later Latin authors in many of the same senses as the modern English word is used. A term for universe among the ancient Greek philosophers from Pythagoras onwards was τὸ πᾶν (tò pân) 'the all', defined as all matter and all space, and τὸ ὅλον (tò hólon) 'all things', which did not necessarily include the void. Another synonym was ὁ κόσμος (ho kósmos) meaning 'the world, the cosmos'. Synonyms are also found in Latin authors (totum, mundus, natura) and survive in modern languages, e.g., the German words Das All, Weltall, and"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_4",
    "chunk": "Natur for universe. The same synonyms are found in English, such as everything (as in the theory of everything), the cosmos (as in cosmology), the world (as in the many-worlds interpretation), and nature (as in natural laws or natural philosophy). The prevailing model for the evolution of the universe is the Big Bang theory. The Big Bang model states that the earliest state of the universe was an extremely hot and dense one, and that the universe subsequently expanded and cooled. The model is based on general relativity and on simplifying assumptions such as the homogeneity and isotropy of space. A version of the model with a cosmological constant (Lambda) and cold dark matter, known as the Lambda-CDM model, is the simplest model that provides a reasonably good account of various observations about the universe. The initial hot, dense state is called the Planck epoch, a brief period extending from time zero to one Planck time unit of approximately 10 seconds. During the Planck epoch, all types of matter and all types of energy were concentrated into a dense state, and gravity—currently the weakest by far of the four known forces—is believed to have been as strong as the other"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_5",
    "chunk": "fundamental forces, and all the forces may have been unified. The physics controlling this very early period (including quantum gravity in the Planck epoch) is not understood, so we cannot say what, if anything, happened before time zero. Since the Planck epoch, the universe has been expanding to its present scale, with a very short but intense period of cosmic inflation speculated to have occurred within the first 10 seconds. This initial period of inflation would explain why space appears to be very flat. Within the first fraction of a second of the universe's existence, the four fundamental forces had separated. As the universe continued to cool from its inconceivably hot state, various types of subatomic particles were able to form in short periods of time known as the quark epoch, the hadron epoch, and the lepton epoch. Together, these epochs encompassed less than 10 seconds of time following the Big Bang. These elementary particles associated stably into ever larger combinations, including stable protons and neutrons, which then formed more complex atomic nuclei through nuclear fusion. This process, known as Big Bang nucleosynthesis, lasted for about 17 minutes and ended about 20 minutes after the Big Bang, so only the"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_6",
    "chunk": "fastest and simplest reactions occurred. About 25% of the protons and all the neutrons in the universe, by mass, were converted to helium, with small amounts of deuterium (a form of hydrogen) and traces of lithium. Any other element was only formed in very tiny quantities. The other 75% of the protons remained unaffected, as hydrogen nuclei. After nucleosynthesis ended, the universe entered a period known as the photon epoch. During this period, the universe was still far too hot for matter to form neutral atoms, so it contained a hot, dense, foggy plasma of negatively charged electrons, neutral neutrinos and positive nuclei. After about 377,000 years, the universe had cooled enough that electrons and nuclei could form the first stable atoms. This is known as recombination for historical reasons; electrons and nuclei were combining for the first time. Unlike plasma, neutral atoms are transparent to many wavelengths of light, so for the first time the universe also became transparent. The photons released (\"decoupled\") when these atoms formed can still be seen today; they form the cosmic microwave background (CMB). As the universe expands, the energy density of electromagnetic radiation decreases more quickly than does that of matter because the"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_7",
    "chunk": "energy of each photon decreases as it is cosmologically redshifted. At around 47,000 years, the energy density of matter became larger than that of photons and neutrinos, and began to dominate the large scale behavior of the universe. This marked the end of the radiation-dominated era and the start of the matter-dominated era. In the earliest stages of the universe, tiny fluctuations within the universe's density led to concentrations of dark matter gradually forming. Ordinary matter, attracted to these by gravity, formed large gas clouds and eventually, stars and galaxies, where the dark matter was most dense, and voids where it was least dense. After around 100–300 million years, the first stars formed, known as Population III stars. These were probably very massive, luminous, non metallic and short-lived. They were responsible for the gradual reionization of the universe between about 200–500 million years and 1 billion years, and also for seeding the universe with elements heavier than helium, through stellar nucleosynthesis. The universe also contains a mysterious energy—possibly a scalar field—called dark energy, the density of which does not change over time. After about 9.8 billion years, the universe had expanded sufficiently so that the density of matter was less"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_8",
    "chunk": "than the density of dark energy, marking the beginning of the present dark-energy-dominated era. In this era, the expansion of the universe is accelerating due to dark energy. Of the four fundamental interactions, gravitation is the dominant at astronomical length scales. Gravity's effects are cumulative; by contrast, the effects of positive and negative charges tend to cancel one another, making electromagnetism relatively insignificant on astronomical length scales. The remaining two interactions, the weak and strong nuclear forces, decline very rapidly with distance; their effects are confined mainly to sub-atomic length scales. The universe appears to have much more matter than antimatter, an asymmetry possibly related to the CP violation. This imbalance between matter and antimatter is partially responsible for the existence of all matter existing today, since matter and antimatter, if equally produced at the Big Bang, would have completely annihilated each other and left only photons as a result of their interaction. Due to the finite speed of light, there is a limit (known as the particle horizon) to how far light can travel over the age of the universe. The spatial region from which we can receive light is called the observable universe. The proper distance (measured at"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_9",
    "chunk": "a fixed time) between Earth and the edge of the observable universe is 46 billion light-years (14 billion parsecs), making the diameter of the observable universe about 93 billion light-years (28 billion parsecs). Although the distance traveled by light from the edge of the observable universe is close to the age of the universe times the speed of light, 13.8 billion light-years (4.2×10^ pc), the proper distance is larger because the edge of the observable universe and the Earth have since moved further apart. For comparison, the diameter of a typical galaxy is 30,000 light-years (9,198 parsecs), and the typical distance between two neighboring galaxies is 3 million light-years (919.8 kiloparsecs). As an example, the Milky Way is roughly 100,000–180,000 light-years in diameter, and the nearest sister galaxy to the Milky Way, the Andromeda Galaxy, is located roughly 2.5 million light-years away. Because humans cannot observe space beyond the edge of the observable universe, it is unknown whether the size of the universe in its totality is finite or infinite. An estimate from 2011 suggests that if the cosmological principle holds, the whole universe must be more than 250 times larger than a Hubble sphere. Some disputed estimates for the"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_10",
    "chunk": "total size of the universe, if finite, reach as high as 10 10 10 122 {\\displaystyle 10^{10^{10^{122}}}} megaparsecs, as implied by a suggested resolution of the No-Boundary Proposal. Assuming that the Lambda-CDM model is correct, the measurements of the parameters using a variety of techniques by numerous experiments yield a best value of the age of the universe at 13.799 ± 0.021 billion years, as of 2015. Over time, the universe and its contents have evolved. For example, the relative population of quasars and galaxies has changed and the universe has expanded. This expansion is inferred from the observation that the light from distant galaxies has been redshifted, which implies that the galaxies are receding from us. Analyses of Type Ia supernovae indicate that the expansion is accelerating. The more matter there is in the universe, the stronger the mutual gravitational pull of the matter. If the universe were too dense then it would re-collapse into a gravitational singularity. However, if the universe contained too little matter then the self-gravity would be too weak for astronomical structures, like galaxies or planets, to form. Since the Big Bang, the universe has expanded monotonically. Perhaps unsurprisingly, our universe has just the right"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_11",
    "chunk": "mass–energy density, equivalent to about 5 protons per cubic meter, which has allowed it to expand for the last 13.8 billion years, giving time to form the universe as observed today. There are dynamical forces acting on the particles in the universe which affect the expansion rate. Before 1998, it was expected that the expansion rate would be decreasing as time went on due to the influence of gravitational interactions in the universe; and thus there is an additional observable quantity in the universe called the deceleration parameter, which most cosmologists expected to be positive and related to the matter density of the universe. In 1998, the deceleration parameter was measured by two different groups to be negative, approximately −0.55, which technically implies that the second derivative of the cosmic scale factor a ¨ {\\displaystyle {\\ddot {a}}} has been positive in the last 5–6 billion years. Modern physics regards events as being organized into spacetime. This idea originated with the special theory of relativity, which predicts that if one observer sees two events happening in different places at the same time, a second observer who is moving relative to the first will see those events happening at different times. The"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_12",
    "chunk": "two observers will disagree on the time T {\\displaystyle T} between the events, and they will disagree about the distance D {\\displaystyle D} separating the events, but they will agree on the speed of light c {\\displaystyle c} , and they will measure the same value for the combination c 2 T 2 − D 2 {\\displaystyle c^{2}T^{2}-D^{2}} . The square root of the absolute value of this quantity is called the interval between the two events. The interval expresses how widely separated events are, not just in space or in time, but in the combined setting of spacetime. The special theory of relativity describes a flat spacetime. Its successor, the general theory of relativity, explains gravity as curvature of spacetime arising due to its energy content. A curved path like an orbit is not the result of a force deflecting a body from an ideal straight-line path, but rather the body's attempt to fall freely through a background that is itself curved by the presence of other masses. A remark by John Archibald Wheeler that has become proverbial among physicists summarizes the theory: \"Spacetime tells matter how to move; matter tells spacetime how to curve\", and therefore there is"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_13",
    "chunk": "no point in considering one without the other. The Newtonian theory of gravity is a good approximation to the predictions of general relativity when gravitational effects are weak and objects are moving slowly compared to the speed of light. The relation between matter distribution and spacetime curvature is given by the Einstein field equations, which require tensor calculus to express. The universe appears to be a smooth spacetime continuum consisting of three spatial dimensions and one temporal (time) dimension. Therefore, an event in the spacetime of the physical universe can be identified by a set of four coordinates: (x, y, z, t). Cosmologists often work with space-like slices of spacetime that are surfaces of constant time in comoving coordinates. The geometry of these spatial slices is set by the density parameter, Omega (Ω), defined as the average matter density of the universe divided by a critical value. This selects one of three possible geometries depending on whether Ω is equal to, less than, or greater than 1. These are called, respectively, the flat, open and closed universes. Observations, including the Cosmic Background Explorer (COBE), Wilkinson Microwave Anisotropy Probe (WMAP), and Planck maps of the CMB, suggest that the universe is"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_14",
    "chunk": "infinite in extent with a finite age, as described by the Friedmann–Lemaître–Robertson–Walker (FLRW) models. These FLRW models thus support inflationary models and the standard model of cosmology, describing a flat, homogeneous universe presently dominated by dark matter and dark energy. The fine-tuned universe hypothesis is the proposition that the conditions that allow the existence of observable life in the universe can only occur when certain universal fundamental physical constants lie within a very narrow range of values. According to this hypothesis, if any of several fundamental constants were only slightly different, the universe would have been unlikely to be conducive to the establishment and development of matter, astronomical structures, elemental diversity, or life as it is understood. Whether this is true, and whether that question is even logically meaningful to ask, are subjects of much debate. The proposition is discussed among philosophers, scientists, theologians, and proponents of creationism. The universe is composed almost completely of dark energy, dark matter, and ordinary matter. Other contents are electromagnetic radiation (estimated to constitute from 0.005% to close to 0.01% of the total mass–energy of the universe) and antimatter. The proportions of all types of matter and energy have changed over the history of"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_15",
    "chunk": "the universe. The total amount of electromagnetic radiation generated within the universe has decreased by 1/2 in the past 2 billion years. Today, ordinary matter, which includes atoms, stars, galaxies, and life, accounts for only 4.9% of the contents of the universe. The present overall density of this type of matter is very low, roughly 4.5 × 10 grams per cubic centimeter, corresponding to a density of the order of only one proton for every four cubic meters of volume. The nature of both dark energy and dark matter is unknown. Dark matter, a mysterious form of matter that has not yet been identified, accounts for 26.8% of the cosmic contents. Dark energy, which is the energy of empty space and is causing the expansion of the universe to accelerate, accounts for the remaining 68.3% of the contents. Matter, dark matter, and dark energy are distributed homogeneously throughout the universe over length scales longer than 300 million light-years (ly) or so. However, over shorter length-scales, matter tends to clump hierarchically; many atoms are condensed into stars, most stars into galaxies, most galaxies into clusters, superclusters and, finally, large-scale galactic filaments. The observable universe contains as many as an estimated 2"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_16",
    "chunk": "trillion galaxies and, overall, as many as an estimated 10 stars – more stars (and earth-like planets) than all the grains of beach sand on planet Earth; but less than the total number of atoms estimated in the universe as 10; and the estimated total number of stars in an inflationary universe (observed and unobserved), as 10. Typical galaxies range from dwarfs with as few as ten million (10) stars up to giants with one trillion (10) stars. Between the larger structures are voids, which are typically 10–150 Mpc (33 million–490 million ly) in diameter. The Milky Way is in the Local Group of galaxies, which in turn is in the Laniakea Supercluster. This supercluster spans over 500 million light-years, while the Local Group spans over 10 million light-years. The universe also has vast regions of relative emptiness; the largest known void measures 1.8 billion ly (550 Mpc) across. The observable universe is isotropic on scales significantly larger than superclusters, meaning that the statistical properties of the universe are the same in all directions as observed from Earth. The universe is bathed in highly isotropic microwave radiation that corresponds to a thermal equilibrium blackbody spectrum of roughly 2.72548 kelvins. The"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_17",
    "chunk": "hypothesis that the large-scale universe is homogeneous and isotropic is known as the cosmological principle. A universe that is both homogeneous and isotropic looks the same from all vantage points and has no center. An explanation for why the expansion of the universe is accelerating remains elusive. It is often attributed to the gravitational influence of \"dark energy\", an unknown form of energy that is hypothesized to permeate space. On a mass–energy equivalence basis, the density of dark energy (~ 7 × 10 g/cm) is much less than the density of ordinary matter or dark matter within galaxies. However, in the present dark-energy era, it dominates the mass–energy of the universe because it is uniform across space. Two proposed forms for dark energy are the cosmological constant, a constant energy density filling space homogeneously, and scalar fields such as quintessence or moduli, dynamic quantities whose energy density can vary in time and space while still permeating them enough to cause the observed rate of expansion. Contributions from scalar fields that are constant in space are usually also included in the cosmological constant. The cosmological constant can be formulated to be equivalent to vacuum energy. Dark matter is a hypothetical kind"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_18",
    "chunk": "of matter that is invisible to the entire electromagnetic spectrum, but which accounts for most of the matter in the universe. The existence and properties of dark matter are inferred from its gravitational effects on visible matter, radiation, and the large-scale structure of the universe. Other than neutrinos, a form of hot dark matter, dark matter has not been detected directly, making it one of the greatest mysteries in modern astrophysics. Dark matter neither emits nor absorbs light or any other electromagnetic radiation at any significant level. Dark matter is estimated to constitute 26.8% of the total mass–energy and 84.5% of the total matter in the universe. The remaining 4.9% of the mass–energy of the universe is ordinary matter, that is, atoms, ions, electrons and the objects they form. This matter includes stars, which produce nearly all of the light we see from galaxies, as well as interstellar gas in the interstellar and intergalactic media, planets, and all the objects from everyday life that we can bump into, touch or squeeze. The great majority of ordinary matter in the universe is unseen, since visible stars and gas inside galaxies and clusters account for less than 10 percent of the ordinary"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_19",
    "chunk": "matter contribution to the mass–energy density of the universe. Ordinary matter commonly exists in four states (or phases): solid, liquid, gas, and plasma. However, advances in experimental techniques have revealed other previously theoretical phases, such as Bose–Einstein condensates and fermionic condensates. Ordinary matter is composed of two types of elementary particles: quarks and leptons. For example, the proton is formed of two up quarks and one down quark; the neutron is formed of two down quarks and one up quark; and the electron is a kind of lepton. An atom consists of an atomic nucleus, made up of protons and neutrons (both of which are baryons), and electrons that orbit the nucleus. Soon after the Big Bang, primordial protons and neutrons formed from the quark–gluon plasma of the early universe as it cooled below two trillion degrees. A few minutes later, in a process known as Big Bang nucleosynthesis, nuclei formed from the primordial protons and neutrons. This nucleosynthesis formed lighter elements, those with small atomic numbers up to lithium and beryllium, but the abundance of heavier elements dropped off sharply with increasing atomic number. Some boron may have been formed at this time, but the next heavier element, carbon,"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_20",
    "chunk": "was not formed in significant amounts. Big Bang nucleosynthesis shut down after about 20 minutes due to the rapid drop in temperature and density of the expanding universe. Subsequent formation of heavier elements resulted from stellar nucleosynthesis and supernova nucleosynthesis. Ordinary matter and the forces that act on matter can be described in terms of elementary particles. These particles are sometimes described as being fundamental, since they have an unknown substructure, and it is unknown whether or not they are composed of smaller and even more fundamental particles. In most contemporary models they are thought of as points in space. All elementary particles are currently best explained by quantum mechanics and exhibit wave–particle duality: their behavior has both particle-like and wave-like aspects, with different features dominating under different circumstances. Of central importance is the Standard Model, a theory that is concerned with electromagnetic interactions and the weak and strong nuclear interactions. The Standard Model is supported by the experimental confirmation of the existence of particles that compose matter: quarks and leptons, and their corresponding \"antimatter\" duals, as well as the force particles that mediate interactions: the photon, the W and Z bosons, and the gluon. The Standard Model predicted the"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_21",
    "chunk": "existence of the recently discovered Higgs boson, a particle that is a manifestation of a field within the universe that can endow particles with mass. Because of its success in explaining a wide variety of experimental results, the Standard Model is sometimes regarded as a \"theory of almost everything\". The Standard Model does not, however, accommodate gravity. A true force–particle \"theory of everything\" has not been attained. A hadron is a composite particle made of quarks held together by the strong force. Hadrons are categorized into two families: baryons (such as protons and neutrons) made of three quarks, and mesons (such as pions) made of one quark and one antiquark. Of the hadrons, protons are stable, and neutrons bound within atomic nuclei are stable. Other hadrons are unstable under ordinary conditions and are thus insignificant constituents of the modern universe. From approximately 10 seconds after the Big Bang, during a period known as the hadron epoch, the temperature of the universe had fallen sufficiently to allow quarks to bind together into hadrons, and the mass of the universe was dominated by hadrons. Initially, the temperature was high enough to allow the formation of hadron–anti-hadron pairs, which kept matter and antimatter"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_22",
    "chunk": "in thermal equilibrium. However, as the temperature of the universe continued to fall, hadron–anti-hadron pairs were no longer produced. Most of the hadrons and anti-hadrons were then eliminated in particle–antiparticle annihilation reactions, leaving a small residual of hadrons by the time the universe was about one second old. A lepton is an elementary, half-integer spin particle that does not undergo strong interactions but is subject to the Pauli exclusion principle; no two leptons of the same species can be in exactly the same state at the same time. Two main classes of leptons exist: charged leptons (also known as the electron-like leptons), and neutral leptons (better known as neutrinos). Electrons are stable and the most common charged lepton in the universe, whereas muons and taus are unstable particles that quickly decay after being produced in high energy collisions, such as those involving cosmic rays or carried out in particle accelerators. Charged leptons can combine with other particles to form various composite particles such as atoms and positronium. The electron governs nearly all of chemistry, as it is found in atoms and is directly tied to all chemical properties. Neutrinos rarely interact with anything, and are consequently rarely observed. Neutrinos stream"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_23",
    "chunk": "throughout the universe but rarely interact with normal matter. The lepton epoch was the period in the evolution of the early universe in which the leptons dominated the mass of the universe. It started roughly 1 second after the Big Bang, after the majority of hadrons and anti-hadrons annihilated each other at the end of the hadron epoch. During the lepton epoch the temperature of the universe was still high enough to create lepton–anti-lepton pairs, so leptons and anti-leptons were in thermal equilibrium. Approximately 10 seconds after the Big Bang, the temperature of the universe had fallen to the point where lepton–anti-lepton pairs were no longer created. Most leptons and anti-leptons were then eliminated in annihilation reactions, leaving a small residue of leptons. The mass of the universe was then dominated by photons as it entered the following photon epoch. A photon is the quantum of light and all other forms of electromagnetic radiation. It is the carrier for the electromagnetic force. The effects of this force are easily observable at the microscopic and at the macroscopic level because the photon has zero rest mass; this allows long distance interactions. The photon epoch started after most leptons and anti-leptons were"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_24",
    "chunk": "annihilated at the end of the lepton epoch, about 10 seconds after the Big Bang. Atomic nuclei were created in the process of nucleosynthesis which occurred during the first few minutes of the photon epoch. For the remainder of the photon epoch the universe contained a hot dense plasma of nuclei, electrons and photons. About 380,000 years after the Big Bang, the temperature of the universe fell to the point where nuclei could combine with electrons to create neutral atoms. As a result, photons no longer interacted frequently with matter and the universe became transparent. The highly redshifted photons from this period form the cosmic microwave background. Tiny variations in the temperature of the CMB correspond to variations in the density of the universe that were the early \"seeds\" from which all subsequent structure formation took place. The frequency of life in the universe has been a frequent point of investigation in astronomy and astrobiology, being the issue of the Drake equation and the different views on it, from identifying the Fermi paradox, the situation of not having found any signs of extraterrestrial life, to arguments for a biophysical cosmology, a view of life being inherent to the physical cosmology"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_25",
    "chunk": "of the universe. General relativity is the geometric theory of gravitation published by Albert Einstein in 1915 and the current description of gravitation in modern physics. It is the basis of current cosmological models of the universe. General relativity generalizes special relativity and Newton's law of universal gravitation, providing a unified description of gravity as a geometric property of space and time, or spacetime. In particular, the curvature of spacetime is directly related to the energy and momentum of whatever matter and radiation are present. The relation is specified by the Einstein field equations, a system of partial differential equations. In general relativity, the distribution of matter and energy determines the geometry of spacetime, which in turn describes the acceleration of matter. Therefore, solutions of the Einstein field equations describe the evolution of the universe. Combined with measurements of the amount, type, and distribution of matter in the universe, the equations of general relativity describe the evolution of the universe over time. With the assumption of the cosmological principle that the universe is homogeneous and isotropic everywhere, a specific solution of the field equations that describes the universe is the metric tensor called the Friedmann–Lemaître–Robertson–Walker metric, where (r, θ, φ)"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_26",
    "chunk": "correspond to a spherical coordinate system. This metric has only two undetermined parameters. An overall dimensionless length scale factor R describes the size scale of the universe as a function of time (an increase in R is the expansion of the universe), and a curvature index k describes the geometry. The index k is defined so that it can take only one of three values: 0, corresponding to flat Euclidean geometry; 1, corresponding to a space of positive curvature; or −1, corresponding to a space of positive or negative curvature. The value of R as a function of time t depends upon k and the cosmological constant Λ. The cosmological constant represents the energy density of the vacuum of space and could be related to dark energy. The equation describing how R varies with time is known as the Friedmann equation after its inventor, Alexander Friedmann. The solutions for R(t) depend on k and Λ, but some qualitative features of such solutions are general. First and most importantly, the length scale R of the universe can remain constant only if the universe is perfectly isotropic with positive curvature (k = 1) and has one precise value of density everywhere, as"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_27",
    "chunk": "first noted by Albert Einstein. Second, all solutions suggest that there was a gravitational singularity in the past, when R went to zero and matter and energy were infinitely dense. It may seem that this conclusion is uncertain because it is based on the questionable assumptions of perfect homogeneity and isotropy (the cosmological principle) and that only the gravitational interaction is significant. However, the Penrose–Hawking singularity theorems show that a singularity should exist for very general conditions. Hence, according to Einstein's field equations, R grew rapidly from an unimaginably hot, dense state that existed immediately following this singularity (when R had a small, finite value); this is the essence of the Big Bang model of the universe. Understanding the singularity of the Big Bang likely requires a quantum theory of gravity, which has not yet been formulated. Third, the curvature index k determines the sign of the curvature of constant-time spatial surfaces averaged over sufficiently large length scales (greater than about a billion light-years). If k = 1, the curvature is positive and the universe has a finite volume. A universe with positive curvature is often visualized as a three-dimensional sphere embedded in a four-dimensional space. Conversely, if k is"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_28",
    "chunk": "zero or negative, the universe has an infinite volume. It may seem counter-intuitive that an infinite and yet infinitely dense universe could be created in a single instant when R = 0, but exactly that is predicted mathematically when k is nonpositive and the cosmological principle is satisfied. By analogy, an infinite plane has zero curvature but infinite area, whereas an infinite cylinder is finite in one direction and a torus is finite in both. The ultimate fate of the universe is still unknown because it depends critically on the curvature index k and the cosmological constant Λ. If the universe were sufficiently dense, k would equal +1, meaning that its average curvature throughout is positive and the universe will eventually recollapse in a Big Crunch, possibly starting a new universe in a Big Bounce. Conversely, if the universe were insufficiently dense, k would equal 0 or −1 and the universe would expand forever, cooling off and eventually reaching the Big Freeze and the heat death of the universe. Modern data suggests that the expansion of the universe is accelerating; if this acceleration is sufficiently rapid, the universe may eventually reach a Big Rip. Observationally, the universe appears to be"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_29",
    "chunk": "flat (k = 0), with an overall density that is very close to the critical value between recollapse and eternal expansion. Some speculative theories have proposed that our universe is but one of a set of disconnected universes, collectively denoted as the multiverse, challenging or enhancing more limited definitions of the universe. Max Tegmark developed a four-part classification scheme for the different types of multiverses that scientists have suggested in response to various problems in physics. An example of such multiverses is the one resulting from the chaotic inflation model of the early universe. Another is the multiverse resulting from the many-worlds interpretation of quantum mechanics. In this interpretation, parallel worlds are generated in a manner similar to quantum superposition and decoherence, with all states of the wave functions being realized in separate worlds. Effectively, in the many-worlds interpretation the multiverse evolves as a universal wavefunction. If the Big Bang that created our multiverse created an ensemble of multiverses, the wave function of the ensemble would be entangled in this sense. Whether scientifically meaningful probabilities can be extracted from this picture has been and continues to be a topic of much debate, and multiple versions of the many-worlds interpretation exist."
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_30",
    "chunk": "The subject of the interpretation of quantum mechanics is in general marked by disagreement. The least controversial, but still highly disputed, category of multiverse in Tegmark's scheme is Level I. The multiverses of this level are composed by distant spacetime events \"in our own universe\". Tegmark and others have argued that, if space is infinite, or sufficiently large and uniform, identical instances of the history of Earth's entire Hubble volume occur every so often, simply by chance. Tegmark calculated that our nearest so-called doppelgänger is 10 metres away from us (a double exponential function larger than a googolplex). However, the arguments used are of speculative nature. It is possible to conceive of disconnected spacetimes, each existing but unable to interact with one another. An easily visualized metaphor of this concept is a group of separate soap bubbles, in which observers living on one soap bubble cannot interact with those on other soap bubbles, even in principle. According to one common terminology, each \"soap bubble\" of spacetime is denoted as a universe, whereas humans' particular spacetime is denoted as the universe, just as humans call Earth's moon the Moon. The entire collection of these separate spacetimes is denoted as the multiverse."
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_31",
    "chunk": "With this terminology, different universes are not causally connected to each other. In principle, the other unconnected universes may have different dimensionalities and topologies of spacetime, different forms of matter and energy, and different physical laws and physical constants, although such possibilities are purely speculative. Others consider each of several bubbles created as part of chaotic inflation to be separate universes, though in this model these universes all share a causal origin. Historically, there have been many ideas of the cosmos (cosmologies) and its origin (cosmogonies). Theories of an impersonal universe governed by physical laws were first proposed by the Greeks and Indians. Ancient Chinese philosophy encompassed the notion of the universe including both all of space and all of time. Over the centuries, improvements in astronomical observations and theories of motion and gravitation led to ever more accurate descriptions of the universe. The modern era of cosmology began with Albert Einstein's 1915 general theory of relativity, which made it possible to quantitatively predict the origin, evolution, and conclusion of the universe as a whole. Most modern, accepted theories of cosmology are based on general relativity and, more specifically, the predicted Big Bang. Many cultures have stories describing the origin"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_32",
    "chunk": "of the world and universe. Cultures generally regard these stories as having some truth. There are however many differing beliefs in how these stories apply amongst those believing in a supernatural origin, ranging from a god directly creating the universe as it is now to a god just setting the \"wheels in motion\" (for example via mechanisms such as the big bang and evolution). Ethnologists and anthropologists who study myths have developed various classification schemes for the various themes that appear in creation stories. For example, in one type of story, the world is born from a world egg; such stories include the Finnish epic poem Kalevala, the Chinese story of Pangu or the Indian Brahmanda Purana. In related stories, the universe is created by a single entity emanating or producing something by him- or herself, as in the Tibetan Buddhism concept of Adi-Buddha, the ancient Greek story of Gaia (Mother Earth), the Aztec goddess Coatlicue myth, the ancient Egyptian god Atum story, and the Judeo-Christian Genesis creation narrative in which the Abrahamic God created the universe. In another type of story, the universe is created from the union of male and female deities, as in the Māori story of"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_33",
    "chunk": "Rangi and Papa. In other stories, the universe is created by crafting it from pre-existing materials, such as the corpse of a dead god—as from Tiamat in the Babylonian epic Enuma Elish or from the giant Ymir in Norse mythology—or from chaotic materials, as in Izanagi and Izanami in Japanese mythology. In other stories, the universe emanates from fundamental principles, such as Brahman and Prakrti, and the creation myth of the Serers. The pre-Socratic Greek philosophers and Indian philosophers developed some of the earliest philosophical concepts of the universe. The earliest Greek philosophers noted that appearances can be deceiving, and sought to understand the underlying reality behind the appearances. In particular, they noted the ability of matter to change forms (e.g., ice to water to steam) and several philosophers proposed that all the physical materials in the world are different forms of a single primordial material, or arche. The first to do so was Thales, who proposed this material to be water. Thales' student, Anaximander, proposed that everything came from the limitless apeiron. Anaximenes proposed the primordial material to be air on account of its perceived attractive and repulsive qualities that cause the arche to condense or dissociate into different"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_34",
    "chunk": "forms. Anaxagoras proposed the principle of Nous (Mind), while Heraclitus proposed fire (and spoke of logos). Empedocles proposed the elements to be earth, water, air and fire. His four-element model became very popular. Like Pythagoras, Plato believed that all things were composed of number, with Empedocles' elements taking the form of the Platonic solids. Democritus, and later philosophers—most notably Leucippus—proposed that the universe is composed of indivisible atoms moving through a void (vacuum), although Aristotle did not believe that to be feasible because air, like water, offers resistance to motion. Air will immediately rush in to fill a void, and moreover, without resistance, it would do so indefinitely fast. Although Heraclitus argued for eternal change, his contemporary Parmenides emphasized changelessness. Parmenides' poem On Nature has been read as saying that all change is an illusion, that the true underlying reality is eternally unchanging and of a single nature, or at least that the essential feature of each thing that exists must exist eternally, without origin, change, or end. His student Zeno of Elea challenged everyday ideas about motion with several famous paradoxes. Aristotle responded to these paradoxes by developing the notion of a potential countable infinity, as well as the"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_35",
    "chunk": "infinitely divisible continuum. The Indian philosopher Kanada, founder of the Vaisheshika school, developed a notion of atomism and proposed that light and heat were varieties of the same substance. In the 5th century AD, the Buddhist atomist philosopher Dignāga proposed atoms to be point-sized, durationless, and made of energy. They denied the existence of substantial matter and proposed that movement consisted of momentary flashes of a stream of energy. The notion of temporal finitism was inspired by the doctrine of creation shared by the three Abrahamic religions: Judaism, Christianity and Islam. The Christian philosopher, John Philoponus, presented the philosophical arguments against the ancient Greek notion of an infinite past and future. Philoponus' arguments against an infinite past were used by the early Muslim philosopher, Al-Kindi (Alkindus); the Jewish philosopher, Saadia Gaon (Saadia ben Joseph); and the Muslim theologian, Al-Ghazali (Algazel). Pantheism is the philosophical religious belief that the universe itself is identical to divinity and a supreme being or entity. The physical universe is thus understood as an all-encompassing, immanent deity. The term 'pantheist' designates one who holds both that everything constitutes a unity and that this unity is divine, consisting of an all-encompassing, manifested god or goddess. The earliest"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_36",
    "chunk": "written records of identifiable predecessors to modern astronomy come from Ancient Egypt and Mesopotamia from around 3000 to 1200 BCE. Babylonian astronomers of the 7th century BCE viewed the world as a flat disk surrounded by the ocean. Later Greek philosophers, observing the motions of the heavenly bodies, were concerned with developing models of the universe based more profoundly on empirical evidence. The first coherent model was proposed by Eudoxus of Cnidos, a student of Plato who followed Plato's idea that heavenly motions had to be circular. In order to account for the known complications of the planets' motions, particularly retrograde movement, Eudoxus' model included 27 different celestial spheres: four for each of the planets visible to the naked eye, three each for the Sun and the Moon, and one for the stars. All of these spheres were centered on the Earth, which remained motionless while they rotated eternally. Aristotle elaborated upon this model, increasing the number of spheres to 55 in order to account for further details of planetary motion. For Aristotle, normal matter was entirely contained within the terrestrial sphere, and it obeyed fundamentally different rules from heavenly material. The post-Aristotle treatise De Mundo (of uncertain authorship and"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_37",
    "chunk": "date) stated, \"Five elements, situated in spheres in five regions, the less being in each case surrounded by the greater—namely, earth surrounded by water, water by air, air by fire, and fire by ether—make up the whole universe\". This model was also refined by Callippus and after concentric spheres were abandoned, it was brought into nearly perfect agreement with astronomical observations by Ptolemy. The success of such a model is largely due to the mathematical fact that any function (such as the position of a planet) can be decomposed into a set of circular functions (the Fourier modes). Other Greek scientists, such as the Pythagorean philosopher Philolaus, postulated (according to Stobaeus' account) that at the center of the universe was a \"central fire\" around which the Earth, Sun, Moon and planets revolved in uniform circular motion. The Greek astronomer Aristarchus of Samos was the first known individual to propose a heliocentric model of the universe. Though the original text has been lost, a reference in Archimedes' book The Sand Reckoner describes Aristarchus's heliocentric model. Archimedes wrote: You, King Gelon, are aware the universe is the name given by most astronomers to the sphere the center of which is the center"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_38",
    "chunk": "of the Earth, while its radius is equal to the straight line between the center of the Sun and the center of the Earth. This is the common account as you have heard from astronomers. But Aristarchus has brought out a book consisting of certain hypotheses, wherein it appears, as a consequence of the assumptions made, that the universe is many times greater than the universe just mentioned. His hypotheses are that the fixed stars and the Sun remain unmoved, that the Earth revolves about the Sun on the circumference of a circle, the Sun lying in the middle of the orbit, and that the sphere of fixed stars, situated about the same center as the Sun, is so great that the circle in which he supposes the Earth to revolve bears such a proportion to the distance of the fixed stars as the center of the sphere bears to its surface. Aristarchus thus believed the stars to be very far away, and saw this as the reason why stellar parallax had not been observed, that is, the stars had not been observed to move relative each other as the Earth moved around the Sun. The stars are in fact"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_39",
    "chunk": "much farther away than the distance that was generally assumed in ancient times, which is why stellar parallax is only detectable with precision instruments. The geocentric model, consistent with planetary parallax, was assumed to be the explanation for the unobservability of stellar parallax. The only other astronomer from antiquity known by name who supported Aristarchus's heliocentric model was Seleucus of Seleucia, a Hellenistic astronomer who lived a century after Aristarchus. According to Plutarch, Seleucus was the first to prove the heliocentric system through reasoning, but it is not known what arguments he used. Seleucus' arguments for a heliocentric cosmology were probably related to the phenomenon of tides. According to Strabo (1.1.9), Seleucus was the first to state that the tides are due to the attraction of the Moon, and that the height of the tides depends on the Moon's position relative to the Sun. Alternatively, he may have proved heliocentricity by determining the constants of a geometric model for it, and by developing methods to compute planetary positions using this model, similar to Nicolaus Copernicus in the 16th century. During the Middle Ages, heliocentric models were also proposed by the Persian astronomers Albumasar and Al-Sijzi. The Aristotelian model was accepted"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_40",
    "chunk": "in the Western world for roughly two millennia, until Copernicus revived Aristarchus's perspective that the astronomical data could be explained more plausibly if the Earth rotated on its axis and if the Sun were placed at the center of the universe. In the center rests the Sun. For who would place this lamp of a very beautiful temple in another or better place than this wherefrom it can illuminate everything at the same time? As noted by Copernicus, the notion that the Earth rotates is very old, dating at least to Philolaus (c. 450 BC), Heraclides Ponticus (c. 350 BC) and Ecphantus the Pythagorean. Roughly a century before Copernicus, the Christian scholar Nicholas of Cusa also proposed that the Earth rotates on its axis in his book, On Learned Ignorance (1440). Al-Sijzi also proposed that the Earth rotates on its axis. Empirical evidence for the Earth's rotation on its axis, using the phenomenon of comets, was given by Tusi (1201–1274) and Ali Qushji (1403–1474). This cosmology was accepted by Isaac Newton, Christiaan Huygens and later scientists. Newton demonstrated that the same laws of motion and gravity apply to earthly and to celestial matter, making Aristotle's division between the two obsolete."
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_41",
    "chunk": "Edmund Halley (1720) and Jean-Philippe de Chéseaux (1744) noted independently that the assumption of an infinite space filled uniformly with stars would lead to the prediction that the nighttime sky would be as bright as the Sun itself; this became known as Olbers' paradox in the 19th century. Newton believed that an infinite space uniformly filled with matter would cause infinite forces and instabilities causing the matter to be crushed inwards under its own gravity. This instability was clarified in 1902 by the Jeans instability criterion. One solution to these paradoxes is the Charlier universe, in which the matter is arranged hierarchically (systems of orbiting bodies that are themselves orbiting in a larger system, ad infinitum) in a fractal way such that the universe has a negligibly small overall density; such a cosmological model had also been proposed earlier in 1761 by Johann Heinrich Lambert. During the 18th century, Immanuel Kant speculated that nebulae could be entire galaxies separate from the Milky Way, and in 1850, Alexander von Humboldt called these separate galaxies Weltinseln, or \"world islands\", a term that later developed into \"island universes\". In 1919, when the Hooker Telescope was completed, the prevailing view was that the universe"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_42",
    "chunk": "consisted entirely of the Milky Way Galaxy. Using the Hooker Telescope, Edwin Hubble identified Cepheid variables in several spiral nebulae and in 1922–1923 proved conclusively that Andromeda Nebula and Triangulum among others, were entire galaxies outside our own, thus proving that the universe consists of a multitude of galaxies. With this Hubble formulated the Hubble constant, which allowed for the first time a calculation of the age of the Universe and size of the Observable Universe, which became increasingly precise with better meassurements, starting at 2 billion years and 280 million light-years, until 2006 when data of the Hubble Space Telescope allowed a very accurate calculation of the age of the Universe and size of the Observable Universe. The modern era of physical cosmology began in 1917, when Albert Einstein first applied his general theory of relativity to model the structure and dynamics of the universe. The discoveries of this era, and the questions that remain unanswered, are outlined in the sections above. \"Two systems of Hindu thought propound physical theories suggestively similar to those of Greece. Kanada, founder of the Vaisheshika philosophy, held that the world is composed of atoms as many in kind as the various elements. The"
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_43",
    "chunk": "Jains more nearly approximated to Democritus by teaching that all atoms were of the same kind, producing different effects by diverse modes of combinations. Kanada believed light and heat to be varieties of the same substance; Udayana taught that all heat comes from the Sun; and Vachaspati, like Newton, interpreted light as composed of minute particles emitted by substances and striking the eye.\" \"The Buddhists denied the existence of substantial matter altogether. Movement consists for them of moments, it is a staccato movement, momentary flashes of a stream of energy... \"Everything is evanescent\",... says the Buddhist, because there is no stuff... Both systems [Sānkhya, and later Indian Buddhism] share in common a tendency to push the analysis of existence up to its minutest, last elements which are imagined as absolute qualities, or things possessing only one unique quality. They are called \"qualities\" (guna-dharma) in both systems in the sense of absolute qualities, a kind of atomic, or intra-atomic, energies of which the empirical things are composed. Both systems, therefore, agree in denying the objective reality of the categories of Substance and Quality,... and of the relation of Inference uniting them. There is in Sānkhya philosophy no separate existence of qualities."
  },
  {
    "source": "Universe.txt",
    "chunk_id": "Universe.txt_44",
    "chunk": "What we call quality is but a particular manifestation of a subtle entity. To every new unit of quality corresponds a subtle quantum of matter which is called guna, \"quality\", but represents a subtle substantive entity. The same applies to early Buddhism where all qualities are substantive... or, more precisely, dynamic entities, although they are also called dharmas ('qualities').\""
  },
  {
    "source": "Visible spectrum.txt",
    "chunk_id": "Visible spectrum.txt_0",
    "chunk": "# Visible spectrum The visible spectrum is the band of the electromagnetic spectrum that is visible to the human eye. Electromagnetic radiation in this range of wavelengths is called visible light (or simply light). The optical spectrum is sometimes considered to be the same as the visible spectrum, but some authors define the term more broadly, to include the ultraviolet and infrared parts of the electromagnetic spectrum as well, known collectively as optical radiation. A typical human eye will respond to wavelengths from about 380 to about 750 nanometers. In terms of frequency, this corresponds to a band in the vicinity of 400–790 terahertz. These boundaries are not sharply defined and may vary per individual. Under optimal conditions, these limits of human perception can extend to 310 nm (ultraviolet) and 1100 nm (near infrared). The spectrum does not contain all the colors that the human visual system can distinguish. Unsaturated colors such as pink, or purple variations like magenta, for example, are absent because they can only be made from a mix of multiple wavelengths. Colors containing only one wavelength are also called pure colors or spectral colors. Visible wavelengths pass largely unattenuated through the Earth's atmosphere via the \"optical"
  },
  {
    "source": "Visible spectrum.txt",
    "chunk_id": "Visible spectrum.txt_1",
    "chunk": "window\" region of the electromagnetic spectrum. An example of this phenomenon is when clean air scatters blue light more than red light, and so the midday sky appears blue (apart from the area around the Sun which appears white because the light is not scattered as much). The optical window is also referred to as the \"visible window\" because it overlaps the human visible response spectrum. The near infrared (NIR) window lies just out of the human vision, as well as the medium wavelength infrared (MWIR) window, and the long-wavelength or far-infrared (LWIR or FIR) window, although other animals may perceive them. Colors that can be produced by visible light of a narrow band of wavelengths (monochromatic light) are called spectral colors. The various color ranges indicated in the illustration are an approximation: The spectrum is continuous, with no clear boundaries between one color and the next. In the 13th century, Roger Bacon theorized that rainbows were produced by a similar process to the passage of light through glass or crystal. In the 17th century, Isaac Newton discovered that prisms could disassemble and reassemble white light, and described the phenomenon in his book Opticks. He was the first to use"
  },
  {
    "source": "Visible spectrum.txt",
    "chunk_id": "Visible spectrum.txt_2",
    "chunk": "the word spectrum (Latin for \"appearance\" or \"apparition\") in this sense in print in 1671 in describing his experiments in optics. Newton observed that, when a narrow beam of sunlight strikes the face of a glass prism at an angle, some is reflected and some of the beam passes into and through the glass, emerging as different-colored bands. Newton hypothesized light to be made up of \"corpuscles\" (particles) of different colors, with the different colors of light moving at different speeds in transparent matter, red light moving more quickly than violet in glass. The result is that red light is bent (refracted) less sharply than violet as it passes through the prism, creating a spectrum of colors. Newton originally divided the spectrum into six named colors: red, orange, yellow, green, blue, and violet. He later added indigo as the seventh color since he believed that seven was a perfect number as derived from the ancient Greek sophists, of there being a connection between the colors, the musical notes, the known objects in the Solar System, and the days of the week. The human eye is relatively insensitive to indigo's frequencies, and some people who have otherwise-good vision cannot distinguish indigo"
  },
  {
    "source": "Visible spectrum.txt",
    "chunk_id": "Visible spectrum.txt_3",
    "chunk": "from blue and violet. For this reason, some later commentators, including Isaac Asimov, have suggested that indigo should not be regarded as a color in its own right but merely as a shade of blue or violet. Evidence indicates that what Newton meant by \"indigo\" and \"blue\" does not correspond to the modern meanings of those color words. Comparing Newton's observation of prismatic colors with a color image of the visible light spectrum shows that \"indigo\" corresponds to what is today called blue, whereas his \"blue\" corresponds to cyan. In the 18th century, Johann Wolfgang von Goethe wrote about optical spectra in his Theory of Colours. Goethe used the word spectrum (Spektrum) to designate a ghostly optical afterimage, as did Schopenhauer in On Vision and Colors. Goethe argued that the continuous spectrum was a compound phenomenon. Where Newton narrowed the beam of light to isolate the phenomenon, Goethe observed that a wider aperture produces not a spectrum but rather reddish-yellow and blue-cyan edges with white between them. The spectrum appears only when these edges are close enough to overlap. In the early 19th century, the concept of the visible spectrum became more definite, as light outside the visible range was"
  },
  {
    "source": "Visible spectrum.txt",
    "chunk_id": "Visible spectrum.txt_4",
    "chunk": "discovered and characterized by William Herschel (infrared) and Johann Wilhelm Ritter (ultraviolet), Thomas Young, Thomas Johann Seebeck, and others. Young was the first to measure the wavelengths of different colors of light, in 1802. The connection between the visible spectrum and color vision was explored by Thomas Young and Hermann von Helmholtz in the early 19th century. Their theory of color vision correctly proposed that the eye uses three distinct receptors to perceive color. The visible spectrum is limited to wavelengths that can both reach the retina and trigger visual phototransduction (excite a visual opsin). Insensitivity to UV light is generally limited by transmission through the lens. Insensitivity to IR light is limited by the spectral sensitivity functions of the visual opsins. The range is defined psychometrically by the luminous efficiency function, which accounts for all of these factors. In humans, there is a separate function for each of two visual systems, one for photopic vision, used in daylight, which is mediated by cone cells, and one for scotopic vision, used in dim light, which is mediated by rod cells. Each of these functions have different visible ranges. However, discussion on the visible range generally assumes photopic vision. The visible"
  },
  {
    "source": "Visible spectrum.txt",
    "chunk_id": "Visible spectrum.txt_5",
    "chunk": "range of most animals evolved to match the optical window, which is the range of light that can pass through the atmosphere. The ozone layer absorbs almost all UV light (below 315 nm). However, this only affects cosmic light (e.g. sunlight), not terrestrial light (e.g. Bioluminescence). Before reaching the retina, light must first transmit through the cornea and lens. UVB light (< 315 nm) is filtered mostly by the cornea, and UVA light (315–400 nm) is filtered mostly by the lens. The lens also yellows with age, attenuating transmission most strongly at the blue part of the spectrum. This can cause xanthopsia as well as a slight truncation of the short-wave (blue) limit of the visible spectrum. Subjects with aphakia are missing a lens, so UVA light can reach the retina and excite the visual opsins; this expands the visible range and may also lead to cyanopsia. Each opsin has a spectral sensitivity function, which defines how likely it is to absorb a photon of each wavelength. The luminous efficiency function is approximately the superposition of the contributing visual opsins. Variance in the position of the individual opsin spectral sensitivity functions therefore affects the luminous efficiency function and the visible"
  },
  {
    "source": "Visible spectrum.txt",
    "chunk_id": "Visible spectrum.txt_6",
    "chunk": "range. For example, the long-wave (red) limit changes proportionally to the position of the L-opsin. The positions are defined by the peak wavelength (wavelength of highest sensitivity), so as the L-opsin peak wavelength blue shifts by 10 nm, the long-wave limit of the visible spectrum also shifts 10 nm. Large deviations of the L-opsin peak wavelength lead to a form of color blindness called protanomaly and a missing L-opsin (protanopia) shortens the visible spectrum by about 30 nm at the long-wave limit. Forms of color blindness affecting the M-opsin and S-opsin do not significantly affect the luminous efficiency function nor the limits of the visible spectrum. Regardless of actual physical and biological variance, the definition of the limits is not standard and will change depending on the industry. For example, some industries may be concerned with practical limits, so would conservatively report 420–680 nm, while others may be concerned with psychometrics and achieving the broadest spectrum would liberally report 380–750, or even 380–800 nm. The luminous efficiency function in the NIR does not have a hard cutoff, but rather an exponential decay, such that the function's value (or vision sensitivity) at 1,050 nm is about 10 times weaker than at"
  },
  {
    "source": "Visible spectrum.txt",
    "chunk_id": "Visible spectrum.txt_7",
    "chunk": "700 nm; much higher intensity is therefore required to perceive 1,050 nm light than 700 nm light. Under ideal laboratory conditions, subjects may perceive infrared light up to at least 1,064 nm. While 1,050 nm NIR light can evoke red, suggesting direct absorption by the L-opsin, there are also reports that pulsed NIR lasers can evoke green, which suggests two-photon absorption may be enabling extended NIR sensitivity. Similarly, young subjects may perceive ultraviolet wavelengths down to about 310–313 nm, but detection of light below 380 nm may be due to fluorescence of the ocular media, rather than direct absorption of UV light by the opsins. As UVA light is absorbed by the ocular media (lens and cornea), it may fluoresce and be released at a lower energy (longer wavelength) that can then be absorbed by the opsins. For example, when the lens absorbs 350 nm light, the fluorescence emission spectrum is centered on 440 nm. In addition to the photopic and scotopic systems, humans have other systems for detecting light that do not contribute to the primary visual system. For example, melanopsin has an absorption range of 420–540 nm and regulates circadian rhythm and other reflexive processes. Since the melanopsin"
  },
  {
    "source": "Visible spectrum.txt",
    "chunk_id": "Visible spectrum.txt_8",
    "chunk": "system does not form images, it is not strictly considered vision and does not contribute to the visible range. The visible spectrum is defined as that visible to humans, but the variance between species is large. Not only can cone opsins be spectrally shifted to alter the visible range, but vertebrates with 4 cones (tetrachromatic) or 2 cones (dichromatic) relative to humans' 3 (trichromatic) will also tend to have a wider or narrower visible spectrum than humans, respectively. Testing the visual systems of animals behaviorally is difficult, so the visible range of animals is usually estimated by comparing the peak wavelengths of opsins with those of typical humans (S-opsin at 420 nm and L-opsin at 560 nm). Most mammals have retained only two opsin classes (LWS and VS), due likely to the nocturnal bottleneck. However, old world primates (including humans) have since evolved two versions in the LWS class to regain trichromacy. Unlike most mammals, rodents' UVS opsins have remained at shorter wavelengths. Along with their lack of UV filters in the lens, mice have a UVS opsin that can detect down to 340 nm. While allowing UV light to reach the retina can lead to retinal damage, the short"
  },
  {
    "source": "Visible spectrum.txt",
    "chunk_id": "Visible spectrum.txt_9",
    "chunk": "lifespan of mice compared with other mammals may minimize this disadvantage relative to the advantage of UV vision. Dogs have two cone opsins at 429 nm and 555 nm, so see almost the entire visible spectrum of humans, despite being dichromatic. Horses have two cone opsins at 428 nm and 539 nm, yielding a slightly more truncated red vision. Most other vertebrates (birds, lizards, fish, etc.) have retained their tetrachromacy, including UVS opsins that extend further into the ultraviolet than humans' VS opsin. The sensitivity of avian UVS opsins vary greatly, from 355–425 nm, and LWS opsins from 560–570 nm. This translates to some birds with a visible spectrum on par with humans, and other birds with greatly expanded sensitivity to UV light. The LWS opsin of birds is sometimes reported to have a peak wavelength above 600 nm, but this is an effective peak wavelength that incorporates the filter of avian oil droplets. The peak wavelength of the LWS opsin alone is the better predictor of the long-wave limit. A possible benefit of avian UV vision involves sex-dependent markings on their plumage that are visible only in the ultraviolet range. Teleosts (bony fish) are generally tetrachromatic. The sensitivity of"
  },
  {
    "source": "Visible spectrum.txt",
    "chunk_id": "Visible spectrum.txt_10",
    "chunk": "fish UVS opsins vary from 347-383 nm, and LWS opsins from 500-570 nm. However, some fish that use alternative chromophores can extend their LWS opsin sensitivity to 625 nm. The popular belief that the common goldfish is the only animal that can see both infrared and ultraviolet light is incorrect, because goldfish cannot see infrared light. The visual systems of invertebrates deviate greatly from vertebrates, so direct comparisons are difficult. However, UV sensitivity has been reported in most insect species. Bees and many other insects can detect ultraviolet light, which helps them find nectar in flowers. Plant species that depend on insect pollination may owe reproductive success to their appearance in ultraviolet light rather than how colorful they appear to humans. Bees' long-wave limit is at about 590 nm. Mantis shrimp exhibit up to 14 opsins, enabling a visible range of less than 300 nm to above 700 nm. Some snakes can \"see\" radiant heat at wavelengths between 5 and 30 μm to a degree of accuracy such that a blind rattlesnake can target vulnerable body parts of the prey at which it strikes, and other snakes with the organ may detect warm bodies from a meter away. It may"
  },
  {
    "source": "Visible spectrum.txt",
    "chunk_id": "Visible spectrum.txt_11",
    "chunk": "also be used in thermoregulation and predator detection. Spectroscopy is the study of objects based on the spectrum of color they emit, absorb or reflect. Visible-light spectroscopy is an important tool in astronomy (as is spectroscopy at other wavelengths), where scientists use it to analyze the properties of distant objects. Chemical elements and small molecules can be detected in astronomical objects by observing emission lines and absorption lines. For example, helium was first detected by analysis of the spectrum of the Sun. The shift in frequency of spectral lines is used to measure the Doppler shift (redshift or blueshift) of distant objects to determine their velocities towards or away from the observer. Astronomical spectroscopy uses high-dispersion diffraction gratings to observe spectra at very high spectral resolutions."
  },
  {
    "source": "Vision for Space Exploration.txt",
    "chunk_id": "Vision for Space Exploration.txt_0",
    "chunk": "# Vision for Space Exploration The Vision for Space Exploration (VSE) was a plan for space exploration announced on January 14, 2004 by President George W. Bush. It was conceived as a response to the Space Shuttle Columbia disaster, the state of human spaceflight at NASA, and as a way to regain public enthusiasm for space exploration. The policy outlined by the \"Vision for Space Exploration\" was replaced first by President Barack Obama's space policy in April 2010, then by President Donald Trump's \"National Space Strategy\" space policy in March 2018, and finally by President Joe Biden's preliminary space policy proposals in spring 2021. The Vision for Space Exploration sought to implement a sustained and affordable human and robotic program to explore the Solar System and beyond; extend human presence across the Solar System, starting with a human return to the Moon by the year 2020, in preparation for human exploration of Mars and other destinations; develop the innovative technologies, knowledge, and infrastructures both to explore and to support decisions about the destinations for human exploration; and to promote international and commercial participation in exploration to further U.S. scientific, security, and economic interests. In pursuit of these goals, the vision"
  },
  {
    "source": "Vision for Space Exploration.txt",
    "chunk_id": "Vision for Space Exploration.txt_1",
    "chunk": "called for the space program to complete the International Space Station by 2010; retire the Space Shuttle by 2010; develop a new Crew Exploration Vehicle (later renamed Orion) by 2008, and conduct its first human spaceflight mission by 2014; explore the Moon with robotic spacecraft missions by 2008 and crewed missions by 2020, and use lunar exploration to develop and test new approaches and technologies useful for supporting sustained exploration of Mars and beyond; explore Mars and other destinations with robotic and crewed missions; pursue commercial transportation to support the International Space Station and missions beyond low Earth orbit. Outlining some of the advantages, U.S. president George W. Bush addressed the following: Establishing an extended human presence on the moon could vastly reduce the costs of further space exploration, making possible ever more ambitious missions. Lifting heavy spacecraft and fuel out of the Earth's gravity is expensive. Spacecraft assembled and provisioned on the moon could escape its far lower gravity using far less energy, and thus, far less cost. Also, the moon is home to abundant resources. Its soil contains raw materials that might be harvested and processed into rocket fuel or breathable air. We can use our time on"
  },
  {
    "source": "Vision for Space Exploration.txt",
    "chunk_id": "Vision for Space Exploration.txt_2",
    "chunk": "the moon to develop and test new approaches and technologies and systems that will allow us to function in other, more challenging environments. One of the stated goals for the Constellation program is to gain significant experience in operating away from Earth's environment, as the White House contended, to embody a \"sustainable course of long-term exploration.\" The Ares boosters are a cost-effective approach – entailing the Ares V's enormous, unprecedented cargo-carrying capacity – transporting future space exploration resources to the Moon's weaker gravity field. While simultaneously serving as a proving ground for a wide range of space operations and processes, the Moon may serve as a cost-effective construction, launching and fueling site for future space exploration missions. For example, future Ares V missions could cost-effectively deliver raw materials for future spacecraft and missions to a Moon-based space dock positioned as a counterweight to a Moon-based space elevator. NASA has also outlined plans for human missions to the far side of the Moon. All of the Apollo missions have landed on the near side. Unique products may be producible in the nearly extreme vacuum of the lunar surface, and the Moon's remoteness is the ultimate isolation for biologically hazardous experiments. The"
  },
  {
    "source": "Vision for Space Exploration.txt",
    "chunk_id": "Vision for Space Exploration.txt_3",
    "chunk": "Moon would also become a proving ground toward the development of In-Situ Resource Utilization, or \"living off the land\" (i.e., self-sufficiency) for permanent human outposts. In a position paper issued by the National Space Society (NSS), a return to the Moon should be considered a high priority space program, to begin development of the knowledge and identification of the industries unique to the Moon. The NSS believes that the Moon may be a repository of the history and possible future of Earth, and that the six Apollo landings only scratched the surface of that \"treasure\". According to NSS, the Moon's far side, permanently shielded from the noisy Earth, is an ideal site for future radio astronomy (for example, signals in the 1–10 MHz range cannot be detected on Earth because of ionosphere interference). When the vision was announced in January 2004, the U.S. Congress and the scientific community gave it a mix of positive and negative reviews. For example, U.S. representative Dave Weldon (Republican–Florida) said, \"I think this is the best thing that has happened to the space program in decades.\" Though physicist and outspoken crewed spaceflight opponent Robert L. Park stated that robotic spacecraft \"are doing so well it's"
  },
  {
    "source": "Vision for Space Exploration.txt",
    "chunk_id": "Vision for Space Exploration.txt_4",
    "chunk": "going to be hard to justify sending a human,\" the vision announced by the president states that \"robotic missions will serve as trailblazers—the advanced guard to the unknown.\" Others, such as the Mars Society, have argued that it makes more sense to avoid going back to the Moon and instead focus on going to Mars first. Throughout much of 2004, it was unclear whether the U.S. Congress would be willing to approve and fund the Vision for Space Exploration. However, in November 2004, Congress passed a spending bill which gave NASA the $16.2 billion that President Bush had sought to kick-start the vision. According to then-NASA chief Sean O'Keefe, that spending bill \"was as strong an endorsement of the space exploration vision, as any of us could have imagined.\" In 2005, Congress passed S.1281, the NASA Authorization Act of 2005, which explicitly endorsed the vision. Former NASA administrator Michael Griffin is a supporter of the vision, but modified it somewhat, saying that he wants to reduce the four-year gap between the retirement of the Space Shuttle and the first crewed mission of the Crew Exploration Vehicle. NASA's \"Lunar Architecture\" forms a key part of its Global Exploration Strategy, also known"
  },
  {
    "source": "Vision for Space Exploration.txt",
    "chunk_id": "Vision for Space Exploration.txt_5",
    "chunk": "as the Vision for Space Exploration. The first part of the Lunar Architecture is the Lunar Reconnaissance Orbiter, which launched in June 2009 on board an Atlas V. The preliminary design review was completed in February 2006 and the critical design review was completed in November 2006. An important function of the orbiter will be to look for further evidence that the increased concentrations of hydrogen discovered at the Moon's poles is in the form of lunar ice. After this the lunar flights will make use of the new Ares I and Ares V rockets. In December 2003, Apollo 11 astronaut Buzz Aldrin voiced criticism for NASA's vision and objectives, stating that the goal of sending astronauts back to the Moon was \"more like reaching for past glory than striving for new triumphs\". In February 2009, the Aerospace Technology Working Group released an in-depth report asserting that the vision had several fundamental problems with regard to politics, financing, and general space policy issues and that the initiative should be rectified or replaced. Another concern noted is that funding for VSE could instead be harnessed to advance science and technology, such as in aeronautics, commercial spacecraft and launch vehicle technology, environmental"
  },
  {
    "source": "Vision for Space Exploration.txt",
    "chunk_id": "Vision for Space Exploration.txt_6",
    "chunk": "monitoring, and biomedical sciences. However, VSE itself is poised to propel a host of beneficial Moon science activities, including lunar telescopes, selenological studies and solar energy beams. With or without VSE, human spaceflight will be made sustainable. However, without VSE, more funds could be directed toward reducing human spaceflight costs sufficiently for the betterment of low Earth orbit research, business, and tourism. Alternatively, VSE could afford advances in other scientific research (astronomy, selenology), in-situ lunar business industries, and lunar-space tourism. The VSE budget required termination the Space Shuttle by 2010 and of any US role in the International Space Station by 2017. This would have required, even in the most optimistic plans, a period of years without human spaceflight capability from the US. Termination of the Space Shuttle program, without any planned alternatives, in 2011 ended virtually all US capability for reusable launch vehicles. This severely limited any future of low Earth orbit or deep space exploration. Ultimately, the lack of proper funding caused the VSE to fall short of its original goals, leaving many projects behind schedule as President George W. Bush's term in office ended. Keith Cowan wrote in 2014, \"The damage done to America and the rest"
  },
  {
    "source": "Vision for Space Exploration.txt",
    "chunk_id": "Vision for Space Exploration.txt_7",
    "chunk": "of the world by unsustainable deficits is real, and any lack of zeal in facing this problem would be a mistake. In that context, this would be a good time for Congress to look again at Bush's plans for NASA to re-establish a human presence in deep space. The outgoing Republican Congress gave its Republican president too much benefit of the doubt on this undertaking. The new Congress must, at the very least, articulate more convincing reasons than have yet been heard for such a colossal expenditure.\" \"A large portion of the scientific community\" concurs that NASA is not \"expanding our scientific understanding of the universe\" in \"the most effective or cost-efficient way.\" Proponents for VSE argue that a permanent settlement on the moon would drastically reduce costs for further space exploration missions. President George W. Bush voiced this sentiment when the vision was first announced (see quote above), and the United States Senate has re-entered testimony by Space Frontier Foundation founder Rick Tumlinson offered previously to the United States Senate Committee on Commerce, Science and Transportation advocating this particular perspective. The reason that the National Space Society regards a return to the Moon as a high space program priority"
  },
  {
    "source": "Vision for Space Exploration.txt",
    "chunk_id": "Vision for Space Exploration.txt_8",
    "chunk": "is to begin development of the knowledge and identification of the industries unique to the Moon, because \"such industries can provide economic leverage and support for NASA activities, saving the government millions.\" As Tumlinson additionally notes, the goal is to \"open space ... to human settlement ... to create ways to harvest the resources ... not only saving this precious planet, but also ... assuring our survival.\" Regarding \"the Moon, NASA should support early exploration now. ... \""
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_0",
    "chunk": "# Voyager program The Voyager program is an American scientific program that employs two interstellar probes, Voyager 1 and Voyager 2. They were launched in 1977 to take advantage of a favorable planetary alignment to explore the two gas giants Jupiter and Saturn and potentially also the ice giants, Uranus and Neptune - to fly near them while collecting data for transmission back to Earth. After Voyager 1 successfully completed its flyby of Saturn and its moon Titan, it was decided to send Voyager 2 on flybys of Uranus and Neptune. After the planetary flybys were complete, decisions were made to keep the probes in operation to explore interstellar space and the outer regions of the Solar System. On 25 August 2012, data from Voyager 1 indicated that it had entered interstellar space. On 5 November 2019, data from Voyager 2 indicated that it also had entered interstellar space. On 4 November 2019, scientists reported that on 5 November 2018, the Voyager 2 probe had officially reached the interstellar medium (ISM), a region of outer space beyond the influence of the solar wind, as did Voyager 1 in 2012. In August 2018, NASA confirmed, based on results by the New"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_1",
    "chunk": "Horizons spacecraft, the existence of a \"hydrogen wall\" at the outer edges of the Solar System that was first detected in 1992 by the two Voyager spacecraft. As of 2024, the Voyagers are still in operation beyond the outer boundary of the heliosphere in interstellar space. Voyager 1 is moving with a velocity of 61,198 kilometers per hour (38,027 mph), or 17 km/s, (10.5 miles/second) relative to the Sun, and is 24,475,900,000 kilometers (1.52086×10 mi) from the Sun reaching a distance of 162 AU (24.2 billion km; 15.1 billion mi) from Earth as of May 25, 2024. As of 2024, Voyager 2 is moving with a velocity of 55,347 kilometers per hour (34,391 mph), or 15 km/s, relative to the Sun, and is 20,439,100,000 kilometers (1.27003×10 mi) from the Sun reaching a distance of 136.627 AU (20.4 billion km; 12.7 billion mi) from Earth as of May 25, 2024. The two Voyagers are the only human-made objects to date that have passed into interstellar space — a record they will hold until at least the 2040s — and Voyager 1 is the farthest human-made object from Earth. Voyager did things no one predicted, found scenes no one expected, and promises"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_2",
    "chunk": "to outlive its inventors. Like a great painting or an abiding institution, it has acquired an existence of its own, a destiny beyond the grasp of its handlers. The two Voyager space probes were originally conceived as part of the Planetary Grand Tour planned during the late 1960s and early 70s that aimed to explore Jupiter, Saturn, Saturn's moon, Titan, Uranus, Neptune, and Pluto. The mission originated from the Grand Tour program, conceptualized by Gary Flandro, an aerospace engineer at the Jet Propulsion Laboratory, in 1964, which leveraged a rare planetary alignment occurring once every 175 years. This alignment allowed a craft to reach all outer planets using gravitational assists. The mission was to send several pairs of probes and gained momentum in 1966 when it was endorsed by NASA's Jet Propulsion Laboratory. However, in December 1971, the Grand Tour mission was canceled when funding was redirected to the Space Shuttle program. In 1972, a scaled-down (four planets, two identical spacecraft) mission was proposed, utilizing a spacecraft derived from the Mariner series, initially intended to be Mariner 11 and Mariner 12. The gravity-assist technique, successfully demonstrated by Mariner 10, would be used to achieve significant velocity changes by maneuvering through"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_3",
    "chunk": "an intermediate planet's gravitational field to minimize time towards Saturn. The spacecrafts were then moved into a separate program named Mariner Jupiter-Saturn (also Mariner Jupiter-Saturn-Uranus, MJS, or MJSU), part of the Mariner program, later renamed because it was thought that the design of the two space probes had progressed sufficiently beyond that of the Mariner family to merit a separate name. On March 4, 1977, NASA announced a competition to rename the mission, believing the existing name was not appropriate as the mission had differed significantly from previous Mariner missions. Voyager was chosen as the new name, referencing an earlier suggestion by William Pickering, who had proposed the name Navigator. Due to the name change occurring close to launch, the probes were still occasionally referred to as Mariner 11 and Mariner 12, or even Voyager 11 and Voyager 12. Two mission trajectories were established: JST aimed at Jupiter, Saturn, and enhancing a Titan flyby, while JSX served as a contingency plan. JST focused on a Titan flyby, while JSX provided a flexible mission plan. If JST succeeded, JSX could proceed with the Grand Tour, but in case of failure, JSX could be redirected for a separate Titan flyby, forfeiting the"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_4",
    "chunk": "Grand Tour opportunity. The second probe, now Voyager 2, followed the JSX trajectory, granting it the option to continue on to Uranus and Neptune. Upon Voyager 1 completing its main objectives at Saturn, Voyager 2 received a mission extension, enabling it to proceed to Uranus and Neptune. This allowed Voyager 2 to diverge from the originally planned JST trajectory. The probes would be launched in August or September 1977, with their main objective being to compare the characteristics of Jupiter and Saturn, such as their atmospheres, magnetic fields, particle environments, ring systems, and moons. They would fly by planets and moons in either a JST or JSX trajectory. After completing their flybys, the probes would communicate with Earth, relaying vital data using their magnetometers, spectrometers, and other instruments to detect interstellar, solar, and cosmic radiation. Their radioisotope thermoelectric generators (RTGs) would limit the maximum communication time with the probes to roughly a decade. Following their primary missions, the probes would continue to drift into interstellar space. Voyager 2 was the first to be launched. Its trajectory was designed to allow flybys of Jupiter, Saturn, Uranus, and Neptune. Voyager 1 was launched after Voyager 2, but along a shorter and faster"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_5",
    "chunk": "trajectory that was designed to provide an optimal flyby of Saturn's moon Titan, which was known to be quite large and to possess a dense atmosphere. This encounter sent Voyager 1 out of the plane of the ecliptic, ending its planetary science mission. Had Voyager 1 been unable to perform the Titan flyby, the trajectory of Voyager 2 could have been altered to explore Titan, forgoing any visit to Uranus and Neptune. Voyager 1 was not launched on a trajectory that would have allowed it to continue to Uranus and Neptune, but could have continued from Saturn to Pluto without exploring Titan. During the 1990s, Voyager 1 overtook the slower deep-space probes Pioneer 10 and Pioneer 11 to become the most distant human-made object from Earth, a record that it will keep for the foreseeable future. The New Horizons probe, which had a higher launch velocity than Voyager 1, is travelling more slowly due to the extra speed Voyager 1 gained from its flybys of Jupiter and Saturn. Voyager 1 and Pioneer 10 are the most widely separated human-made objects anywhere since they are travelling in roughly opposite directions from the Solar System. In December 2004, Voyager 1 crossed the"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_6",
    "chunk": "termination shock, where the solar wind is slowed to subsonic speed, and entered the heliosheath, where the solar wind is compressed and made turbulent due to interactions with the interstellar medium. On 10 December 2007, Voyager 2 also reached the termination shock, about 1.6 billion kilometres (1 billion miles) closer to the Sun than from where Voyager 1 first crossed it, indicating that the Solar System is asymmetrical. In 2010 Voyager 1 reported that the outward velocity of the solar wind had dropped to zero, and scientists predicted it was nearing interstellar space. In 2011, data from the Voyagers determined that the heliosheath is not smooth, but filled with giant magnetic bubbles, theorized to form when the magnetic field of the Sun becomes warped at the edge of the Solar System. In June 2012, Scientists at NASA reported that Voyager 1 was very close to entering interstellar space, indicated by a sharp rise in high-energy particles from outside the Solar System. In September 2013, NASA announced that Voyager 1 had crossed the heliopause on 25 August 2012, making it the first spacecraft to enter interstellar space. In December 2018, NASA announced that Voyager 2 had crossed the heliopause on 5"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_7",
    "chunk": "November 2018, making it the second spacecraft to enter interstellar space. As of 2017 Voyager 1 and Voyager 2 continue to monitor conditions in the outer expanses of the Solar System. The Voyager spacecraft are expected to be able to operate science instruments through 2020, when limited power will require instruments to be deactivated one by one. Sometime around 2025, there will no longer be sufficient power to operate any science instruments. In July 2019, a revised power management plan was implemented to better manage the two probes' dwindling power supply. The Voyager spacecraft each weighed 815 kilograms (1,797 pounds) at launch, but after fuel usage are now about 733 kilograms (1,616 pounds). Of this weight, each spacecraft carries 105 kilograms (231 pounds) of scientific instruments. The identical Voyager spacecraft use three-axis-stabilized guidance systems that use gyroscopic and accelerometer inputs to their attitude control computers to point their high-gain antennas towards the Earth and their scientific instruments towards their targets, sometimes with the help of a movable instrument platform for the smaller instruments and the electronic photography system. The diagram shows the high-gain antenna (HGA) with a 3.7 m (12 ft) diameter dish attached to the hollow decagonal electronics container."
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_8",
    "chunk": "There is also a spherical tank that contains the hydrazine monopropellant fuel. The Voyager Golden Record is attached to one of the bus sides. The angled square panel to the right is the optical calibration target and excess heat radiator. The three radioisotope thermoelectric generators (RTGs) are mounted end-to-end on the lower boom. The scan platform comprises: the Infrared Interferometer Spectrometer (IRIS) (largest camera at top right); the Ultraviolet Spectrometer (UVS) just above the IRIS; the two Imaging Science Subsystem (ISS) vidicon cameras to the left of the UVS; and the Photopolarimeter System (PPS) under the ISS. Only five investigation teams are still supported, though data is collected for two additional instruments. The Flight Data Subsystem (FDS) and a single eight-track digital tape recorder (DTR) provide the data handling functions. The FDS configures each instrument and controls instrument operations. It also collects engineering and science data and formats the data for transmission. The DTR is used to record high-rate Plasma Wave Subsystem (PWS) data, which is played back every six months. The Imaging Science Subsystem made up of a wide-angle and a narrow-angle camera is a modified version of the slow scan vidicon camera designs that were used in the"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_9",
    "chunk": "earlier Mariner flights. The Imaging Science Subsystem consists of two television-type cameras, each with eight filters in a commandable filter wheel mounted in front of the vidicons. One has a low resolution 200 mm (7.9 in) focal length wide-angle lens with an aperture of f/3 (the wide-angle camera), while the other uses a higher resolution 1,500 mm (59 in) narrow-angle f/8.5 lens (the narrow-angle camera). Three spacecraft were built, Voyager 1 (VGR 77-1), Voyager 2 (VGR 77-3), and test spare model (VGR 77-2). There are three different computer types on the Voyager spacecraft, two of each kind, sometimes used for redundancy. They are proprietary, custom-built computers built from CMOS and TTL medium-scale CMOS integrated circuits and discrete components, mostly from the 7400 series of Texas Instruments. Total number of words among the six computers is about 32K. Voyager 1 and Voyager 2 have identical computer systems. The Computer Command System (CCS), the central controller of the spacecraft, has two 18-bit word, interrupt-type processors with 4096 words each of non-volatile plated-wire memory. During most of the Voyager mission the two CCS computers on each spacecraft were used non-redundantly to increase the command and processing capability of the spacecraft. The CCS is"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_10",
    "chunk": "nearly identical to the system flown on the Viking spacecraft. The Flight Data System (FDS) is two 16-bit word machines with modular memories and 8198 words each. The Attitude and Articulation Control System (AACS) is two 18-bit word machines with 4096 words each. Unlike the other on-board instruments, the operation of the cameras for visible light is not autonomous, but rather it is controlled by an imaging parameter table contained in one of the on-board digital computers, the Flight Data Subsystem (FDS). More recent space probes, since about 1990, usually have completely autonomous cameras. The computer command subsystem (CCS) controls the cameras. The CCS contains fixed computer programs such as command decoding, fault detection, and correction routines, antenna-pointing routines, and spacecraft sequencing routines. This computer is an improved version of the one that was used in the Viking orbiter. The hardware in both custom-built CCS subsystems in the Voyagers is identical. There is only a minor software modification for one of them that has a scientific subsystem that the other lacks. According to Guinness Book of Records, CCS holds record of \"longest period of continual operation for a computer\". It has been running continuously since 20 August 1977. The Attitude"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_11",
    "chunk": "and Articulation Control Subsystem (AACS) controls the spacecraft orientation (its attitude). It keeps the high-gain antenna pointing towards the Earth, controls attitude changes, and points the scan platform. The custom-built AACS systems on both craft are identical. It has been erroneously reported on the Internet that the Voyager space probes were controlled by a version of the RCA 1802 (RCA CDP1802 \"COSMAC\" microprocessor), but such claims are not supported by the primary design documents. The CDP1802 microprocessor was used later in the Galileo space probe, which was designed and built years later. The digital control electronics of the Voyagers were not based on a microprocessor integrated-circuit chip. The uplink communications are executed via S-band microwave communications. The downlink communications are carried out by an X-band microwave transmitter on board the spacecraft, with an S-band transmitter as a back-up. All long-range communications to and from the two Voyagers have been carried out using their 3.7-meter (12 ft) high-gain antennas. The high-gain antenna has a beamwidth of 0.5° for X-band, and 2.3° for S-band. (The low-gain antenna has a 7 dB gain and 60° beamwidth.) Because of the inverse-square law in radio communications, the digital data rates used in the downlinks from"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_12",
    "chunk": "the Voyagers have been continually decreasing the farther that they get from the Earth. For example, the data rate used from Jupiter was about 115,000 bits per second. That was halved at the distance of Saturn, and it has gone down continually since then. Some measures were taken on the ground along the way to reduce the effects of the inverse-square law. In between 1982 and 1985, the diameters of the three main parabolic dish antennas of the Deep Space Network were increased from 64 to 70 m (210 to 230 ft) dramatically increasing their areas for gathering weak microwave signals. Whilst the craft were between Saturn and Uranus the onboard software was upgraded to do a degree of image compression and to use a more efficient Reed-Solomon error-correcting encoding. Then between 1986 and 1989, new techniques were brought into play to combine the signals from multiple antennas on the ground into one, more powerful signal, in a kind of an antenna array. This was done at Goldstone, California, Canberra (Australia), and Madrid (Spain) using the additional dish antennas available there. Also, in Australia, the Parkes Radio Telescope was brought into the array in time for the fly-by of Neptune"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_13",
    "chunk": "in 1989. In the United States, the Very Large Array in New Mexico was brought into temporary use along with the antennas of the Deep Space Network at Goldstone. Using this new technology of antenna arrays helped to compensate for the immense radio distance from Neptune to the Earth. Electrical power is supplied by three MHW-RTG radioisotope thermoelectric generators (RTGs). They are powered by plutonium-238 (distinct from the Pu-239 isotope used in nuclear weapons) and provided approximately 470 W at 30 volts DC when the spacecraft was launched. Plutonium-238 decays with a half-life of 87.74 years, so RTGs using Pu-238 will lose a factor of 1−0.5 = 0.79% of their power output per year. In 2011, 34 years after launch, the thermal power generated by such an RTG would be reduced to (1/2) ≈ 76% of its initial power. The RTG thermocouples, which convert thermal power into electricity, also degrade over time reducing available electric power below this calculated level. By 7 October 2011 the power generated by Voyager 1 and Voyager 2 had dropped to 267.9 W and 269.2 W respectively, about 57% of the power at launch. The level of power output was better than pre-launch predictions based"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_14",
    "chunk": "on a conservative thermocouple degradation model. As the electrical power decreases, spacecraft loads must be turned off, eliminating some capabilities. There may be insufficient power for communications by 2032. The Voyager primary mission was completed in 1989, with the close flyby of Neptune by Voyager 2. The Voyager Interstellar Mission (VIM) is a mission extension, which began when the two spacecraft had already been in flight for over 12 years. The Heliophysics Division of the NASA Science Mission Directorate conducted a Heliophysics Senior Review in 2008. The panel found that the VIM \"is a mission that is absolutely imperative to continue\" and that VIM \"funding near the optimal level and increased DSN (Deep Space Network) support is warranted.\" The main objective of the VIM was to extend the exploration of the Solar System beyond the outer planets to the heliopause (the farthest extent at which the Sun's radiation predominates over interstellar winds) and if possible even beyond. Voyager 1 crossed the heliopause boundary in 2012, followed by Voyager 2 in 2018. Passing through the heliopause boundary has allowed both spacecraft to make measurements of the interstellar fields, particles and waves unaffected by the solar wind. Two significant findings so far"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_15",
    "chunk": "have been the discovery of a region of magnetic bubbles and no indication of an expected shift in the Solar magnetic field. The entire Voyager 2 scan platform, including all of the platform instruments, was switched off in 1998. All platform instruments on Voyager 1, except for the ultraviolet spectrometer (UVS) have also been switched off. The Voyager 1 scan platform was scheduled to go off-line in late 2000 but has been left on to investigate UV emission from the upwind direction. UVS data are still captured but scans are no longer possible. Gyro operations ended in 2016 for Voyager 2 and in 2017 for Voyager 1. Gyro operations are used to rotate the probe 360 degrees six times per year to measure the magnetic field of the spacecraft, which is then subtracted from the magnetometer science data. The two spacecraft continue to operate, with some loss in subsystem redundancy but retain the capability to return scientific data from a full complement of Voyager Interstellar Mission (VIM) science instruments. Both spacecraft also have adequate electrical power and attitude control propellant to continue operating until around 2025, after which there may not be electrical power to support science instrument operation; science"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_16",
    "chunk": "data return and spacecraft operations will cease. By the start of VIM, Voyager 1 was at a distance of 40 AU from the Earth, while Voyager 2 was at 31 AU. VIM is in three phases: termination shock, heliosheath exploration, and interstellar exploration phase. The spacecraft began VIM in an environment controlled by the Sun's magnetic field, with the plasma particles being dominated by those contained in the expanding supersonic solar wind. This is the characteristic environment of the termination shock phase. At some distance from the Sun, the supersonic solar wind will be held back from further expansion by the interstellar wind. The first feature encountered by a spacecraft as a result of this interaction – between interstellar wind and solar wind – was the termination shock, where the solar wind slows to subsonic speed, and large changes in plasma flow direction and magnetic field orientation occur. Voyager 1 completed the phase of termination shock in December 2004 at a distance of 94 AU, while Voyager 2 completed it in August 2007 at a distance of 84 AU. After entering into the heliosheath, the spacecraft were in an area that is dominated by the Sun's magnetic field and solar"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_17",
    "chunk": "wind particles. After passing through the heliosheath, the two Voyagers began the phase of interstellar exploration. The outer boundary of the heliosheath is called the heliopause. This is the region where the Sun's influence begins to decrease and interstellar space can be detected. Voyager 1 is escaping the Solar System at the speed of 3.6 AU per year 35° north of the ecliptic in the general direction of the solar apex in Hercules, while Voyager 2's speed is about 3.3 AU per year, heading 48° south of the ecliptic. The Voyager spacecraft will eventually go on to the stars. In about 40,000 years, Voyager 1 will be within 1.6 light years (ly) of AC+79 3888, also known as Gliese 445, which is approaching the Sun. In 40,000 years Voyager 2 will be within 1.7 ly of Ross 248 (another star which is approaching the Sun), and in 296,000 years it will pass within 4.6 ly of Sirius, which is the brightest star in the night-sky. The spacecraft are not expected to collide with a star for 1 sextillion (10) years. In October 2020, astronomers reported a significant unexpected increase in density in the space beyond the Solar System, as detected"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_18",
    "chunk": "by the Voyager space probes. According to the researchers, this implies that \"the density gradient is a large-scale feature of the VLISM (very local interstellar medium) in the general direction of the heliospheric nose\". Both spacecraft carry a 12-inch (30 cm) golden phonograph record that contains pictures and sounds of Earth, symbolic directions on the cover for playing the record, and data detailing the location of Earth. The record is intended as a combination time capsule and an interstellar message to any civilization, alien or far-future human, that may recover either of the Voyagers. The contents of this record were selected by a committee that included Timothy Ferris and was chaired by Carl Sagan. Pale Blue Dot is a photograph of Earth taken on February 14, 1990, by the Voyager 1 space probe from a distance of approximately 6 billion kilometers (3.7 billion miles, 40.5 AU), as part of that day's Family Portrait series of images of the Solar System. The Voyager program's discoveries during the primary phase of its mission, including new close-up color photos of the major planets, were regularly documented by print and electronic media outlets. Among the best-known of these is an image of the Earth"
  },
  {
    "source": "Voyager program.txt",
    "chunk_id": "Voyager program.txt_19",
    "chunk": "as a Pale Blue Dot, taken in 1990 by Voyager 1, and popularized by Carl Sagan, Consider again that dot. That's here. That's home. That's us....The Earth is a very small stage in a vast cosmic arena.... To my mind, there is perhaps no better demonstration of the folly of human conceits than this distant image of our tiny world. To me, it underscores our responsibility to deal more kindly and compassionately with one another and to preserve and cherish that pale blue dot, the only home we've ever known."
  },
  {
    "source": "Walter H. F. Smith.txt",
    "chunk_id": "Walter H. F. Smith.txt_0",
    "chunk": "# Walter H. F. Smith Walter H. F. Smith is a geophysicist, currently working in NOAA's Laboratory for Satellite Altimetry. He was formerly Chair of the scientific and technical sub-committee of GEBCO from 2003 to 2013. Smith earned a BSc at the University of Southern California, and an MA, MPhil and PhD degrees at Columbia University. He was a post-doctoral fellow at the Institute for Geophysics and Planetary Physics of the Scripps Institution of Oceanography until joining NOAA in 1992. Smith is a fellow of the American Geophysical Union, nominated for his contributions to marine geodesy. Along with Pål Wessel, Smith created the Generic Mapping Tools, an open-source collection of computer software tools for processing and displaying geographic and Cartesian datasets. Smith and Wessel developed and maintain the Global Self-consistent, Hierarchical, High-resolution Geography Database."
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_0",
    "chunk": "# Wavelength In physics and mathematics, wavelength or spatial period of a wave or periodic function is the distance over which the wave's shape repeats. In other words, it is the distance between consecutive corresponding points of the same phase on the wave, such as two adjacent crests, troughs, or zero crossings. Wavelength is a characteristic of both traveling waves and standing waves, as well as other spatial wave patterns. The inverse of the wavelength is called the spatial frequency. Wavelength is commonly designated by the Greek letter lambda (λ). For a modulated wave, wavelength may refer to the carrier wavelength of the signal. The term wavelength may also apply to the repeating envelope of modulated waves or waves formed by interference of several sinusoids. Assuming a sinusoidal wave moving at a fixed wave speed, wavelength is inversely proportional to the frequency of the wave: waves with higher frequencies have shorter wavelengths, and lower frequencies have longer wavelengths. Wavelength depends on the medium (for example, vacuum, air, or water) that a wave travels through. Examples of waves are sound waves, light, water waves and periodic electrical signals in a conductor. A sound wave is a variation in air pressure, while"
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_1",
    "chunk": "in light and other electromagnetic radiation the strength of the electric and the magnetic field vary. Water waves are variations in the height of a body of water. In a crystal lattice vibration, atomic positions vary. The range of wavelengths or frequencies for wave phenomena is called a spectrum. The name originated with the visible light spectrum but now can be applied to the entire electromagnetic spectrum as well as to a sound spectrum or vibration spectrum. In linear media, any wave pattern can be described in terms of the independent propagation of sinusoidal components. The wavelength λ of a sinusoidal waveform traveling at constant speed v {\\displaystyle v} is given by λ = v f , {\\displaystyle \\lambda ={\\frac {v}{f}}\\,\\,,} where v {\\displaystyle v} is called the phase speed (magnitude of the phase velocity) of the wave and f {\\displaystyle f} is the wave's frequency. In a dispersive medium, the phase speed itself depends upon the frequency of the wave, making the relationship between wavelength and frequency nonlinear. In the case of electromagnetic radiation—such as light—in free space, the phase speed is the speed of light, about 3×10 m/s. Thus the wavelength of a 100 MHz electromagnetic (radio) wave"
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_2",
    "chunk": "is about: 3×10 m/s divided by 10 Hz = 3 m. The wavelength of visible light ranges from deep red, roughly 700 nm, to violet, roughly 400 nm (for other examples, see electromagnetic spectrum). For sound waves in air, the speed of sound is 343 m/s (at room temperature and atmospheric pressure). The wavelengths of sound frequencies audible to the human ear (20 Hz–20 kHz) are thus between approximately 17 m and 17 mm, respectively. Somewhat higher frequencies are used by bats so they can resolve targets smaller than 17 mm. Wavelengths in audible sound are much longer than those in visible light. A standing wave is an undulatory motion that stays in one place. A sinusoidal standing wave includes stationary points of no motion, called nodes, and the wavelength is twice the distance between nodes. The upper figure shows three standing waves in a box. The walls of the box are considered to require the wave to have nodes at the walls of the box (an example of boundary conditions), thus determining the allowed wavelengths. For example, for an electromagnetic wave, if the box has ideal conductive walls, the condition for nodes at the walls results because the conductive"
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_3",
    "chunk": "walls cannot support a tangential electric field, forcing the wave to have zero amplitude at the wall. The stationary wave can be viewed as the sum of two traveling sinusoidal waves of oppositely directed velocities. Consequently, wavelength, period, and wave velocity are related just as for a traveling wave. For example, the speed of light can be determined from observation of standing waves in a metal box containing an ideal vacuum. Traveling sinusoidal waves are often represented mathematically in terms of their velocity v (in the x direction), frequency f and wavelength λ as: y ( x , t ) = A cos ⁡ ( 2 π ( x λ − f t ) ) = A cos ⁡ ( 2 π λ ( x − v t ) ) {\\displaystyle y(x,\\ t)=A\\cos \\left(2\\pi \\left({\\frac {x}{\\lambda }}-ft\\right)\\right)=A\\cos \\left({\\frac {2\\pi }{\\lambda }}(x-vt)\\right)} where y is the value of the wave at any position x and time t, and A is the amplitude of the wave. They are also commonly expressed in terms of wavenumber k (2π times the reciprocal of wavelength) and angular frequency ω (2π times the frequency) as: y ( x , t ) = A cos ⁡ ("
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_4",
    "chunk": "k x − ω t ) = A cos ⁡ ( k ( x − v t ) ) {\\displaystyle y(x,\\ t)=A\\cos \\left(kx-\\omega t\\right)=A\\cos \\left(k(x-vt)\\right)} in which wavelength and wavenumber are related to velocity and frequency as: k = 2 π λ = 2 π f v = ω v , {\\displaystyle k={\\frac {2\\pi }{\\lambda }}={\\frac {2\\pi f}{v}}={\\frac {\\omega }{v}},} or λ = 2 π k = 2 π v ω = v f . {\\displaystyle \\lambda ={\\frac {2\\pi }{k}}={\\frac {2\\pi v}{\\omega }}={\\frac {v}{f}}.} In the second form given above, the phase (kx − ωt) is often generalized to (k ⋅ r − ωt), by replacing the wavenumber k with a wave vector that specifies the direction and wavenumber of a plane wave in 3-space, parameterized by position vector r. In that case, the wavenumber k, the magnitude of k, is still in the same relationship with wavelength as shown above, with v being interpreted as scalar speed in the direction of the wave vector. The first form, using reciprocal wavelength in the phase, does not generalize as easily to a wave in an arbitrary direction. Generalizations to sinusoids of other phases, and to complex exponentials, are also common; see"
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_5",
    "chunk": "plane wave. The typical convention of using the cosine phase instead of the sine phase when describing a wave is based on the fact that the cosine is the real part of the complex exponential in the wave A e i ( k x − ω t ) . {\\displaystyle Ae^{i\\left(kx-\\omega t\\right)}.} The speed of a wave depends upon the medium in which it propagates. In particular, the speed of light in a medium is less than in vacuum, which means that the same frequency will correspond to a shorter wavelength in the medium than in vacuum, as shown in the figure at right. This change in speed upon entering a medium causes refraction, or a change in direction of waves that encounter the interface between media at an angle. For electromagnetic waves, this change in the angle of propagation is governed by Snell's law. The wave velocity in one medium not only may differ from that in another, but the velocity typically varies with wavelength. As a result, the change in direction upon entering a different medium changes with the wavelength of the wave. For electromagnetic waves the speed in a medium is governed by its refractive index according"
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_6",
    "chunk": "to v = c n ( λ 0 ) , {\\displaystyle v={\\frac {c}{n(\\lambda _{0})}},} where c is the speed of light in vacuum and n(λ0) is the refractive index of the medium at wavelength λ0, where the latter is measured in vacuum rather than in the medium. The corresponding wavelength in the medium is λ = λ 0 n ( λ 0 ) . {\\displaystyle \\lambda ={\\frac {\\lambda _{0}}{n(\\lambda _{0})}}.} When wavelengths of electromagnetic radiation are quoted, the wavelength in vacuum usually is intended unless the wavelength is specifically identified as the wavelength in some other medium. In acoustics, where a medium is essential for the waves to exist, the wavelength value is given for a specified medium. The variation in speed of light with wavelength is known as dispersion, and is also responsible for the familiar phenomenon in which light is separated into component colours by a prism. Separation occurs when the refractive index inside the prism varies with wavelength, so different wavelengths propagate at different speeds inside the prism, causing them to refract at different angles. The mathematical relationship that describes how the speed of light within a medium varies with wavelength is known as a dispersion relation."
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_7",
    "chunk": "Wavelength can be a useful concept even if the wave is not periodic in space. For example, in an ocean wave approaching shore, shown in the figure, the incoming wave undulates with a varying local wavelength that depends in part on the depth of the sea floor compared to the wave height. The analysis of the wave can be based upon comparison of the local wavelength with the local water depth. Waves that are sinusoidal in time but propagate through a medium whose properties vary with position (an inhomogeneous medium) may propagate at a velocity that varies with position, and as a result may not be sinusoidal in space. The figure at right shows an example. As the wave slows down, the wavelength gets shorter and the amplitude increases; after a place of maximum response, the short wavelength is associated with a high loss and the wave dies out. The analysis of differential equations of such systems is often done approximately, using the WKB method (also known as the Liouville–Green method). The method integrates phase through space using a local wavenumber, which can be interpreted as indicating a \"local wavelength\" of the solution as a function of time and space."
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_8",
    "chunk": "This method treats the system locally as if it were uniform with the local properties; in particular, the local wave velocity associated with a frequency is the only thing needed to estimate the corresponding local wavenumber or wavelength. In addition, the method computes a slowly changing amplitude to satisfy other constraints of the equations or of the physical system, such as for conservation of energy in the wave. Waves in crystalline solids are not continuous, because they are composed of vibrations of discrete particles arranged in a regular lattice. This produces aliasing because the same vibration can be considered to have a variety of different wavelengths, as shown in the figure. Descriptions using more than one of these wavelengths are redundant; it is conventional to choose the longest wavelength that fits the phenomenon. The range of wavelengths sufficient to provide a description of all possible waves in a crystalline medium corresponds to the wave vectors confined to the Brillouin zone. This indeterminacy in wavelength in solids is important in the analysis of wave phenomena such as energy bands and lattice vibrations. It is mathematically equivalent to the aliasing of a signal that is sampled at discrete intervals. The concept of"
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_9",
    "chunk": "wavelength is most often applied to sinusoidal, or nearly sinusoidal, waves, because in a linear system the sinusoid is the unique shape that propagates with no shape change – just a phase change and potentially an amplitude change. The wavelength (or alternatively wavenumber or wave vector) is a characterization of the wave in space, that is functionally related to its frequency, as constrained by the physics of the system. Sinusoids are the simplest traveling wave solutions, and more complex solutions can be built up by superposition. In the special case of dispersion-free and uniform media, waves other than sinusoids propagate with unchanging shape and constant velocity. In certain circumstances, waves of unchanging shape also can occur in nonlinear media; for example, the figure shows ocean waves in shallow water that have sharper crests and flatter troughs than those of a sinusoid, typical of a cnoidal wave, a traveling wave so named because it is described by the Jacobi elliptic function of mth order, usually denoted as cn(x; m). Large-amplitude ocean waves with certain shapes can propagate unchanged, because of properties of the nonlinear surface-wave medium. If a traveling wave has a fixed shape that repeats in space or in time,"
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_10",
    "chunk": "it is a periodic wave. Such waves are sometimes regarded as having a wavelength even though they are not sinusoidal. As shown in the figure, wavelength is measured between consecutive corresponding points on the waveform. Localized wave packets, \"bursts\" of wave action where each wave packet travels as a unit, find application in many fields of physics. A wave packet has an envelope that describes the overall amplitude of the wave; within the envelope, the distance between adjacent peaks or troughs is sometimes called a local wavelength. An example is shown in the figure. In general, the envelope of the wave packet moves at a speed different from the constituent waves. Using Fourier analysis, wave packets can be analyzed into infinite sums (or integrals) of sinusoidal waves of different wavenumbers or wavelengths. Louis de Broglie postulated that all particles with a specific value of momentum p have a wavelength λ = h/p, where h is the Planck constant. This hypothesis was at the basis of quantum mechanics. Nowadays, this wavelength is called the de Broglie wavelength. For example, the electrons in a CRT display have a De Broglie wavelength of about 10 m. To prevent the wave function for such"
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_11",
    "chunk": "a particle being spread over all space, de Broglie proposed using wave packets to represent particles that are localized in space. The spatial spread of the wave packet, and the spread of the wavenumbers of sinusoids that make up the packet, correspond to the uncertainties in the particle's position and momentum, the product of which is bounded by Heisenberg uncertainty principle. When sinusoidal waveforms add, they may reinforce each other (constructive interference) or cancel each other (destructive interference) depending upon their relative phase. This phenomenon is used in the interferometer. A simple example is an experiment due to Young where light is passed through two slits. As shown in the figure, light is passed through two slits and shines on a screen. The path of the light to a position on the screen is different for the two slits, and depends upon the angle θ the path makes with the screen. If we suppose the screen is far enough from the slits (that is, s is large compared to the slit separation d) then the paths are nearly parallel, and the path difference is simply d sin θ. Accordingly, the condition for constructive interference is: d sin ⁡ θ ="
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_12",
    "chunk": "m λ , {\\displaystyle d\\sin \\theta =m\\lambda \\ ,} where m is an integer, and for destructive interference is: d sin ⁡ θ = ( m + 1 / 2 ) λ . {\\displaystyle d\\sin \\theta =(m+1/2)\\lambda \\ .} Thus, if the wavelength of the light is known, the slit separation can be determined from the interference pattern or fringes, and vice versa. For multiple slits, the pattern is I q = I 1 sin 2 ⁡ ( q π g sin ⁡ α λ ) / sin 2 ⁡ ( π g sin ⁡ α λ ) , {\\displaystyle I_{q}=I_{1}\\sin ^{2}\\left({\\frac {q\\pi g\\sin \\alpha }{\\lambda }}\\right)/\\sin ^{2}\\left({\\frac {\\pi g\\sin \\alpha }{\\lambda }}\\right)\\ ,} where q is the number of slits, and g is the grating constant. The first factor, I1, is the single-slit result, which modulates the more rapidly varying second factor that depends upon the number of slits and their spacing. In the figure I1 has been set to unity, a very rough approximation. The effect of interference is to redistribute the light, so the energy contained in the light is not altered, just where it shows up. The notion of path difference and constructive or destructive interference"
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_13",
    "chunk": "used above for the double-slit experiment applies as well to the display of a single slit of light intercepted on a screen. The main result of this interference is to spread out the light from the narrow slit into a broader image on the screen. This distribution of wave energy is called diffraction. Two types of diffraction are distinguished, depending upon the separation between the source and the screen: Fraunhofer diffraction or far-field diffraction at large separations and Fresnel diffraction or near-field diffraction at close separations. In the analysis of the single slit, the non-zero width of the slit is taken into account, and each point in the aperture is taken as the source of one contribution to the beam of light (Huygens' wavelets). On the screen, the light arriving from each position within the slit has a different path length, albeit possibly a very small difference. Consequently, interference occurs. In the Fraunhofer diffraction pattern sufficiently far from a single slit, within a small-angle approximation, the intensity spread S is related to position x via a squared sinc function: S ( u ) = s i n c 2 ( u ) = ( sin ⁡ π u π u"
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_14",
    "chunk": ") 2 ; {\\displaystyle S(u)=\\mathrm {sinc} ^{2}(u)=\\left({\\frac {\\sin \\pi u}{\\pi u}}\\right)^{2}\\ ;} with u = x L λ R , {\\displaystyle u={\\frac {xL}{\\lambda R}}\\ ,} where L is the slit width, R is the distance of the pattern (on the screen) from the slit, and λ is the wavelength of light used. The function S has zeros where u is a non-zero integer, where are at x values at a separation proportion to wavelength. Diffraction is the fundamental limitation on the resolving power of optical instruments, such as telescopes (including radiotelescopes) and microscopes. For a circular aperture, the diffraction-limited image spot is known as an Airy disk; the distance x in the single-slit diffraction formula is replaced by radial distance r and the sine is replaced by 2J1, where J1 is a first order Bessel function. The resolvable spatial size of objects viewed through a microscope is limited according to the Rayleigh criterion, the radius to the first null of the Airy disk, to a size proportional to the wavelength of the light used, and depending on the numerical aperture: r A i r y = 1.22 λ 2 N A , {\\displaystyle r_{Airy}=1.22{\\frac {\\lambda }{2\\,\\mathrm {NA} }}\\ ,} where"
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_15",
    "chunk": "the numerical aperture is defined as N A = n sin ⁡ θ {\\displaystyle \\mathrm {NA} =n\\sin \\theta \\;} for θ being the half-angle of the cone of rays accepted by the microscope objective. The angular size of the central bright portion (radius to first null of the Airy disk) of the image diffracted by a circular aperture, a measure most commonly used for telescopes and cameras, is: δ = 1.22 λ D , {\\displaystyle \\delta =1.22{\\frac {\\lambda }{D}}\\ ,} where λ is the wavelength of the waves that are focused for imaging, D the entrance pupil diameter of the imaging system, in the same units, and the angular resolution δ is in radians. As with other diffraction patterns, the pattern scales in proportion to wavelength, so shorter wavelengths can lead to higher resolution. The term subwavelength is used to describe an object having one or more dimensions smaller than the length of the wave with which the object interacts. For example, the term subwavelength-diameter optical fibre means an optical fibre whose diameter is less than the wavelength of light propagating through it. A subwavelength particle is a particle smaller than the wavelength of light with which it interacts (see"
  },
  {
    "source": "Wavelength.txt",
    "chunk_id": "Wavelength.txt_16",
    "chunk": "Rayleigh scattering). Subwavelength apertures are holes smaller than the wavelength of light propagating through them. Such structures have applications in extraordinary optical transmission, and zero-mode waveguides, among other areas of photonics. Subwavelength may also refer to a phenomenon involving subwavelength objects; for example, subwavelength imaging. A quantity related to the wavelength is the angular wavelength (also known as reduced wavelength), usually symbolized by ƛ (\"lambda-bar\" or barred lambda). It is equal to the ordinary wavelength reduced by a factor of 2π (ƛ = λ/2π), with SI units of meter per radian. It is the inverse of angular wavenumber (k = 2π/λ). It is usually encountered in quantum mechanics, where it is used in combination with the reduced Planck constant (symbol ħ, h-bar) and the angular frequency (symbol ω = 2πf)."
  },
  {
    "source": "Wiley-Blackwell.txt",
    "chunk_id": "Wiley-Blackwell.txt_0",
    "chunk": "# Wiley-Blackwell Wiley-Blackwell is an international scientific, technical, medical, and scholarly publishing business of John Wiley & Sons. It was formed by the merger of John Wiley & Sons Global Scientific, Technical, and Medical business with Blackwell Publishing in 2007. Wiley-Blackwell is now an imprint that publishes a diverse range of academic and professional fields, including biology, medicine, physical sciences, technology, social science, and the humanities. Blackwell Publishing was formed by the 2001 merger of two Oxford-based academic publishing companies, Blackwell Science, founded in 1939 as Blackwell Scientific Publishing, and Blackwell Publishers, founded in 1922 as Basil Blackwell & Mott. Blackwell Publishers, founded in 1926, had its origins in the 19th century Blackwell's family bookshop and publishing business. The merger between the two publishing companies created the world's leading learned society publisher. The group then acquired BMJ Books from the BMJ Publishing Group, publisher of The BMJ, a British medical journal, in 2004. Blackwell published over 805 journals and 650 text and reference books in 2006, across a wide range of academic, medical, and professional subjects. On November 17, 2006, John Wiley & Sons announced it had \"entered into a definitive agreement to acquire\" Blackwell Publishing. The acquisition was completed"
  },
  {
    "source": "Wiley-Blackwell.txt",
    "chunk_id": "Wiley-Blackwell.txt_1",
    "chunk": "in February 2007, at a purchase price of £572 million. Blackwell Publishing was merged into Wiley's Global Scientific, Technical, and Medical business to create Wiley-Blackwell. From June 30, 2008, the journals previously on Blackwell Synergy were delivered through Wiley InterScience. In April 2022, the journal Science reported that a Ukrainian company, International Publisher Ltd., run by Ksenia Badziun, operates a Russian website where academics can purchase authorships in soon to be published academic papers. During the 2 year period analyzed by researchers, they found that at least 419 articles \"appeared to match manuscripts that later appeared in dozens of different journals\" and that \"More than 100 of these identified papers were published in 68 journals run by established publishers, including Elsevier, Oxford University Press, Springer Nature, Taylor & Francis, Wolters Kluwer, and Wiley-Blackwell.\" Wiley-Blackwell claimed that they were examining the specific papers that were identified and brought to their attention."
  },
  {
    "source": "Williamina Fleming.txt",
    "chunk_id": "Williamina Fleming.txt_0",
    "chunk": "# Williamina Fleming Williamina Paton Stevens Fleming (15 May 1857 – 21 May 1911) was a pioneering Scottish astronomer, who made significant contributions to the field despite facing gender biases. She was a single mother hired by the director of the Harvard College Observatory to help in the photographic classification of stellar spectra. She helped develop a common designation system for stars and cataloged more than ten thousand stars, 59 gaseous nebulae, over 310 variable stars, and 10 novae and other astronomical phenomena. Among several career achievements that advanced astronomy, Fleming is noted for her discovery of the Horsehead Nebula in 1888. Fleming's work has had a lasting impact on our understanding of the universe. Williamina Paton Stevens was born in Dundee, Scotland, at 86 Nethergate, on 15 May 1857 to Mary Walker and Robert Stevens, a carver and gilder. She was one of six children. Her younger sister, Johanna Stevens, would also later work at Harvard College Observatory. Starting at the age of fourteen, she went to work as a pupil-teacher. In 1877, she married James Orr Fleming, an accountant and widower, also of Dundee. The couple had one son, Edward P. Fleming. In 1878, aged 21, she and"
  },
  {
    "source": "Williamina Fleming.txt",
    "chunk_id": "Williamina Fleming.txt_1",
    "chunk": "her husband emigrated to Boston, Massachusetts, US. After her husband abandoned her and her young son, she worked as a maid in the home of Professor Edward Charles Pickering, the director of the Harvard College Observatory (HCO). Pickering's wife Elizabeth recommended Williamina as having talents beyond custodial and maternal arts, and in 1879, Pickering hired Fleming to conduct part-time administrative work at the observatory. In 1881, Pickering formally invited Fleming to join the HCO and taught her how to analyze stellar spectra. She became one of the founding members of the Harvard Computers, an all-women cadre of human computers hired by Pickering to compute mathematical classifications and edit the observatory's publications. In 1886, Mary Anna Draper, the wealthy widow of astronomer Henry Draper, started the Henry Draper Memorial to fund the HCO's research. In response, the HCO began work on the first Henry Draper Catalogue, a long-term project to obtain the optical spectra of as many stars as possible and to index and classify stars by spectra. Fleming was placed in charge of the Draper Catalogue project. A disagreement soon developed regarding classifying the stars. The analysis had been started by Nettie Farrar, but she left a few months later"
  },
  {
    "source": "Williamina Fleming.txt",
    "chunk_id": "Williamina Fleming.txt_2",
    "chunk": "to be married. Antonia Maury advocated for a complex classification scheme. Fleming, however, wanted a much more simple, straightforward approach. The latest Harvard College Observatory images contained photographed spectra of stars that extended into the ultraviolet range, which allowed much more accurate classifications than recording spectra by hand through an instrument at night. Fleming devised a system for classifying stars according to the relative amount of hydrogen observed in their spectra, known as the Pickering-Fleming system. Stars showing hydrogen as the most abundant element were classified A; those of hydrogen as the second-most abundant element, B; and so on. Later, her colleague Annie Jump Cannon reordered the classification system based on the surface temperature of stars, resulting in the Harvard spectral classification, which is still in use today. In 1890, the HCO published the first Henry Draper Catalogue due to years of work by their female computer team, a catalog with more than 10,000 stars classified according to their spectrum. Fleming did the majority of these classifications. Fleming also made it possible to go back and compare recorded plates by organizing thousands of photographs by telescope along with other identifying factors. In 1898, she was appointed Curator of Astronomical Photographs"
  },
  {
    "source": "Williamina Fleming.txt",
    "chunk_id": "Williamina Fleming.txt_3",
    "chunk": "at Harvard, the first woman to hold the position. At the 1893 World's Fair in Chicago, Fleming openly advocated for other women in the sciences in her talk \"A Field for Woman's Work in Astronomy\", where she openly promoted hiring female assistants in astronomy. Her speech suggested she agreed with the prevailing idea that women were inferior but felt that, if given more significant opportunities, they would be able to become equals; in other words, the sex differences in this regard were more culturally constructed than biologically grounded. During her career, Fleming discovered a total of 59 gaseous nebulae, over 310 variable stars, and 10 novae. Most notably, in 1888, Fleming discovered the Horsehead Nebula on a telescope-photogrammetry plate made by astronomer W. H. Pickering, brother of E.C. Pickering. She described the bright nebula (later known as IC 434) as having \"a semicircular indentation 5 minutes in diameter 30 minutes south of Zeta Orionis\". Subsequent professional publications neglected to give credit to Fleming for the discovery. The first Dreyer Index Catalogue omitted Fleming's name from the list of contributors, having then discovered sky objects at Harvard, attributing the entire work merely to \"Pickering\". However, by the time the second Dreyer"
  },
  {
    "source": "Williamina Fleming.txt",
    "chunk_id": "Williamina Fleming.txt_4",
    "chunk": "Index Catalogue was published in 1908, Fleming and her female colleagues at the HCO were sufficiently well-known and received proper credit for their discoveries. The first person who knew of the existence of white dwarfs was Mrs. Fleming; the next two, an hour or two later, Professor E. C. Pickering and I. With characteristic generosity, Pickering had volunteered to have the spectra of the stars which I had observed for parallax looked up on the Harvard plates. All those of faint absolute magnitude turned out to be of class G or later. Moved with curiosity I asked him about the companion of 40 Eridani. Characteristically, again, he telephoned to Mrs. Fleming who reported within an hour or so, that it was of Class A. In 1910, Fleming published her discovery of white dwarf stars . Her other notable publications include A Photographic Study of Variable Stars (1907), a list of 222 variable stars she had discovered; and Spectra and Photographic Magnitudes of Stars in Standard Regions (1911). The women of the Harvard Computers were famous during their lifetimes but were largely forgotten in the following century. In 2015, Lindsay Smith Zrull, curator of Harvard's Plate Stacks collection, was working to"
  },
  {
    "source": "Williamina Fleming.txt",
    "chunk_id": "Williamina Fleming.txt_5",
    "chunk": "catalog and digitize the astronomical plates for Digital Access to a Sky Century @ Harvard (DASCH)and discovered about 118 boxes, each containing 20 to 30 notebooks, from women computers and early Harvard astronomers. Smith Zrull realized that the 2,500+ volumes were outside the scope of her work with DASCH but wanted to see the material preserved and made accessible. Smith Zrull contacted librarians at the Harvard–Smithsonian Center for Astrophysics. In response, the Wolbach Library launched Project PHaEDRA (Preserving Harvard's Early Data and Research in Astronomy). Daina Bouquin, Wolbach's Head Librarian, explained that the objective is to enable full-text search of the research: \"If you search for Williamina Fleming, you're not going to just find a mention of her in a publication where she wasn't the author of her work. You're going to find her work.\" In July 2017, the Wolbach Library at the Center for Astrophysics | Harvard & Smithsonian unveiled a display showcasing Fleming's work, including the log book containing the Horsehead Nebula discovery. The library has dozens of volumes of Fleming's work in its PHaEDRA collection. As of August 2017, about 200 of over 2,500 volumes had been transcribed. The task is expected to take years to complete"
  },
  {
    "source": "Williamina Fleming.txt",
    "chunk_id": "Williamina Fleming.txt_6",
    "chunk": "fully. Some of the notebooks are listed via the Smithsonian Digital Volunteers Web site, which encourages volunteers to transcribe them."
  },
  {
    "source": "Wolter telescope.txt",
    "chunk_id": "Wolter telescope.txt_0",
    "chunk": "# Wolter telescope A Wolter telescope is a telescope for X-rays that only uses grazing incidence optics – mirrors that reflect X-rays at very shallow angles. Conventional telescope designs require reflection or refraction in a manner that does not work well for X-rays. Visible light optical systems use either lenses or mirrors aligned for nearly normal incidence – that is, the light waves travel nearly perpendicular to the reflecting or refracting surface. Conventional mirror telescopes work poorly with X-rays, since X-rays that strike mirror surfaces nearly perpendicularly are either transmitted or absorbed – not reflected. Lenses for visible light are made of transparent materials with an index of refraction substantially different from 1, but all known X-ray-transparent materials have index of refraction essentially the same as 1, so a long series of X-ray lenses, known as compound refractive lenses, are required in order to achieve focusing without significant attenuation. X-ray mirrors can be built, but only if the angle from the plane of reflection is very low (typically 10 arc-minutes to 2 degrees). These are called glancing (or grazing) incidence mirrors. In 1952, Hans Wolter outlined three ways a telescope could be built using only this kind of mirror. These"
  },
  {
    "source": "Wolter telescope.txt",
    "chunk_id": "Wolter telescope.txt_1",
    "chunk": "are called Wolter telescopes of type I, II, and III. Each has different advantages and disadvantages. Wolter's key innovation was that by using two mirrors it is possible to create a telescope with a usably wide field of view. In contrast, a grazing incidence telescope with just one parabolic mirror could focus X-rays, but only very close to the centre of the field of view. The rest of the image would suffer from extreme coma."
  },
  {
    "source": "zbMATH Open.txt",
    "chunk_id": "zbMATH Open.txt_0",
    "chunk": "# zbMATH Open zbMATH Open, formerly Zentralblatt MATH, is a major reviewing service providing reviews and abstracts for articles in pure and applied mathematics, produced by the Berlin office of FIZ Karlsruhe – Leibniz Institute for Information Infrastructure GmbH. Editors are the European Mathematical Society, FIZ Karlsruhe, and the Heidelberg Academy of Sciences. zbMATH is distributed by Springer Science+Business Media. It uses the Mathematics Subject Classification codes for organising reviews by topic. Mathematicians Richard Courant, Otto Neugebauer, and Harald Bohr, together with the publisher Ferdinand Springer, took the initiative for a new mathematical reviewing journal. Harald Bohr worked in Copenhagen. Courant and Neugebauer were professors at the University of Göttingen. At that time, Göttingen was considered one of the central places for mathematical research, having appointed mathematicians like David Hilbert, Hermann Minkowski, Carl Runge, and Felix Klein, the great organiser of mathematics and physics in Göttingen. His dream of a building for an independent mathematical institute with a spacious and rich reference library was realised four years after his death. The credit for this achievement is particularly due to Richard Courant, who convinced the Rockefeller Foundation to donate a large amount of money for the construction. The service was founded"
  },
  {
    "source": "zbMATH Open.txt",
    "chunk_id": "zbMATH Open.txt_1",
    "chunk": "in 1931, by Otto Neugebauer as Zentralblatt für Mathematik und ihre Grenzgebiete. It contained the bibliographical data of all recently published mathematical articles and book, together with peer reviews done by mathematicians over the world. In the preface to the first volume, the intentions of Zentralblatt are formulated as follows: Zentralblatt für Mathematik und ihre Grenzgebiete aims to publish—in an efficient and reliable manner—reviews of the entire world literature in mathematics and related areas in issues initially appearing monthly. As the name suggests, the main focus of the journal is mathematics. However, those areas that are closely related to mathematics will be treated as seriously as the so-called pure mathematics. Zentralblatt and the Jahrbuch über die Fortschritte der Mathematik had in essence the same agenda, but Zentralblatt published several issues per year. An issue was published as soon as sufficiently many reviews were available, in a frequency of three or four weeks. In the late 1930s, it began rejecting some Jewish reviewers and a number of reviewers in England and United States resigned in protest. Some of them helped start Mathematical Reviews, a competing publication. The electronic form was provided under the name INKA-MATH (acronym for Information System Karlsruhe-Database on"
  },
  {
    "source": "zbMATH Open.txt",
    "chunk_id": "zbMATH Open.txt_2",
    "chunk": "Mathematics) since at least 1980. The name was later shortened to Zentralblatt MATH. In addition to the print issue, the services were offered online under the name zbMATH since 1996. Since 2004 older issues were incorporated back to 1826. The printed issue was discontinued in 2013. Since January 2021, the access to the database is now open under the name zbMATH Open. The Jahrbuch über die Fortschritte der Mathematik (Yearbook on the Progress of Mathematics) was internationally the first comprehensive journal of abstracts in the history of mathematics. It contains information about almost all of the most important publications in mathematics and their areas of application from the period 1868 to 1942. The Jahrbuch was written in 1868 by the mathematicians Carl Ohrtmann (1839–1885) and Felix Müller (1843–1928). It appeared annually with a few exceptions and initially contained 880 references per year (1868) and up to 7000 references in the later phase (around 1930). Some of the mathematical abstracts were written by famous mathematicians such as Felix Klein, Sophus Lie, Richard Courant, or Emmy Noether. During WW II publication of the Jahrbuch was stopped. The Jahrbuch's founding concept was characterized by its documentary completeness. The Jahrbuch only appeared when all"
  },
  {
    "source": "zbMATH Open.txt",
    "chunk_id": "zbMATH Open.txt_3",
    "chunk": "papers in a year had been completely processed. This was later paid for with a great loss of relevance. In addition, there was since 1931 the Zentralblatt MATH, which surpassed the Jahrbuch in terms of speed of publication. The Zentralblatt MATH abstracting service provides reviews (brief accounts of contents) of current articles, conference papers, books and other publications in mathematics, its applications, and related areas. The reviews are predominantly in English, with occasional entries in German and French. Reviewers are volunteers invited by the editors based on their published work or a recommendation by an existing reviewer. Zentralblatt MATH is provided both over the Web and in printed form. The service reviews more than 2,300 journals and serials worldwide, as well as books and conference proceedings. Zentralblatt MATH is now edited by the European Mathematical Society, FIZ Karlsruhe, and the Heidelberg Academy of Sciences. The database also incorporates the 200,000 entries of the earlier similar publication Jahrbuch über die Fortschritte der Mathematik from 1868 to 1942, added in 2003. As of January 2021, the complete database is accessible for free. Previously, only the first three records in a search were available without a subscription."
  }
]